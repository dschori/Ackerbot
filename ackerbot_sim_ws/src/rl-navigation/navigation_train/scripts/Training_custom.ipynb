{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from navigation_gym.tasks.navigation_discrete_task import NavigationDiscreteTask"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dschori/MasterThesis/ackerbot_sim_ws/src/rl-navigation/navigation_gym/src/navigation_gym/tasks/navigation_discrete_task.py:25: UserWarning: 'rl-navigation_42356_1615204664884' is not a legal ROS base name. This may cause problems with other ROS tools.\n",
      "  rospy.init_node('rl-navigation', anonymous=True, log_level=rospy.INFO)\n",
      "[ERROR] [1615204665.034579, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1615204665.038017, 0.000000]: Start Init ControllersConnection\n",
      "[WARN] [1615204665.038879, 0.000000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0.0, 1.0, (84, 84, 4), float32)\n",
      "Action Space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Navigation-v0')\n",
    "\n",
    "print(\"Observation Space: {}\".format(env.observation_space))\n",
    "print(\"Action Space: {}\".format(env.action_space))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Environment State"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(4):\n",
    "    obs, _, _, _ = env.step(action=2)\n",
    "plt.imshow(obs[:, :, 0])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ray Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1296x648 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space, lr):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = Conv2D(16, 8, strides=(4, 4),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\", input_shape=input_shape)(X_input)\n",
    "    X = Conv2D(32, 4, strides=(2, 2),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten(input_shape=input_shape)(X_input)\n",
    "\n",
    "    X = Dense(265, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(256, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(64, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    action = Dense(action_space, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "    value = Dense(1, activation='linear', kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    def ppo_loss(y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 5e-3\n",
    "\n",
    "        prob = y_pred * actions\n",
    "        old_prob = actions * prediction_picks\n",
    "        r = prob/(old_prob + 1e-10)\n",
    "        p1 = r * advantages\n",
    "        p2 = K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "        loss =  -K.mean(K.minimum(p1, p2) + ENTROPY_LOSS * -(prob * K.log(prob + 1e-10)))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    Actor = Model(inputs = X_input, outputs = action)\n",
    "    Actor.compile(loss=ppo_loss, optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    Critic = Model(inputs = X_input, outputs = value)\n",
    "    Critic.compile(loss='mse', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    return Actor, Critic\n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = 'Navigation-v0'\n",
    "        self.env = env\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.episode, self.max_average = 1000, 0, -100.0 # specific for pong\n",
    "        self.lock = Lock() # lock all to update parameters without other thread interruption\n",
    "        self.lr = 0.0001\n",
    "\n",
    "        self.ROWS = 84\n",
    "        self.COLS = 84\n",
    "        self.REM_STEP = 4\n",
    "        self.EPOCHS = 10\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_APPO_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.Actor, self.Critic = OurModel(input_shape=self.state_size, action_space = self.action_size, lr=self.lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        return action, prediction\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = np.vstack(self.discount_rewards(rewards))\n",
    "\n",
    "        # Get Critic network predictions\n",
    "        values = self.Critic.predict(states)\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "\n",
    "        '''\n",
    "        pylab.plot(discounted_r,'-')\n",
    "        pylab.plot(advantages,'.')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "\n",
    "        # training Actor and Critic networks\n",
    "        self.Actor.fit(states, y_true, epochs=self.EPOCHS, verbose=0, shuffle=True, batch_size=len(rewards))\n",
    "        self.Critic.fit(states, discounted_r, epochs=self.EPOCHS, verbose=0, shuffle=True, batch_size=len(rewards))\n",
    "\n",
    "    def load(self, Actor_name, Critic_name):\n",
    "        self.Actor = load_model(Actor_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.save(self.Model_name + '_Actor.h5')\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame, image_memory):\n",
    "        if image_memory.shape == (1,*self.state_size):\n",
    "            image_memory = np.squeeze(image_memory)\n",
    "\n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function\n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        image_memory = np.roll(image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame\n",
    "        #self.imshow(image_memory,0)\n",
    "        #self.imshow(image_memory,1)\n",
    "        #self.imshow(image_memory,2)\n",
    "        #self.imshow(image_memory,3)\n",
    "\n",
    "        return np.expand_dims(image_memory, axis=0)\n",
    "\n",
    "    def reset(self, env):\n",
    "        image_memory = np.zeros(self.state_size)\n",
    "        frame = env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame, image_memory)\n",
    "        return state\n",
    "\n",
    "    def step(self, action, env, image_memory):\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = self.GetImage(next_state, image_memory)\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset(self.env)\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            # Instantiate or reset games memory\n",
    "            states, actions, rewards, predictions = [], [], [], []\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.step(action, self.env, state)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[action] = 1\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    # saving best models\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save()\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    self.replay(states, actions, rewards, predictions)\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "    def train(self):\n",
    "        while self.episode < self.EPISODES:\n",
    "            # Reset episode\n",
    "            score, done, SAVING = 0, False, ''\n",
    "            state = self.reset(env)\n",
    "            # Instantiate or reset games memory\n",
    "            states, actions, rewards, predictions = [], [], [], []\n",
    "            while not done:\n",
    "                action, prediction = self.act(state)\n",
    "                next_state, reward, done, _ = self.step(action, self.env, state)\n",
    "\n",
    "                states.append(state)\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[action] = 1\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                predictions.append(prediction)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "            self.replay(states, actions, rewards, predictions)\n",
    "\n",
    "            average = self.PlotModel(score, self.episode)\n",
    "            # saving best models\n",
    "            if average >= self.max_average:\n",
    "                self.max_average = average\n",
    "                self.save()\n",
    "                SAVING = \"SAVING\"\n",
    "            else:\n",
    "                SAVING = \"\"\n",
    "            print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "            if(self.episode < self.EPISODES):\n",
    "                self.episode += 1\n",
    "        env.close()\n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "agent = PPOAgent(env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "episode: 0/1000, score: -109.42181735665659, average: -109.42 \n",
      "episode: 1/1000, score: -108.87223345848932, average: -109.15 \n",
      "episode: 2/1000, score: -108.40457257145177, average: -108.90 \n",
      "episode: 3/1000, score: -105.58355223715208, average: -108.07 \n",
      "episode: 4/1000, score: -108.29767976486828, average: -108.12 \n",
      "episode: 5/1000, score: -90.75259496651005, average: -105.22 \n",
      "episode: 6/1000, score: -121.85866512509024, average: -107.60 \n",
      "episode: 7/1000, score: -105.98046913753103, average: -107.40 \n",
      "episode: 8/1000, score: -107.17927060337178, average: -107.37 \n",
      "episode: 9/1000, score: -107.27702955780188, average: -107.36 \n",
      "episode: 10/1000, score: -109.75920997874033, average: -107.58 \n",
      "episode: 11/1000, score: -91.73325897380367, average: -106.26 \n",
      "episode: 12/1000, score: -108.19054403845902, average: -106.41 \n",
      "episode: 13/1000, score: -109.10673070613717, average: -106.60 \n",
      "episode: 14/1000, score: -90.5159281923346, average: -105.53 \n",
      "episode: 15/1000, score: -106.5360468605548, average: -105.59 \n",
      "episode: 16/1000, score: -106.49015433702189, average: -105.64 \n",
      "episode: 17/1000, score: -109.58156080205269, average: -105.86 \n",
      "episode: 18/1000, score: -107.95842996351018, average: -105.97 \n",
      "episode: 19/1000, score: -106.1951204387075, average: -105.98 \n",
      "episode: 20/1000, score: -104.39182956951774, average: -105.91 \n",
      "episode: 21/1000, score: -106.65876012448796, average: -105.94 \n",
      "episode: 22/1000, score: -106.69187907590643, average: -105.98 \n",
      "episode: 23/1000, score: -106.3351922622173, average: -105.99 \n",
      "episode: 24/1000, score: -105.05223677923881, average: -105.95 \n",
      "episode: 25/1000, score: -93.18292736688626, average: -105.46 \n",
      "episode: 26/1000, score: -106.2339377131598, average: -105.49 \n",
      "episode: 27/1000, score: -105.98101947718372, average: -105.51 \n",
      "episode: 28/1000, score: -90.96531429658899, average: -105.01 \n",
      "episode: 29/1000, score: -108.39884581784742, average: -105.12 \n",
      "episode: 30/1000, score: -98.28323987734215, average: -104.90 \n",
      "episode: 31/1000, score: -114.19625680880759, average: -105.19 \n",
      "episode: 32/1000, score: -92.5264303533353, average: -104.81 \n",
      "episode: 33/1000, score: -108.59989837388048, average: -104.92 \n",
      "episode: 34/1000, score: -94.8346579635596, average: -104.63 \n",
      "episode: 35/1000, score: -92.50613150948978, average: -104.29 \n",
      "episode: 36/1000, score: -90.45475766864, average: -103.92 \n",
      "episode: 37/1000, score: -90.63039098134219, average: -103.57 \n",
      "episode: 38/1000, score: -108.28161118392985, average: -103.69 \n",
      "episode: 39/1000, score: -107.13435452812557, average: -103.78 \n",
      "episode: 40/1000, score: -108.38434295252979, average: -103.89 \n",
      "episode: 41/1000, score: -106.71000114582417, average: -103.96 \n",
      "episode: 42/1000, score: -105.96866020843764, average: -104.00 \n",
      "episode: 43/1000, score: -111.05149786259238, average: -104.16 \n",
      "episode: 44/1000, score: -107.8192039539309, average: -104.24 \n",
      "episode: 45/1000, score: -90.54319271668989, average: -103.95 \n",
      "episode: 46/1000, score: -104.72599046763011, average: -103.96 \n",
      "episode: 47/1000, score: -108.57771241503224, average: -104.06 \n",
      "episode: 48/1000, score: -96.26413787162332, average: -103.90 \n",
      "episode: 49/1000, score: -98.09843721231186, average: -103.78 \n",
      "episode: 50/1000, score: -105.28319458868437, average: -103.70 \n",
      "episode: 51/1000, score: -105.91143788232296, average: -103.64 \n",
      "episode: 52/1000, score: -92.14947651222634, average: -103.32 \n",
      "episode: 53/1000, score: -108.50558582110067, average: -103.37 \n",
      "episode: 54/1000, score: -93.01910154526567, average: -103.07 \n",
      "episode: 55/1000, score: -108.73783345626477, average: -103.43 \n",
      "episode: 56/1000, score: -105.94154285100028, average: -103.11 \n",
      "episode: 57/1000, score: -91.79236889037828, average: -102.83 \n",
      "episode: 58/1000, score: -107.98287301149489, average: -102.84 \n",
      "episode: 59/1000, score: -108.43032337889068, average: -102.87 \n",
      "episode: 60/1000, score: -110.15243597753438, average: -102.87 \n",
      "episode: 61/1000, score: -106.45832650910381, average: -103.17 \n",
      "episode: 62/1000, score: -105.9553751839958, average: -103.12 \n",
      "episode: 63/1000, score: -107.48701858439574, average: -103.09 \n",
      "episode: 64/1000, score: -97.72616461540608, average: -103.24 \n",
      "episode: 65/1000, score: -108.25408964899749, average: -103.27 \n",
      "episode: 66/1000, score: -106.07454832504573, average: -103.26 \n",
      "episode: 67/1000, score: -106.878680219987, average: -103.21 \n",
      "episode: 68/1000, score: -106.94412525906287, average: -103.19 \n",
      "episode: 69/1000, score: -107.29952412686768, average: -103.21 \n",
      "episode: 70/1000, score: -93.78023215934071, average: -103.00 \n",
      "episode: 71/1000, score: -111.29963426982349, average: -103.09 \n",
      "episode: 72/1000, score: -107.04201993452588, average: -103.10 \n",
      "episode: 73/1000, score: -108.23767395731086, average: -103.13 \n",
      "episode: 74/1000, score: -108.2465258908132, average: -103.20 \n",
      "episode: 75/1000, score: -108.22128548647582, average: -103.50 \n",
      "episode: 76/1000, score: -101.38388169040516, average: -103.40 \n",
      "episode: 77/1000, score: -107.18358456269101, average: -103.43 \n",
      "episode: 78/1000, score: -106.52518150353778, average: -103.74 \n",
      "episode: 79/1000, score: -112.59819407135866, average: -103.82 \n",
      "episode: 80/1000, score: -107.87758975339923, average: -104.01 \n",
      "episode: 81/1000, score: -100.32507223528523, average: -103.74 \n",
      "episode: 82/1000, score: -105.15074307490414, average: -103.99 \n",
      "episode: 83/1000, score: -107.08045168082515, average: -103.96 \n",
      "episode: 84/1000, score: -90.44733961322447, average: -103.87 \n",
      "episode: 85/1000, score: -99.60081774334004, average: -104.01 \n",
      "episode: 86/1000, score: -100.45843735662625, average: -104.21 \n",
      "episode: 87/1000, score: -91.13318710272617, average: -104.22 \n",
      "episode: 88/1000, score: -108.05807257995816, average: -104.22 \n",
      "episode: 89/1000, score: -106.76939632476825, average: -104.21 \n",
      "episode: 90/1000, score: -104.47063569086866, average: -104.13 \n",
      "episode: 91/1000, score: -92.36771179682447, average: -103.85 \n",
      "episode: 92/1000, score: -109.24254700305856, average: -103.91 \n",
      "episode: 93/1000, score: -106.1463134528092, average: -103.81 \n",
      "episode: 94/1000, score: -111.24335096497425, average: -103.88 \n",
      "episode: 95/1000, score: -108.58841764898634, average: -104.24 \n",
      "episode: 96/1000, score: -92.95318088311407, average: -104.01 \n",
      "episode: 97/1000, score: -91.65508658709587, average: -103.67 \n",
      "episode: 98/1000, score: -108.61596124867917, average: -103.92 \n",
      "episode: 99/1000, score: -108.30599716256002, average: -104.12 \n",
      "episode: 100/1000, score: -107.63901249306367, average: -104.17 \n",
      "episode: 101/1000, score: -112.98119145480263, average: -104.31 \n",
      "episode: 102/1000, score: -107.53459562729617, average: -104.62 \n",
      "episode: 103/1000, score: -99.06221765192691, average: -104.43 \n",
      "episode: 104/1000, score: -107.38581355932835, average: -104.71 \n",
      "episode: 105/1000, score: -108.32070899734472, average: -104.71 \n",
      "episode: 106/1000, score: -109.17115628704076, average: -104.77 \n",
      "episode: 107/1000, score: -100.79247071380675, average: -104.95 \n",
      "episode: 108/1000, score: -98.88020774210527, average: -104.77 \n",
      "episode: 109/1000, score: -90.83368120276285, average: -104.42 \n",
      "episode: 110/1000, score: -97.77970416934015, average: -104.17 \n",
      "episode: 111/1000, score: -108.33340059267275, average: -104.21 \n",
      "episode: 112/1000, score: -111.9231784855613, average: -104.33 \n",
      "episode: 113/1000, score: -91.09595769575265, average: -104.00 \n",
      "episode: 114/1000, score: -90.71755606024698, average: -103.86 \n",
      "episode: 115/1000, score: -91.0083896479322, average: -103.51 \n",
      "episode: 116/1000, score: -108.64908401038562, average: -103.56 \n",
      "episode: 117/1000, score: -107.3829259969954, average: -103.57 \n",
      "episode: 118/1000, score: -107.56305106481557, average: -103.59 \n",
      "episode: 119/1000, score: -108.17180959743536, average: -103.60 \n",
      "episode: 120/1000, score: -107.86958353004023, average: -103.89 \n",
      "episode: 121/1000, score: -111.16368906241149, average: -103.88 \n",
      "episode: 122/1000, score: -105.8122337638521, average: -103.86 \n",
      "episode: 123/1000, score: -104.93821758571538, average: -103.79 \n",
      "episode: 124/1000, score: -107.77290617061747, average: -103.78 \n",
      "episode: 125/1000, score: -108.96654221605567, average: -103.80 \n",
      "episode: 126/1000, score: -106.154585185739, average: -103.89 \n",
      "episode: 127/1000, score: -106.9818321036826, average: -103.89 \n",
      "episode: 128/1000, score: -94.75111557393788, average: -103.65 \n",
      "episode: 129/1000, score: -112.45975750198777, average: -103.65 \n",
      "episode: 130/1000, score: -109.73911478985286, average: -103.69 \n",
      "episode: 131/1000, score: -106.28468380792181, average: -103.81 \n",
      "episode: 132/1000, score: -104.69145722761648, average: -103.80 \n",
      "episode: 133/1000, score: -105.32409892210543, average: -103.76 \n",
      "episode: 134/1000, score: -111.3161556181752, average: -104.18 \n",
      "episode: 135/1000, score: -106.21598488835859, average: -104.31 \n",
      "episode: 136/1000, score: -107.32274356246398, average: -104.45 \n",
      "episode: 137/1000, score: -90.35971398947892, average: -104.44 \n",
      "episode: 138/1000, score: -105.12320779307757, average: -104.38 \n",
      "episode: 139/1000, score: -90.42393160071038, average: -104.05 \n",
      "episode: 140/1000, score: -93.41820625461385, average: -103.83 \n",
      "episode: 141/1000, score: -104.61693080428334, average: -104.07 \n",
      "episode: 142/1000, score: -108.23368871100568, average: -104.05 \n",
      "episode: 143/1000, score: -106.64491991599567, average: -104.06 \n",
      "episode: 144/1000, score: -106.16117906151499, average: -103.96 \n",
      "episode: 145/1000, score: -105.65050666733096, average: -103.90 \n",
      "episode: 146/1000, score: -99.5234262310329, average: -104.03 \n",
      "episode: 147/1000, score: -108.19211206221584, average: -104.37 \n",
      "episode: 148/1000, score: -108.57471251234199, average: -104.36 \n",
      "episode: 149/1000, score: -108.65928104982434, average: -104.37 \n",
      "episode: 150/1000, score: -111.25402267741589, average: -104.44 \n",
      "episode: 151/1000, score: -107.12090574353499, average: -104.33 \n",
      "episode: 152/1000, score: -112.77356386998179, average: -104.43 \n",
      "episode: 153/1000, score: -107.54116040346977, average: -104.60 \n",
      "episode: 154/1000, score: -115.21581069969416, average: -104.76 \n",
      "episode: 155/1000, score: -109.39862741509152, average: -104.78 \n",
      "episode: 156/1000, score: -90.62189877222174, average: -104.41 \n",
      "episode: 157/1000, score: -106.33078085913155, average: -104.52 \n",
      "episode: 158/1000, score: -91.84382310626026, average: -104.38 \n",
      "episode: 159/1000, score: -109.0576395376684, average: -104.74 \n",
      "episode: 160/1000, score: -106.80713860730529, average: -104.92 \n",
      "episode: 161/1000, score: -90.6052488969606, average: -104.57 \n",
      "episode: 162/1000, score: -105.04937156409841, average: -104.43 \n",
      "episode: 163/1000, score: -106.66271488278832, average: -104.74 \n",
      "episode: 164/1000, score: -109.36208717606416, average: -105.12 \n",
      "episode: 165/1000, score: -106.31495117038708, average: -105.42 \n",
      "episode: 166/1000, score: -93.86836863295463, average: -105.13 \n",
      "episode: 167/1000, score: -108.63766274080228, average: -105.15 \n",
      "episode: 168/1000, score: -108.26629616486584, average: -105.17 \n",
      "episode: 169/1000, score: -107.19745548699547, average: -105.15 \n",
      "episode: 170/1000, score: -99.41203154740477, average: -104.98 \n",
      "episode: 171/1000, score: -105.89310327806834, average: -104.87 \n",
      "episode: 172/1000, score: -91.69965135238012, average: -104.59 \n",
      "episode: 173/1000, score: -109.04630313926002, average: -104.67 \n",
      "episode: 174/1000, score: -107.89018248992437, average: -104.67 \n",
      "episode: 175/1000, score: -90.61987784276322, average: -104.31 \n",
      "episode: 176/1000, score: -106.12742538096377, average: -104.31 \n",
      "episode: 177/1000, score: -106.44199950612894, average: -104.29 \n",
      "episode: 178/1000, score: -113.55818257834807, average: -104.67 \n",
      "episode: 179/1000, score: -110.37622938958106, average: -104.63 \n",
      "episode: 180/1000, score: -107.58288462844052, average: -104.59 \n",
      "episode: 181/1000, score: -92.95522710288789, average: -104.32 \n",
      "episode: 182/1000, score: -107.86247928868491, average: -104.38 \n",
      "episode: 183/1000, score: -90.82644003257879, average: -104.09 \n",
      "episode: 184/1000, score: -106.61804717961539, average: -104.00 \n",
      "episode: 185/1000, score: -104.44365540884192, average: -103.96 \n",
      "episode: 186/1000, score: -123.29486477854672, average: -104.28 \n",
      "episode: 187/1000, score: -107.82415527313779, average: -104.63 \n",
      "episode: 188/1000, score: -106.16679823130022, average: -104.65 \n",
      "episode: 189/1000, score: -105.20458569599383, average: -104.95 \n",
      "episode: 190/1000, score: -93.7169505961263, average: -104.95 \n",
      "episode: 191/1000, score: -112.55257961697333, average: -105.11 \n",
      "episode: 192/1000, score: -102.87896331284824, average: -105.01 \n",
      "episode: 193/1000, score: -106.5145535813754, average: -105.00 \n",
      "episode: 194/1000, score: -105.8010763763159, average: -105.00 \n",
      "episode: 195/1000, score: -106.83619078349025, average: -105.02 \n",
      "episode: 196/1000, score: -92.87976306577073, average: -104.89 \n",
      "episode: 197/1000, score: -111.25006275439364, average: -104.95 \n",
      "episode: 198/1000, score: -99.52078484331712, average: -104.77 \n",
      "episode: 199/1000, score: -107.06306301489693, average: -104.74 \n",
      "episode: 200/1000, score: -108.85557248030561, average: -104.69 \n",
      "episode: 201/1000, score: -93.68503369143052, average: -104.42 \n",
      "episode: 202/1000, score: -106.47178743891271, average: -104.29 \n",
      "episode: 203/1000, score: -99.48544689179909, average: -104.13 \n",
      "episode: 204/1000, score: -97.8552089999231, average: -103.78 \n",
      "episode: 205/1000, score: -92.9192724929478, average: -103.46 \n",
      "episode: 206/1000, score: -107.41312190057272, average: -103.79 \n",
      "episode: 207/1000, score: -108.59754352175243, average: -103.84 \n",
      "episode: 208/1000, score: -102.34278728441862, average: -104.05 \n",
      "episode: 209/1000, score: -103.4818316867433, average: -103.93 \n",
      "episode: 210/1000, score: -107.16740958612151, average: -103.94 \n",
      "episode: 211/1000, score: -108.83104450885064, average: -104.31 \n",
      "episode: 212/1000, score: -109.60676386869986, average: -104.40 \n",
      "episode: 213/1000, score: -106.408089480657, average: -104.39 \n",
      "episode: 214/1000, score: -107.9086319965871, average: -104.36 \n",
      "episode: 215/1000, score: -105.86681670940162, average: -104.35 \n",
      "episode: 216/1000, score: -93.15174000920966, average: -104.34 \n",
      "episode: 217/1000, score: -106.83043617571165, average: -104.30 \n",
      "episode: 218/1000, score: -110.70816010165146, average: -104.35 \n",
      "episode: 219/1000, score: -92.98043256605168, average: -104.07 \n",
      "episode: 220/1000, score: -107.58107489819594, average: -104.23 \n",
      "episode: 221/1000, score: -121.6642078483574, average: -104.55 \n",
      "episode: 222/1000, score: -102.23233817137634, average: -104.76 \n",
      "episode: 223/1000, score: -106.76513920908096, average: -104.71 \n",
      "episode: 224/1000, score: -91.39378002537177, average: -104.38 \n",
      "episode: 225/1000, score: -104.70437887604808, average: -104.66 \n",
      "episode: 226/1000, score: -109.19511400247549, average: -104.73 \n",
      "episode: 227/1000, score: -106.37602298829387, average: -104.72 \n",
      "episode: 228/1000, score: -108.44746660739314, average: -104.62 \n",
      "episode: 229/1000, score: -104.33115826698165, average: -104.50 \n",
      "episode: 230/1000, score: -106.29226933117084, average: -104.48 \n",
      "episode: 231/1000, score: -107.71550947516256, average: -104.77 \n",
      "episode: 232/1000, score: -113.57459544283888, average: -104.88 \n",
      "episode: 233/1000, score: -104.92812992639541, average: -105.17 \n",
      "episode: 234/1000, score: -92.50027249764301, average: -104.88 \n",
      "episode: 235/1000, score: -103.55209595222539, average: -104.87 \n",
      "episode: 236/1000, score: -106.68740420251544, average: -104.53 \n",
      "episode: 237/1000, score: -109.30061740870579, average: -104.56 \n",
      "episode: 238/1000, score: -108.0206224590032, average: -104.60 \n",
      "episode: 239/1000, score: -92.63556544098492, average: -104.35 \n",
      "episode: 240/1000, score: -106.16893404705803, average: -104.60 \n",
      "episode: 241/1000, score: -106.18970452595082, average: -104.47 \n",
      "episode: 242/1000, score: -108.09765846626789, average: -104.58 \n",
      "episode: 243/1000, score: -109.77814569312696, average: -104.64 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}