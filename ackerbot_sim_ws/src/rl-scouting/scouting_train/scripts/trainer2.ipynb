{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from gym.spaces import Discrete, Tuple\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models import ModelCatalog, ModelV2\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.utils import get_filter_config\n",
    "from ray.rllib.utils import override\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1611907619.790632, 175.666000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1611907619.796088, 0.009000]: Start Init ControllersConnection\n",
      "[WARN] [1611907619.797393, 0.009000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Box(0.0, 4.0, (18,), float32), Box(-10.0, 10.0, (2,), float32), Box(-1.0, 1.0, (2,), float32))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.,  1.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "plt.figure()\n",
    "thetas = np.linspace(-2.44, 2.44, 18)\n",
    "thetas = np.linspace(-3.14, 3.14, 18)\n",
    "plt.polar(thetas, obs[0])\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978138\n",
      "3.99\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(40):\n",
    "    #obs, reward, done, _ = env.step(1)\n",
    "    obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs[0].min())\n",
    "print(obs[0].max())\n",
    "\n",
    "#plt.imshow(obs[0].squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#print(obs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "class CNNPlusFCConcatModel(TFModelV2):\n",
    "    \"\"\"TFModelV2 concat'ing CNN outputs to flat input(s), followed by FC(s).\n",
    "\n",
    "    Note: This model should be used for complex (Dict or Tuple) observation\n",
    "    spaces that have one or more image components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        # TODO: (sven) Support Dicts as well.\n",
    "        assert isinstance(obs_space.original_space, (Tuple)), \\\n",
    "            \"`obs_space.original_space` must be Tuple!\"\n",
    "\n",
    "        super().__init__(obs_space, action_space, num_outputs, model_config,\n",
    "                         name)\n",
    "\n",
    "        # Build the CNN(s) given obs_space's image components.\n",
    "        self.cnns = {}\n",
    "        concat_size = 0\n",
    "        for i, component in enumerate(obs_space.original_space):\n",
    "            # Image space.\n",
    "            if len(component.shape) == 3:\n",
    "                config = {\n",
    "                    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [5, 16], 1]],\n",
    "                    \"conv_activation\": model_config.get(\"conv_activation\"),\n",
    "                }\n",
    "                cnn = ModelCatalog.get_model_v2(\n",
    "                    component,\n",
    "                    action_space,\n",
    "                    num_outputs=None,\n",
    "                    model_config=config,\n",
    "                    framework=\"tf\",\n",
    "                    name=\"cnn_{}\".format(i))\n",
    "                concat_size += cnn.num_outputs\n",
    "                self.cnns[i] = cnn\n",
    "                self.register_variables(cnn.base_model.variables)\n",
    "            # Discrete inputs -> One-hot encode.\n",
    "            elif isinstance(component, Discrete):\n",
    "                concat_size += component.n\n",
    "            # TODO: (sven) Multidiscrete (see e.g. our auto-LSTM wrappers).\n",
    "            # Everything else (1D Box).\n",
    "            else:\n",
    "                assert len(component.shape) == 1, \\\n",
    "                    \"Only input Box 1D or 3D spaces allowed!\"\n",
    "                concat_size += component.shape[-1]\n",
    "\n",
    "        self.logits_and_value_model = None\n",
    "        self._value_out = None\n",
    "        if num_outputs:\n",
    "            # Action-distribution head.\n",
    "            concat_layer = tf.keras.layers.Input((concat_size, ))\n",
    "            logits_layer = tf.keras.layers.Dense(\n",
    "                num_outputs,\n",
    "                activation=tf.keras.activations.linear,\n",
    "                name=\"logits\")(concat_layer)\n",
    "\n",
    "            # Create the value branch model.\n",
    "            value_layer = tf.keras.layers.Dense(\n",
    "                1,\n",
    "                name=\"value_out\",\n",
    "                activation=None,\n",
    "                kernel_initializer=normc_initializer(0.01))(concat_layer)\n",
    "            self.logits_and_value_model = tf.keras.models.Model(\n",
    "                concat_layer, [logits_layer, value_layer])\n",
    "        else:\n",
    "            self.num_outputs = concat_size\n",
    "        self.register_variables(self.logits_and_value_model.variables)\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Push image observations through our CNNs.\n",
    "        outs = []\n",
    "        for i, component in enumerate(input_dict[\"obs\"]):\n",
    "            if i in self.cnns:\n",
    "                cnn_out, _ = self.cnns[i]({\"obs\": component})\n",
    "                outs.append(cnn_out)\n",
    "            else:\n",
    "                outs.append(component)\n",
    "        # Concat all outputs and the non-image inputs.\n",
    "        out = tf.concat(outs, axis=1)\n",
    "        if not self.logits_and_value_model:\n",
    "            return out, []\n",
    "\n",
    "        # Value branch.\n",
    "        logits, values = self.logits_and_value_model(out)\n",
    "        self._value_out = tf.reshape(values, [-1])\n",
    "        return logits, []\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "\n",
    "ModelCatalog.register_custom_model(\"CNNPlusFCConcatModel\", CNNPlusFCConcatModel)\n",
    "\n",
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"model\": {\n",
    "        #\"custom_model\": \"CNNPlusFCConcatModel\",\n",
    "        #\"conv_filters\": [[16, [4, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [5, 16], 1]],\n",
    "        \"fcnet_hiddens\": [256, ],\n",
    "        #\"use_lstm\": True,\n",
    "        #\"lstm_cell_size\": 256,\n",
    "        #\"fcnet_hiddens\": tune.grid_search([[64, 64, ], [128, 128, ], [256, 256, ]])\n",
    "    }\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 4500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 09:07:03,045\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-01-29_09-07-02_512709_2092754/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-01-29_09-07-02_512709_2092754/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-01-29_09-07-02_512709_2092754',\n 'metrics_export_port': 65382,\n 'node_id': 'ac29f043d79dc8125c2fb75e3b000f1bcb566726'}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True, restore=restorepath)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 8.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m 2021-01-29 09:07:08,572\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m 2021-01-29 09:07:08,572\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m [ERROR] [1611907632.006361, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m [WARN] [1611907632.010011, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m [WARN] [1611907632.011133, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m 2021-01-29 09:07:17,396\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m 2021-01-29 09:07:17,483\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-01-29_09-07-05/PPO_ScoutingDiscreteTask_f98e4_00000_0_2021-01-29_09-07-05/tmp7oq4ga12restore_from_object/checkpoint-375\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m 2021-01-29 09:07:17,484\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 375, '_timesteps_total': None, '_time_total': 58624.67053723335, '_episodes_total': 3501}\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m 2021-01-29 09:07:18,654\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2092936)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=2092931)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-10-15\n",
      "  done: false\n",
      "  episode_len_mean: 433.22222222222223\n",
      "  episode_reward_max: 118.35527246806276\n",
      "  episode_reward_mean: -68.50581263682233\n",
      "  episode_reward_min: -97.03698004165872\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3510\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6172436475753784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007835383526980877\n",
      "        model: {}\n",
      "        policy_loss: -0.01635926403105259\n",
      "        total_loss: 1153.6846923828125\n",
      "        vf_explained_var: 0.060151997953653336\n",
      "        vf_loss: 1153.6995849609375\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.94921259842519\n",
      "    ram_util_percent: 31.855118110236223\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06879105981484976\n",
      "    mean_env_wait_ms: 39.984641448166094\n",
      "    mean_inference_ms: 1.010688118146855\n",
      "    mean_raw_obs_processing_ms: 2.7116968225938205\n",
      "  time_since_restore: 177.59182024002075\n",
      "  time_this_iter_s: 177.59182024002075\n",
      "  time_total_s: 58802.26235747337\n",
      "  timers:\n",
      "    learn_throughput: 1836.969\n",
      "    learn_time_ms: 2177.5\n",
      "    load_throughput: 96496.759\n",
      "    load_time_ms: 41.452\n",
      "    sample_throughput: 22.827\n",
      "    sample_time_ms: 175227.623\n",
      "    update_time_ms: 1.159\n",
      "  timestamp: 1611907815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 453.4117647058824\n",
      "  episode_reward_max: 118.35527246806276\n",
      "  episode_reward_mean: -56.88717946927571\n",
      "  episode_reward_min: -99.59410772673796\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3518\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.682252049446106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013735052198171616\n",
      "        model: {}\n",
      "        policy_loss: -0.01652289554476738\n",
      "        total_loss: 425.2005310058594\n",
      "        vf_explained_var: 0.4865763187408447\n",
      "        vf_loss: 425.2143249511719\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7172839506173\n",
      "    ram_util_percent: 31.89465020576131\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06854660657265538\n",
      "    mean_env_wait_ms: 39.651039960738885\n",
      "    mean_inference_ms: 0.9988533090068142\n",
      "    mean_raw_obs_processing_ms: 2.6515035528807926\n",
      "  time_since_restore: 347.99575424194336\n",
      "  time_this_iter_s: 170.4039340019226\n",
      "  time_total_s: 58972.666291475296\n",
      "  timers:\n",
      "    learn_throughput: 1896.626\n",
      "    learn_time_ms: 2109.009\n",
      "    load_throughput: 143708.219\n",
      "    load_time_ms: 27.834\n",
      "    sample_throughput: 23.29\n",
      "    sample_time_ms: 171744.094\n",
      "    update_time_ms: 1.239\n",
      "  timestamp: 1611907985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 377\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 491.4583333333333\n",
      "  episode_reward_max: 118.35527246806276\n",
      "  episode_reward_mean: -58.32305242373912\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3525\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6937344670295715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004539493005722761\n",
      "        model: {}\n",
      "        policy_loss: -0.009166860021650791\n",
      "        total_loss: 689.1268310546875\n",
      "        vf_explained_var: 0.14175789058208466\n",
      "        vf_loss: 689.1350708007812\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.30562248995983\n",
      "    ram_util_percent: 31.9004016064257\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684381341217383\n",
      "    mean_env_wait_ms: 39.57787179291554\n",
      "    mean_inference_ms: 0.995370897719941\n",
      "    mean_raw_obs_processing_ms: 2.5974432849262503\n",
      "  time_since_restore: 522.1978871822357\n",
      "  time_this_iter_s: 174.20213294029236\n",
      "  time_total_s: 59146.86842441559\n",
      "  timers:\n",
      "    learn_throughput: 1848.396\n",
      "    learn_time_ms: 2164.038\n",
      "    load_throughput: 184190.94\n",
      "    load_time_ms: 21.717\n",
      "    sample_throughput: 23.288\n",
      "    sample_time_ms: 171764.75\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1611908159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 511.8709677419355\n",
      "  episode_reward_max: 118.36337826375762\n",
      "  episode_reward_mean: -45.39016153321407\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3532\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6513456106185913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025289352983236313\n",
      "        model: {}\n",
      "        policy_loss: -0.021628551185131073\n",
      "        total_loss: 690.5341186523438\n",
      "        vf_explained_var: 0.22799746692180634\n",
      "        vf_loss: 690.55322265625\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.67470817120623\n",
      "    ram_util_percent: 31.91867704280156\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06846310431219894\n",
      "    mean_env_wait_ms: 39.63840595330168\n",
      "    mean_inference_ms: 0.9956620334810102\n",
      "    mean_raw_obs_processing_ms: 2.5523944184090093\n",
      "  time_since_restore: 702.5356152057648\n",
      "  time_this_iter_s: 180.33772802352905\n",
      "  time_total_s: 59327.20615243912\n",
      "  timers:\n",
      "    learn_throughput: 1827.964\n",
      "    learn_time_ms: 2188.227\n",
      "    load_throughput: 199738.273\n",
      "    load_time_ms: 20.026\n",
      "    sample_throughput: 23.08\n",
      "    sample_time_ms: 173310.287\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1611908340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-21-57\n",
      "  done: false\n",
      "  episode_len_mean: 512.5263157894736\n",
      "  episode_reward_max: 118.37129247789888\n",
      "  episode_reward_mean: -31.882689210589767\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3539\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6657984852790833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007725424598902464\n",
      "        model: {}\n",
      "        policy_loss: -0.010169154033064842\n",
      "        total_loss: 284.1852722167969\n",
      "        vf_explained_var: 0.6627137660980225\n",
      "        vf_loss: 284.19427490234375\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.40513833992095\n",
      "    ram_util_percent: 31.9\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06847827412690008\n",
      "    mean_env_wait_ms: 39.699548118138765\n",
      "    mean_inference_ms: 0.9961353027833081\n",
      "    mean_raw_obs_processing_ms: 2.5164716016241973\n",
      "  time_since_restore: 879.6951494216919\n",
      "  time_this_iter_s: 177.15953421592712\n",
      "  time_total_s: 59504.365686655045\n",
      "  timers:\n",
      "    learn_throughput: 1855.691\n",
      "    learn_time_ms: 2155.531\n",
      "    load_throughput: 221831.058\n",
      "    load_time_ms: 18.032\n",
      "    sample_throughput: 23.035\n",
      "    sample_time_ms: 173648.422\n",
      "    update_time_ms: 1.309\n",
      "  timestamp: 1611908517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 504.3191489361702\n",
      "  episode_reward_max: 118.37129247789888\n",
      "  episode_reward_mean: -39.024406783723926\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3548\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6796349883079529\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006132856477051973\n",
      "        model: {}\n",
      "        policy_loss: -0.007172168232500553\n",
      "        total_loss: 367.0489807128906\n",
      "        vf_explained_var: 0.6040593981742859\n",
      "        vf_loss: 367.05523681640625\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.8759336099585\n",
      "    ram_util_percent: 31.9207468879668\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.068418548963101\n",
      "    mean_env_wait_ms: 39.6850503814017\n",
      "    mean_inference_ms: 0.9948475290989176\n",
      "    mean_raw_obs_processing_ms: 2.499631811333906\n",
      "  time_since_restore: 1048.5827360153198\n",
      "  time_this_iter_s: 168.88758659362793\n",
      "  time_total_s: 59673.25327324867\n",
      "  timers:\n",
      "    learn_throughput: 1882.5\n",
      "    learn_time_ms: 2124.834\n",
      "    load_throughput: 223100.288\n",
      "    load_time_ms: 17.929\n",
      "    sample_throughput: 23.187\n",
      "    sample_time_ms: 172507.771\n",
      "    update_time_ms: 1.276\n",
      "  timestamp: 1611908686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 381\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 506.0\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -35.914560498613874\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3556\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.710681676864624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016615141183137894\n",
      "        model: {}\n",
      "        policy_loss: -0.01609439216554165\n",
      "        total_loss: 250.73092651367188\n",
      "        vf_explained_var: 0.7047773003578186\n",
      "        vf_loss: 250.7445526123047\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.47722007722007\n",
      "    ram_util_percent: 31.92046332046332\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06842849234872916\n",
      "    mean_env_wait_ms: 39.70439467096833\n",
      "    mean_inference_ms: 0.995176306024684\n",
      "    mean_raw_obs_processing_ms: 2.4906545900251245\n",
      "  time_since_restore: 1229.7507247924805\n",
      "  time_this_iter_s: 181.16798877716064\n",
      "  time_total_s: 59854.42126202583\n",
      "  timers:\n",
      "    learn_throughput: 1822.957\n",
      "    learn_time_ms: 2194.237\n",
      "    load_throughput: 235469.226\n",
      "    load_time_ms: 16.987\n",
      "    sample_throughput: 23.074\n",
      "    sample_time_ms: 173353.156\n",
      "    update_time_ms: 1.365\n",
      "  timestamp: 1611908867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 517.1967213114754\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -34.23778341971313\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3562\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7319035530090332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014204469509422779\n",
      "        model: {}\n",
      "        policy_loss: -0.018068263307213783\n",
      "        total_loss: 276.6529846191406\n",
      "        vf_explained_var: 0.6848458051681519\n",
      "        vf_loss: 276.6689147949219\n",
      "    num_steps_sampled: 1532000\n",
      "    num_steps_trained: 1532000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.99019607843137\n",
      "    ram_util_percent: 31.943921568627445\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06847460161058906\n",
      "    mean_env_wait_ms: 39.73145168787416\n",
      "    mean_inference_ms: 0.9960149598011899\n",
      "    mean_raw_obs_processing_ms: 2.478352629373769\n",
      "  time_since_restore: 1408.6162667274475\n",
      "  time_this_iter_s: 178.86554193496704\n",
      "  time_total_s: 60033.2868039608\n",
      "  timers:\n",
      "    learn_throughput: 1811.891\n",
      "    learn_time_ms: 2207.638\n",
      "    load_throughput: 248371.509\n",
      "    load_time_ms: 16.105\n",
      "    sample_throughput: 23.023\n",
      "    sample_time_ms: 173736.505\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1611909046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1532000\n",
      "  training_iteration: 383\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-33-49\n",
      "  done: false\n",
      "  episode_len_mean: 517.4202898550725\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -28.745066101435835\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3570\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6709219217300415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01323049608618021\n",
      "        model: {}\n",
      "        policy_loss: -0.019829774275422096\n",
      "        total_loss: 675.2766723632812\n",
      "        vf_explained_var: 0.38697028160095215\n",
      "        vf_loss: 675.2944946289062\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.56551724137931\n",
      "    ram_util_percent: 31.942528735632184\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06855032843573759\n",
      "    mean_env_wait_ms: 39.780249965051645\n",
      "    mean_inference_ms: 0.997554489333089\n",
      "    mean_raw_obs_processing_ms: 2.466207775770928\n",
      "  time_since_restore: 1591.5066990852356\n",
      "  time_this_iter_s: 182.8904323577881\n",
      "  time_total_s: 60216.17723631859\n",
      "  timers:\n",
      "    learn_throughput: 1802.948\n",
      "    learn_time_ms: 2218.589\n",
      "    load_throughput: 266783.532\n",
      "    load_time_ms: 14.993\n",
      "    sample_throughput: 22.925\n",
      "    sample_time_ms: 174482.655\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1611909229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-36-50\n",
      "  done: false\n",
      "  episode_len_mean: 517.1578947368421\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -31.89275238500552\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3577\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6990854740142822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008443648926913738\n",
      "        model: {}\n",
      "        policy_loss: -0.011304982006549835\n",
      "        total_loss: 461.0989074707031\n",
      "        vf_explained_var: 0.364148885011673\n",
      "        vf_loss: 461.1090087890625\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.3980694980695\n",
      "    ram_util_percent: 31.927799227799223\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06861894818792279\n",
      "    mean_env_wait_ms: 39.82610450035961\n",
      "    mean_inference_ms: 0.9990588854210208\n",
      "    mean_raw_obs_processing_ms: 2.4561849251170793\n",
      "  time_since_restore: 1773.0055248737335\n",
      "  time_this_iter_s: 181.49882578849792\n",
      "  time_total_s: 60397.676062107086\n",
      "  timers:\n",
      "    learn_throughput: 1797.416\n",
      "    learn_time_ms: 2225.417\n",
      "    load_throughput: 278816.346\n",
      "    load_time_ms: 14.346\n",
      "    sample_throughput: 22.865\n",
      "    sample_time_ms: 174939.238\n",
      "    update_time_ms: 1.468\n",
      "  timestamp: 1611909410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 515.797619047619\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -37.41895231186817\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3585\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7117969393730164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007021606899797916\n",
      "        model: {}\n",
      "        policy_loss: -0.012310575693845749\n",
      "        total_loss: 519.491455078125\n",
      "        vf_explained_var: 0.45827585458755493\n",
      "        vf_loss: 519.502685546875\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.34163424124515\n",
      "    ram_util_percent: 31.985992217898833\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06868328780910563\n",
      "    mean_env_wait_ms: 39.87334793630415\n",
      "    mean_inference_ms: 1.0006587707404249\n",
      "    mean_raw_obs_processing_ms: 2.4478803552191764\n",
      "  time_since_restore: 1952.8530070781708\n",
      "  time_this_iter_s: 179.84748220443726\n",
      "  time_total_s: 60577.52354431152\n",
      "  timers:\n",
      "    learn_throughput: 1792.647\n",
      "    learn_time_ms: 2231.338\n",
      "    load_throughput: 352484.736\n",
      "    load_time_ms: 11.348\n",
      "    sample_throughput: 22.836\n",
      "    sample_time_ms: 175163.267\n",
      "    update_time_ms: 1.529\n",
      "  timestamp: 1611909590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 516.8586956521739\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -41.68320300541118\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3593\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7150391340255737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009188898839056492\n",
      "        model: {}\n",
      "        policy_loss: -0.008598417043685913\n",
      "        total_loss: 436.4555969238281\n",
      "        vf_explained_var: 0.6290063261985779\n",
      "        vf_loss: 436.46282958984375\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.41081081081082\n",
      "    ram_util_percent: 31.98030888030888\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06873942802095821\n",
      "    mean_env_wait_ms: 39.91716921184665\n",
      "    mean_inference_ms: 1.0022734890651968\n",
      "    mean_raw_obs_processing_ms: 2.4422074455479987\n",
      "  time_since_restore: 2134.3398220539093\n",
      "  time_this_iter_s: 181.48681497573853\n",
      "  time_total_s: 60759.01035928726\n",
      "  timers:\n",
      "    learn_throughput: 1740.39\n",
      "    learn_time_ms: 2298.336\n",
      "    load_throughput: 361715.133\n",
      "    load_time_ms: 11.058\n",
      "    sample_throughput: 22.701\n",
      "    sample_time_ms: 176199.983\n",
      "    update_time_ms: 1.576\n",
      "  timestamp: 1611909772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 387\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-45-40\n",
      "  done: false\n",
      "  episode_len_mean: 522.7878787878788\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -42.857358327076355\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3600\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6720461845397949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010093034245073795\n",
      "        model: {}\n",
      "        policy_loss: -0.017328942194581032\n",
      "        total_loss: 521.9522705078125\n",
      "        vf_explained_var: 0.40815773606300354\n",
      "        vf_loss: 521.9680786132812\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.67\n",
      "    ram_util_percent: 31.96583333333333\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06876877691905346\n",
      "    mean_env_wait_ms: 39.938638675594696\n",
      "    mean_inference_ms: 1.0031596559017342\n",
      "    mean_raw_obs_processing_ms: 2.436832414685845\n",
      "  time_since_restore: 2302.860621213913\n",
      "  time_this_iter_s: 168.52079916000366\n",
      "  time_total_s: 60927.531158447266\n",
      "  timers:\n",
      "    learn_throughput: 1748.288\n",
      "    learn_time_ms: 2287.953\n",
      "    load_throughput: 334864.545\n",
      "    load_time_ms: 11.945\n",
      "    sample_throughput: 22.774\n",
      "    sample_time_ms: 175641.149\n",
      "    update_time_ms: 1.548\n",
      "  timestamp: 1611909940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-48-19\n",
      "  done: false\n",
      "  episode_len_mean: 533.33\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -39.13990602311472\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3607\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7316601872444153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013325005769729614\n",
      "        model: {}\n",
      "        policy_loss: -0.019603410735726357\n",
      "        total_loss: 277.29510498046875\n",
      "        vf_explained_var: 0.6750472187995911\n",
      "        vf_loss: 277.31268310546875\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60486725663718\n",
      "    ram_util_percent: 31.991592920353984\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06877166816991684\n",
      "    mean_env_wait_ms: 39.93418867052438\n",
      "    mean_inference_ms: 1.003059705321775\n",
      "    mean_raw_obs_processing_ms: 2.414354762392472\n",
      "  time_since_restore: 2461.183283805847\n",
      "  time_this_iter_s: 158.3226625919342\n",
      "  time_total_s: 61085.8538210392\n",
      "  timers:\n",
      "    learn_throughput: 1780.342\n",
      "    learn_time_ms: 2246.759\n",
      "    load_throughput: 326923.351\n",
      "    load_time_ms: 12.235\n",
      "    sample_throughput: 23.057\n",
      "    sample_time_ms: 173482.65\n",
      "    update_time_ms: 1.529\n",
      "  timestamp: 1611910099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 389\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-50-56\n",
      "  done: false\n",
      "  episode_len_mean: 543.38\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -34.9435057467959\n",
      "  episode_reward_min: -106.86660315904774\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3613\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.725292980670929\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004784788936376572\n",
      "        model: {}\n",
      "        policy_loss: -0.010835077613592148\n",
      "        total_loss: 181.39910888671875\n",
      "        vf_explained_var: 0.7129315733909607\n",
      "        vf_loss: 181.40927124023438\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.48839285714287\n",
      "    ram_util_percent: 31.991964285714285\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06877446837515322\n",
      "    mean_env_wait_ms: 39.93491588441021\n",
      "    mean_inference_ms: 1.0033329435507778\n",
      "    mean_raw_obs_processing_ms: 2.3946778638844632\n",
      "  time_since_restore: 2617.9969189167023\n",
      "  time_this_iter_s: 156.8136351108551\n",
      "  time_total_s: 61242.667456150055\n",
      "  timers:\n",
      "    learn_throughput: 1794.125\n",
      "    learn_time_ms: 2229.499\n",
      "    load_throughput: 312917.856\n",
      "    load_time_ms: 12.783\n",
      "    sample_throughput: 23.328\n",
      "    sample_time_ms: 171468.34\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1611910256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-53-48\n",
      "  done: false\n",
      "  episode_len_mean: 535.08\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -36.487362282093414\n",
      "  episode_reward_min: -102.66418584546958\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3623\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7203707098960876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01342129148542881\n",
      "        model: {}\n",
      "        policy_loss: -0.02040886878967285\n",
      "        total_loss: 283.7178955078125\n",
      "        vf_explained_var: 0.7000872492790222\n",
      "        vf_loss: 283.7373352050781\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.65569105691057\n",
      "    ram_util_percent: 32.04634146341464\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06879863834398893\n",
      "    mean_env_wait_ms: 39.9572875140545\n",
      "    mean_inference_ms: 1.004796792566392\n",
      "    mean_raw_obs_processing_ms: 2.3786448088175547\n",
      "  time_since_restore: 2790.5688829421997\n",
      "  time_this_iter_s: 172.57196402549744\n",
      "  time_total_s: 61415.23942017555\n",
      "  timers:\n",
      "    learn_throughput: 1742.093\n",
      "    learn_time_ms: 2296.089\n",
      "    load_throughput: 343923.676\n",
      "    load_time_ms: 11.63\n",
      "    sample_throughput: 23.288\n",
      "    sample_time_ms: 171765.58\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1611910428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-56-47\n",
      "  done: false\n",
      "  episode_len_mean: 528.07\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -34.686790191320725\n",
      "  episode_reward_min: -102.66418584546958\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3630\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6658025979995728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009817421436309814\n",
      "        model: {}\n",
      "        policy_loss: -0.013848932459950447\n",
      "        total_loss: 195.86471557617188\n",
      "        vf_explained_var: 0.6935856938362122\n",
      "        vf_loss: 195.87783813476562\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.54745098039217\n",
      "    ram_util_percent: 32.1121568627451\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06880620428606496\n",
      "    mean_env_wait_ms: 39.950859258298166\n",
      "    mean_inference_ms: 1.0054596560282796\n",
      "    mean_raw_obs_processing_ms: 2.3742465331634492\n",
      "  time_since_restore: 2968.921833753586\n",
      "  time_this_iter_s: 178.3529508113861\n",
      "  time_total_s: 61593.59237098694\n",
      "  timers:\n",
      "    learn_throughput: 1772.524\n",
      "    learn_time_ms: 2256.669\n",
      "    load_throughput: 338683.236\n",
      "    load_time_ms: 11.81\n",
      "    sample_throughput: 23.321\n",
      "    sample_time_ms: 171518.74\n",
      "    update_time_ms: 2.579\n",
      "  timestamp: 1611910607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_09-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 525.8\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -36.57920036520641\n",
      "  episode_reward_min: -102.66418584546958\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3638\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6840798258781433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017682000994682312\n",
      "        model: {}\n",
      "        policy_loss: -0.015245222486555576\n",
      "        total_loss: 467.61895751953125\n",
      "        vf_explained_var: 0.5515915155410767\n",
      "        vf_loss: 467.6329040527344\n",
      "    num_steps_sampled: 1572000\n",
      "    num_steps_trained: 1572000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.13401639344262\n",
      "    ram_util_percent: 32.09959016393444\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06880459181759767\n",
      "    mean_env_wait_ms: 39.922282553581354\n",
      "    mean_inference_ms: 1.0058523791700507\n",
      "    mean_raw_obs_processing_ms: 2.373643109671893\n",
      "  time_since_restore: 3140.260447025299\n",
      "  time_this_iter_s: 171.33861327171326\n",
      "  time_total_s: 61764.93098425865\n",
      "  timers:\n",
      "    learn_throughput: 1810.69\n",
      "    learn_time_ms: 2209.103\n",
      "    load_throughput: 309916.689\n",
      "    load_time_ms: 12.907\n",
      "    sample_throughput: 23.417\n",
      "    sample_time_ms: 170815.629\n",
      "    update_time_ms: 2.549\n",
      "  timestamp: 1611910778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1572000\n",
      "  training_iteration: 393\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-02-16\n",
      "  done: false\n",
      "  episode_len_mean: 528.29\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -32.25069156301316\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3646\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6599298715591431\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005454260855913162\n",
      "        model: {}\n",
      "        policy_loss: -0.0080944262444973\n",
      "        total_loss: 250.45045471191406\n",
      "        vf_explained_var: 0.7484527230262756\n",
      "        vf_loss: 250.45811462402344\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.37566371681416\n",
      "    ram_util_percent: 32.10929203539824\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06881371481723775\n",
      "    mean_env_wait_ms: 39.898323043514004\n",
      "    mean_inference_ms: 1.0064582756233096\n",
      "    mean_raw_obs_processing_ms: 2.3694094895913635\n",
      "  time_since_restore: 3298.191476583481\n",
      "  time_this_iter_s: 157.93102955818176\n",
      "  time_total_s: 61922.86201381683\n",
      "  timers:\n",
      "    learn_throughput: 1848.018\n",
      "    learn_time_ms: 2164.481\n",
      "    load_throughput: 281559.744\n",
      "    load_time_ms: 14.207\n",
      "    sample_throughput: 23.758\n",
      "    sample_time_ms: 168366.197\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1611910936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-04-52\n",
      "  done: false\n",
      "  episode_len_mean: 541.23\n",
      "  episode_reward_max: 118.38339855581667\n",
      "  episode_reward_mean: -25.88352777074817\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3652\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6546499133110046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00898362509906292\n",
      "        model: {}\n",
      "        policy_loss: -0.008115137927234173\n",
      "        total_loss: 250.08114624023438\n",
      "        vf_explained_var: 0.6833428740501404\n",
      "        vf_loss: 250.0885772705078\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.6054054054054\n",
      "    ram_util_percent: 32.10045045045045\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06879877719583721\n",
      "    mean_env_wait_ms: 39.86388088068685\n",
      "    mean_inference_ms: 1.0064021629227236\n",
      "    mean_raw_obs_processing_ms: 2.3637071403154852\n",
      "  time_since_restore: 3453.804407119751\n",
      "  time_this_iter_s: 155.61293053627014\n",
      "  time_total_s: 62078.4749443531\n",
      "  timers:\n",
      "    learn_throughput: 1887.891\n",
      "    learn_time_ms: 2118.766\n",
      "    load_throughput: 265406.845\n",
      "    load_time_ms: 15.071\n",
      "    sample_throughput: 24.122\n",
      "    sample_time_ms: 165826.931\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1611911092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 395\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-07-49\n",
      "  done: false\n",
      "  episode_len_mean: 530.39\n",
      "  episode_reward_max: 118.37598321298464\n",
      "  episode_reward_mean: -29.762718035337603\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3661\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6709092855453491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009057204239070415\n",
      "        model: {}\n",
      "        policy_loss: -0.009248638525605202\n",
      "        total_loss: 296.1432800292969\n",
      "        vf_explained_var: 0.7162513136863708\n",
      "        vf_loss: 296.15191650390625\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.64229249011858\n",
      "    ram_util_percent: 32.07470355731226\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06875175149242216\n",
      "    mean_env_wait_ms: 39.80115391750534\n",
      "    mean_inference_ms: 1.0058252496621254\n",
      "    mean_raw_obs_processing_ms: 2.360252804011465\n",
      "  time_since_restore: 3630.9516727924347\n",
      "  time_this_iter_s: 177.14726567268372\n",
      "  time_total_s: 62255.62221002579\n",
      "  timers:\n",
      "    learn_throughput: 1886.916\n",
      "    learn_time_ms: 2119.862\n",
      "    load_throughput: 267006.438\n",
      "    load_time_ms: 14.981\n",
      "    sample_throughput: 24.161\n",
      "    sample_time_ms: 165554.379\n",
      "    update_time_ms: 2.419\n",
      "  timestamp: 1611911269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 529.96\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: -27.58183777201996\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3668\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.633033275604248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014534109272062778\n",
      "        model: {}\n",
      "        policy_loss: -0.01767726056277752\n",
      "        total_loss: 180.0063934326172\n",
      "        vf_explained_var: 0.7970470786094666\n",
      "        vf_loss: 180.0229949951172\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.22807692307694\n",
      "    ram_util_percent: 32.0\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06869812069715933\n",
      "    mean_env_wait_ms: 39.74452696608258\n",
      "    mean_inference_ms: 1.005036795369278\n",
      "    mean_raw_obs_processing_ms: 2.3589456326284086\n",
      "  time_since_restore: 3813.265072822571\n",
      "  time_this_iter_s: 182.3134000301361\n",
      "  time_total_s: 62437.93561005592\n",
      "  timers:\n",
      "    learn_throughput: 1924.146\n",
      "    learn_time_ms: 2078.845\n",
      "    load_throughput: 259956.708\n",
      "    load_time_ms: 15.387\n",
      "    sample_throughput: 24.143\n",
      "    sample_time_ms: 165677.966\n",
      "    update_time_ms: 2.386\n",
      "  timestamp: 1611911451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-13-33\n",
      "  done: false\n",
      "  episode_len_mean: 529.86\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: -17.558512907558192\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3676\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6185231804847717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009040676057338715\n",
      "        model: {}\n",
      "        policy_loss: -0.013133601285517216\n",
      "        total_loss: 158.41029357910156\n",
      "        vf_explained_var: 0.8730352520942688\n",
      "        vf_loss: 158.42276000976562\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.79523809523808\n",
      "    ram_util_percent: 32.01038961038961\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06861049327852715\n",
      "    mean_env_wait_ms: 39.660537960646735\n",
      "    mean_inference_ms: 1.003535522976985\n",
      "    mean_raw_obs_processing_ms: 2.3585995757312217\n",
      "  time_since_restore: 3975.0126955509186\n",
      "  time_this_iter_s: 161.74762272834778\n",
      "  time_total_s: 62599.68323278427\n",
      "  timers:\n",
      "    learn_throughput: 1954.066\n",
      "    learn_time_ms: 2047.013\n",
      "    load_throughput: 258053.847\n",
      "    load_time_ms: 15.501\n",
      "    sample_throughput: 24.237\n",
      "    sample_time_ms: 165034.903\n",
      "    update_time_ms: 2.36\n",
      "  timestamp: 1611911613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-16-15\n",
      "  done: false\n",
      "  episode_len_mean: 533.18\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: -7.59409522830946\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3684\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6905609965324402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014698036015033722\n",
      "        model: {}\n",
      "        policy_loss: -0.017587266862392426\n",
      "        total_loss: 156.48553466796875\n",
      "        vf_explained_var: 0.8319247364997864\n",
      "        vf_loss: 156.50205993652344\n",
      "    num_steps_sampled: 1596000\n",
      "    num_steps_trained: 1596000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44718614718613\n",
      "    ram_util_percent: 32.0\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06850847083272532\n",
      "    mean_env_wait_ms: 39.56153642771654\n",
      "    mean_inference_ms: 1.0015613574503666\n",
      "    mean_raw_obs_processing_ms: 2.3581060015896314\n",
      "  time_since_restore: 4136.744872331619\n",
      "  time_this_iter_s: 161.73217678070068\n",
      "  time_total_s: 62761.41540956497\n",
      "  timers:\n",
      "    learn_throughput: 1914.503\n",
      "    learn_time_ms: 2089.315\n",
      "    load_throughput: 260426.002\n",
      "    load_time_ms: 15.359\n",
      "    sample_throughput: 24.194\n",
      "    sample_time_ms: 165331.449\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1611911775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1596000\n",
      "  training_iteration: 399\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 534.0\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: -3.9662946102970476\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3690\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6682301759719849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006313785444945097\n",
      "        model: {}\n",
      "        policy_loss: -0.008931894786655903\n",
      "        total_loss: 358.1108703613281\n",
      "        vf_explained_var: 0.5669906735420227\n",
      "        vf_loss: 358.1192321777344\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.48976377952756\n",
      "    ram_util_percent: 32.0263779527559\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06843856524470261\n",
      "    mean_env_wait_ms: 39.48858981307382\n",
      "    mean_inference_ms: 1.0000036164168684\n",
      "    mean_raw_obs_processing_ms: 2.3558688952380646\n",
      "  time_since_restore: 4314.935156822205\n",
      "  time_this_iter_s: 178.19028449058533\n",
      "  time_total_s: 62939.60569405556\n",
      "  timers:\n",
      "    learn_throughput: 1873.781\n",
      "    learn_time_ms: 2134.721\n",
      "    load_throughput: 269811.712\n",
      "    load_time_ms: 14.825\n",
      "    sample_throughput: 23.892\n",
      "    sample_time_ms: 167419.219\n",
      "    update_time_ms: 2.47\n",
      "  timestamp: 1611911953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 534.23\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 5.273875574206616\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3698\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6770862340927124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015406185761094093\n",
      "        model: {}\n",
      "        policy_loss: -0.014809041284024715\n",
      "        total_loss: 238.99099731445312\n",
      "        vf_explained_var: 0.7678606510162354\n",
      "        vf_loss: 239.00469970703125\n",
      "    num_steps_sampled: 1604000\n",
      "    num_steps_trained: 1604000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.74153225806452\n",
      "    ram_util_percent: 32.077016129032266\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06835291724334258\n",
      "    mean_env_wait_ms: 39.39918949782223\n",
      "    mean_inference_ms: 0.9980765554765526\n",
      "    mean_raw_obs_processing_ms: 2.353927334668616\n",
      "  time_since_restore: 4488.845426797867\n",
      "  time_this_iter_s: 173.91026997566223\n",
      "  time_total_s: 63113.51596403122\n",
      "  timers:\n",
      "    learn_throughput: 1929.567\n",
      "    learn_time_ms: 2073.004\n",
      "    load_throughput: 260324.979\n",
      "    load_time_ms: 15.365\n",
      "    sample_throughput: 23.864\n",
      "    sample_time_ms: 167618.803\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1611912127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1604000\n",
      "  training_iteration: 401\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 524.56\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 3.1083367053670434\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3706\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6356001496315002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006004988681524992\n",
      "        model: {}\n",
      "        policy_loss: -0.009747171774506569\n",
      "        total_loss: 471.7366027832031\n",
      "        vf_explained_var: 0.5226055979728699\n",
      "        vf_loss: 471.7460021972656\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.8789256198347\n",
      "    ram_util_percent: 32.073966942148765\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06829042769955014\n",
      "    mean_env_wait_ms: 39.33046916299986\n",
      "    mean_inference_ms: 0.9965806914326646\n",
      "    mean_raw_obs_processing_ms: 2.353700738956569\n",
      "  time_since_restore: 4658.425466775894\n",
      "  time_this_iter_s: 169.58003997802734\n",
      "  time_total_s: 63283.09600400925\n",
      "  timers:\n",
      "    learn_throughput: 1944.683\n",
      "    learn_time_ms: 2056.89\n",
      "    load_throughput: 267470.849\n",
      "    load_time_ms: 14.955\n",
      "    sample_throughput: 23.986\n",
      "    sample_time_ms: 166764.849\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1611912297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 402\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-27-35\n",
      "  done: false\n",
      "  episode_len_mean: 518.79\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: -1.170693760447101\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3713\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7123825550079346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014708118513226509\n",
      "        model: {}\n",
      "        policy_loss: -0.013899054378271103\n",
      "        total_loss: 211.26866149902344\n",
      "        vf_explained_var: 0.761581540107727\n",
      "        vf_loss: 211.28146362304688\n",
      "    num_steps_sampled: 1612000\n",
      "    num_steps_trained: 1612000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.41814159292035\n",
      "    ram_util_percent: 32.060619469026555\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06824651671550158\n",
      "    mean_env_wait_ms: 39.28388515608935\n",
      "    mean_inference_ms: 0.9955139220846035\n",
      "    mean_raw_obs_processing_ms: 2.3552077675668164\n",
      "  time_since_restore: 4816.494696855545\n",
      "  time_this_iter_s: 158.06923007965088\n",
      "  time_total_s: 63441.1652340889\n",
      "  timers:\n",
      "    learn_throughput: 1944.349\n",
      "    learn_time_ms: 2057.244\n",
      "    load_throughput: 289609.377\n",
      "    load_time_ms: 13.812\n",
      "    sample_throughput: 24.178\n",
      "    sample_time_ms: 165439.765\n",
      "    update_time_ms: 1.341\n",
      "  timestamp: 1611912455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1612000\n",
      "  training_iteration: 403\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-30-14\n",
      "  done: false\n",
      "  episode_len_mean: 528.61\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 8.623131560806392\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3721\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6670997738838196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011954431422054768\n",
      "        model: {}\n",
      "        policy_loss: -0.011539160273969173\n",
      "        total_loss: 83.81355285644531\n",
      "        vf_explained_var: 0.899836003780365\n",
      "        vf_loss: 83.82420349121094\n",
      "    num_steps_sampled: 1616000\n",
      "    num_steps_trained: 1616000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.77797356828194\n",
      "    ram_util_percent: 32.07797356828194\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06819619912510565\n",
      "    mean_env_wait_ms: 39.23098494569507\n",
      "    mean_inference_ms: 0.9942858489818776\n",
      "    mean_raw_obs_processing_ms: 2.3540967899970577\n",
      "  time_since_restore: 4975.110552072525\n",
      "  time_this_iter_s: 158.61585521697998\n",
      "  time_total_s: 63599.78108930588\n",
      "  timers:\n",
      "    learn_throughput: 1943.306\n",
      "    learn_time_ms: 2058.348\n",
      "    load_throughput: 296801.093\n",
      "    load_time_ms: 13.477\n",
      "    sample_throughput: 24.169\n",
      "    sample_time_ms: 165503.182\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1611912614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1616000\n",
      "  training_iteration: 404\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-32-53\n",
      "  done: false\n",
      "  episode_len_mean: 526.22\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 8.178603860444364\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3729\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6579095125198364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009689023718237877\n",
      "        model: {}\n",
      "        policy_loss: -0.015002934262156487\n",
      "        total_loss: 254.31414794921875\n",
      "        vf_explained_var: 0.7309481501579285\n",
      "        vf_loss: 254.3284454345703\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.00176991150443\n",
      "    ram_util_percent: 32.099115044247796\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06813228458687677\n",
      "    mean_env_wait_ms: 39.165946842702354\n",
      "    mean_inference_ms: 0.9927193215665401\n",
      "    mean_raw_obs_processing_ms: 2.353803373555826\n",
      "  time_since_restore: 5134.047560214996\n",
      "  time_this_iter_s: 158.9370081424713\n",
      "  time_total_s: 63758.71809744835\n",
      "  timers:\n",
      "    learn_throughput: 1940.828\n",
      "    learn_time_ms: 2060.976\n",
      "    load_throughput: 291837.052\n",
      "    load_time_ms: 13.706\n",
      "    sample_throughput: 24.121\n",
      "    sample_time_ms: 165832.471\n",
      "    update_time_ms: 1.334\n",
      "  timestamp: 1611912773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 405\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-35-30\n",
      "  done: false\n",
      "  episode_len_mean: 528.96\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 3.7004345062971566\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3736\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6315211653709412\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012678894214332104\n",
      "        model: {}\n",
      "        policy_loss: -0.012378073297441006\n",
      "        total_loss: 379.9542541503906\n",
      "        vf_explained_var: 0.6200718879699707\n",
      "        vf_loss: 379.9656677246094\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1624000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.27688888888889\n",
      "    ram_util_percent: 32.085777777777786\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06806925933810129\n",
      "    mean_env_wait_ms: 39.103373101297315\n",
      "    mean_inference_ms: 0.9911884995527376\n",
      "    mean_raw_obs_processing_ms: 2.352948160537545\n",
      "  time_since_restore: 5291.067179679871\n",
      "  time_this_iter_s: 157.01961946487427\n",
      "  time_total_s: 63915.73771691322\n",
      "  timers:\n",
      "    learn_throughput: 1976.74\n",
      "    learn_time_ms: 2023.533\n",
      "    load_throughput: 278909.976\n",
      "    load_time_ms: 14.342\n",
      "    sample_throughput: 24.411\n",
      "    sample_time_ms: 163858.935\n",
      "    update_time_ms: 1.298\n",
      "  timestamp: 1611912930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 406\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 535.19\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 5.679653341839879\n",
      "  episode_reward_min: -107.96406149488739\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3742\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7011871933937073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0131944315508008\n",
      "        model: {}\n",
      "        policy_loss: -0.009777803905308247\n",
      "        total_loss: 129.11184692382812\n",
      "        vf_explained_var: 0.8314991593360901\n",
      "        vf_loss: 129.12063598632812\n",
      "    num_steps_sampled: 1628000\n",
      "    num_steps_trained: 1628000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60720720720721\n",
      "    ram_util_percent: 32.086036036036035\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06801803007013979\n",
      "    mean_env_wait_ms: 39.053011542267434\n",
      "    mean_inference_ms: 0.9899490695382491\n",
      "    mean_raw_obs_processing_ms: 2.3510169049757876\n",
      "  time_since_restore: 5447.148740530014\n",
      "  time_this_iter_s: 156.08156085014343\n",
      "  time_total_s: 64071.81927776337\n",
      "  timers:\n",
      "    learn_throughput: 2023.677\n",
      "    learn_time_ms: 1976.6\n",
      "    load_throughput: 290192.462\n",
      "    load_time_ms: 13.784\n",
      "    sample_throughput: 24.801\n",
      "    sample_time_ms: 161286.417\n",
      "    update_time_ms: 1.31\n",
      "  timestamp: 1611913086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1628000\n",
      "  training_iteration: 407\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 540.91\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 5.712717683045724\n",
      "  episode_reward_min: -105.12459272274782\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3749\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6787577867507935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010654086247086525\n",
      "        model: {}\n",
      "        policy_loss: -0.01118322741240263\n",
      "        total_loss: 167.65914916992188\n",
      "        vf_explained_var: 0.8176932334899902\n",
      "        vf_loss: 167.66954040527344\n",
      "    num_steps_sampled: 1632000\n",
      "    num_steps_trained: 1632000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.31555555555556\n",
      "    ram_util_percent: 32.08711111111112\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06796271773612221\n",
      "    mean_env_wait_ms: 38.99866081293805\n",
      "    mean_inference_ms: 0.9886078344305668\n",
      "    mean_raw_obs_processing_ms: 2.3490721581502445\n",
      "  time_since_restore: 5604.435450553894\n",
      "  time_this_iter_s: 157.28671002388\n",
      "  time_total_s: 64229.10598778725\n",
      "  timers:\n",
      "    learn_throughput: 2022.278\n",
      "    learn_time_ms: 1977.968\n",
      "    load_throughput: 286847.877\n",
      "    load_time_ms: 13.945\n",
      "    sample_throughput: 24.87\n",
      "    sample_time_ms: 160834.198\n",
      "    update_time_ms: 1.333\n",
      "  timestamp: 1611913243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1632000\n",
      "  training_iteration: 408\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 540.11\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 12.984268050426545\n",
      "  episode_reward_min: -105.12459272274782\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3756\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6674298048019409\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008998067118227482\n",
      "        model: {}\n",
      "        policy_loss: -0.013268054462969303\n",
      "        total_loss: 142.21531677246094\n",
      "        vf_explained_var: 0.8325807452201843\n",
      "        vf_loss: 142.22792053222656\n",
      "    num_steps_sampled: 1636000\n",
      "    num_steps_trained: 1636000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.10210084033612\n",
      "    ram_util_percent: 32.08025210084034\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06790950431354387\n",
      "    mean_env_wait_ms: 38.94931869015909\n",
      "    mean_inference_ms: 0.9873501078681005\n",
      "    mean_raw_obs_processing_ms: 2.3471472022454947\n",
      "  time_since_restore: 5771.023134231567\n",
      "  time_this_iter_s: 166.58768367767334\n",
      "  time_total_s: 64395.69367146492\n",
      "  timers:\n",
      "    learn_throughput: 2048.392\n",
      "    learn_time_ms: 1952.751\n",
      "    load_throughput: 301633.478\n",
      "    load_time_ms: 13.261\n",
      "    sample_throughput: 24.791\n",
      "    sample_time_ms: 161347.194\n",
      "    update_time_ms: 1.294\n",
      "  timestamp: 1611913410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1636000\n",
      "  training_iteration: 409\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 552.53\n",
      "  episode_reward_max: 118.3776589846874\n",
      "  episode_reward_mean: 10.684536972509948\n",
      "  episode_reward_min: -105.12459272274782\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3762\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6590847373008728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008498048409819603\n",
      "        model: {}\n",
      "        policy_loss: -0.007302898447960615\n",
      "        total_loss: 349.7015380859375\n",
      "        vf_explained_var: 0.5142794847488403\n",
      "        vf_loss: 349.70819091796875\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.16502057613168\n",
      "    ram_util_percent: 32.02139917695473\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06785937075836582\n",
      "    mean_env_wait_ms: 38.90595225739877\n",
      "    mean_inference_ms: 0.9862238799880518\n",
      "    mean_raw_obs_processing_ms: 2.344371980787996\n",
      "  time_since_restore: 5941.361886978149\n",
      "  time_this_iter_s: 170.33875274658203\n",
      "  time_total_s: 64566.0324242115\n",
      "  timers:\n",
      "    learn_throughput: 2040.816\n",
      "    learn_time_ms: 1960.0\n",
      "    load_throughput: 311481.733\n",
      "    load_time_ms: 12.842\n",
      "    sample_throughput: 24.914\n",
      "    sample_time_ms: 160555.054\n",
      "    update_time_ms: 1.272\n",
      "  timestamp: 1611913580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 410\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 551.51\n",
      "  episode_reward_max: 118.37292344152567\n",
      "  episode_reward_mean: 2.167667181179839\n",
      "  episode_reward_min: -105.53763266062248\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3771\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6301990747451782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012076349928975105\n",
      "        model: {}\n",
      "        policy_loss: -0.015821507200598717\n",
      "        total_loss: 417.8067626953125\n",
      "        vf_explained_var: 0.5600265860557556\n",
      "        vf_loss: 417.8217468261719\n",
      "    num_steps_sampled: 1644000\n",
      "    num_steps_trained: 1644000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.04694656488549\n",
      "    ram_util_percent: 32.036641221374055\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778758796815358\n",
      "    mean_env_wait_ms: 38.84393033435868\n",
      "    mean_inference_ms: 0.9845503943096336\n",
      "    mean_raw_obs_processing_ms: 2.3416945710306423\n",
      "  time_since_restore: 6125.351510763168\n",
      "  time_this_iter_s: 183.98962378501892\n",
      "  time_total_s: 64750.02204799652\n",
      "  timers:\n",
      "    learn_throughput: 2011.088\n",
      "    learn_time_ms: 1988.973\n",
      "    load_throughput: 328491.605\n",
      "    load_time_ms: 12.177\n",
      "    sample_throughput: 24.763\n",
      "    sample_time_ms: 161532.352\n",
      "    update_time_ms: 1.272\n",
      "  timestamp: 1611913764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1644000\n",
      "  training_iteration: 411\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-52-27\n",
      "  done: false\n",
      "  episode_len_mean: 557.22\n",
      "  episode_reward_max: 118.37292344152567\n",
      "  episode_reward_mean: -6.350762298570007\n",
      "  episode_reward_min: -105.53763266062248\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3777\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6086533069610596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007563981227576733\n",
      "        model: {}\n",
      "        policy_loss: -0.007186282891780138\n",
      "        total_loss: 238.8955841064453\n",
      "        vf_explained_var: 0.6722081899642944\n",
      "        vf_loss: 238.90219116210938\n",
      "    num_steps_sampled: 1648000\n",
      "    num_steps_trained: 1648000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.83371647509578\n",
      "    ram_util_percent: 32.05785440613027\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06775413007704147\n",
      "    mean_env_wait_ms: 38.81406797063758\n",
      "    mean_inference_ms: 0.9837275401731769\n",
      "    mean_raw_obs_processing_ms: 2.3390208588241808\n",
      "  time_since_restore: 6307.658053636551\n",
      "  time_this_iter_s: 182.30654287338257\n",
      "  time_total_s: 64932.3285908699\n",
      "  timers:\n",
      "    learn_throughput: 1989.739\n",
      "    learn_time_ms: 2010.314\n",
      "    load_throughput: 328352.096\n",
      "    load_time_ms: 12.182\n",
      "    sample_throughput: 24.573\n",
      "    sample_time_ms: 162778.946\n",
      "    update_time_ms: 1.281\n",
      "  timestamp: 1611913947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1648000\n",
      "  training_iteration: 412\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 565.79\n",
      "  episode_reward_max: 118.37292344152567\n",
      "  episode_reward_mean: -8.381721665174153\n",
      "  episode_reward_min: -105.53763266062248\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3783\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6134225726127625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007413472048938274\n",
      "        model: {}\n",
      "        policy_loss: -0.005931416060775518\n",
      "        total_loss: 280.5923767089844\n",
      "        vf_explained_var: 0.6007890105247498\n",
      "        vf_loss: 280.59771728515625\n",
      "    num_steps_sampled: 1652000\n",
      "    num_steps_trained: 1652000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.71867704280156\n",
      "    ram_util_percent: 32.01206225680934\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06773099880422012\n",
      "    mean_env_wait_ms: 38.79420472482778\n",
      "    mean_inference_ms: 0.9831597658024276\n",
      "    mean_raw_obs_processing_ms: 2.3354803787658924\n",
      "  time_since_restore: 6487.798520326614\n",
      "  time_this_iter_s: 180.14046669006348\n",
      "  time_total_s: 65112.46905755997\n",
      "  timers:\n",
      "    learn_throughput: 1905.95\n",
      "    learn_time_ms: 2098.691\n",
      "    load_throughput: 338546.551\n",
      "    load_time_ms: 11.815\n",
      "    sample_throughput: 24.258\n",
      "    sample_time_ms: 164891.259\n",
      "    update_time_ms: 1.316\n",
      "  timestamp: 1611914127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1652000\n",
      "  training_iteration: 413\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_10-58-09\n",
      "  done: false\n",
      "  episode_len_mean: 558.74\n",
      "  episode_reward_max: 118.37292344152567\n",
      "  episode_reward_mean: -9.009596534656936\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3791\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6480724215507507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00841866061091423\n",
      "        model: {}\n",
      "        policy_loss: -0.015359284356236458\n",
      "        total_loss: 447.74224853515625\n",
      "        vf_explained_var: 0.5285189151763916\n",
      "        vf_loss: 447.7569580078125\n",
      "    num_steps_sampled: 1656000\n",
      "    num_steps_trained: 1656000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.19437229437229\n",
      "    ram_util_percent: 32.01948051948052\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06768431224433737\n",
      "    mean_env_wait_ms: 38.75776384901653\n",
      "    mean_inference_ms: 0.9821625226283166\n",
      "    mean_raw_obs_processing_ms: 2.3323960064677904\n",
      "  time_since_restore: 6649.549217700958\n",
      "  time_this_iter_s: 161.75069737434387\n",
      "  time_total_s: 65274.21975493431\n",
      "  timers:\n",
      "    learn_throughput: 1907.817\n",
      "    learn_time_ms: 2096.638\n",
      "    load_throughput: 326262.796\n",
      "    load_time_ms: 12.26\n",
      "    sample_throughput: 24.212\n",
      "    sample_time_ms: 165208.53\n",
      "    update_time_ms: 1.327\n",
      "  timestamp: 1611914289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1656000\n",
      "  training_iteration: 414\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 563.69\n",
      "  episode_reward_max: 118.37292344152567\n",
      "  episode_reward_mean: -14.919267323165938\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3797\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07500000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6061630845069885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004594167694449425\n",
      "        model: {}\n",
      "        policy_loss: -0.0071702091954648495\n",
      "        total_loss: 205.5405731201172\n",
      "        vf_explained_var: 0.6804428696632385\n",
      "        vf_loss: 205.5474090576172\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.53542600896861\n",
      "    ram_util_percent: 32.002242152466366\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06764421622714742\n",
      "    mean_env_wait_ms: 38.724858086982984\n",
      "    mean_inference_ms: 0.9813321794979589\n",
      "    mean_raw_obs_processing_ms: 2.3293961403607892\n",
      "  time_since_restore: 6805.975045204163\n",
      "  time_this_iter_s: 156.42582750320435\n",
      "  time_total_s: 65430.645582437515\n",
      "  timers:\n",
      "    learn_throughput: 1906.364\n",
      "    learn_time_ms: 2098.235\n",
      "    load_throughput: 330674.151\n",
      "    load_time_ms: 12.097\n",
      "    sample_throughput: 24.249\n",
      "    sample_time_ms: 164957.819\n",
      "    update_time_ms: 1.321\n",
      "  timestamp: 1611914445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 415\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 567.61\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -15.244081113242927\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3805\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6206978559494019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00617828406393528\n",
      "        model: {}\n",
      "        policy_loss: -0.009499427862465382\n",
      "        total_loss: 241.64859008789062\n",
      "        vf_explained_var: 0.6926683783531189\n",
      "        vf_loss: 241.65786743164062\n",
      "    num_steps_sampled: 1664000\n",
      "    num_steps_trained: 1664000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.02412280701755\n",
      "    ram_util_percent: 32.025438596491234\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06758644468370369\n",
      "    mean_env_wait_ms: 38.67790515814203\n",
      "    mean_inference_ms: 0.9801933227020591\n",
      "    mean_raw_obs_processing_ms: 2.3254345754404953\n",
      "  time_since_restore: 6965.668355703354\n",
      "  time_this_iter_s: 159.69331049919128\n",
      "  time_total_s: 65590.3388929367\n",
      "  timers:\n",
      "    learn_throughput: 1906.021\n",
      "    learn_time_ms: 2098.613\n",
      "    load_throughput: 332594.869\n",
      "    load_time_ms: 12.027\n",
      "    sample_throughput: 24.209\n",
      "    sample_time_ms: 165226.866\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1611914605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1664000\n",
      "  training_iteration: 416\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-06-01\n",
      "  done: false\n",
      "  episode_len_mean: 569.42\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -9.573008473714463\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3811\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.609058141708374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009131696075201035\n",
      "        model: {}\n",
      "        policy_loss: -0.0031433692201972008\n",
      "        total_loss: 167.64862060546875\n",
      "        vf_explained_var: 0.7821667194366455\n",
      "        vf_loss: 167.65142822265625\n",
      "    num_steps_sampled: 1668000\n",
      "    num_steps_trained: 1668000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.55855855855856\n",
      "    ram_util_percent: 32.01216216216216\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06754542154119068\n",
      "    mean_env_wait_ms: 38.64436955512663\n",
      "    mean_inference_ms: 0.9793950358590828\n",
      "    mean_raw_obs_processing_ms: 2.3221236468751623\n",
      "  time_since_restore: 7121.037269592285\n",
      "  time_this_iter_s: 155.36891388893127\n",
      "  time_total_s: 65745.70780682564\n",
      "  timers:\n",
      "    learn_throughput: 1900.784\n",
      "    learn_time_ms: 2104.395\n",
      "    load_throughput: 316477.202\n",
      "    load_time_ms: 12.639\n",
      "    sample_throughput: 24.22\n",
      "    sample_time_ms: 165149.85\n",
      "    update_time_ms: 1.298\n",
      "  timestamp: 1611914761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1668000\n",
      "  training_iteration: 417\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-08-39\n",
      "  done: false\n",
      "  episode_len_mean: 575.69\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -15.636949459559911\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3818\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5803655385971069\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0062149083241820335\n",
      "        model: {}\n",
      "        policy_loss: -0.008184392936527729\n",
      "        total_loss: 394.9765625\n",
      "        vf_explained_var: 0.44947606325149536\n",
      "        vf_loss: 394.9844970703125\n",
      "    num_steps_sampled: 1672000\n",
      "    num_steps_trained: 1672000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.27244444444445\n",
      "    ram_util_percent: 32.01822222222222\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0675030649499263\n",
      "    mean_env_wait_ms: 38.6080582454309\n",
      "    mean_inference_ms: 0.9785476958334283\n",
      "    mean_raw_obs_processing_ms: 2.3179158876912687\n",
      "  time_since_restore: 7279.107141017914\n",
      "  time_this_iter_s: 158.06987142562866\n",
      "  time_total_s: 65903.77767825127\n",
      "  timers:\n",
      "    learn_throughput: 1899.962\n",
      "    learn_time_ms: 2105.305\n",
      "    load_throughput: 349197.233\n",
      "    load_time_ms: 11.455\n",
      "    sample_throughput: 24.209\n",
      "    sample_time_ms: 165229.969\n",
      "    update_time_ms: 1.287\n",
      "  timestamp: 1611914919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1672000\n",
      "  training_iteration: 418\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 582.56\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -19.92170700130461\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3825\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.620570719242096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016181161627173424\n",
      "        model: {}\n",
      "        policy_loss: -0.016560440883040428\n",
      "        total_loss: 230.17282104492188\n",
      "        vf_explained_var: 0.6970689296722412\n",
      "        vf_loss: 230.18878173828125\n",
      "    num_steps_sampled: 1676000\n",
      "    num_steps_trained: 1676000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.48056680161942\n",
      "    ram_util_percent: 32.06032388663968\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06747033026840661\n",
      "    mean_env_wait_ms: 38.57963722286389\n",
      "    mean_inference_ms: 0.9778792734166563\n",
      "    mean_raw_obs_processing_ms: 2.3133150003108143\n",
      "  time_since_restore: 7451.914039373398\n",
      "  time_this_iter_s: 172.806898355484\n",
      "  time_total_s: 66076.58457660675\n",
      "  timers:\n",
      "    learn_throughput: 1873.598\n",
      "    learn_time_ms: 2134.929\n",
      "    load_throughput: 327539.26\n",
      "    load_time_ms: 12.212\n",
      "    sample_throughput: 24.123\n",
      "    sample_time_ms: 165818.196\n",
      "    update_time_ms: 1.288\n",
      "  timestamp: 1611915092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1676000\n",
      "  training_iteration: 419\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 581.56\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -20.035332013127157\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3832\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5897539258003235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010707413777709007\n",
      "        model: {}\n",
      "        policy_loss: -0.014566482976078987\n",
      "        total_loss: 249.37278747558594\n",
      "        vf_explained_var: 0.6388334631919861\n",
      "        vf_loss: 249.38693237304688\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.49027237354086\n",
      "    ram_util_percent: 32.08949416342413\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.067448463631868\n",
      "    mean_env_wait_ms: 38.561538915826894\n",
      "    mean_inference_ms: 0.9774577510652808\n",
      "    mean_raw_obs_processing_ms: 2.3087002228121083\n",
      "  time_since_restore: 7632.065415620804\n",
      "  time_this_iter_s: 180.151376247406\n",
      "  time_total_s: 66256.73595285416\n",
      "  timers:\n",
      "    learn_throughput: 1885.054\n",
      "    learn_time_ms: 2121.955\n",
      "    load_throughput: 303160.695\n",
      "    load_time_ms: 13.194\n",
      "    sample_throughput: 23.979\n",
      "    sample_time_ms: 166811.206\n",
      "    update_time_ms: 1.274\n",
      "  timestamp: 1611915272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 420\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-17-30\n",
      "  done: false\n",
      "  episode_len_mean: 584.76\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -24.599465319740634\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3839\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6866175532341003\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01588120311498642\n",
      "        model: {}\n",
      "        policy_loss: -0.010558494366705418\n",
      "        total_loss: 368.431396484375\n",
      "        vf_explained_var: 0.3757074773311615\n",
      "        vf_loss: 368.4414367675781\n",
      "    num_steps_sampled: 1684000\n",
      "    num_steps_trained: 1684000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.51294117647059\n",
      "    ram_util_percent: 32.0964705882353\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06743658798949018\n",
      "    mean_env_wait_ms: 38.55377397978316\n",
      "    mean_inference_ms: 0.9772742880658469\n",
      "    mean_raw_obs_processing_ms: 2.304649082956947\n",
      "  time_since_restore: 7810.588978767395\n",
      "  time_this_iter_s: 178.5235631465912\n",
      "  time_total_s: 66435.25951600075\n",
      "  timers:\n",
      "    learn_throughput: 1887.959\n",
      "    learn_time_ms: 2118.69\n",
      "    load_throughput: 299504.896\n",
      "    load_time_ms: 13.355\n",
      "    sample_throughput: 24.058\n",
      "    sample_time_ms: 166266.927\n",
      "    update_time_ms: 1.272\n",
      "  timestamp: 1611915450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1684000\n",
      "  training_iteration: 421\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-20-26\n",
      "  done: false\n",
      "  episode_len_mean: 584.39\n",
      "  episode_reward_max: 118.36833022583072\n",
      "  episode_reward_mean: -28.997098498952486\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 3844\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6142454743385315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011725565418601036\n",
      "        model: {}\n",
      "        policy_loss: -0.005854496732354164\n",
      "        total_loss: 253.00015258789062\n",
      "        vf_explained_var: 0.5389917492866516\n",
      "        vf_loss: 253.0055389404297\n",
      "    num_steps_sampled: 1688000\n",
      "    num_steps_trained: 1688000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.58645418326692\n",
      "    ram_util_percent: 32.095617529880485\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0674348875036153\n",
      "    mean_env_wait_ms: 38.554840247254255\n",
      "    mean_inference_ms: 0.9773190500172355\n",
      "    mean_raw_obs_processing_ms: 2.3015806088338295\n",
      "  time_since_restore: 7986.180397033691\n",
      "  time_this_iter_s: 175.5914182662964\n",
      "  time_total_s: 66610.85093426704\n",
      "  timers:\n",
      "    learn_throughput: 1889.491\n",
      "    learn_time_ms: 2116.973\n",
      "    load_throughput: 307451.268\n",
      "    load_time_ms: 13.01\n",
      "    sample_throughput: 24.155\n",
      "    sample_time_ms: 165600.09\n",
      "    update_time_ms: 1.27\n",
      "  timestamp: 1611915626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1688000\n",
      "  training_iteration: 422\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-23-09\n",
      "  done: false\n",
      "  episode_len_mean: 580.59\n",
      "  episode_reward_max: 118.35038295614653\n",
      "  episode_reward_mean: -39.14560575716708\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3852\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03750000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7433052062988281\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023156898096203804\n",
      "        model: {}\n",
      "        policy_loss: -0.018213143572211266\n",
      "        total_loss: 327.54296875\n",
      "        vf_explained_var: 0.5080050230026245\n",
      "        vf_loss: 327.5603332519531\n",
      "    num_steps_sampled: 1692000\n",
      "    num_steps_trained: 1692000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9974025974026\n",
      "    ram_util_percent: 32.08658008658009\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06743363557815009\n",
      "    mean_env_wait_ms: 38.55810081634035\n",
      "    mean_inference_ms: 0.9774381765138825\n",
      "    mean_raw_obs_processing_ms: 2.2973560374628637\n",
      "  time_since_restore: 8148.571672201157\n",
      "  time_this_iter_s: 162.3912751674652\n",
      "  time_total_s: 66773.24220943451\n",
      "  timers:\n",
      "    learn_throughput: 1967.999\n",
      "    learn_time_ms: 2032.521\n",
      "    load_throughput: 278182.988\n",
      "    load_time_ms: 14.379\n",
      "    sample_throughput: 24.403\n",
      "    sample_time_ms: 163914.583\n",
      "    update_time_ms: 1.261\n",
      "  timestamp: 1611915789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1692000\n",
      "  training_iteration: 423\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 583.87\n",
      "  episode_reward_max: 118.34735320725822\n",
      "  episode_reward_mean: -47.47360066305428\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 3858\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7251632809638977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006666491739451885\n",
      "        model: {}\n",
      "        policy_loss: -0.00788965169340372\n",
      "        total_loss: 191.7905731201172\n",
      "        vf_explained_var: 0.6965402364730835\n",
      "        vf_loss: 191.79811096191406\n",
      "    num_steps_sampled: 1696000\n",
      "    num_steps_trained: 1696000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.40762331838566\n",
      "    ram_util_percent: 32.097309417040364\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06743174193059316\n",
      "    mean_env_wait_ms: 38.55738439436574\n",
      "    mean_inference_ms: 0.9774901128200668\n",
      "    mean_raw_obs_processing_ms: 2.294071348652313\n",
      "  time_since_restore: 8304.577031612396\n",
      "  time_this_iter_s: 156.00535941123962\n",
      "  time_total_s: 66929.24756884575\n",
      "  timers:\n",
      "    learn_throughput: 1966.39\n",
      "    learn_time_ms: 2034.184\n",
      "    load_throughput: 299350.455\n",
      "    load_time_ms: 13.362\n",
      "    sample_throughput: 24.489\n",
      "    sample_time_ms: 163338.34\n",
      "    update_time_ms: 1.234\n",
      "  timestamp: 1611915945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1696000\n",
      "  training_iteration: 424\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 585.73\n",
      "  episode_reward_max: 118.34735320725822\n",
      "  episode_reward_mean: -43.84238289588675\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3865\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6458662152290344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01310559269040823\n",
      "        model: {}\n",
      "        policy_loss: -0.012983090244233608\n",
      "        total_loss: 156.1481170654297\n",
      "        vf_explained_var: 0.7527575492858887\n",
      "        vf_loss: 156.16033935546875\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.42499999999998\n",
      "    ram_util_percent: 32.11607142857144\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06742376311996963\n",
      "    mean_env_wait_ms: 38.54938591881491\n",
      "    mean_inference_ms: 0.9774223428078552\n",
      "    mean_raw_obs_processing_ms: 2.2902633523098297\n",
      "  time_since_restore: 8461.690431833267\n",
      "  time_this_iter_s: 157.11340022087097\n",
      "  time_total_s: 67086.36096906662\n",
      "  timers:\n",
      "    learn_throughput: 1965.293\n",
      "    learn_time_ms: 2035.32\n",
      "    load_throughput: 299948.26\n",
      "    load_time_ms: 13.336\n",
      "    sample_throughput: 24.479\n",
      "    sample_time_ms: 163405.581\n",
      "    update_time_ms: 1.233\n",
      "  timestamp: 1611916102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 425\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-31-00\n",
      "  done: false\n",
      "  episode_len_mean: 584.57\n",
      "  episode_reward_max: 118.34735320725822\n",
      "  episode_reward_mean: -46.019587431976\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3873\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6780170798301697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008370458148419857\n",
      "        model: {}\n",
      "        policy_loss: -0.013391097076237202\n",
      "        total_loss: 238.70167541503906\n",
      "        vf_explained_var: 0.6124398708343506\n",
      "        vf_loss: 238.71463012695312\n",
      "    num_steps_sampled: 1704000\n",
      "    num_steps_trained: 1704000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.35840707964601\n",
      "    ram_util_percent: 32.08893805309736\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06740400070003379\n",
      "    mean_env_wait_ms: 38.52994022838029\n",
      "    mean_inference_ms: 0.977144522790778\n",
      "    mean_raw_obs_processing_ms: 2.2857750349781627\n",
      "  time_since_restore: 8619.320796728134\n",
      "  time_this_iter_s: 157.63036489486694\n",
      "  time_total_s: 67243.99133396149\n",
      "  timers:\n",
      "    learn_throughput: 1937.479\n",
      "    learn_time_ms: 2064.538\n",
      "    load_throughput: 301931.494\n",
      "    load_time_ms: 13.248\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 163168.094\n",
      "    update_time_ms: 1.222\n",
      "  timestamp: 1611916260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1704000\n",
      "  training_iteration: 426\n",
      "  trial_id: f98e4_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_f98e4_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-33-37\n",
      "  done: false\n",
      "  episode_len_mean: 575.7\n",
      "  episode_reward_max: 118.34735320725822\n",
      "  episode_reward_mean: -50.22180343461458\n",
      "  episode_reward_min: -106.48882377164827\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3880\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7190980315208435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011336714960634708\n",
      "        model: {}\n",
      "        policy_loss: -0.018945829942822456\n",
      "        total_loss: 181.3560791015625\n",
      "        vf_explained_var: 0.6788073778152466\n",
      "        vf_loss: 181.3743438720703\n",
      "    num_steps_sampled: 1708000\n",
      "    num_steps_trained: 1708000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58348214285714\n",
      "    ram_util_percent: 32.109821428571436\n",
      "  pid: 2092936\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06737646258432614\n",
      "    mean_env_wait_ms: 38.502468767325524\n",
      "    mean_inference_ms: 0.9766592975529356\n",
      "    mean_raw_obs_processing_ms: 2.282669843156248\n",
      "  time_since_restore: 8776.590459346771\n",
      "  time_this_iter_s: 157.26966261863708\n",
      "  time_total_s: 67401.26099658012\n",
      "  timers:\n",
      "    learn_throughput: 1942.476\n",
      "    learn_time_ms: 2059.228\n",
      "    load_throughput: 316804.689\n",
      "    load_time_ms: 12.626\n",
      "    sample_throughput: 24.485\n",
      "    sample_time_ms: 163362.922\n",
      "    update_time_ms: 1.257\n",
      "  timestamp: 1611916417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1708000\n",
      "  training_iteration: 427\n",
      "  trial_id: f98e4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         58802.3</td><td style=\"text-align: right;\">1504000</td><td style=\"text-align: right;\">-68.5058</td><td style=\"text-align: right;\">             118.355</td><td style=\"text-align: right;\">             -97.037</td><td style=\"text-align: right;\">           433.222</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         58972.7</td><td style=\"text-align: right;\">1508000</td><td style=\"text-align: right;\">-56.8872</td><td style=\"text-align: right;\">             118.355</td><td style=\"text-align: right;\">            -99.5941</td><td style=\"text-align: right;\">           453.412</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         59146.9</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">-58.3231</td><td style=\"text-align: right;\">             118.355</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           491.458</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         59327.2</td><td style=\"text-align: right;\">1516000</td><td style=\"text-align: right;\">-45.3902</td><td style=\"text-align: right;\">             118.363</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           511.871</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         59504.4</td><td style=\"text-align: right;\">1520000</td><td style=\"text-align: right;\">-31.8827</td><td style=\"text-align: right;\">             118.371</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           512.526</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         59673.3</td><td style=\"text-align: right;\">1524000</td><td style=\"text-align: right;\">-39.0244</td><td style=\"text-align: right;\">             118.371</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           504.319</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         59854.4</td><td style=\"text-align: right;\">1528000</td><td style=\"text-align: right;\">-35.9146</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">               506</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         60033.3</td><td style=\"text-align: right;\">1532000</td><td style=\"text-align: right;\">-34.2378</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           517.197</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         60216.2</td><td style=\"text-align: right;\">1536000</td><td style=\"text-align: right;\">-28.7451</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">            517.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         60397.7</td><td style=\"text-align: right;\">1540000</td><td style=\"text-align: right;\">-31.8928</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           517.158</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         60577.5</td><td style=\"text-align: right;\">1544000</td><td style=\"text-align: right;\"> -37.419</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           515.798</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">           60759</td><td style=\"text-align: right;\">1548000</td><td style=\"text-align: right;\">-41.6832</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           516.859</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         60927.5</td><td style=\"text-align: right;\">1552000</td><td style=\"text-align: right;\">-42.8574</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">           522.788</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         61085.9</td><td style=\"text-align: right;\">1556000</td><td style=\"text-align: right;\">-39.1399</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">            533.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         61242.7</td><td style=\"text-align: right;\">1560000</td><td style=\"text-align: right;\">-34.9435</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">            543.38</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         61415.2</td><td style=\"text-align: right;\">1564000</td><td style=\"text-align: right;\">-36.4874</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -102.664</td><td style=\"text-align: right;\">            535.08</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         61593.6</td><td style=\"text-align: right;\">1568000</td><td style=\"text-align: right;\">-34.6868</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -102.664</td><td style=\"text-align: right;\">            528.07</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         61764.9</td><td style=\"text-align: right;\">1572000</td><td style=\"text-align: right;\">-36.5792</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -102.664</td><td style=\"text-align: right;\">             525.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         61922.9</td><td style=\"text-align: right;\">1576000</td><td style=\"text-align: right;\">-32.2507</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            528.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         62078.5</td><td style=\"text-align: right;\">1580000</td><td style=\"text-align: right;\">-25.8835</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            541.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         62255.6</td><td style=\"text-align: right;\">1584000</td><td style=\"text-align: right;\">-29.7627</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            530.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         62437.9</td><td style=\"text-align: right;\">1588000</td><td style=\"text-align: right;\">-27.5818</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            529.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         62599.7</td><td style=\"text-align: right;\">1592000</td><td style=\"text-align: right;\">-17.5585</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            529.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         62761.4</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\"> -7.5941</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            533.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         62939.6</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">-3.96629</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">               534</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         63113.5</td><td style=\"text-align: right;\">1604000</td><td style=\"text-align: right;\"> 5.27388</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            534.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         63283.1</td><td style=\"text-align: right;\">1608000</td><td style=\"text-align: right;\"> 3.10834</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            524.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         63441.2</td><td style=\"text-align: right;\">1612000</td><td style=\"text-align: right;\">-1.17069</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            518.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         63599.8</td><td style=\"text-align: right;\">1616000</td><td style=\"text-align: right;\"> 8.62313</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            528.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         63758.7</td><td style=\"text-align: right;\">1620000</td><td style=\"text-align: right;\">  8.1786</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            526.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         63915.7</td><td style=\"text-align: right;\">1624000</td><td style=\"text-align: right;\"> 3.70043</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            528.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         64071.8</td><td style=\"text-align: right;\">1628000</td><td style=\"text-align: right;\"> 5.67965</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.964</td><td style=\"text-align: right;\">            535.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         64229.1</td><td style=\"text-align: right;\">1632000</td><td style=\"text-align: right;\"> 5.71272</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -105.125</td><td style=\"text-align: right;\">            540.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         64395.7</td><td style=\"text-align: right;\">1636000</td><td style=\"text-align: right;\"> 12.9843</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -105.125</td><td style=\"text-align: right;\">            540.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">           64566</td><td style=\"text-align: right;\">1640000</td><td style=\"text-align: right;\"> 10.6845</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -105.125</td><td style=\"text-align: right;\">            552.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">           64750</td><td style=\"text-align: right;\">1644000</td><td style=\"text-align: right;\"> 2.16767</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -105.538</td><td style=\"text-align: right;\">            551.51</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         64932.3</td><td style=\"text-align: right;\">1648000</td><td style=\"text-align: right;\">-6.35076</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -105.538</td><td style=\"text-align: right;\">            557.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         65112.5</td><td style=\"text-align: right;\">1652000</td><td style=\"text-align: right;\">-8.38172</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -105.538</td><td style=\"text-align: right;\">            565.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         65274.2</td><td style=\"text-align: right;\">1656000</td><td style=\"text-align: right;\"> -9.0096</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            558.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         65430.6</td><td style=\"text-align: right;\">1660000</td><td style=\"text-align: right;\">-14.9193</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            563.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         65590.3</td><td style=\"text-align: right;\">1664000</td><td style=\"text-align: right;\">-15.2441</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            567.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         65745.7</td><td style=\"text-align: right;\">1668000</td><td style=\"text-align: right;\">-9.57301</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            569.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         65903.8</td><td style=\"text-align: right;\">1672000</td><td style=\"text-align: right;\">-15.6369</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            575.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         66076.6</td><td style=\"text-align: right;\">1676000</td><td style=\"text-align: right;\">-19.9217</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            582.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         66256.7</td><td style=\"text-align: right;\">1680000</td><td style=\"text-align: right;\">-20.0353</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            581.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         66435.3</td><td style=\"text-align: right;\">1684000</td><td style=\"text-align: right;\">-24.5995</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            584.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         66610.9</td><td style=\"text-align: right;\">1688000</td><td style=\"text-align: right;\">-28.9971</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            584.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         66773.2</td><td style=\"text-align: right;\">1692000</td><td style=\"text-align: right;\">-39.1456</td><td style=\"text-align: right;\">              118.35</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            580.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         66929.2</td><td style=\"text-align: right;\">1696000</td><td style=\"text-align: right;\">-47.4736</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            583.87</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         67086.4</td><td style=\"text-align: right;\">1700000</td><td style=\"text-align: right;\">-43.8424</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            585.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">           67244</td><td style=\"text-align: right;\">1704000</td><td style=\"text-align: right;\">-46.0196</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">            584.57</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.11 GiB heap, 0.0/4.83 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_09-07-05<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_f98e4_00000</td><td>RUNNING </td><td>192.168.178.60:2092936</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         67401.3</td><td style=\"text-align: right;\">1708000</td><td style=\"text-align: right;\">-50.2218</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -106.489</td><td style=\"text-align: right;\">             575.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/PPO_2021-01-28_15-51-43/PPO_ScoutingDiscreteTask_56467_00000_0_2021-01-28_15-51-43/checkpoint_375/checkpoint-375')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-01-28_15-51-43/PPO_ScoutingDiscreteTask_56467_00000_0_2021-01-28_15-51-43/checkpoint_375/checkpoint-375'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m non-resource variables are not supported in the long term\n",
      "2021-01-29 08:46:24,136\tWARNING worker.py:1034 -- Failed to unpickle actor class 'RolloutWorker' for actor ID 88866c7d01000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    logger.exception(\"Failed to load actor class %s.\", class_name)\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n",
      "    from ray.rllib.env.base_env import BaseEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 1, in <module>\n",
      "    from ray.rllib.env.base_env import BaseEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/base_env.py\", line 3, in <module>\n",
      "    from ray.rllib.env.external_env import ExternalEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/external_env.py\", line 7, in <module>\n",
      "    from ray.rllib.utils.annotations import PublicAPI\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/utils/__init__.py\", line 18, in <module>\n",
      "    from ray.tune.utils import merge_dicts, deep_update\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/__init__.py\", line 2, in <module>\n",
      "    from ray.tune.tune import run_experiments, run\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/tune.py\", line 10, in <module>\n",
      "    from ray.tune.ray_trial_executor import RayTrialExecutor\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 11, in <module>\n",
      "    from ray.exceptions import RayTimeoutError\n",
      "ImportError: cannot import name 'RayTimeoutError' from 'ray.exceptions' (/home/dschori/.local/lib/python3.8/site-packages/ray/exceptions.py)\n",
      "\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m 2021-01-29 08:46:24,130\tERROR function_manager.py:498 -- Failed to load actor class RolloutWorker.\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     logger.exception(\"Failed to load actor class %s.\", class_name)\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.base_env import BaseEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 1, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.base_env import BaseEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/base_env.py\", line 3, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.external_env import ExternalEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/external_env.py\", line 7, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.utils.annotations import PublicAPI\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/utils/__init__.py\", line 18, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.utils import merge_dicts, deep_update\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/__init__.py\", line 2, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.tune import run_experiments, run\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/tune.py\", line 10, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.ray_trial_executor import RayTrialExecutor\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 11, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.exceptions import RayTimeoutError\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m ImportError: cannot import name 'RayTimeoutError' from 'ray.exceptions' (/home/dschori/.local/lib/python3.8/site-packages/ray/exceptions.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m None\n"
     ]
    },
    {
     "ename": "RayTaskError(AssertionError)",
     "evalue": "\u001B[36mray::RolloutWorker.foreach_policy()\u001B[39m (pid=2023600, ip=192.168.178.60)\n  File \"python/ray/_raylet.pyx\", line 422, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 442, in ray._raylet.execute_task\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 310, in deserialize_objects\n    except DeserializationError:\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 248, in _deserialize_object\n    # Check if the object should be returned as raw bytes.\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 226, in _deserialize_msgpack_data\n    python_objects = []\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 216, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n    from ray.rllib.env.base_env import BaseEnv\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 11, in <module>\n    from ray.rllib.env.policy_client import PolicyClient\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/policy_client.py\", line 13, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/__init__.py\", line 2, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 73, in <module>\n    class RolloutWorker(ParallelIteratorWorker):\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in RolloutWorker\n    @ray.method(num_return_vals=2)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/actor.py\", line 45, in method\nAssertionError",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRayTaskError(AssertionError)\u001B[0m              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-7650ad861f1f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-13-8712695bee7b>\u001B[0m in \u001B[0;36mload\u001B[0;34m(checkpoint_path, config)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;34m:\u001B[0m\u001B[0mparam\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mPath\u001B[0m \u001B[0mpointing\u001B[0m \u001B[0mto\u001B[0m \u001B[0mthe\u001B[0m \u001B[0magent\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0msaved\u001B[0m \u001B[0mcheckpoint\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0monly\u001B[0m \u001B[0mused\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mRLlib\u001B[0m \u001B[0magents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \"\"\"\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPPOTrainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrestore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, env, logger_creator)\u001B[0m\n\u001B[1;32m    104\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_exec_impl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexecution_plan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mworkers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mafter_init\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 106\u001B[0;31m                 \u001B[0mafter_init\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0moverride\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTrainer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, env, logger_creator)\u001B[0m\n\u001B[1;32m    463\u001B[0m             \u001B[0mtimestr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoday\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrftime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%Y-%m-%d_%H-%M-%S\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m             logdir_prefix = \"{}_{}_{}\".format(self._name, self._env_id,\n\u001B[0;32m--> 465\u001B[0;31m                                               timestr)\n\u001B[0m\u001B[1;32m    466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    467\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mdefault_logger_creator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, logger_creator)\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0mFileNotFoundError\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mdirectory\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfound\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m         \"\"\"\n\u001B[0;32m---> 96\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Path does not exist\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36msetup\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    627\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"callbacks\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         \u001B[0mlog_level\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"log_level\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 629\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mlog_level\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"WARN\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"ERROR\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    630\u001B[0m             logger.info(\"Current log_level is {}. For more information, \"\n\u001B[1;32m    631\u001B[0m                         \u001B[0;34m\"set 'log_level': 'INFO' / 'DEBUG' or use the -v and \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001B[0m in \u001B[0;36m_init\u001B[0;34m(self, config, env_creator)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         \u001B[0mArguments\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m             \u001B[0moverrides\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0muse\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mto\u001B[0m \u001B[0moverride\u001B[0m \u001B[0many\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m                 \u001B[0moriginally\u001B[0m \u001B[0mpassed\u001B[0m \u001B[0mto\u001B[0m \u001B[0mbuild_trainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m         \"\"\"\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36m_make_workers\u001B[0;34m(self, env_creator, validate_env, policy_class, config, num_workers)\u001B[0m\n\u001B[1;32m    698\u001B[0m                 \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m             \u001B[0mpolicy\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mPolicy\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mto\u001B[0m \u001B[0muse\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcreating\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mpolicies\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 700\u001B[0;31m                 \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mworkers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    701\u001B[0m             \u001B[0mconfig\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mTrainer\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m             \u001B[0mnum_workers\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mNumber\u001B[0m \u001B[0mof\u001B[0m \u001B[0mremote\u001B[0m \u001B[0mrollout\u001B[0m \u001B[0mworkers\u001B[0m \u001B[0mto\u001B[0m \u001B[0mcreate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, logdir, _setup)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mremote_workers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"ActorHandle\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;34m\"\"\"Return a list of remote rollout workers.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_remote_workers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msync_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(object_refs, timeout)\u001B[0m\n\u001B[1;32m   1377\u001B[0m             \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprinter_thread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1378\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mworker\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logger_thread\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1379\u001B[0;31m             \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogger_thread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1380\u001B[0m         \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mthreads_stopped\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1381\u001B[0m         \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_session_index\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRayTaskError(AssertionError)\u001B[0m: \u001B[36mray::RolloutWorker.foreach_policy()\u001B[39m (pid=2023600, ip=192.168.178.60)\n  File \"python/ray/_raylet.pyx\", line 422, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 442, in ray._raylet.execute_task\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 310, in deserialize_objects\n    except DeserializationError:\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 248, in _deserialize_object\n    # Check if the object should be returned as raw bytes.\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 226, in _deserialize_msgpack_data\n    python_objects = []\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 216, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n    from ray.rllib.env.base_env import BaseEnv\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 11, in <module>\n    from ray.rllib.env.policy_client import PolicyClient\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/policy_client.py\", line 13, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/__init__.py\", line 2, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 73, in <module>\n    class RolloutWorker(ParallelIteratorWorker):\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in RolloutWorker\n    @ray.method(num_return_vals=2)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/actor.py\", line 45, in method\nAssertionError"
     ]
    }
   ],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', (array([3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,\n       3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99], dtype=float32), array([9.99      , 0.22813019]), array([-1.,  1.])))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mcheck_shape\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_obs_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m                     raise ValueError(\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py\u001B[0m in \u001B[0;36mcontains\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    127\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Promote list to array for contains check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlow\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhigh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-d77bb593b827>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mepisode_reward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mepisode_reward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-8712695bee7b>\u001B[0m in \u001B[0;36mtest\u001B[0;34m(agent, env)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0mobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m         \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mepisode_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36mcompute_action\u001B[0;34m(self, observation, state, prev_action, prev_reward, info, policy_id, full_fetch, explore)\u001B[0m\n\u001B[1;32m    816\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 818\u001B[0;31m         preprocessed = self.workers.local_worker().preprocessors[\n\u001B[0m\u001B[1;32m    819\u001B[0m             policy_id].transform(observation)\n\u001B[1;32m    820\u001B[0m         filtered_obs = self.workers.local_worker().filters[policy_id](\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0moverride\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPreprocessor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensorType\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mcheck_shape\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m     65\u001B[0m                         observation, self._obs_space)\n\u001B[1;32m     66\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m                 raise ValueError(\n\u001B[0m\u001B[1;32m     68\u001B[0m                     \u001B[0;34m\"Observation for a Box/MultiBinary/MultiDiscrete space \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m                     \"should be an np.array, not a Python list.\", observation)\n",
      "\u001B[0;31mValueError\u001B[0m: ('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', (array([3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,\n       3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99], dtype=float32), array([9.99      , 0.22813019]), array([-1.,  1.])))"
     ]
    }
   ],
   "source": [
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}