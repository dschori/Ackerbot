{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1611745285.854981, 31.755000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1611745285.861055, 0.020000]: Start Init ControllersConnection\n",
      "[WARN] [1611745285.862040, 0.020000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 8.0, (12,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7753428816795349\n",
      "7.99\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(20):\n",
    "    #obs, reward, done, _ = env.step(1)\n",
    "    obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98392397 0.79180807 0.77534288 0.80873847 1.02237475 1.80956209\n",
      " 1.86166608 2.06839633 2.9133904  2.31194305 7.99       7.99      ]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 12:01:29,727\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8266\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:13851',\n 'object_store_address': '/tmp/ray/session_2021-01-27_12-01-29_107005_3139395/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-01-27_12-01-29_107005_3139395/sockets/raylet',\n 'webui_url': '127.0.0.1:8266',\n 'session_dir': '/tmp/ray/session_2021-01-27_12-01-29_107005_3139395',\n 'metrics_export_port': 59556,\n 'node_id': 'dd7e1e5d6c0a02bd0911b0ca3c45e6bb18230633'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [256, ],\n",
    "        #\"fcnet_hiddens\": tune.grid_search([[64, 64, ], [128, 128, ], [256, 256, ]])\n",
    "    }\n",
    "    #\"model\": {\n",
    "    #    \"dim\": 40,\n",
    "    #    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [512, [5, 16], 1]]\n",
    "    #}\n",
    "    #\"model\": {\n",
    "    #    \"use_lstm\": True,\n",
    "    #    # Max seq len f\n",
    "    #    or training the LSTM, defaults to 20.\n",
    "    #    \"max_seq_len\": 20,\n",
    "    #    # Size of the LSTM cell.\n",
    "    #    \"lstm_cell_size\": 256\n",
    "    #}\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 2500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m 2021-01-27 12:01:34,884\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m 2021-01-27 12:01:34,884\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m [ERROR] [1611745298.389321, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m [WARN] [1611745298.392558, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m [WARN] [1611745298.393445, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m 2021-01-27 12:01:43,699\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m 2021-01-27 12:01:44,821\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3139643)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=3139639)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 41.02061855670103\n",
      "  episode_reward_max: -93.22261968069297\n",
      "  episode_reward_mean: -98.54206849810636\n",
      "  episode_reward_min: -101.95727175831794\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 97\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.448979377746582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004820636473596096\n",
      "        model: {}\n",
      "        policy_loss: -0.005949055776000023\n",
      "        total_loss: 4568.89501953125\n",
      "        vf_explained_var: 0.0038130630273371935\n",
      "        vf_loss: 4568.90087890625\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.14355670103093\n",
      "    ram_util_percent: 41.42783505154639\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10051366181053002\n",
      "    mean_env_wait_ms: 37.64578259846354\n",
      "    mean_inference_ms: 0.9613836207886807\n",
      "    mean_raw_obs_processing_ms: 28.445524801107926\n",
      "  time_since_restore: 271.3516685962677\n",
      "  time_this_iter_s: 271.3516685962677\n",
      "  time_total_s: 271.3516685962677\n",
      "  timers:\n",
      "    learn_throughput: 1651.073\n",
      "    learn_time_ms: 2422.667\n",
      "    load_throughput: 111667.206\n",
      "    load_time_ms: 35.821\n",
      "    sample_throughput: 14.884\n",
      "    sample_time_ms: 268742.345\n",
      "    update_time_ms: 1.281\n",
      "  timestamp: 1611745575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-10-23\n",
      "  done: false\n",
      "  episode_len_mean: 43.99\n",
      "  episode_reward_max: -93.30861338939559\n",
      "  episode_reward_mean: -98.68304404496901\n",
      "  episode_reward_min: -102.6055173997879\n",
      "  episodes_this_iter: 91\n",
      "  episodes_total: 188\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3989331722259521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01771976798772812\n",
      "        model: {}\n",
      "        policy_loss: -0.011264290660619736\n",
      "        total_loss: 3227.29248046875\n",
      "        vf_explained_var: -0.0006989683024585247\n",
      "        vf_loss: 3227.30224609375\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.81830985915492\n",
      "    ram_util_percent: 41.3681690140845\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09956009096131915\n",
      "    mean_env_wait_ms: 36.207836315042506\n",
      "    mean_inference_ms: 0.9528047390909987\n",
      "    mean_raw_obs_processing_ms: 27.38222087727143\n",
      "  time_since_restore: 520.0926895141602\n",
      "  time_this_iter_s: 248.74102091789246\n",
      "  time_total_s: 520.0926895141602\n",
      "  timers:\n",
      "    learn_throughput: 1803.053\n",
      "    learn_time_ms: 2218.46\n",
      "    load_throughput: 145042.543\n",
      "    load_time_ms: 27.578\n",
      "    sample_throughput: 15.524\n",
      "    sample_time_ms: 257668.432\n",
      "    update_time_ms: 1.242\n",
      "  timestamp: 1611745823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-14-16\n",
      "  done: false\n",
      "  episode_len_mean: 50.28\n",
      "  episode_reward_max: -93.67355744817786\n",
      "  episode_reward_mean: -98.4805909178005\n",
      "  episode_reward_min: -101.77112794554233\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 266\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3510221242904663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01776990108191967\n",
      "        model: {}\n",
      "        policy_loss: -0.012945766560733318\n",
      "        total_loss: 2028.1412353515625\n",
      "        vf_explained_var: -0.0024179681204259396\n",
      "        vf_loss: 2028.1524658203125\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.78765060240964\n",
      "    ram_util_percent: 41.46777108433735\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09942019424064405\n",
      "    mean_env_wait_ms: 35.57865119618089\n",
      "    mean_inference_ms: 0.9483847623796222\n",
      "    mean_raw_obs_processing_ms: 25.98672824206484\n",
      "  time_since_restore: 752.8784022331238\n",
      "  time_this_iter_s: 232.78571271896362\n",
      "  time_total_s: 752.8784022331238\n",
      "  timers:\n",
      "    learn_throughput: 1758.744\n",
      "    learn_time_ms: 2274.351\n",
      "    load_throughput: 179981.505\n",
      "    load_time_ms: 22.225\n",
      "    sample_throughput: 16.095\n",
      "    sample_time_ms: 248523.747\n",
      "    update_time_ms: 1.364\n",
      "  timestamp: 1611746056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 57.25\n",
      "  episode_reward_max: -92.04000911341653\n",
      "  episode_reward_mean: -98.24482812137941\n",
      "  episode_reward_min: -102.72279845615441\n",
      "  episodes_this_iter: 67\n",
      "  episodes_total: 333\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2999777793884277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012839493341743946\n",
      "        model: {}\n",
      "        policy_loss: -0.012850945815443993\n",
      "        total_loss: 1321.3460693359375\n",
      "        vf_explained_var: -0.0028742821887135506\n",
      "        vf_loss: 1321.3577880859375\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.93037974683544\n",
      "    ram_util_percent: 41.425632911392405\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09928489930994826\n",
      "    mean_env_wait_ms: 35.268622455016256\n",
      "    mean_inference_ms: 0.9469281351679493\n",
      "    mean_raw_obs_processing_ms: 24.548185443183584\n",
      "  time_since_restore: 974.0451519489288\n",
      "  time_this_iter_s: 221.16674971580505\n",
      "  time_total_s: 974.0451519489288\n",
      "  timers:\n",
      "    learn_throughput: 1749.627\n",
      "    learn_time_ms: 2286.201\n",
      "    load_throughput: 204435.027\n",
      "    load_time_ms: 19.566\n",
      "    sample_throughput: 16.593\n",
      "    sample_time_ms: 241067.775\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1611746277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 74.28\n",
      "  episode_reward_max: -86.461546876921\n",
      "  episode_reward_mean: -97.3626908743876\n",
      "  episode_reward_min: -102.72279845615441\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 376\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1799415349960327\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01860361546278\n",
      "        model: {}\n",
      "        policy_loss: -0.021443478763103485\n",
      "        total_loss: 949.5281372070312\n",
      "        vf_explained_var: 0.02242984063923359\n",
      "        vf_loss: 949.5476684570312\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9659420289855\n",
      "    ram_util_percent: 41.47717391304348\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09917699457590246\n",
      "    mean_env_wait_ms: 35.114083920923115\n",
      "    mean_inference_ms: 0.9461726790490279\n",
      "    mean_raw_obs_processing_ms: 23.02078262968585\n",
      "  time_since_restore: 1167.5421710014343\n",
      "  time_this_iter_s: 193.4970190525055\n",
      "  time_total_s: 1167.5421710014343\n",
      "  timers:\n",
      "    learn_throughput: 1793.828\n",
      "    learn_time_ms: 2229.868\n",
      "    load_throughput: 209617.728\n",
      "    load_time_ms: 19.082\n",
      "    sample_throughput: 17.306\n",
      "    sample_time_ms: 231127.053\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1611746471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-24-25\n",
      "  done: false\n",
      "  episode_len_mean: 86.83\n",
      "  episode_reward_max: -86.04476789105763\n",
      "  episode_reward_mean: -96.59474989823333\n",
      "  episode_reward_min: -101.90909282684326\n",
      "  episodes_this_iter: 44\n",
      "  episodes_total: 420\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1339644193649292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018697673454880714\n",
      "        model: {}\n",
      "        policy_loss: -0.010494321584701538\n",
      "        total_loss: 852.8063354492188\n",
      "        vf_explained_var: 0.06369651854038239\n",
      "        vf_loss: 852.8150634765625\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.00253623188406\n",
      "    ram_util_percent: 41.43188405797102\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09909415471578704\n",
      "    mean_env_wait_ms: 34.978220235563136\n",
      "    mean_inference_ms: 0.9457176647333846\n",
      "    mean_raw_obs_processing_ms: 21.33391184310706\n",
      "  time_since_restore: 1361.2923214435577\n",
      "  time_this_iter_s: 193.7501504421234\n",
      "  time_total_s: 1361.2923214435577\n",
      "  timers:\n",
      "    learn_throughput: 1780.765\n",
      "    learn_time_ms: 2246.226\n",
      "    load_throughput: 228734.471\n",
      "    load_time_ms: 17.488\n",
      "    sample_throughput: 17.818\n",
      "    sample_time_ms: 224485.924\n",
      "    update_time_ms: 1.348\n",
      "  timestamp: 1611746665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-27-15\n",
      "  done: false\n",
      "  episode_len_mean: 104.94\n",
      "  episode_reward_max: -85.90976392720904\n",
      "  episode_reward_mean: -95.89001653589219\n",
      "  episode_reward_min: -101.7827640760541\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 443\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9954847097396851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014270384795963764\n",
      "        model: {}\n",
      "        policy_loss: -0.01635049469769001\n",
      "        total_loss: 679.797119140625\n",
      "        vf_explained_var: 0.08457302302122116\n",
      "        vf_loss: 679.8120727539062\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.16255144032922\n",
      "    ram_util_percent: 41.420164609053494\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09905404767963738\n",
      "    mean_env_wait_ms: 34.903992391335095\n",
      "    mean_inference_ms: 0.9454325054637568\n",
      "    mean_raw_obs_processing_ms: 20.241950049166572\n",
      "  time_since_restore: 1531.648645401001\n",
      "  time_this_iter_s: 170.35632395744324\n",
      "  time_total_s: 1531.648645401001\n",
      "  timers:\n",
      "    learn_throughput: 1809.313\n",
      "    learn_time_ms: 2210.784\n",
      "    load_throughput: 243279.016\n",
      "    load_time_ms: 16.442\n",
      "    sample_throughput: 18.48\n",
      "    sample_time_ms: 216451.654\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1611746835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 130.67\n",
      "  episode_reward_max: -82.77635929969199\n",
      "  episode_reward_mean: -94.81086927417716\n",
      "  episode_reward_min: -101.7827640760541\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 468\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9642605185508728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017391132190823555\n",
      "        model: {}\n",
      "        policy_loss: -0.012594268657267094\n",
      "        total_loss: 710.3380737304688\n",
      "        vf_explained_var: 0.1272985339164734\n",
      "        vf_loss: 710.3488159179688\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8890243902439\n",
      "    ram_util_percent: 41.43943089430895\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09898868992900114\n",
      "    mean_env_wait_ms: 34.82672716705152\n",
      "    mean_inference_ms: 0.9452188405525451\n",
      "    mean_raw_obs_processing_ms: 19.039323524591993\n",
      "  time_since_restore: 1703.7394154071808\n",
      "  time_this_iter_s: 172.0907700061798\n",
      "  time_total_s: 1703.7394154071808\n",
      "  timers:\n",
      "    learn_throughput: 1828.758\n",
      "    learn_time_ms: 2187.277\n",
      "    load_throughput: 253717.315\n",
      "    load_time_ms: 15.766\n",
      "    sample_throughput: 18.99\n",
      "    sample_time_ms: 210638.495\n",
      "    update_time_ms: 1.287\n",
      "  timestamp: 1611747007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-32-58\n",
      "  done: false\n",
      "  episode_len_mean: 143.1\n",
      "  episode_reward_max: -82.77635929969199\n",
      "  episode_reward_mean: -93.9876962547453\n",
      "  episode_reward_min: -101.09301269543171\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 491\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8883280754089355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02118608169257641\n",
      "        model: {}\n",
      "        policy_loss: -0.015701811760663986\n",
      "        total_loss: 654.9038696289062\n",
      "        vf_explained_var: 0.1441185623407364\n",
      "        vf_loss: 654.9173583984375\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.03073770491804\n",
      "    ram_util_percent: 41.40491803278687\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891389891820758\n",
      "    mean_env_wait_ms: 34.768366055422746\n",
      "    mean_inference_ms: 0.9450332229577795\n",
      "    mean_raw_obs_processing_ms: 17.89986542990204\n",
      "  time_since_restore: 1874.720175743103\n",
      "  time_this_iter_s: 170.98076033592224\n",
      "  time_total_s: 1874.720175743103\n",
      "  timers:\n",
      "    learn_throughput: 1814.374\n",
      "    learn_time_ms: 2204.617\n",
      "    load_throughput: 255099.533\n",
      "    load_time_ms: 15.68\n",
      "    sample_throughput: 19.422\n",
      "    sample_time_ms: 205954.572\n",
      "    update_time_ms: 1.335\n",
      "  timestamp: 1611747178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-35-43\n",
      "  done: false\n",
      "  episode_len_mean: 168.56\n",
      "  episode_reward_max: -82.77635929969199\n",
      "  episode_reward_mean: -93.22126099819374\n",
      "  episode_reward_min: -101.09301269543171\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 509\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9010077118873596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010658673010766506\n",
      "        model: {}\n",
      "        policy_loss: -0.013501723296940327\n",
      "        total_loss: 608.8966674804688\n",
      "        vf_explained_var: 0.120561383664608\n",
      "        vf_loss: 608.9085693359375\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.93404255319149\n",
      "    ram_util_percent: 41.44765957446809\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09886173225012578\n",
      "    mean_env_wait_ms: 34.72454222963117\n",
      "    mean_inference_ms: 0.9448608294911783\n",
      "    mean_raw_obs_processing_ms: 16.914350993678617\n",
      "  time_since_restore: 2039.6620421409607\n",
      "  time_this_iter_s: 164.94186639785767\n",
      "  time_total_s: 2039.6620421409607\n",
      "  timers:\n",
      "    learn_throughput: 1803.196\n",
      "    learn_time_ms: 2218.284\n",
      "    load_throughput: 261419.378\n",
      "    load_time_ms: 15.301\n",
      "    sample_throughput: 19.841\n",
      "    sample_time_ms: 201601.741\n",
      "    update_time_ms: 1.352\n",
      "  timestamp: 1611747343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 180.37\n",
      "  episode_reward_max: 116.15071898696984\n",
      "  episode_reward_mean: -90.58034126560207\n",
      "  episode_reward_min: -101.09301269543171\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 529\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8959893584251404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01411921251565218\n",
      "        model: {}\n",
      "        policy_loss: -0.01608162187039852\n",
      "        total_loss: 912.7093505859375\n",
      "        vf_explained_var: 0.13661564886569977\n",
      "        vf_loss: 912.7233276367188\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.01008403361345\n",
      "    ram_util_percent: 41.41806722689074\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0988077808451042\n",
      "    mean_env_wait_ms: 34.67532961055009\n",
      "    mean_inference_ms: 0.9447124779832211\n",
      "    mean_raw_obs_processing_ms: 15.830057256560176\n",
      "  time_since_restore: 2206.2249932289124\n",
      "  time_this_iter_s: 166.56295108795166\n",
      "  time_total_s: 2206.2249932289124\n",
      "  timers:\n",
      "    learn_throughput: 1810.826\n",
      "    learn_time_ms: 2208.937\n",
      "    load_throughput: 313090.707\n",
      "    load_time_ms: 12.776\n",
      "    sample_throughput: 20.927\n",
      "    sample_time_ms: 191136.528\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1611747510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-41-17\n",
      "  done: false\n",
      "  episode_len_mean: 189.11\n",
      "  episode_reward_max: 117.77433618543697\n",
      "  episode_reward_mean: -81.94175425805724\n",
      "  episode_reward_min: -101.09301269543171\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 549\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8617241978645325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011931991204619408\n",
      "        model: {}\n",
      "        policy_loss: -0.011643978767096996\n",
      "        total_loss: 1068.5107421875\n",
      "        vf_explained_var: 0.15493524074554443\n",
      "        vf_loss: 1068.5206298828125\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.0155462184874\n",
      "    ram_util_percent: 41.400840336134436\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09875599146312357\n",
      "    mean_env_wait_ms: 34.63295247278502\n",
      "    mean_inference_ms: 0.9446043938700606\n",
      "    mean_raw_obs_processing_ms: 14.906200121951516\n",
      "  time_since_restore: 2372.7985124588013\n",
      "  time_this_iter_s: 166.57351922988892\n",
      "  time_total_s: 2372.7985124588013\n",
      "  timers:\n",
      "    learn_throughput: 1810.031\n",
      "    learn_time_ms: 2209.906\n",
      "    load_throughput: 312317.865\n",
      "    load_time_ms: 12.807\n",
      "    sample_throughput: 21.868\n",
      "    sample_time_ms: 182917.509\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1611747677\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-44-10\n",
      "  done: false\n",
      "  episode_len_mean: 190.04\n",
      "  episode_reward_max: 117.77433618543697\n",
      "  episode_reward_mean: -79.83628462881586\n",
      "  episode_reward_min: -101.62742706751823\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 571\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8864009380340576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005724149290472269\n",
      "        model: {}\n",
      "        policy_loss: -0.0067674689926207066\n",
      "        total_loss: 815.7389526367188\n",
      "        vf_explained_var: 0.19431902468204498\n",
      "        vf_loss: 815.7449340820312\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.55725806451612\n",
      "    ram_util_percent: 41.483467741935485\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09875797736445949\n",
      "    mean_env_wait_ms: 34.61306535500964\n",
      "    mean_inference_ms: 0.9449390488380258\n",
      "    mean_raw_obs_processing_ms: 14.019597483140794\n",
      "  time_since_restore: 2546.4301886558533\n",
      "  time_this_iter_s: 173.631676197052\n",
      "  time_total_s: 2546.4301886558533\n",
      "  timers:\n",
      "    learn_throughput: 1823.966\n",
      "    learn_time_ms: 2193.024\n",
      "    load_throughput: 317449.688\n",
      "    load_time_ms: 12.6\n",
      "    sample_throughput: 22.596\n",
      "    sample_time_ms: 177020.447\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1611747850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 188.06\n",
      "  episode_reward_max: 117.77433618543697\n",
      "  episode_reward_mean: -80.10637977279515\n",
      "  episode_reward_min: -101.62742706751823\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 595\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8868808746337891\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006943258922547102\n",
      "        model: {}\n",
      "        policy_loss: -0.0072816284373402596\n",
      "        total_loss: 646.9322509765625\n",
      "        vf_explained_var: 0.2554526925086975\n",
      "        vf_loss: 646.9385375976562\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.29178571428571\n",
      "    ram_util_percent: 41.48392857142857\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0989239602622365\n",
      "    mean_env_wait_ms: 34.69597752669638\n",
      "    mean_inference_ms: 0.9475312932422528\n",
      "    mean_raw_obs_processing_ms: 13.230917935489854\n",
      "  time_since_restore: 2742.525941848755\n",
      "  time_this_iter_s: 196.0957531929016\n",
      "  time_total_s: 2742.525941848755\n",
      "  timers:\n",
      "    learn_throughput: 1803.785\n",
      "    learn_time_ms: 2217.559\n",
      "    load_throughput: 319774.939\n",
      "    load_time_ms: 12.509\n",
      "    sample_throughput: 22.924\n",
      "    sample_time_ms: 174487.085\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1611748046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-52-00\n",
      "  done: false\n",
      "  episode_len_mean: 180.64\n",
      "  episode_reward_max: 117.77433618543697\n",
      "  episode_reward_mean: -79.95743928149653\n",
      "  episode_reward_min: -101.62742706751823\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 618\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8865146040916443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00581198837608099\n",
      "        model: {}\n",
      "        policy_loss: -0.010756556876003742\n",
      "        total_loss: 574.2926635742188\n",
      "        vf_explained_var: 0.29077044129371643\n",
      "        vf_loss: 574.3025512695312\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.46974358974359\n",
      "    ram_util_percent: 41.79\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09921021042202964\n",
      "    mean_env_wait_ms: 35.166501664417815\n",
      "    mean_inference_ms: 0.9520984600182948\n",
      "    mean_raw_obs_processing_ms: 12.662198430334115\n",
      "  time_since_restore: 3015.7706248760223\n",
      "  time_this_iter_s: 273.24468302726746\n",
      "  time_total_s: 3015.7706248760223\n",
      "  timers:\n",
      "    learn_throughput: 1757.061\n",
      "    learn_time_ms: 2276.529\n",
      "    load_throughput: 338774.877\n",
      "    load_time_ms: 11.807\n",
      "    sample_throughput: 21.93\n",
      "    sample_time_ms: 182400.473\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1611748320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: '03325_00000'\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_03325_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_12-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 184.71\n",
      "  episode_reward_max: 118.14299664158997\n",
      "  episode_reward_mean: -77.90714680503605\n",
      "  episode_reward_min: -101.62742706751823\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 637\n",
      "  experiment_id: ae70aee8670745c387b63ebbd6aacd9d\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.760062038898468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012057650834321976\n",
      "        model: {}\n",
      "        policy_loss: -0.015514175407588482\n",
      "        total_loss: 1082.60986328125\n",
      "        vf_explained_var: 0.1509733945131302\n",
      "        vf_loss: 1082.6236572265625\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.31314878892734\n",
      "    ram_util_percent: 41.74705882352941\n",
      "  pid: 3139643\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09955788464002563\n",
      "    mean_env_wait_ms: 35.64178627594171\n",
      "    mean_inference_ms: 0.9573975174225609\n",
      "    mean_raw_obs_processing_ms: 12.264588050089035\n",
      "  time_since_restore: 3218.6738798618317\n",
      "  time_this_iter_s: 202.90325498580933\n",
      "  time_total_s: 3218.6738798618317\n",
      "  timers:\n",
      "    learn_throughput: 1762.473\n",
      "    learn_time_ms: 2269.539\n",
      "    load_throughput: 332451.853\n",
      "    load_time_ms: 12.032\n",
      "    sample_throughput: 21.819\n",
      "    sample_time_ms: 183323.938\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1611748523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: '03325_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         271.352</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-98.5421</td><td style=\"text-align: right;\">            -93.2226</td><td style=\"text-align: right;\">            -101.957</td><td style=\"text-align: right;\">           41.0206</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         520.093</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -98.683</td><td style=\"text-align: right;\">            -93.3086</td><td style=\"text-align: right;\">            -102.606</td><td style=\"text-align: right;\">             43.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         752.878</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-98.4806</td><td style=\"text-align: right;\">            -93.6736</td><td style=\"text-align: right;\">            -101.771</td><td style=\"text-align: right;\">             50.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         974.045</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-98.2448</td><td style=\"text-align: right;\">              -92.04</td><td style=\"text-align: right;\">            -102.723</td><td style=\"text-align: right;\">             57.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1167.54</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-97.3627</td><td style=\"text-align: right;\">            -86.4615</td><td style=\"text-align: right;\">            -102.723</td><td style=\"text-align: right;\">             74.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         1361.29</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-96.5947</td><td style=\"text-align: right;\">            -86.0448</td><td style=\"text-align: right;\">            -101.909</td><td style=\"text-align: right;\">             86.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         1531.65</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  -95.89</td><td style=\"text-align: right;\">            -85.9098</td><td style=\"text-align: right;\">            -101.783</td><td style=\"text-align: right;\">            104.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1703.74</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-94.8109</td><td style=\"text-align: right;\">            -82.7764</td><td style=\"text-align: right;\">            -101.783</td><td style=\"text-align: right;\">            130.67</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1874.72</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-93.9877</td><td style=\"text-align: right;\">            -82.7764</td><td style=\"text-align: right;\">            -101.093</td><td style=\"text-align: right;\">             143.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2039.66</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-93.2213</td><td style=\"text-align: right;\">            -82.7764</td><td style=\"text-align: right;\">            -101.093</td><td style=\"text-align: right;\">            168.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         2206.22</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-90.5803</td><td style=\"text-align: right;\">             116.151</td><td style=\"text-align: right;\">            -101.093</td><td style=\"text-align: right;\">            180.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          2372.8</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-81.9418</td><td style=\"text-align: right;\">             117.774</td><td style=\"text-align: right;\">            -101.093</td><td style=\"text-align: right;\">            189.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         2546.43</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-79.8363</td><td style=\"text-align: right;\">             117.774</td><td style=\"text-align: right;\">            -101.627</td><td style=\"text-align: right;\">            190.04</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         2742.53</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-80.1064</td><td style=\"text-align: right;\">             117.774</td><td style=\"text-align: right;\">            -101.627</td><td style=\"text-align: right;\">            188.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         3015.77</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-79.9574</td><td style=\"text-align: right;\">             117.774</td><td style=\"text-align: right;\">            -101.627</td><td style=\"text-align: right;\">            180.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/12.65 GiB heap, 0.0/4.35 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_12-01-31<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_03325_00000</td><td>RUNNING </td><td>192.168.178.60:3139643</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         3218.67</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-77.9071</td><td style=\"text-align: right;\">             118.143</td><td style=\"text-align: right;\">            -101.627</td><td style=\"text-align: right;\">            184.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/PPO_2021-01-26_21-34-32/PPO_ScoutingDiscreteTask_e5473_00000_0_2021-01-26_21-34-32/checkpoint_100/checkpoint-100')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)\n",
    "\n",
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}