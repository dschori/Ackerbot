{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from gym.spaces import Discrete, Tuple\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.impala import ImpalaTrainer\n",
    "from ray.rllib.models import ModelCatalog, ModelV2\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.utils import get_filter_config\n",
    "from ray.rllib.utils import override\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1612470463.964583, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1612470463.970717, 0.000000]: Start Init ControllersConnection\n",
      "[WARN] [1612470463.972168, 0.000000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (84, 84, 4), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5882353186607361\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f351054d460>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs.max())\n",
    "plt.imshow(obs[:, :, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fcea478c640>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = None\n",
    "for _ in range(1):\n",
    "    obs, _, _, _ = env.step(action=1)\n",
    "plt.imshow(obs[:, :, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fbe2c160910>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = None\n",
    "for _ in range(1):\n",
    "    obs, _, _, _ = env.step(action=1)\n",
    "plt.imshow(obs[:, :, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5707918087715123 1.1475518445038917 0.42323996426762056\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (18,) and (64, 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-6567a5411d3f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mthetas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinspace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2.44\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2.44\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m18\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mthetas\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinspace\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m3.14\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3.14\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m18\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpolar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthetas\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001B[0m in \u001B[0;36mpolar\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   2204\u001B[0m                                  'that does not have a polar projection.')\n\u001B[1;32m   2205\u001B[0m     \u001B[0max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgca\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpolar\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2206\u001B[0;31m     \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2207\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mret\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2208\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36mplot\u001B[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1663\u001B[0m         \"\"\"\n\u001B[1;32m   1664\u001B[0m         \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormalize_kwargs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmlines\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLine2D\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_alias_map\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1665\u001B[0;31m         \u001B[0mlines\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_lines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1666\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1667\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_line\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m                 \u001B[0mthis\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m                 \u001B[0margs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 225\u001B[0;31m             \u001B[0;32myield\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_plot_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    226\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_next_color\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36m_plot_args\u001B[0;34m(self, tup, kwargs)\u001B[0m\n\u001B[1;32m    389\u001B[0m             \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindex_of\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtup\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 391\u001B[0;31m         \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_xy_from_xy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    392\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    393\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcommand\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'plot'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001B[0m in \u001B[0;36m_xy_from_xy\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m    267\u001B[0m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_1d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 269\u001B[0;31m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001B[0m\u001B[1;32m    270\u001B[0m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001B[1;32m    271\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (18,) and (64, 3)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "plt.figure()\n",
    "thetas = np.linspace(-2.44, 2.44, 18)\n",
    "thetas = np.linspace(-3.14, 3.14, 18)\n",
    "plt.polar(thetas, obs[0])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.] [0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-e2419b8cd6f9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m \u001B[0mpix_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpix_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;31m#pix_x, pix_y = target_to_pixels(target_x=obs[1][0], target_y=obs[1][1], angle=obs[1][2])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcircle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mpix_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpix_y\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mradius\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m55\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m55\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthickness\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-e2419b8cd6f9>\u001B[0m in \u001B[0;36mnew\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpol2cart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;36m2.\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeg2rad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrad2deg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrad2deg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m0.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m     \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtarget_to_pixels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-e2419b8cd6f9>\u001B[0m in \u001B[0;36mtarget_to_pixels\u001B[0;34m(target_x, target_y, angle)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mtarget_to_pixels\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mangle\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mtarget_x\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_x\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m3.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m     \u001B[0mtarget_y\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_y\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m3.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;31m#print(target_x, target_y)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)\n",
    "\n",
    "def target_to_pixels(target_x, target_y, angle):\n",
    "\n",
    "    target_x = max(min(target_x, 3.), -3.)\n",
    "    target_y = max(min(target_y, 3.), -3.)\n",
    "    #print(target_x, target_y)\n",
    "    #target_x *= math.cos(angle)\n",
    "    #target_y *= math.sin(angle)\n",
    "    #print(target_x, target_y)\n",
    "    pix_x = int((30. / 3.) * target_x)\n",
    "    pix_y = int((30. / 3.) * target_y)\n",
    "    return pix_x + 30, pix_y + 30\n",
    "\n",
    "def new():\n",
    "    (x, y) = pol2cart(obs[1][4]/2., np.deg2rad(np.rad2deg(obs[1][2])-np.rad2deg(obs[1][3])-0.))\n",
    "    print(x, y)\n",
    "    x, y = target_to_pixels(x, y, 0)\n",
    "    print(x, y)\n",
    "    return x, y\n",
    "\n",
    "pix_x, pix_y = new()\n",
    "#pix_x, pix_y = target_to_pixels(target_x=obs[1][0], target_y=obs[1][1], angle=obs[1][2])\n",
    "image = cv2.circle(obs[0], (pix_x, pix_y), radius=2, color=(55, 55, 0), thickness=2)\n",
    "plt.imshow(image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7ff00ddc2b80>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env.reset()\n",
    "for _ in range(10):\n",
    "    obs, reward, done, _ = env.step(1)\n",
    "    #obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs[0].min())\n",
    "print(obs[0].max())\n",
    "\n",
    "plt.imshow(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#print(obs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "class CNNPlusFCConcatModel(TFModelV2):\n",
    "    \"\"\"TFModelV2 concat'ing CNN outputs to flat input(s), followed by FC(s).\n",
    "\n",
    "    Note: This model should be used for complex (Dict or Tuple) observation\n",
    "    spaces that have one or more image components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        # TODO: (sven) Support Dicts as well.\n",
    "        assert isinstance(obs_space.original_space, (Tuple)), \\\n",
    "            \"`obs_space.original_space` must be Tuple!\"\n",
    "\n",
    "        super().__init__(obs_space, action_space, num_outputs, model_config,\n",
    "                         name)\n",
    "\n",
    "        # Build the CNN(s) given obs_space's image components.\n",
    "        self.cnns = {}\n",
    "        concat_size = 0\n",
    "        for i, component in enumerate(obs_space.original_space):\n",
    "            # Image space.\n",
    "            if len(component.shape) == 3:\n",
    "                config = {\n",
    "                    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [5, 16], 1]],\n",
    "                    \"conv_activation\": model_config.get(\"conv_activation\"),\n",
    "                }\n",
    "                cnn = ModelCatalog.get_model_v2(\n",
    "                    component,\n",
    "                    action_space,\n",
    "                    num_outputs=None,\n",
    "                    model_config=config,\n",
    "                    framework=\"tf\",\n",
    "                    name=\"cnn_{}\".format(i))\n",
    "                concat_size += cnn.num_outputs\n",
    "                self.cnns[i] = cnn\n",
    "                self.register_variables(cnn.base_model.variables)\n",
    "            # Discrete inputs -> One-hot encode.\n",
    "            elif isinstance(component, Discrete):\n",
    "                concat_size += component.n\n",
    "            # TODO: (sven) Multidiscrete (see e.g. our auto-LSTM wrappers).\n",
    "            # Everything else (1D Box).\n",
    "            else:\n",
    "                assert len(component.shape) == 1, \\\n",
    "                    \"Only input Box 1D or 3D spaces allowed!\"\n",
    "                concat_size += component.shape[-1]\n",
    "\n",
    "        self.logits_and_value_model = None\n",
    "        self._value_out = None\n",
    "        if num_outputs:\n",
    "            # Action-distribution head.\n",
    "            concat_layer = tf.keras.layers.Input((concat_size, ))\n",
    "            logits_layer = tf.keras.layers.Dense(\n",
    "                num_outputs,\n",
    "                activation=tf.keras.activations.linear,\n",
    "                name=\"logits\")(concat_layer)\n",
    "\n",
    "            # Create the value branch model.\n",
    "            value_layer = tf.keras.layers.Dense(\n",
    "                1,\n",
    "                name=\"value_out\",\n",
    "                activation=None,\n",
    "                kernel_initializer=normc_initializer(0.01))(concat_layer)\n",
    "            self.logits_and_value_model = tf.keras.models.Model(\n",
    "                concat_layer, [logits_layer, value_layer])\n",
    "        else:\n",
    "            self.num_outputs = concat_size\n",
    "        self.register_variables(self.logits_and_value_model.variables)\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Push image observations through our CNNs.\n",
    "        outs = []\n",
    "        for i, component in enumerate(input_dict[\"obs\"]):\n",
    "            if i in self.cnns:\n",
    "                cnn_out, _ = self.cnns[i]({\"obs\": component})\n",
    "                outs.append(cnn_out)\n",
    "            else:\n",
    "                outs.append(component)\n",
    "        # Concat all outputs and the non-image inputs.\n",
    "        out = tf.concat(outs, axis=1)\n",
    "        if not self.logits_and_value_model:\n",
    "            return out, []\n",
    "\n",
    "        # Value branch.\n",
    "        logits, values = self.logits_and_value_model(out)\n",
    "        self._value_out = tf.reshape(values, [-1])\n",
    "        return logits, []\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "\n",
    "ModelCatalog.register_custom_model(\"CNNPlusFCConcatModel\", CNNPlusFCConcatModel)\n",
    "\n",
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    #\"model\": {\n",
    "        #\"vf_share_layers\": True\n",
    "        #\"custom_model\": \"CNNPlusFCConcatModel\",\n",
    "        #\"conv_filters\": [[8, [3, 3], 2], [16, [3, 3], 2], [32, [3, 3], 2], [64, [8, 8], 2], [128, [8, 8], 1]],\n",
    "        #\"fcnet_hiddens\": [256, 256, ],\n",
    "        #\"use_lstm\": True,\n",
    "        #\"lstm_cell_size\": 128,\n",
    "        #\"fcnet_hiddens\": tune.grid_search([[64, 64, ], [128, 128, ], [256, 256, ]])\n",
    "    #}\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 4500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 21:27:47,323\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-02-04_21-27-46_836850_1843983/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-02-04_21-27-46_836850_1843983/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-02-04_21-27-46_836850_1843983',\n 'metrics_export_port': 63803,\n 'node_id': '387d9e9fd4ebb5fed2176904221fe1fae88ab21f'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 7.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m 2021-02-04 21:27:51,973\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m 2021-02-04 21:27:51,974\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m [ERROR] [1612470475.271243, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m [WARN] [1612470475.274505, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m [WARN] [1612470475.275737, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m 2021-02-04 21:28:02,995\tINFO trainable.py:99 -- Trainable.setup took 11.022 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m 2021-02-04 21:28:02,995\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m 2021-02-04 21:28:04,192\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2021-02-06 01:27:31,391\tINFO tune.py:448 -- Total run time: 100782.49 seconds (100781.68 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1844151)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=1844148)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_21-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 121.12121212121212\n",
      "  episode_reward_max: -89.57480473520062\n",
      "  episode_reward_mean: -101.989527058615\n",
      "  episode_reward_min: -141.7091238836413\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 33\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0772981643676758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021499352529644966\n",
      "        model: {}\n",
      "        policy_loss: -0.061058904975652695\n",
      "        total_loss: 2028.3983154296875\n",
      "        vf_explained_var: 0.036660194396972656\n",
      "        vf_loss: 2028.455322265625\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.54609665427509\n",
      "    ram_util_percent: 29.25303593556382\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07315976296148131\n",
      "    mean_env_wait_ms: 126.55109311604136\n",
      "    mean_inference_ms: 1.5762707377278606\n",
      "    mean_raw_obs_processing_ms: 10.06695372675157\n",
      "  time_since_restore: 565.4976713657379\n",
      "  time_this_iter_s: 565.4976713657379\n",
      "  time_total_s: 565.4976713657379\n",
      "  timers:\n",
      "    learn_throughput: 353.846\n",
      "    learn_time_ms: 11304.342\n",
      "    load_throughput: 7627.235\n",
      "    load_time_ms: 524.436\n",
      "    sample_throughput: 7.227\n",
      "    sample_time_ms: 553483.326\n",
      "    update_time_ms: 3.419\n",
      "  timestamp: 1612471048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_21-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 107.79729729729729\n",
      "  episode_reward_max: -87.58737052859588\n",
      "  episode_reward_mean: -99.35933162751594\n",
      "  episode_reward_min: -141.7091238836413\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 74\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0490062236785889\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02172921597957611\n",
      "        model: {}\n",
      "        policy_loss: -0.06566911935806274\n",
      "        total_loss: 1687.489990234375\n",
      "        vf_explained_var: 0.13553734123706818\n",
      "        vf_loss: 1687.5494384765625\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.13500627352572\n",
      "    ram_util_percent: 31.674654956085316\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0728233355973103\n",
      "    mean_env_wait_ms: 125.48327291395604\n",
      "    mean_inference_ms: 1.5770035414988979\n",
      "    mean_raw_obs_processing_ms: 10.708631806053198\n",
      "  time_since_restore: 1124.092319726944\n",
      "  time_this_iter_s: 558.594648361206\n",
      "  time_total_s: 1124.092319726944\n",
      "  timers:\n",
      "    learn_throughput: 359.95\n",
      "    learn_time_ms: 11112.646\n",
      "    load_throughput: 8515.424\n",
      "    load_time_ms: 469.736\n",
      "    sample_throughput: 7.269\n",
      "    sample_time_ms: 550310.47\n",
      "    update_time_ms: 3.475\n",
      "  timestamp: 1612471607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_21-56-01\n",
      "  done: false\n",
      "  episode_len_mean: 111.22\n",
      "  episode_reward_max: -86.21658999302315\n",
      "  episode_reward_mean: -100.03769711100011\n",
      "  episode_reward_min: -149.80068782632588\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 106\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.023102879524231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020299741998314857\n",
      "        model: {}\n",
      "        policy_loss: -0.08530482649803162\n",
      "        total_loss: 979.4820556640625\n",
      "        vf_explained_var: 0.23558303713798523\n",
      "        vf_loss: 979.5581665039062\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.88331226295827\n",
      "    ram_util_percent: 31.735398230088492\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07274664032719924\n",
      "    mean_env_wait_ms: 125.10112812055736\n",
      "    mean_inference_ms: 1.5763807143892745\n",
      "    mean_raw_obs_processing_ms: 10.755500146516884\n",
      "  time_since_restore: 1678.2304520606995\n",
      "  time_this_iter_s: 554.1381323337555\n",
      "  time_total_s: 1678.2304520606995\n",
      "  timers:\n",
      "    learn_throughput: 361.886\n",
      "    learn_time_ms: 11053.2\n",
      "    load_throughput: 9004.367\n",
      "    load_time_ms: 444.229\n",
      "    sample_throughput: 7.302\n",
      "    sample_time_ms: 547770.128\n",
      "    update_time_ms: 3.425\n",
      "  timestamp: 1612472161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-05-15\n",
      "  done: false\n",
      "  episode_len_mean: 121.7\n",
      "  episode_reward_max: 118.37474086508318\n",
      "  episode_reward_mean: -98.28887163585911\n",
      "  episode_reward_min: -149.80068782632588\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 132\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0235015153884888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016919491812586784\n",
      "        model: {}\n",
      "        policy_loss: -0.06689644604921341\n",
      "        total_loss: 795.1310424804688\n",
      "        vf_explained_var: 0.4623516798019409\n",
      "        vf_loss: 795.1863403320312\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.38975979772441\n",
      "    ram_util_percent: 31.764601769911504\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07260941943622341\n",
      "    mean_env_wait_ms: 124.6704145986175\n",
      "    mean_inference_ms: 1.5759117597470595\n",
      "    mean_raw_obs_processing_ms: 10.745476360045835\n",
      "  time_since_restore: 2232.5958440303802\n",
      "  time_this_iter_s: 554.3653919696808\n",
      "  time_total_s: 2232.5958440303802\n",
      "  timers:\n",
      "    learn_throughput: 362.803\n",
      "    learn_time_ms: 11025.258\n",
      "    load_throughput: 9272.008\n",
      "    load_time_ms: 431.406\n",
      "    sample_throughput: 7.319\n",
      "    sample_time_ms: 546545.548\n",
      "    update_time_ms: 3.46\n",
      "  timestamp: 1612472715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 132.67\n",
      "  episode_reward_max: 118.37474086508318\n",
      "  episode_reward_mean: -100.45886087175853\n",
      "  episode_reward_min: -149.80068782632588\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 158\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0141583681106567\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012465625070035458\n",
      "        model: {}\n",
      "        policy_loss: -0.056296247988939285\n",
      "        total_loss: 392.7535705566406\n",
      "        vf_explained_var: 0.5571572184562683\n",
      "        vf_loss: 392.8015441894531\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.38371212121213\n",
      "    ram_util_percent: 31.78409090909091\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258934618524855\n",
      "    mean_env_wait_ms: 124.78809163148085\n",
      "    mean_inference_ms: 1.5749957217856883\n",
      "    mean_raw_obs_processing_ms: 10.340310824531649\n",
      "  time_since_restore: 2787.55278468132\n",
      "  time_this_iter_s: 554.9569406509399\n",
      "  time_total_s: 2787.55278468132\n",
      "  timers:\n",
      "    learn_throughput: 363.282\n",
      "    learn_time_ms: 11010.721\n",
      "    load_throughput: 9589.734\n",
      "    load_time_ms: 417.113\n",
      "    sample_throughput: 7.327\n",
      "    sample_time_ms: 545933.173\n",
      "    update_time_ms: 3.497\n",
      "  timestamp: 1612473270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-23-46\n",
      "  done: false\n",
      "  episode_len_mean: 138.47\n",
      "  episode_reward_max: 118.37474086508318\n",
      "  episode_reward_mean: -96.70370132225193\n",
      "  episode_reward_min: -148.25240074814383\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 191\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9842020869255066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012789796106517315\n",
      "        model: {}\n",
      "        policy_loss: -0.06306453049182892\n",
      "        total_loss: 1101.1817626953125\n",
      "        vf_explained_var: 0.24579092860221863\n",
      "        vf_loss: 1101.236083984375\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.77780580075661\n",
      "    ram_util_percent: 31.765699873896597\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258807097532037\n",
      "    mean_env_wait_ms: 124.94054956454725\n",
      "    mean_inference_ms: 1.5745080184350548\n",
      "    mean_raw_obs_processing_ms: 9.917587858034187\n",
      "  time_since_restore: 3343.348611354828\n",
      "  time_this_iter_s: 555.7958266735077\n",
      "  time_total_s: 3343.348611354828\n",
      "  timers:\n",
      "    learn_throughput: 363.672\n",
      "    learn_time_ms: 10998.908\n",
      "    load_throughput: 9759.279\n",
      "    load_time_ms: 409.866\n",
      "    sample_throughput: 7.33\n",
      "    sample_time_ms: 545670.796\n",
      "    update_time_ms: 3.451\n",
      "  timestamp: 1612473826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 147.81\n",
      "  episode_reward_max: 118.37474086508318\n",
      "  episode_reward_mean: -93.58109953475437\n",
      "  episode_reward_min: -152.39224959179603\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 217\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9794202446937561\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01356146764010191\n",
      "        model: {}\n",
      "        policy_loss: -0.0663742870092392\n",
      "        total_loss: 731.8023071289062\n",
      "        vf_explained_var: 0.3727956712245941\n",
      "        vf_loss: 731.859619140625\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.68375959079283\n",
      "    ram_util_percent: 31.69769820971867\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257718488107341\n",
      "    mean_env_wait_ms: 125.01046445616558\n",
      "    mean_inference_ms: 1.5743460888841607\n",
      "    mean_raw_obs_processing_ms: 9.661060322774597\n",
      "  time_since_restore: 3891.2037074565887\n",
      "  time_this_iter_s: 547.8550961017609\n",
      "  time_total_s: 3891.2037074565887\n",
      "  timers:\n",
      "    learn_throughput: 363.988\n",
      "    learn_time_ms: 10989.387\n",
      "    load_throughput: 9927.777\n",
      "    load_time_ms: 402.91\n",
      "    sample_throughput: 7.348\n",
      "    sample_time_ms: 544346.567\n",
      "    update_time_ms: 3.463\n",
      "  timestamp: 1612474374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 130.05\n",
      "  episode_reward_max: 118.21417339992553\n",
      "  episode_reward_mean: -82.5457970633982\n",
      "  episode_reward_min: -152.39224959179603\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 249\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9488598704338074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016666840761899948\n",
      "        model: {}\n",
      "        policy_loss: -0.07062018662691116\n",
      "        total_loss: 1458.60595703125\n",
      "        vf_explained_var: 0.3097323775291443\n",
      "        vf_loss: 1458.66552734375\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.098847631242\n",
      "    ram_util_percent: 31.744046094750317\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07256668004284808\n",
      "    mean_env_wait_ms: 124.87880423689036\n",
      "    mean_inference_ms: 1.5743146194226927\n",
      "    mean_raw_obs_processing_ms: 9.556589521688378\n",
      "  time_since_restore: 4438.197489500046\n",
      "  time_this_iter_s: 546.993782043457\n",
      "  time_total_s: 4438.197489500046\n",
      "  timers:\n",
      "    learn_throughput: 364.206\n",
      "    learn_time_ms: 10982.788\n",
      "    load_throughput: 9997.153\n",
      "    load_time_ms: 400.114\n",
      "    sample_throughput: 7.363\n",
      "    sample_time_ms: 543247.808\n",
      "    update_time_ms: 3.462\n",
      "  timestamp: 1612474921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_22-51-15\n",
      "  done: false\n",
      "  episode_len_mean: 145.88\n",
      "  episode_reward_max: 118.21417339992553\n",
      "  episode_reward_mean: -86.95651581557073\n",
      "  episode_reward_min: -152.39224959179603\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 272\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9866807460784912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0134500190615654\n",
      "        model: {}\n",
      "        policy_loss: -0.057302966713905334\n",
      "        total_loss: 479.4631042480469\n",
      "        vf_explained_var: 0.49050548672676086\n",
      "        vf_loss: 479.51141357421875\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.2479746835443\n",
      "    ram_util_percent: 31.716075949367085\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07256860274593196\n",
      "    mean_env_wait_ms: 124.8313640054017\n",
      "    mean_inference_ms: 1.5741917293641159\n",
      "    mean_raw_obs_processing_ms: 9.452591067611982\n",
      "  time_since_restore: 4991.763355016708\n",
      "  time_this_iter_s: 553.5658655166626\n",
      "  time_total_s: 4991.763355016708\n",
      "  timers:\n",
      "    learn_throughput: 364.354\n",
      "    learn_time_ms: 10978.339\n",
      "    load_throughput: 10099.721\n",
      "    load_time_ms: 396.051\n",
      "    sample_throughput: 7.365\n",
      "    sample_time_ms: 543123.07\n",
      "    update_time_ms: 3.488\n",
      "  timestamp: 1612475475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-00-31\n",
      "  done: false\n",
      "  episode_len_mean: 152.98\n",
      "  episode_reward_max: 118.21417339992553\n",
      "  episode_reward_mean: -84.15895553012669\n",
      "  episode_reward_min: -152.39224959179603\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 296\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9743451476097107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014825825579464436\n",
      "        model: {}\n",
      "        policy_loss: -0.06780017912387848\n",
      "        total_loss: 876.2578735351562\n",
      "        vf_explained_var: 0.443137526512146\n",
      "        vf_loss: 876.315673828125\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.19105793450882\n",
      "    ram_util_percent: 31.72392947103274\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07256809061616223\n",
      "    mean_env_wait_ms: 124.85534527972445\n",
      "    mean_inference_ms: 1.5740798995269991\n",
      "    mean_raw_obs_processing_ms: 9.305470014228517\n",
      "  time_since_restore: 5548.3718292713165\n",
      "  time_this_iter_s: 556.6084742546082\n",
      "  time_total_s: 5548.3718292713165\n",
      "  timers:\n",
      "    learn_throughput: 364.555\n",
      "    learn_time_ms: 10972.293\n",
      "    load_throughput: 10123.322\n",
      "    load_time_ms: 395.127\n",
      "    sample_throughput: 7.362\n",
      "    sample_time_ms: 543327.989\n",
      "    update_time_ms: 3.481\n",
      "  timestamp: 1612476031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 153.16\n",
      "  episode_reward_max: 118.21417339992553\n",
      "  episode_reward_mean: -85.51536176446069\n",
      "  episode_reward_min: -147.0462610698898\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 325\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9431465864181519\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014470876194536686\n",
      "        model: {}\n",
      "        policy_loss: -0.07045608013868332\n",
      "        total_loss: 740.26611328125\n",
      "        vf_explained_var: 0.5249317288398743\n",
      "        vf_loss: 740.326904296875\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.45743073047859\n",
      "    ram_util_percent: 31.710705289672546\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07255812389630824\n",
      "    mean_env_wait_ms: 124.94367786718502\n",
      "    mean_inference_ms: 1.57409985137463\n",
      "    mean_raw_obs_processing_ms: 9.175087845104269\n",
      "  time_since_restore: 6104.518327951431\n",
      "  time_this_iter_s: 556.1464986801147\n",
      "  time_total_s: 6104.518327951431\n",
      "  timers:\n",
      "    learn_throughput: 365.821\n",
      "    learn_time_ms: 10934.312\n",
      "    load_throughput: 10520.289\n",
      "    load_time_ms: 380.218\n",
      "    sample_throughput: 7.374\n",
      "    sample_time_ms: 542451.451\n",
      "    update_time_ms: 3.46\n",
      "  timestamp: 1612476588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-19-01\n",
      "  done: false\n",
      "  episode_len_mean: 157.94\n",
      "  episode_reward_max: 118.3306663644839\n",
      "  episode_reward_mean: -89.09714558031892\n",
      "  episode_reward_min: -147.0462610698898\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 351\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9063995480537415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01871694065630436\n",
      "        model: {}\n",
      "        policy_loss: -0.08122757077217102\n",
      "        total_loss: 800.510986328125\n",
      "        vf_explained_var: 0.5252761244773865\n",
      "        vf_loss: 800.57958984375\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.50354879594424\n",
      "    ram_util_percent: 31.731178707224338\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257133592025493\n",
      "    mean_env_wait_ms: 125.08890903957052\n",
      "    mean_inference_ms: 1.574238347809105\n",
      "    mean_raw_obs_processing_ms: 9.035195686065958\n",
      "  time_since_restore: 6657.745717287064\n",
      "  time_this_iter_s: 553.2273893356323\n",
      "  time_total_s: 6657.745717287064\n",
      "  timers:\n",
      "    learn_throughput: 365.832\n",
      "    learn_time_ms: 10933.991\n",
      "    load_throughput: 10577.811\n",
      "    load_time_ms: 378.15\n",
      "    sample_throughput: 7.381\n",
      "    sample_time_ms: 541915.77\n",
      "    update_time_ms: 3.423\n",
      "  timestamp: 1612477141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 144.42\n",
      "  episode_reward_max: 118.3306663644839\n",
      "  episode_reward_mean: -83.97989072252665\n",
      "  episode_reward_min: -145.19388299423906\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 380\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9158598184585571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019094053655862808\n",
      "        model: {}\n",
      "        policy_loss: -0.06937514245510101\n",
      "        total_loss: 794.3336181640625\n",
      "        vf_explained_var: 0.40281611680984497\n",
      "        vf_loss: 794.39013671875\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.5819216182048\n",
      "    ram_util_percent: 31.74108723135272\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258140613330342\n",
      "    mean_env_wait_ms: 125.1641086045132\n",
      "    mean_inference_ms: 1.5745696449752105\n",
      "    mean_raw_obs_processing_ms: 8.966006319901128\n",
      "  time_since_restore: 7211.807156085968\n",
      "  time_this_iter_s: 554.0614387989044\n",
      "  time_total_s: 7211.807156085968\n",
      "  timers:\n",
      "    learn_throughput: 365.832\n",
      "    learn_time_ms: 10933.967\n",
      "    load_throughput: 10639.807\n",
      "    load_time_ms: 375.947\n",
      "    sample_throughput: 7.381\n",
      "    sample_time_ms: 541908.977\n",
      "    update_time_ms: 3.395\n",
      "  timestamp: 1612477695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 142.29\n",
      "  episode_reward_max: 118.3306663644839\n",
      "  episode_reward_mean: -78.86310397013978\n",
      "  episode_reward_min: -140.627557964434\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 409\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8805199861526489\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01477861125022173\n",
      "        model: {}\n",
      "        policy_loss: -0.05966709554195404\n",
      "        total_loss: 1000.92822265625\n",
      "        vf_explained_var: 0.6123405694961548\n",
      "        vf_loss: 1000.97802734375\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.62217997465146\n",
      "    ram_util_percent: 31.659315589353614\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257336230433571\n",
      "    mean_env_wait_ms: 125.17843337960534\n",
      "    mean_inference_ms: 1.5747868950880655\n",
      "    mean_raw_obs_processing_ms: 8.935222080096738\n",
      "  time_since_restore: 7765.055373668671\n",
      "  time_this_iter_s: 553.2482175827026\n",
      "  time_total_s: 7765.055373668671\n",
      "  timers:\n",
      "    learn_throughput: 365.827\n",
      "    learn_time_ms: 10934.132\n",
      "    load_throughput: 10665.391\n",
      "    load_time_ms: 375.045\n",
      "    sample_throughput: 7.383\n",
      "    sample_time_ms: 541800.085\n",
      "    update_time_ms: 3.344\n",
      "  timestamp: 1612478248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-46-38\n",
      "  done: false\n",
      "  episode_len_mean: 138.85\n",
      "  episode_reward_max: 118.3306663644839\n",
      "  episode_reward_mean: -74.51363607014612\n",
      "  episode_reward_min: -130.10300849786023\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 439\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8536105155944824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017553042620420456\n",
      "        model: {}\n",
      "        policy_loss: -0.0764918252825737\n",
      "        total_loss: 739.887939453125\n",
      "        vf_explained_var: 0.5975854396820068\n",
      "        vf_loss: 739.9525146484375\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.77359693877551\n",
      "    ram_util_percent: 31.68150510204082\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257221677356941\n",
      "    mean_env_wait_ms: 125.1483109139876\n",
      "    mean_inference_ms: 1.574962817247383\n",
      "    mean_raw_obs_processing_ms: 8.9220709066531\n",
      "  time_since_restore: 8314.462483167648\n",
      "  time_this_iter_s: 549.4071094989777\n",
      "  time_total_s: 8314.462483167648\n",
      "  timers:\n",
      "    learn_throughput: 365.965\n",
      "    learn_time_ms: 10930.013\n",
      "    load_throughput: 10614.135\n",
      "    load_time_ms: 376.856\n",
      "    sample_throughput: 7.39\n",
      "    sample_time_ms: 541249.585\n",
      "    update_time_ms: 3.332\n",
      "  timestamp: 1612478798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-04_23-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 141.71\n",
      "  episode_reward_max: 118.32757309095052\n",
      "  episode_reward_mean: -75.62430902232002\n",
      "  episode_reward_min: -130.10300849786023\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 460\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9002994894981384\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01504561211913824\n",
      "        model: {}\n",
      "        policy_loss: -0.0633593499660492\n",
      "        total_loss: 450.9750671386719\n",
      "        vf_explained_var: 0.6187621355056763\n",
      "        vf_loss: 451.0282897949219\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.28883248730965\n",
      "    ram_util_percent: 31.820558375634516\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0725696216055342\n",
      "    mean_env_wait_ms: 125.14677774963806\n",
      "    mean_inference_ms: 1.575025768785536\n",
      "    mean_raw_obs_processing_ms: 8.89032720439061\n",
      "  time_since_restore: 8866.23798084259\n",
      "  time_this_iter_s: 551.775497674942\n",
      "  time_total_s: 8866.23798084259\n",
      "  timers:\n",
      "    learn_throughput: 365.979\n",
      "    learn_time_ms: 10929.587\n",
      "    load_throughput: 10584.108\n",
      "    load_time_ms: 377.925\n",
      "    sample_throughput: 7.396\n",
      "    sample_time_ms: 540846.712\n",
      "    update_time_ms: 3.337\n",
      "  timestamp: 1612479350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 149.53\n",
      "  episode_reward_max: 118.34697643539712\n",
      "  episode_reward_mean: -71.82641298965277\n",
      "  episode_reward_min: -128.91925922917596\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 489\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8159337639808655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015157832764089108\n",
      "        model: {}\n",
      "        policy_loss: -0.06258884072303772\n",
      "        total_loss: 600.689697265625\n",
      "        vf_explained_var: 0.741688072681427\n",
      "        vf_loss: 600.7420043945312\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.60494923857868\n",
      "    ram_util_percent: 31.821065989847718\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07255528181946427\n",
      "    mean_env_wait_ms: 125.13966564137621\n",
      "    mean_inference_ms: 1.5751459646604338\n",
      "    mean_raw_obs_processing_ms: 8.84864047865584\n",
      "  time_since_restore: 9418.599950551987\n",
      "  time_this_iter_s: 552.3619697093964\n",
      "  time_total_s: 9418.599950551987\n",
      "  timers:\n",
      "    learn_throughput: 365.994\n",
      "    learn_time_ms: 10929.141\n",
      "    load_throughput: 10559.672\n",
      "    load_time_ms: 378.8\n",
      "    sample_throughput: 7.39\n",
      "    sample_time_ms: 541299.018\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1612479902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 155.22\n",
      "  episode_reward_max: 118.36648300198468\n",
      "  episode_reward_mean: -76.51325944092851\n",
      "  episode_reward_min: -128.91925922917596\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 515\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.858328104019165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01771327666938305\n",
      "        model: {}\n",
      "        policy_loss: -0.0768163874745369\n",
      "        total_loss: 602.9507446289062\n",
      "        vf_explained_var: 0.6987802386283875\n",
      "        vf_loss: 603.015625\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.48071065989848\n",
      "    ram_util_percent: 31.844035532994923\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07255746789362336\n",
      "    mean_env_wait_ms: 125.146227987666\n",
      "    mean_inference_ms: 1.5752926661264188\n",
      "    mean_raw_obs_processing_ms: 8.800778840013063\n",
      "  time_since_restore: 9970.527886629105\n",
      "  time_this_iter_s: 551.9279360771179\n",
      "  time_total_s: 9970.527886629105\n",
      "  timers:\n",
      "    learn_throughput: 366.034\n",
      "    learn_time_ms: 10927.947\n",
      "    load_throughput: 10582.184\n",
      "    load_time_ms: 377.994\n",
      "    sample_throughput: 7.383\n",
      "    sample_time_ms: 541793.979\n",
      "    update_time_ms: 3.309\n",
      "  timestamp: 1612480454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 146.9\n",
      "  episode_reward_max: 118.36648300198468\n",
      "  episode_reward_mean: -66.6899462698769\n",
      "  episode_reward_min: -128.78383581296544\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 545\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7646830081939697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019279388710856438\n",
      "        model: {}\n",
      "        policy_loss: -0.08217025548219681\n",
      "        total_loss: 934.0054321289062\n",
      "        vf_explained_var: 0.6931751370429993\n",
      "        vf_loss: 934.0745849609375\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.83482142857143\n",
      "    ram_util_percent: 31.89183673469388\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07255870998894137\n",
      "    mean_env_wait_ms: 125.14825478895949\n",
      "    mean_inference_ms: 1.5755427062297713\n",
      "    mean_raw_obs_processing_ms: 8.757549375000574\n",
      "  time_since_restore: 10520.421685218811\n",
      "  time_this_iter_s: 549.8937985897064\n",
      "  time_total_s: 10520.421685218811\n",
      "  timers:\n",
      "    learn_throughput: 366.104\n",
      "    learn_time_ms: 10925.858\n",
      "    load_throughput: 10554.332\n",
      "    load_time_ms: 378.991\n",
      "    sample_throughput: 7.388\n",
      "    sample_time_ms: 541428.006\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1612481004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-32-38\n",
      "  done: false\n",
      "  episode_len_mean: 133.33\n",
      "  episode_reward_max: 118.37287643211215\n",
      "  episode_reward_mean: -61.020786713216204\n",
      "  episode_reward_min: -128.78383581296544\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 576\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7683061957359314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02003057673573494\n",
      "        model: {}\n",
      "        policy_loss: -0.08301916718482971\n",
      "        total_loss: 330.36541748046875\n",
      "        vf_explained_var: 0.8846016526222229\n",
      "        vf_loss: 330.43487548828125\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.89772151898734\n",
      "    ram_util_percent: 31.851772151898736\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07254949549487283\n",
      "    mean_env_wait_ms: 125.10947302195328\n",
      "    mean_inference_ms: 1.5757126600705136\n",
      "    mean_raw_obs_processing_ms: 8.761449245251622\n",
      "  time_since_restore: 11073.767119884491\n",
      "  time_this_iter_s: 553.3454346656799\n",
      "  time_total_s: 11073.767119884491\n",
      "  timers:\n",
      "    learn_throughput: 362.233\n",
      "    learn_time_ms: 11042.607\n",
      "    load_throughput: 10481.42\n",
      "    load_time_ms: 381.628\n",
      "    sample_throughput: 7.394\n",
      "    sample_time_ms: 540978.72\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1612481558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-41-44\n",
      "  done: false\n",
      "  episode_len_mean: 144.7\n",
      "  episode_reward_max: 118.3729683535172\n",
      "  episode_reward_mean: -57.25460405564244\n",
      "  episode_reward_min: -128.78383581296544\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 603\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7490825653076172\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017426801845431328\n",
      "        model: {}\n",
      "        policy_loss: -0.09214477986097336\n",
      "        total_loss: 235.72695922851562\n",
      "        vf_explained_var: 0.9070287942886353\n",
      "        vf_loss: 235.80145263671875\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.7148717948718\n",
      "    ram_util_percent: 31.89833333333333\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07254350516379741\n",
      "    mean_env_wait_ms: 125.06106675016983\n",
      "    mean_inference_ms: 1.5757487902382155\n",
      "    mean_raw_obs_processing_ms: 8.763828044397549\n",
      "  time_since_restore: 11620.164612293243\n",
      "  time_this_iter_s: 546.3974924087524\n",
      "  time_total_s: 11620.164612293243\n",
      "  timers:\n",
      "    learn_throughput: 362.254\n",
      "    learn_time_ms: 11041.98\n",
      "    load_throughput: 10503.174\n",
      "    load_time_ms: 380.837\n",
      "    sample_throughput: 7.407\n",
      "    sample_time_ms: 540005.033\n",
      "    update_time_ms: 3.324\n",
      "  timestamp: 1612482104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_00-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 135.24\n",
      "  episode_reward_max: 118.3729683535172\n",
      "  episode_reward_mean: -52.01402104214714\n",
      "  episode_reward_min: -123.18714427017059\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 630\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7638378739356995\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0163580272346735\n",
      "        model: {}\n",
      "        policy_loss: -0.08534321188926697\n",
      "        total_loss: 622.6608276367188\n",
      "        vf_explained_var: 0.6979730725288391\n",
      "        vf_loss: 622.729736328125\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.55069708491762\n",
      "    ram_util_percent: 31.86032953105196\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07253460245097529\n",
      "    mean_env_wait_ms: 125.02778100746393\n",
      "    mean_inference_ms: 1.5758146309447971\n",
      "    mean_raw_obs_processing_ms: 8.76285776505877\n",
      "  time_since_restore: 12173.529799461365\n",
      "  time_this_iter_s: 553.3651871681213\n",
      "  time_total_s: 12173.529799461365\n",
      "  timers:\n",
      "    learn_throughput: 362.274\n",
      "    learn_time_ms: 11041.364\n",
      "    load_throughput: 10508.306\n",
      "    load_time_ms: 380.651\n",
      "    sample_throughput: 7.407\n",
      "    sample_time_ms: 540017.531\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1612482657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-00-01\n",
      "  done: false\n",
      "  episode_len_mean: 143.6\n",
      "  episode_reward_max: 118.3729683535172\n",
      "  episode_reward_mean: -52.44240821864978\n",
      "  episode_reward_min: -123.18714427017059\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 655\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6967030763626099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01540356408804655\n",
      "        model: {}\n",
      "        policy_loss: -0.07003103941679001\n",
      "        total_loss: 456.2743225097656\n",
      "        vf_explained_var: 0.8460292816162109\n",
      "        vf_loss: 456.3287658691406\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.76469072164949\n",
      "    ram_util_percent: 31.91623711340206\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07252072682906313\n",
      "    mean_env_wait_ms: 124.99627593802961\n",
      "    mean_inference_ms: 1.5758316441520361\n",
      "    mean_raw_obs_processing_ms: 8.744359555599221\n",
      "  time_since_restore: 12717.187819957733\n",
      "  time_this_iter_s: 543.6580204963684\n",
      "  time_total_s: 12717.187819957733\n",
      "  timers:\n",
      "    learn_throughput: 362.295\n",
      "    learn_time_ms: 11040.729\n",
      "    load_throughput: 10489.497\n",
      "    load_time_ms: 381.334\n",
      "    sample_throughput: 7.421\n",
      "    sample_time_ms: 538976.922\n",
      "    update_time_ms: 3.334\n",
      "  timestamp: 1612483201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 151.16\n",
      "  episode_reward_max: 118.3729683535172\n",
      "  episode_reward_mean: -57.32757794809433\n",
      "  episode_reward_min: -125.37821059337388\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 679\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8234769701957703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015347247943282127\n",
      "        model: {}\n",
      "        policy_loss: -0.08869138360023499\n",
      "        total_loss: 439.5354309082031\n",
      "        vf_explained_var: 0.7343553900718689\n",
      "        vf_loss: 439.6086120605469\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.34974683544304\n",
      "    ram_util_percent: 31.926075949367085\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07252026224018905\n",
      "    mean_env_wait_ms: 124.989854405656\n",
      "    mean_inference_ms: 1.5758599093600614\n",
      "    mean_raw_obs_processing_ms: 8.709554561708604\n",
      "  time_since_restore: 13270.634379148483\n",
      "  time_this_iter_s: 553.4465591907501\n",
      "  time_total_s: 13270.634379148483\n",
      "  timers:\n",
      "    learn_throughput: 362.383\n",
      "    learn_time_ms: 11038.055\n",
      "    load_throughput: 10395.157\n",
      "    load_time_ms: 384.795\n",
      "    sample_throughput: 7.421\n",
      "    sample_time_ms: 538996.052\n",
      "    update_time_ms: 3.332\n",
      "  timestamp: 1612483755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-18-27\n",
      "  done: false\n",
      "  episode_len_mean: 150.45\n",
      "  episode_reward_max: 118.37292438260835\n",
      "  episode_reward_mean: -53.30875307347361\n",
      "  episode_reward_min: -125.37821059337388\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 709\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7059909105300903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012296861037611961\n",
      "        model: {}\n",
      "        policy_loss: -0.05814923718571663\n",
      "        total_loss: 436.3019714355469\n",
      "        vf_explained_var: 0.8652090430259705\n",
      "        vf_loss: 436.34771728515625\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.69022842639595\n",
      "    ram_util_percent: 31.926395939086294\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07252450188017294\n",
      "    mean_env_wait_ms: 124.98823989159838\n",
      "    mean_inference_ms: 1.5759841653189262\n",
      "    mean_raw_obs_processing_ms: 8.67993821360023\n",
      "  time_since_restore: 13822.825020551682\n",
      "  time_this_iter_s: 552.1906414031982\n",
      "  time_total_s: 13822.825020551682\n",
      "  timers:\n",
      "    learn_throughput: 362.394\n",
      "    learn_time_ms: 11037.723\n",
      "    load_throughput: 10426.441\n",
      "    load_time_ms: 383.64\n",
      "    sample_throughput: 7.417\n",
      "    sample_time_ms: 539275.613\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1612484307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 152.25\n",
      "  episode_reward_max: 118.39088690688746\n",
      "  episode_reward_mean: -42.619027279112025\n",
      "  episode_reward_min: -125.37821059337388\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 733\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6942565441131592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01597510278224945\n",
      "        model: {}\n",
      "        policy_loss: -0.08317320048809052\n",
      "        total_loss: 538.5311889648438\n",
      "        vf_explained_var: 0.8178655505180359\n",
      "        vf_loss: 538.5982055664062\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.76958762886598\n",
      "    ram_util_percent: 31.992010309278346\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07252695440558353\n",
      "    mean_env_wait_ms: 124.97412959985631\n",
      "    mean_inference_ms: 1.5760320300240926\n",
      "    mean_raw_obs_processing_ms: 8.64997813087793\n",
      "  time_since_restore: 14366.171002864838\n",
      "  time_this_iter_s: 543.3459823131561\n",
      "  time_total_s: 14366.171002864838\n",
      "  timers:\n",
      "    learn_throughput: 362.44\n",
      "    learn_time_ms: 11036.304\n",
      "    load_throughput: 10253.273\n",
      "    load_time_ms: 390.119\n",
      "    sample_throughput: 7.429\n",
      "    sample_time_ms: 538426.109\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1612484850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-36-30\n",
      "  done: false\n",
      "  episode_len_mean: 150.3\n",
      "  episode_reward_max: 118.39088690688746\n",
      "  episode_reward_mean: -37.34886837510011\n",
      "  episode_reward_min: -122.90017219626094\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 761\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6453499794006348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013204590417444706\n",
      "        model: {}\n",
      "        policy_loss: -0.07780508697032928\n",
      "        total_loss: 694.9550170898438\n",
      "        vf_explained_var: 0.7875512838363647\n",
      "        vf_loss: 695.0193481445312\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.09987012987013\n",
      "    ram_util_percent: 31.857532467532472\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07254316512491814\n",
      "    mean_env_wait_ms: 124.93443348397628\n",
      "    mean_inference_ms: 1.5761376041975996\n",
      "    mean_raw_obs_processing_ms: 8.63041394821176\n",
      "  time_since_restore: 14905.931308031082\n",
      "  time_this_iter_s: 539.7603051662445\n",
      "  time_total_s: 14905.931308031082\n",
      "  timers:\n",
      "    learn_throughput: 362.481\n",
      "    learn_time_ms: 11035.066\n",
      "    load_throughput: 10214.557\n",
      "    load_time_ms: 391.598\n",
      "    sample_throughput: 7.447\n",
      "    sample_time_ms: 537164.704\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1612485390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 138.38\n",
      "  episode_reward_max: 118.39088690688746\n",
      "  episode_reward_mean: -31.710273560705083\n",
      "  episode_reward_min: -121.18964790467723\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 793\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6797612905502319\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01668965257704258\n",
      "        model: {}\n",
      "        policy_loss: -0.08969087898731232\n",
      "        total_loss: 350.9443054199219\n",
      "        vf_explained_var: 0.889649510383606\n",
      "        vf_loss: 351.01715087890625\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.22980645161292\n",
      "    ram_util_percent: 31.906322580645163\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07255698177990569\n",
      "    mean_env_wait_ms: 124.84057250703587\n",
      "    mean_inference_ms: 1.576300028378174\n",
      "    mean_raw_obs_processing_ms: 8.631624951802852\n",
      "  time_since_restore: 15449.511172771454\n",
      "  time_this_iter_s: 543.5798647403717\n",
      "  time_total_s: 15449.511172771454\n",
      "  timers:\n",
      "    learn_throughput: 362.45\n",
      "    learn_time_ms: 11036.012\n",
      "    load_throughput: 10093.857\n",
      "    load_time_ms: 396.281\n",
      "    sample_throughput: 7.458\n",
      "    sample_time_ms: 536320.042\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1612485934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_01-54-40\n",
      "  done: false\n",
      "  episode_len_mean: 149.58\n",
      "  episode_reward_max: 118.39088690688746\n",
      "  episode_reward_mean: -38.82397286023452\n",
      "  episode_reward_min: -121.18964790467723\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 816\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7538915872573853\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01586633175611496\n",
      "        model: {}\n",
      "        policy_loss: -0.08666154742240906\n",
      "        total_loss: 487.71295166015625\n",
      "        vf_explained_var: 0.7737557888031006\n",
      "        vf_loss: 487.7835693359375\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.60346153846153\n",
      "    ram_util_percent: 31.83474358974359\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07256231448258604\n",
      "    mean_env_wait_ms: 124.7808861065372\n",
      "    mean_inference_ms: 1.5763778704795417\n",
      "    mean_raw_obs_processing_ms: 8.621094952370113\n",
      "  time_since_restore: 15995.691041231155\n",
      "  time_this_iter_s: 546.1798684597015\n",
      "  time_total_s: 15995.691041231155\n",
      "  timers:\n",
      "    learn_throughput: 362.471\n",
      "    learn_time_ms: 11035.368\n",
      "    load_throughput: 10067.934\n",
      "    load_time_ms: 397.301\n",
      "    sample_throughput: 7.463\n",
      "    sample_time_ms: 535947.322\n",
      "    update_time_ms: 3.297\n",
      "  timestamp: 1612486480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 151.47\n",
      "  episode_reward_max: 118.3794355638386\n",
      "  episode_reward_mean: -34.79538784154075\n",
      "  episode_reward_min: -121.18964790467723\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 839\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6893161535263062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015107863582670689\n",
      "        model: {}\n",
      "        policy_loss: -0.0800752192735672\n",
      "        total_loss: 583.001953125\n",
      "        vf_explained_var: 0.8028636574745178\n",
      "        vf_loss: 583.066650390625\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.95723514211886\n",
      "    ram_util_percent: 31.78178294573643\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0725710373168935\n",
      "    mean_env_wait_ms: 124.72777534614126\n",
      "    mean_inference_ms: 1.5764659898537792\n",
      "    mean_raw_obs_processing_ms: 8.606348353950205\n",
      "  time_since_restore: 16538.338808774948\n",
      "  time_this_iter_s: 542.6477675437927\n",
      "  time_total_s: 16538.338808774948\n",
      "  timers:\n",
      "    learn_throughput: 362.497\n",
      "    learn_time_ms: 11034.579\n",
      "    load_throughput: 10072.454\n",
      "    load_time_ms: 397.123\n",
      "    sample_throughput: 7.478\n",
      "    sample_time_ms: 534874.256\n",
      "    update_time_ms: 3.343\n",
      "  timestamp: 1612487023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-12-47\n",
      "  done: false\n",
      "  episode_len_mean: 154.03\n",
      "  episode_reward_max: 118.36617063941824\n",
      "  episode_reward_mean: -41.31156465874447\n",
      "  episode_reward_min: -121.18964790467723\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 864\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6687061190605164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01751045696437359\n",
      "        model: {}\n",
      "        policy_loss: -0.09235639870166779\n",
      "        total_loss: 461.63751220703125\n",
      "        vf_explained_var: 0.8672016859054565\n",
      "        vf_loss: 461.7121276855469\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.82177835051546\n",
      "    ram_util_percent: 31.806443298969075\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0725756558638839\n",
      "    mean_env_wait_ms: 124.68778607821118\n",
      "    mean_inference_ms: 1.5765421715273447\n",
      "    mean_raw_obs_processing_ms: 8.581550317864382\n",
      "  time_since_restore: 17081.99335169792\n",
      "  time_this_iter_s: 543.6545429229736\n",
      "  time_total_s: 17081.99335169792\n",
      "  timers:\n",
      "    learn_throughput: 362.511\n",
      "    learn_time_ms: 11034.155\n",
      "    load_throughput: 10083.161\n",
      "    load_time_ms: 396.701\n",
      "    sample_throughput: 7.482\n",
      "    sample_time_ms: 534600.757\n",
      "    update_time_ms: 3.291\n",
      "  timestamp: 1612487567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 162.86\n",
      "  episode_reward_max: 118.36617063941824\n",
      "  episode_reward_mean: -36.91904130714199\n",
      "  episode_reward_min: -120.70487992649295\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 890\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6652873158454895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016578761860728264\n",
      "        model: {}\n",
      "        policy_loss: -0.09108319878578186\n",
      "        total_loss: 661.5873413085938\n",
      "        vf_explained_var: 0.793250322341919\n",
      "        vf_loss: 661.6618041992188\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.87487113402062\n",
      "    ram_util_percent: 31.83994845360825\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257367565513388\n",
      "    mean_env_wait_ms: 124.66351963598227\n",
      "    mean_inference_ms: 1.5766241793704956\n",
      "    mean_raw_obs_processing_ms: 8.541946917899656\n",
      "  time_since_restore: 17626.073068857193\n",
      "  time_this_iter_s: 544.0797171592712\n",
      "  time_total_s: 17626.073068857193\n",
      "  timers:\n",
      "    learn_throughput: 362.52\n",
      "    learn_time_ms: 11033.884\n",
      "    load_throughput: 10153.217\n",
      "    load_time_ms: 393.964\n",
      "    sample_throughput: 7.495\n",
      "    sample_time_ms: 533676.54\n",
      "    update_time_ms: 3.326\n",
      "  timestamp: 1612488111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-30-58\n",
      "  done: false\n",
      "  episode_len_mean: 164.08\n",
      "  episode_reward_max: 118.39823260910373\n",
      "  episode_reward_mean: -28.369714550080662\n",
      "  episode_reward_min: -120.76360720692769\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 914\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7122521996498108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015174468979239464\n",
      "        model: {}\n",
      "        policy_loss: -0.0810065045952797\n",
      "        total_loss: 361.1776123046875\n",
      "        vf_explained_var: 0.869043231010437\n",
      "        vf_loss: 361.2431640625\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.59654289372598\n",
      "    ram_util_percent: 31.828809218950067\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07257805911971724\n",
      "    mean_env_wait_ms: 124.64113740897632\n",
      "    mean_inference_ms: 1.5767184727171826\n",
      "    mean_raw_obs_processing_ms: 8.508242740722286\n",
      "  time_since_restore: 18172.848105192184\n",
      "  time_this_iter_s: 546.7750363349915\n",
      "  time_total_s: 18172.848105192184\n",
      "  timers:\n",
      "    learn_throughput: 362.56\n",
      "    learn_time_ms: 11032.654\n",
      "    load_throughput: 10202.385\n",
      "    load_time_ms: 392.065\n",
      "    sample_throughput: 7.491\n",
      "    sample_time_ms: 533991.89\n",
      "    update_time_ms: 3.34\n",
      "  timestamp: 1612488658\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-40-02\n",
      "  done: false\n",
      "  episode_len_mean: 156.22\n",
      "  episode_reward_max: 118.39823260910373\n",
      "  episode_reward_mean: -25.331659089253577\n",
      "  episode_reward_min: -121.49591280911645\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 940\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6489320993423462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017256062477827072\n",
      "        model: {}\n",
      "        policy_loss: -0.09227285534143448\n",
      "        total_loss: 559.311279296875\n",
      "        vf_explained_var: 0.8264219760894775\n",
      "        vf_loss: 559.3861694335938\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.80450450450451\n",
      "    ram_util_percent: 31.863063063063063\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258396138931272\n",
      "    mean_env_wait_ms: 124.61730930285505\n",
      "    mean_inference_ms: 1.5767698377589243\n",
      "    mean_raw_obs_processing_ms: 8.482374320066025\n",
      "  time_since_restore: 18717.516945123672\n",
      "  time_this_iter_s: 544.668839931488\n",
      "  time_total_s: 18717.516945123672\n",
      "  timers:\n",
      "    learn_throughput: 362.439\n",
      "    learn_time_ms: 11036.349\n",
      "    load_throughput: 10324.716\n",
      "    load_time_ms: 387.42\n",
      "    sample_throughput: 7.503\n",
      "    sample_time_ms: 533115.069\n",
      "    update_time_ms: 3.358\n",
      "  timestamp: 1612489202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-49-08\n",
      "  done: false\n",
      "  episode_len_mean: 158.39\n",
      "  episode_reward_max: 118.39823260910373\n",
      "  episode_reward_mean: -30.088347863332334\n",
      "  episode_reward_min: -121.49591280911645\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 965\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.674640953540802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01682453416287899\n",
      "        model: {}\n",
      "        policy_loss: -0.08461618423461914\n",
      "        total_loss: 248.38088989257812\n",
      "        vf_explained_var: 0.9200524687767029\n",
      "        vf_loss: 248.44850158691406\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.80616174582798\n",
      "    ram_util_percent: 31.893196405648265\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07258440892158488\n",
      "    mean_env_wait_ms: 124.60101550983772\n",
      "    mean_inference_ms: 1.5767886952045642\n",
      "    mean_raw_obs_processing_ms: 8.459560208764024\n",
      "  time_since_restore: 19263.44418144226\n",
      "  time_this_iter_s: 545.9272363185883\n",
      "  time_total_s: 19263.44418144226\n",
      "  timers:\n",
      "    learn_throughput: 362.322\n",
      "    learn_time_ms: 11039.913\n",
      "    load_throughput: 10339.538\n",
      "    load_time_ms: 386.864\n",
      "    sample_throughput: 7.512\n",
      "    sample_time_ms: 532486.21\n",
      "    update_time_ms: 3.364\n",
      "  timestamp: 1612489748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_02-58-11\n",
      "  done: false\n",
      "  episode_len_mean: 152.44\n",
      "  episode_reward_max: 118.39823260910373\n",
      "  episode_reward_mean: -27.717872886498476\n",
      "  episode_reward_min: -121.49591280911645\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 993\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6119527220726013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016350574791431427\n",
      "        model: {}\n",
      "        policy_loss: -0.09280195832252502\n",
      "        total_loss: 636.1643676757812\n",
      "        vf_explained_var: 0.8144463300704956\n",
      "        vf_loss: 636.2406005859375\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.02222222222221\n",
      "    ram_util_percent: 31.8874677002584\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07259076229859969\n",
      "    mean_env_wait_ms: 124.5759144965114\n",
      "    mean_inference_ms: 1.5768014778344213\n",
      "    mean_raw_obs_processing_ms: 8.440108763310265\n",
      "  time_since_restore: 19805.570668697357\n",
      "  time_this_iter_s: 542.1264872550964\n",
      "  time_total_s: 19805.570668697357\n",
      "  timers:\n",
      "    learn_throughput: 362.206\n",
      "    learn_time_ms: 11043.451\n",
      "    load_throughput: 10491.354\n",
      "    load_time_ms: 381.266\n",
      "    sample_throughput: 7.514\n",
      "    sample_time_ms: 532366.925\n",
      "    update_time_ms: 3.36\n",
      "  timestamp: 1612490291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 156.73\n",
      "  episode_reward_max: 118.38917529748329\n",
      "  episode_reward_mean: -25.633899897946115\n",
      "  episode_reward_min: -121.49591280911645\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1017\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.700288712978363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01208523754030466\n",
      "        model: {}\n",
      "        policy_loss: -0.06873747706413269\n",
      "        total_loss: 562.939208984375\n",
      "        vf_explained_var: 0.7804058194160461\n",
      "        vf_loss: 562.9957275390625\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.45095057034221\n",
      "    ram_util_percent: 31.811787072243344\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07259840960904204\n",
      "    mean_env_wait_ms: 124.56540846451816\n",
      "    mean_inference_ms: 1.5768657597339193\n",
      "    mean_raw_obs_processing_ms: 8.424633350234021\n",
      "  time_since_restore: 20358.526723623276\n",
      "  time_this_iter_s: 552.9560549259186\n",
      "  time_total_s: 20358.526723623276\n",
      "  timers:\n",
      "    learn_throughput: 362.188\n",
      "    learn_time_ms: 11043.993\n",
      "    load_throughput: 10501.174\n",
      "    load_time_ms: 380.91\n",
      "    sample_throughput: 7.495\n",
      "    sample_time_ms: 533682.888\n",
      "    update_time_ms: 3.375\n",
      "  timestamp: 1612490844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 163.01\n",
      "  episode_reward_max: 118.38917529748329\n",
      "  episode_reward_mean: -23.32718715884216\n",
      "  episode_reward_min: -119.52935466386951\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1038\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6470019817352295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016508150845766068\n",
      "        model: {}\n",
      "        policy_loss: -0.08749708533287048\n",
      "        total_loss: 233.08470153808594\n",
      "        vf_explained_var: 0.9182560443878174\n",
      "        vf_loss: 233.15548706054688\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.17427122940431\n",
      "    ram_util_percent: 31.705323193916346\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07259970646432302\n",
      "    mean_env_wait_ms: 124.57579140661274\n",
      "    mean_inference_ms: 1.5769424801846492\n",
      "    mean_raw_obs_processing_ms: 8.403209533519854\n",
      "  time_since_restore: 20911.32491493225\n",
      "  time_this_iter_s: 552.7981913089752\n",
      "  time_total_s: 20911.32491493225\n",
      "  timers:\n",
      "    learn_throughput: 362.152\n",
      "    learn_time_ms: 11045.073\n",
      "    load_throughput: 10675.744\n",
      "    load_time_ms: 374.681\n",
      "    sample_throughput: 7.482\n",
      "    sample_time_ms: 534612.445\n",
      "    update_time_ms: 3.358\n",
      "  timestamp: 1612491396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-25-44\n",
      "  done: false\n",
      "  episode_len_mean: 161.36\n",
      "  episode_reward_max: 118.38917529748329\n",
      "  episode_reward_mean: -20.48264359158213\n",
      "  episode_reward_min: -119.35257963823824\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1063\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6468459367752075\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01827414147555828\n",
      "        model: {}\n",
      "        policy_loss: -0.10255274921655655\n",
      "        total_loss: 654.4535522460938\n",
      "        vf_explained_var: 0.7994177937507629\n",
      "        vf_loss: 654.5375366210938\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.8179257362356\n",
      "    ram_util_percent: 31.694622279129323\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07260980421460748\n",
      "    mean_env_wait_ms: 124.58727131176192\n",
      "    mean_inference_ms: 1.5770194932062203\n",
      "    mean_raw_obs_processing_ms: 8.378993611033513\n",
      "  time_since_restore: 21458.412856578827\n",
      "  time_this_iter_s: 547.0879416465759\n",
      "  time_total_s: 21458.412856578827\n",
      "  timers:\n",
      "    learn_throughput: 358.251\n",
      "    learn_time_ms: 11165.36\n",
      "    load_throughput: 10637.273\n",
      "    load_time_ms: 376.036\n",
      "    sample_throughput: 7.482\n",
      "    sample_time_ms: 534581.014\n",
      "    update_time_ms: 3.398\n",
      "  timestamp: 1612491944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 174.1\n",
      "  episode_reward_max: 118.38917529748329\n",
      "  episode_reward_mean: -28.1516467602353\n",
      "  episode_reward_min: -123.91248320624763\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1085\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7341217398643494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015190019272267818\n",
      "        model: {}\n",
      "        policy_loss: -0.09156434237957001\n",
      "        total_loss: 515.6961669921875\n",
      "        vf_explained_var: 0.7852199673652649\n",
      "        vf_loss: 515.7722778320312\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.28790931989926\n",
      "    ram_util_percent: 31.746599496221663\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07261838772861724\n",
      "    mean_env_wait_ms: 124.62424379449087\n",
      "    mean_inference_ms: 1.577045616205375\n",
      "    mean_raw_obs_processing_ms: 8.349580272392455\n",
      "  time_since_restore: 22014.698434591293\n",
      "  time_this_iter_s: 556.2855780124664\n",
      "  time_total_s: 22014.698434591293\n",
      "  timers:\n",
      "    learn_throughput: 358.125\n",
      "    learn_time_ms: 11169.294\n",
      "    load_throughput: 10664.201\n",
      "    load_time_ms: 375.087\n",
      "    sample_throughput: 7.463\n",
      "    sample_time_ms: 535945.51\n",
      "    update_time_ms: 3.377\n",
      "  timestamp: 1612492500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-44-00\n",
      "  done: false\n",
      "  episode_len_mean: 162.97\n",
      "  episode_reward_max: 118.3927268305383\n",
      "  episode_reward_mean: -32.980370813393705\n",
      "  episode_reward_min: -123.91248320624763\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1113\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.604529082775116\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019762828946113586\n",
      "        model: {}\n",
      "        policy_loss: -0.10369981080293655\n",
      "        total_loss: 582.9034423828125\n",
      "        vf_explained_var: 0.8246853351593018\n",
      "        vf_loss: 582.9871215820312\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.16857142857143\n",
      "    ram_util_percent: 31.80155844155844\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07262203155635846\n",
      "    mean_env_wait_ms: 124.64446444753544\n",
      "    mean_inference_ms: 1.577044453758711\n",
      "    mean_raw_obs_processing_ms: 8.319087401524989\n",
      "  time_since_restore: 22554.65952897072\n",
      "  time_this_iter_s: 539.961094379425\n",
      "  time_total_s: 22554.65952897072\n",
      "  timers:\n",
      "    learn_throughput: 358.088\n",
      "    learn_time_ms: 11170.427\n",
      "    load_throughput: 10644.909\n",
      "    load_time_ms: 375.766\n",
      "    sample_throughput: 7.469\n",
      "    sample_time_ms: 535571.932\n",
      "    update_time_ms: 3.387\n",
      "  timestamp: 1612493040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_03-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 161.65\n",
      "  episode_reward_max: 118.3927268305383\n",
      "  episode_reward_mean: -40.91623327578305\n",
      "  episode_reward_min: -123.91248320624763\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1137\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6470796465873718\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017398666590452194\n",
      "        model: {}\n",
      "        policy_loss: -0.0876656100153923\n",
      "        total_loss: 423.27813720703125\n",
      "        vf_explained_var: 0.8589585423469543\n",
      "        vf_loss: 423.3481750488281\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.76405648267009\n",
      "    ram_util_percent: 31.76007702182285\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0726285004969986\n",
      "    mean_env_wait_ms: 124.6424840103586\n",
      "    mean_inference_ms: 1.5771091806435964\n",
      "    mean_raw_obs_processing_ms: 8.29990537732279\n",
      "  time_since_restore: 23100.660297870636\n",
      "  time_this_iter_s: 546.0007688999176\n",
      "  time_total_s: 23100.660297870636\n",
      "  timers:\n",
      "    learn_throughput: 354.296\n",
      "    learn_time_ms: 11290.005\n",
      "    load_throughput: 10569.117\n",
      "    load_time_ms: 378.461\n",
      "    sample_throughput: 7.468\n",
      "    sample_time_ms: 535636.457\n",
      "    update_time_ms: 3.396\n",
      "  timestamp: 1612493586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-02-12\n",
      "  done: false\n",
      "  episode_len_mean: 161.3\n",
      "  episode_reward_max: 118.3927268305383\n",
      "  episode_reward_mean: -45.4642121702095\n",
      "  episode_reward_min: -123.91248320624763\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1161\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6515577435493469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01581556908786297\n",
      "        model: {}\n",
      "        policy_loss: -0.08612679690122604\n",
      "        total_loss: 396.2294921875\n",
      "        vf_explained_var: 0.8534255623817444\n",
      "        vf_loss: 396.29962158203125\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.63697047496791\n",
      "    ram_util_percent: 31.83388960205391\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07263518940092573\n",
      "    mean_env_wait_ms: 124.63953400459734\n",
      "    mean_inference_ms: 1.577233059590104\n",
      "    mean_raw_obs_processing_ms: 8.28198552601172\n",
      "  time_since_restore: 23646.690155506134\n",
      "  time_this_iter_s: 546.029857635498\n",
      "  time_total_s: 23646.690155506134\n",
      "  timers:\n",
      "    learn_throughput: 354.188\n",
      "    learn_time_ms: 11293.452\n",
      "    load_throughput: 10556.475\n",
      "    load_time_ms: 378.914\n",
      "    sample_throughput: 7.469\n",
      "    sample_time_ms: 535556.019\n",
      "    update_time_ms: 3.407\n",
      "  timestamp: 1612494132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 159.19\n",
      "  episode_reward_max: 118.3927268305383\n",
      "  episode_reward_mean: -31.683672248049703\n",
      "  episode_reward_min: -119.15276177871813\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1185\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6089690923690796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016677318140864372\n",
      "        model: {}\n",
      "        policy_loss: -0.08751507848501205\n",
      "        total_loss: 576.2803344726562\n",
      "        vf_explained_var: 0.814337968826294\n",
      "        vf_loss: 576.3508911132812\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.54557124518614\n",
      "    ram_util_percent: 31.896277278562255\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07263789027415908\n",
      "    mean_env_wait_ms: 124.62143127188328\n",
      "    mean_inference_ms: 1.5774217565746766\n",
      "    mean_raw_obs_processing_ms: 8.267586611975075\n",
      "  time_since_restore: 24192.720225572586\n",
      "  time_this_iter_s: 546.030070066452\n",
      "  time_total_s: 24192.720225572586\n",
      "  timers:\n",
      "    learn_throughput: 354.216\n",
      "    learn_time_ms: 11292.55\n",
      "    load_throughput: 10566.597\n",
      "    load_time_ms: 378.551\n",
      "    sample_throughput: 7.467\n",
      "    sample_time_ms: 535693.948\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1612494678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-20-21\n",
      "  done: false\n",
      "  episode_len_mean: 159.1\n",
      "  episode_reward_max: 118.39948585864008\n",
      "  episode_reward_mean: -24.89462187235163\n",
      "  episode_reward_min: -119.15276177871813\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1214\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5624330043792725\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015448620542883873\n",
      "        model: {}\n",
      "        policy_loss: -0.0847347155213356\n",
      "        total_loss: 745.5010375976562\n",
      "        vf_explained_var: 0.7954680919647217\n",
      "        vf_loss: 745.5699462890625\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.21251612903225\n",
      "    ram_util_percent: 31.954322580645158\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07264282237191522\n",
      "    mean_env_wait_ms: 124.60042765902874\n",
      "    mean_inference_ms: 1.5775834824101964\n",
      "    mean_raw_obs_processing_ms: 8.253796023776127\n",
      "  time_since_restore: 24735.357544898987\n",
      "  time_this_iter_s: 542.6373193264008\n",
      "  time_total_s: 24735.357544898987\n",
      "  timers:\n",
      "    learn_throughput: 350.677\n",
      "    learn_time_ms: 11406.503\n",
      "    load_throughput: 10411.911\n",
      "    load_time_ms: 384.175\n",
      "    sample_throughput: 7.473\n",
      "    sample_time_ms: 535242.398\n",
      "    update_time_ms: 3.642\n",
      "  timestamp: 1612495221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-29-24\n",
      "  done: false\n",
      "  episode_len_mean: 145.45\n",
      "  episode_reward_max: 118.39948585864008\n",
      "  episode_reward_mean: -21.601903818804555\n",
      "  episode_reward_min: -111.63344687857422\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1246\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5638947486877441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01711886376142502\n",
      "        model: {}\n",
      "        policy_loss: -0.0892491564154625\n",
      "        total_loss: 533.3318481445312\n",
      "        vf_explained_var: 0.8557261824607849\n",
      "        vf_loss: 533.40380859375\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.3795865633075\n",
      "    ram_util_percent: 31.934366925064595\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07264865266998895\n",
      "    mean_env_wait_ms: 124.5586377843962\n",
      "    mean_inference_ms: 1.5776994438490235\n",
      "    mean_raw_obs_processing_ms: 8.257132245388998\n",
      "  time_since_restore: 25278.148739099503\n",
      "  time_this_iter_s: 542.7911942005157\n",
      "  time_total_s: 25278.148739099503\n",
      "  timers:\n",
      "    learn_throughput: 350.735\n",
      "    learn_time_ms: 11404.606\n",
      "    load_throughput: 10368.617\n",
      "    load_time_ms: 385.78\n",
      "    sample_throughput: 7.472\n",
      "    sample_time_ms: 535308.139\n",
      "    update_time_ms: 3.662\n",
      "  timestamp: 1612495764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 144.01\n",
      "  episode_reward_max: 118.39948585864008\n",
      "  episode_reward_mean: -19.715993859328353\n",
      "  episode_reward_min: -107.99116230983047\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1272\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.58588045835495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01685330830514431\n",
      "        model: {}\n",
      "        policy_loss: -0.09060464799404144\n",
      "        total_loss: 767.633056640625\n",
      "        vf_explained_var: 0.7569916248321533\n",
      "        vf_loss: 767.7066650390625\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.86666666666667\n",
      "    ram_util_percent: 31.98622908622909\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07264973020351788\n",
      "    mean_env_wait_ms: 124.52003621849794\n",
      "    mean_inference_ms: 1.577741222680409\n",
      "    mean_raw_obs_processing_ms: 8.263732745460608\n",
      "  time_since_restore: 25822.22322487831\n",
      "  time_this_iter_s: 544.0744857788086\n",
      "  time_total_s: 25822.22322487831\n",
      "  timers:\n",
      "    learn_throughput: 350.704\n",
      "    learn_time_ms: 11405.63\n",
      "    load_throughput: 10432.324\n",
      "    load_time_ms: 383.424\n",
      "    sample_throughput: 7.485\n",
      "    sample_time_ms: 534421.605\n",
      "    update_time_ms: 3.638\n",
      "  timestamp: 1612496308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 143.27\n",
      "  episode_reward_max: 118.39948585864008\n",
      "  episode_reward_mean: -15.062630103088441\n",
      "  episode_reward_min: -105.8539139123067\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1299\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5536649823188782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01707777939736843\n",
      "        model: {}\n",
      "        policy_loss: -0.08797617256641388\n",
      "        total_loss: 530.5689086914062\n",
      "        vf_explained_var: 0.8348113894462585\n",
      "        vf_loss: 530.6395874023438\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.88516129032259\n",
      "    ram_util_percent: 32.036516129032265\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07265286495911569\n",
      "    mean_env_wait_ms: 124.4800098539123\n",
      "    mean_inference_ms: 1.577775233182461\n",
      "    mean_raw_obs_processing_ms: 8.272171045628268\n",
      "  time_since_restore: 26365.67674970627\n",
      "  time_this_iter_s: 543.4535248279572\n",
      "  time_total_s: 26365.67674970627\n",
      "  timers:\n",
      "    learn_throughput: 350.786\n",
      "    learn_time_ms: 11402.952\n",
      "    load_throughput: 10393.6\n",
      "    load_time_ms: 384.852\n",
      "    sample_throughput: 7.498\n",
      "    sample_time_ms: 533489.444\n",
      "    update_time_ms: 3.621\n",
      "  timestamp: 1612496852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_04-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 141.82\n",
      "  episode_reward_max: 118.38719874684419\n",
      "  episode_reward_mean: -23.75954234505401\n",
      "  episode_reward_min: -105.8539139123067\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1329\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5438542366027832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018621524795889854\n",
      "        model: {}\n",
      "        policy_loss: -0.0924660712480545\n",
      "        total_loss: 684.8733520507812\n",
      "        vf_explained_var: 0.8122291564941406\n",
      "        vf_loss: 684.9470825195312\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.21088082901554\n",
      "    ram_util_percent: 32.09119170984456\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07265671660962698\n",
      "    mean_env_wait_ms: 124.43665975100065\n",
      "    mean_inference_ms: 1.5778230955685675\n",
      "    mean_raw_obs_processing_ms: 8.28006604861252\n",
      "  time_since_restore: 26906.667660713196\n",
      "  time_this_iter_s: 540.9909110069275\n",
      "  time_total_s: 26906.667660713196\n",
      "  timers:\n",
      "    learn_throughput: 354.462\n",
      "    learn_time_ms: 11284.707\n",
      "    load_throughput: 10369.389\n",
      "    load_time_ms: 385.751\n",
      "    sample_throughput: 7.505\n",
      "    sample_time_ms: 532998.302\n",
      "    update_time_ms: 3.546\n",
      "  timestamp: 1612497393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-05-35\n",
      "  done: false\n",
      "  episode_len_mean: 148.44\n",
      "  episode_reward_max: 118.39588250638838\n",
      "  episode_reward_mean: -6.7579488207248595\n",
      "  episode_reward_min: -105.8539139123067\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1354\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5258134007453918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01705305278301239\n",
      "        model: {}\n",
      "        policy_loss: -0.09529557824134827\n",
      "        total_loss: 402.7589416503906\n",
      "        vf_explained_var: 0.879513144493103\n",
      "        vf_loss: 402.8370056152344\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.83385012919896\n",
      "    ram_util_percent: 32.067441860465124\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07265877477865479\n",
      "    mean_env_wait_ms: 124.40572946141774\n",
      "    mean_inference_ms: 1.5778415537850143\n",
      "    mean_raw_obs_processing_ms: 8.280138682312597\n",
      "  time_since_restore: 27449.096165657043\n",
      "  time_this_iter_s: 542.4285049438477\n",
      "  time_total_s: 27449.096165657043\n",
      "  timers:\n",
      "    learn_throughput: 358.281\n",
      "    learn_time_ms: 11164.432\n",
      "    load_throughput: 10473.413\n",
      "    load_time_ms: 381.919\n",
      "    sample_throughput: 7.523\n",
      "    sample_time_ms: 531735.459\n",
      "    update_time_ms: 3.529\n",
      "  timestamp: 1612497935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-14-28\n",
      "  done: false\n",
      "  episode_len_mean: 151.14\n",
      "  episode_reward_max: 118.39588250638838\n",
      "  episode_reward_mean: -2.2725383358532643\n",
      "  episode_reward_min: -105.14510744872584\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1377\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5230282545089722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01456835400313139\n",
      "        model: {}\n",
      "        policy_loss: -0.07776286453008652\n",
      "        total_loss: 432.0383605957031\n",
      "        vf_explained_var: 0.8816011548042297\n",
      "        vf_loss: 432.1014404296875\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.11655716162943\n",
      "    ram_util_percent: 32.09093298291722\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07266500934379277\n",
      "    mean_env_wait_ms: 124.36988260366142\n",
      "    mean_inference_ms: 1.5778615171215984\n",
      "    mean_raw_obs_processing_ms: 8.27566989763783\n",
      "  time_since_restore: 27982.138587236404\n",
      "  time_this_iter_s: 533.042421579361\n",
      "  time_total_s: 27982.138587236404\n",
      "  timers:\n",
      "    learn_throughput: 358.248\n",
      "    learn_time_ms: 11165.439\n",
      "    load_throughput: 10477.614\n",
      "    load_time_ms: 381.766\n",
      "    sample_throughput: 7.532\n",
      "    sample_time_ms: 531041.07\n",
      "    update_time_ms: 3.519\n",
      "  timestamp: 1612498468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 159.91\n",
      "  episode_reward_max: 118.39588250638838\n",
      "  episode_reward_mean: 2.03628137301441\n",
      "  episode_reward_min: -105.14510744872584\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1398\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6159635782241821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015435895882546902\n",
      "        model: {}\n",
      "        policy_loss: -0.07790268212556839\n",
      "        total_loss: 524.193603515625\n",
      "        vf_explained_var: 0.810483992099762\n",
      "        vf_loss: 524.255859375\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.52951030927834\n",
      "    ram_util_percent: 32.119587628865986\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07267246304101789\n",
      "    mean_env_wait_ms: 124.34468823962234\n",
      "    mean_inference_ms: 1.5778662514809603\n",
      "    mean_raw_obs_processing_ms: 8.26425480709455\n",
      "  time_since_restore: 28525.949059963226\n",
      "  time_this_iter_s: 543.8104727268219\n",
      "  time_total_s: 28525.949059963226\n",
      "  timers:\n",
      "    learn_throughput: 362.07\n",
      "    learn_time_ms: 11047.581\n",
      "    load_throughput: 10536.383\n",
      "    load_time_ms: 379.637\n",
      "    sample_throughput: 7.534\n",
      "    sample_time_ms: 530947.523\n",
      "    update_time_ms: 3.473\n",
      "  timestamp: 1612499012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 162.43\n",
      "  episode_reward_max: 118.39598039140296\n",
      "  episode_reward_mean: 0.11594400372119623\n",
      "  episode_reward_min: -105.14510744872584\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1428\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5597380995750427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0209366362541914\n",
      "        model: {}\n",
      "        policy_loss: -0.09996334463357925\n",
      "        total_loss: 575.7612915039062\n",
      "        vf_explained_var: 0.8374968767166138\n",
      "        vf_loss: 575.8400268554688\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.13565891472868\n",
      "    ram_util_percent: 32.0328165374677\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07267928886813226\n",
      "    mean_env_wait_ms: 124.3114746945052\n",
      "    mean_inference_ms: 1.5778668109049971\n",
      "    mean_raw_obs_processing_ms: 8.248210083044018\n",
      "  time_since_restore: 29067.975616931915\n",
      "  time_this_iter_s: 542.026556968689\n",
      "  time_total_s: 29067.975616931915\n",
      "  timers:\n",
      "    learn_throughput: 362.206\n",
      "    learn_time_ms: 11043.448\n",
      "    load_throughput: 10472.627\n",
      "    load_time_ms: 381.948\n",
      "    sample_throughput: 7.539\n",
      "    sample_time_ms: 530549.38\n",
      "    update_time_ms: 3.471\n",
      "  timestamp: 1612499554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 154.84\n",
      "  episode_reward_max: 118.39598039140296\n",
      "  episode_reward_mean: -16.657855613678453\n",
      "  episode_reward_min: -103.32784372138347\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1457\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5898687839508057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012709559872746468\n",
      "        model: {}\n",
      "        policy_loss: -0.08001719415187836\n",
      "        total_loss: 601.7896118164062\n",
      "        vf_explained_var: 0.7901181578636169\n",
      "        vf_loss: 601.8502807617188\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.23149100257069\n",
      "    ram_util_percent: 32.02275064267352\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07268728019469005\n",
      "    mean_env_wait_ms: 124.27761609298763\n",
      "    mean_inference_ms: 1.5778526287411068\n",
      "    mean_raw_obs_processing_ms: 8.240592515498758\n",
      "  time_since_restore: 29613.22019791603\n",
      "  time_this_iter_s: 545.2445809841156\n",
      "  time_total_s: 29613.22019791603\n",
      "  timers:\n",
      "    learn_throughput: 358.413\n",
      "    learn_time_ms: 11160.297\n",
      "    load_throughput: 10399.692\n",
      "    load_time_ms: 384.627\n",
      "    sample_throughput: 7.542\n",
      "    sample_time_ms: 530347.716\n",
      "    update_time_ms: 3.524\n",
      "  timestamp: 1612500100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-50-43\n",
      "  done: false\n",
      "  episode_len_mean: 146.31\n",
      "  episode_reward_max: 118.39598039140296\n",
      "  episode_reward_mean: -29.86333715556781\n",
      "  episode_reward_min: -114.55756918305846\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1483\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5786467790603638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013634150847792625\n",
      "        model: {}\n",
      "        policy_loss: -0.09277414530515671\n",
      "        total_loss: 476.18853759765625\n",
      "        vf_explained_var: 0.8584484457969666\n",
      "        vf_loss: 476.2606201171875\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.85463917525773\n",
      "    ram_util_percent: 32.04845360824743\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07269168593359879\n",
      "    mean_env_wait_ms: 124.25456760232626\n",
      "    mean_inference_ms: 1.5778422284396054\n",
      "    mean_raw_obs_processing_ms: 8.240400985610053\n",
      "  time_since_restore: 30156.607145547867\n",
      "  time_this_iter_s: 543.3869476318359\n",
      "  time_total_s: 30156.607145547867\n",
      "  timers:\n",
      "    learn_throughput: 362.108\n",
      "    learn_time_ms: 11046.413\n",
      "    load_throughput: 10431.895\n",
      "    load_time_ms: 383.439\n",
      "    sample_throughput: 7.539\n",
      "    sample_time_ms: 530539.857\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1612500643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_05-59-50\n",
      "  done: false\n",
      "  episode_len_mean: 151.11\n",
      "  episode_reward_max: 118.39398723782314\n",
      "  episode_reward_mean: -21.953573665237446\n",
      "  episode_reward_min: -114.55756918305846\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1507\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5825847387313843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014799654483795166\n",
      "        model: {}\n",
      "        policy_loss: -0.09557856619358063\n",
      "        total_loss: 585.5525512695312\n",
      "        vf_explained_var: 0.8373094201087952\n",
      "        vf_loss: 585.6256713867188\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.7431322207959\n",
      "    ram_util_percent: 32.05481386392811\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07269307348375595\n",
      "    mean_env_wait_ms: 124.23691780737946\n",
      "    mean_inference_ms: 1.5778478076345783\n",
      "    mean_raw_obs_processing_ms: 8.241406456944906\n",
      "  time_since_restore: 30702.807052135468\n",
      "  time_this_iter_s: 546.1999065876007\n",
      "  time_total_s: 30702.807052135468\n",
      "  timers:\n",
      "    learn_throughput: 362.17\n",
      "    learn_time_ms: 11044.531\n",
      "    load_throughput: 10545.378\n",
      "    load_time_ms: 379.313\n",
      "    sample_throughput: 7.535\n",
      "    sample_time_ms: 530886.785\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1612501190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 157.71\n",
      "  episode_reward_max: 118.37193176863194\n",
      "  episode_reward_mean: -15.951218549043048\n",
      "  episode_reward_min: -127.33712953933949\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1529\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5894385576248169\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012235159985721111\n",
      "        model: {}\n",
      "        policy_loss: -0.08043991774320602\n",
      "        total_loss: 437.12493896484375\n",
      "        vf_explained_var: 0.8493401408195496\n",
      "        vf_loss: 437.1867980957031\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.92682291666667\n",
      "    ram_util_percent: 32.075260416666666\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07269631986449471\n",
      "    mean_env_wait_ms: 124.22363630492178\n",
      "    mean_inference_ms: 1.5778556605620235\n",
      "    mean_raw_obs_processing_ms: 8.23419477173708\n",
      "  time_since_restore: 31240.956570863724\n",
      "  time_this_iter_s: 538.1495187282562\n",
      "  time_total_s: 31240.956570863724\n",
      "  timers:\n",
      "    learn_throughput: 362.209\n",
      "    learn_time_ms: 11043.348\n",
      "    load_throughput: 10524.64\n",
      "    load_time_ms: 380.061\n",
      "    sample_throughput: 7.543\n",
      "    sample_time_ms: 530298.934\n",
      "    update_time_ms: 3.319\n",
      "  timestamp: 1612501728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-17-48\n",
      "  done: false\n",
      "  episode_len_mean: 172.28\n",
      "  episode_reward_max: 118.37193176863194\n",
      "  episode_reward_mean: -9.973890064412451\n",
      "  episode_reward_min: -128.00312509949964\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1548\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5962624549865723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01269841194152832\n",
      "        model: {}\n",
      "        policy_loss: -0.07348529249429703\n",
      "        total_loss: 441.7516784667969\n",
      "        vf_explained_var: 0.8495548963546753\n",
      "        vf_loss: 441.8059387207031\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.77924773022049\n",
      "    ram_util_percent: 32.04889753566796\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07269891125402748\n",
      "    mean_env_wait_ms: 124.21835456653413\n",
      "    mean_inference_ms: 1.5778651101360628\n",
      "    mean_raw_obs_processing_ms: 8.217984408806082\n",
      "  time_since_restore: 31781.106332063675\n",
      "  time_this_iter_s: 540.1497611999512\n",
      "  time_total_s: 31781.106332063675\n",
      "  timers:\n",
      "    learn_throughput: 358.413\n",
      "    learn_time_ms: 11160.296\n",
      "    load_throughput: 10440.269\n",
      "    load_time_ms: 383.132\n",
      "    sample_throughput: 7.549\n",
      "    sample_time_ms: 529841.551\n",
      "    update_time_ms: 3.394\n",
      "  timestamp: 1612502268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-26-54\n",
      "  done: false\n",
      "  episode_len_mean: 179.76\n",
      "  episode_reward_max: 118.37193176863194\n",
      "  episode_reward_mean: -12.465849938789313\n",
      "  episode_reward_min: -128.00312509949964\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1572\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5962300300598145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012574665248394012\n",
      "        model: {}\n",
      "        policy_loss: -0.08449487388134003\n",
      "        total_loss: 647.60107421875\n",
      "        vf_explained_var: 0.7968217730522156\n",
      "        vf_loss: 647.66650390625\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.77740693196407\n",
      "    ram_util_percent: 31.98883183568678\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07269930569056939\n",
      "    mean_env_wait_ms: 124.21538813383744\n",
      "    mean_inference_ms: 1.5778622458440923\n",
      "    mean_raw_obs_processing_ms: 8.195119660421046\n",
      "  time_since_restore: 32326.862001895905\n",
      "  time_this_iter_s: 545.7556698322296\n",
      "  time_total_s: 32326.862001895905\n",
      "  timers:\n",
      "    learn_throughput: 358.454\n",
      "    learn_time_ms: 11159.037\n",
      "    load_throughput: 10530.876\n",
      "    load_time_ms: 379.835\n",
      "    sample_throughput: 7.543\n",
      "    sample_time_ms: 530322.843\n",
      "    update_time_ms: 3.407\n",
      "  timestamp: 1612502814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-35-59\n",
      "  done: false\n",
      "  episode_len_mean: 171.55\n",
      "  episode_reward_max: 118.39896255238874\n",
      "  episode_reward_mean: -9.732901543935496\n",
      "  episode_reward_min: -128.00312509949964\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1598\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5556591153144836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01564640738070011\n",
      "        model: {}\n",
      "        policy_loss: -0.09240683168172836\n",
      "        total_loss: 438.0298156738281\n",
      "        vf_explained_var: 0.8797011971473694\n",
      "        vf_loss: 438.09844970703125\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.68637532133675\n",
      "    ram_util_percent: 32.5357326478149\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07270518564534777\n",
      "    mean_env_wait_ms: 124.21072066748832\n",
      "    mean_inference_ms: 1.5779250371117934\n",
      "    mean_raw_obs_processing_ms: 8.172347846575649\n",
      "  time_since_restore: 32871.97388458252\n",
      "  time_this_iter_s: 545.111882686615\n",
      "  time_total_s: 32871.97388458252\n",
      "  timers:\n",
      "    learn_throughput: 358.399\n",
      "    learn_time_ms: 11160.76\n",
      "    load_throughput: 10518.62\n",
      "    load_time_ms: 380.278\n",
      "    sample_throughput: 7.539\n",
      "    sample_time_ms: 530593.3\n",
      "    update_time_ms: 3.401\n",
      "  timestamp: 1612503359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 180.25\n",
      "  episode_reward_max: 118.39896255238874\n",
      "  episode_reward_mean: -1.4944834274595098\n",
      "  episode_reward_min: -128.00312509949964\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1617\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5550108551979065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012994809076189995\n",
      "        model: {}\n",
      "        policy_loss: -0.091889888048172\n",
      "        total_loss: 274.7970886230469\n",
      "        vf_explained_var: 0.9136096835136414\n",
      "        vf_loss: 274.8692626953125\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.74516971279374\n",
      "    ram_util_percent: 32.72597911227154\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0727103957864329\n",
      "    mean_env_wait_ms: 124.20726620741323\n",
      "    mean_inference_ms: 1.5779609433057455\n",
      "    mean_raw_obs_processing_ms: 8.152498031906335\n",
      "  time_since_restore: 33409.0087723732\n",
      "  time_this_iter_s: 537.0348877906799\n",
      "  time_total_s: 33409.0087723732\n",
      "  timers:\n",
      "    learn_throughput: 358.422\n",
      "    learn_time_ms: 11160.017\n",
      "    load_throughput: 10535.098\n",
      "    load_time_ms: 379.683\n",
      "    sample_throughput: 7.533\n",
      "    sample_time_ms: 530995.053\n",
      "    update_time_ms: 3.447\n",
      "  timestamp: 1612503896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_06-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 173.9\n",
      "  episode_reward_max: 118.39896255238874\n",
      "  episode_reward_mean: 5.2158108236988845\n",
      "  episode_reward_min: -122.83309194273369\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1642\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5341144800186157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01209219079464674\n",
      "        model: {}\n",
      "        policy_loss: -0.08305031061172485\n",
      "        total_loss: 649.3587036132812\n",
      "        vf_explained_var: 0.7888708710670471\n",
      "        vf_loss: 649.4235229492188\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.79021879021879\n",
      "    ram_util_percent: 32.72149292149292\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07271392444917864\n",
      "    mean_env_wait_ms: 124.20360903291166\n",
      "    mean_inference_ms: 1.5779897370644542\n",
      "    mean_raw_obs_processing_ms: 8.132677548644596\n",
      "  time_since_restore: 33953.46446681023\n",
      "  time_this_iter_s: 544.455694437027\n",
      "  time_total_s: 33953.46446681023\n",
      "  timers:\n",
      "    learn_throughput: 358.479\n",
      "    learn_time_ms: 11158.27\n",
      "    load_throughput: 10545.899\n",
      "    load_time_ms: 379.294\n",
      "    sample_throughput: 7.532\n",
      "    sample_time_ms: 531062.082\n",
      "    update_time_ms: 3.476\n",
      "  timestamp: 1612504441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 172.15\n",
      "  episode_reward_max: 118.39896255238874\n",
      "  episode_reward_mean: 16.228547859564625\n",
      "  episode_reward_min: -122.83309194273369\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1665\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5055462718009949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01345458161085844\n",
      "        model: {}\n",
      "        policy_loss: -0.07924971729516983\n",
      "        total_loss: 449.4091796875\n",
      "        vf_explained_var: 0.8480197787284851\n",
      "        vf_loss: 449.4679870605469\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.23723684210526\n",
      "    ram_util_percent: 32.736315789473686\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07271989676067042\n",
      "    mean_env_wait_ms: 124.18900343586847\n",
      "    mean_inference_ms: 1.5780271926454765\n",
      "    mean_raw_obs_processing_ms: 8.117543785511469\n",
      "  time_since_restore: 34485.77057504654\n",
      "  time_this_iter_s: 532.3061082363129\n",
      "  time_total_s: 34485.77057504654\n",
      "  timers:\n",
      "    learn_throughput: 358.405\n",
      "    learn_time_ms: 11160.569\n",
      "    load_throughput: 10514.835\n",
      "    load_time_ms: 380.415\n",
      "    sample_throughput: 7.546\n",
      "    sample_time_ms: 530087.309\n",
      "    update_time_ms: 3.45\n",
      "  timestamp: 1612504973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-11-49\n",
      "  done: false\n",
      "  episode_len_mean: 174.0\n",
      "  episode_reward_max: 118.39271620707677\n",
      "  episode_reward_mean: 14.027005509046813\n",
      "  episode_reward_min: -122.83309194273369\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1687\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5259451270103455\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015930751338601112\n",
      "        model: {}\n",
      "        policy_loss: -0.09473392367362976\n",
      "        total_loss: 496.2794189453125\n",
      "        vf_explained_var: 0.8492617011070251\n",
      "        vf_loss: 496.3499755859375\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.20692810457517\n",
      "    ram_util_percent: 32.74300653594771\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07272649200862286\n",
      "    mean_env_wait_ms: 124.16963254862647\n",
      "    mean_inference_ms: 1.5780210871733455\n",
      "    mean_raw_obs_processing_ms: 8.0996873687725\n",
      "  time_since_restore: 35021.5172457695\n",
      "  time_this_iter_s: 535.7466707229614\n",
      "  time_total_s: 35021.5172457695\n",
      "  timers:\n",
      "    learn_throughput: 358.349\n",
      "    learn_time_ms: 11162.305\n",
      "    load_throughput: 10505.974\n",
      "    load_time_ms: 380.736\n",
      "    sample_throughput: 7.559\n",
      "    sample_time_ms: 529136.385\n",
      "    update_time_ms: 3.467\n",
      "  timestamp: 1612505509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-20-42\n",
      "  done: false\n",
      "  episode_len_mean: 177.14\n",
      "  episode_reward_max: 118.39271620707677\n",
      "  episode_reward_mean: 6.077999946947442\n",
      "  episode_reward_min: -113.89728681773411\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1709\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5340998768806458\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013281844556331635\n",
      "        model: {}\n",
      "        policy_loss: -0.0876181423664093\n",
      "        total_loss: 556.9920654296875\n",
      "        vf_explained_var: 0.8166420459747314\n",
      "        vf_loss: 557.0594482421875\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.36943346508563\n",
      "    ram_util_percent: 32.76903820816864\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07273019246413295\n",
      "    mean_env_wait_ms: 124.14279486798404\n",
      "    mean_inference_ms: 1.5780066925177545\n",
      "    mean_raw_obs_processing_ms: 8.081373518685519\n",
      "  time_since_restore: 35553.810210227966\n",
      "  time_this_iter_s: 532.2929644584656\n",
      "  time_total_s: 35553.810210227966\n",
      "  timers:\n",
      "    learn_throughput: 354.732\n",
      "    learn_time_ms: 11276.121\n",
      "    load_throughput: 10512.528\n",
      "    load_time_ms: 380.498\n",
      "    sample_throughput: 7.577\n",
      "    sample_time_ms: 527909.977\n",
      "    update_time_ms: 3.504\n",
      "  timestamp: 1612506042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 173.62\n",
      "  episode_reward_max: 118.39271620707677\n",
      "  episode_reward_mean: 16.191847528977483\n",
      "  episode_reward_min: -113.89728681773411\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1735\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4664571285247803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014301544055342674\n",
      "        model: {}\n",
      "        policy_loss: -0.09253620356321335\n",
      "        total_loss: 630.53271484375\n",
      "        vf_explained_var: 0.7967574000358582\n",
      "        vf_loss: 630.603515625\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.28391699092087\n",
      "    ram_util_percent: 32.78249027237354\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07273766502663402\n",
      "    mean_env_wait_ms: 124.10472796203139\n",
      "    mean_inference_ms: 1.5780118151445577\n",
      "    mean_raw_obs_processing_ms: 8.065615228004063\n",
      "  time_since_restore: 36093.41268467903\n",
      "  time_this_iter_s: 539.6024744510651\n",
      "  time_total_s: 36093.41268467903\n",
      "  timers:\n",
      "    learn_throughput: 350.95\n",
      "    learn_time_ms: 11397.651\n",
      "    load_throughput: 10449.766\n",
      "    load_time_ms: 382.784\n",
      "    sample_throughput: 7.588\n",
      "    sample_time_ms: 527122.986\n",
      "    update_time_ms: 3.557\n",
      "  timestamp: 1612506582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-38-34\n",
      "  done: false\n",
      "  episode_len_mean: 164.86\n",
      "  episode_reward_max: 118.39175844818104\n",
      "  episode_reward_mean: 15.968612006711087\n",
      "  episode_reward_min: -114.82924389463392\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1760\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4956229627132416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013459742069244385\n",
      "        model: {}\n",
      "        policy_loss: -0.09054109454154968\n",
      "        total_loss: 471.30914306640625\n",
      "        vf_explained_var: 0.8578674793243408\n",
      "        vf_loss: 471.3792419433594\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.36126482213439\n",
      "    ram_util_percent: 32.8167325428195\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07274624178118452\n",
      "    mean_env_wait_ms: 124.06488484650826\n",
      "    mean_inference_ms: 1.57802337031252\n",
      "    mean_raw_obs_processing_ms: 8.052216303302014\n",
      "  time_since_restore: 36625.885133743286\n",
      "  time_this_iter_s: 532.4724490642548\n",
      "  time_total_s: 36625.885133743286\n",
      "  timers:\n",
      "    learn_throughput: 350.943\n",
      "    learn_time_ms: 11397.862\n",
      "    load_throughput: 10306.046\n",
      "    load_time_ms: 388.122\n",
      "    sample_throughput: 7.597\n",
      "    sample_time_ms: 526549.346\n",
      "    update_time_ms: 3.585\n",
      "  timestamp: 1612507114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 163.64\n",
      "  episode_reward_max: 118.39172250116374\n",
      "  episode_reward_mean: 34.75526549639209\n",
      "  episode_reward_min: -114.82924389463392\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1785\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.41121524572372437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01266419980674982\n",
      "        model: {}\n",
      "        policy_loss: -0.08113492280244827\n",
      "        total_loss: 279.6927185058594\n",
      "        vf_explained_var: 0.8717822432518005\n",
      "        vf_loss: 279.754638671875\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.119921875\n",
      "    ram_util_percent: 32.728125\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07275278907327182\n",
      "    mean_env_wait_ms: 124.02791453981536\n",
      "    mean_inference_ms: 1.5780891097493481\n",
      "    mean_raw_obs_processing_ms: 8.042508095145903\n",
      "  time_since_restore: 37164.15636563301\n",
      "  time_this_iter_s: 538.2712318897247\n",
      "  time_total_s: 37164.15636563301\n",
      "  timers:\n",
      "    learn_throughput: 354.694\n",
      "    learn_time_ms: 11277.328\n",
      "    load_throughput: 10256.208\n",
      "    load_time_ms: 390.008\n",
      "    sample_throughput: 7.598\n",
      "    sample_time_ms: 526482.233\n",
      "    update_time_ms: 3.511\n",
      "  timestamp: 1612507652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_07-56-27\n",
      "  done: false\n",
      "  episode_len_mean: 164.93\n",
      "  episode_reward_max: 118.39172250116374\n",
      "  episode_reward_mean: 43.0166106834919\n",
      "  episode_reward_min: -114.82924389463392\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1806\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49965640902519226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01398203894495964\n",
      "        model: {}\n",
      "        policy_loss: -0.09074382483959198\n",
      "        total_loss: 349.5885925292969\n",
      "        vf_explained_var: 0.8843551278114319\n",
      "        vf_loss: 349.6581115722656\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55242463958061\n",
      "    ram_util_percent: 32.58781127129751\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07276256465320777\n",
      "    mean_env_wait_ms: 124.00029798483014\n",
      "    mean_inference_ms: 1.5781647326956931\n",
      "    mean_raw_obs_processing_ms: 8.034488308239457\n",
      "  time_since_restore: 37698.36499905586\n",
      "  time_this_iter_s: 534.2086334228516\n",
      "  time_total_s: 37698.36499905586\n",
      "  timers:\n",
      "    learn_throughput: 351.125\n",
      "    learn_time_ms: 11391.943\n",
      "    load_throughput: 10120.805\n",
      "    load_time_ms: 395.225\n",
      "    sample_throughput: 7.616\n",
      "    sample_time_ms: 525204.02\n",
      "    update_time_ms: 3.56\n",
      "  timestamp: 1612508187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 170.01\n",
      "  episode_reward_max: 118.39172250116374\n",
      "  episode_reward_mean: 51.406367881224654\n",
      "  episode_reward_min: -114.82924389463392\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1829\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46553969383239746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013246501795947552\n",
      "        model: {}\n",
      "        policy_loss: -0.08415625989437103\n",
      "        total_loss: 503.64202880859375\n",
      "        vf_explained_var: 0.8069676160812378\n",
      "        vf_loss: 503.70611572265625\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.59050131926121\n",
      "    ram_util_percent: 32.605540897097626\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07277526259068572\n",
      "    mean_env_wait_ms: 123.96838088323578\n",
      "    mean_inference_ms: 1.57829656944867\n",
      "    mean_raw_obs_processing_ms: 8.022999398772832\n",
      "  time_since_restore: 38229.77461338043\n",
      "  time_this_iter_s: 531.4096143245697\n",
      "  time_total_s: 38229.77461338043\n",
      "  timers:\n",
      "    learn_throughput: 351.278\n",
      "    learn_time_ms: 11386.977\n",
      "    load_throughput: 10061.326\n",
      "    load_time_ms: 397.562\n",
      "    sample_throughput: 7.636\n",
      "    sample_time_ms: 523836.537\n",
      "    update_time_ms: 3.552\n",
      "  timestamp: 1612508718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-14-04\n",
      "  done: false\n",
      "  episode_len_mean: 171.66\n",
      "  episode_reward_max: 118.39565837344934\n",
      "  episode_reward_mean: 57.941928000597976\n",
      "  episode_reward_min: -103.52546017737247\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1852\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44269299507141113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012389971874654293\n",
      "        model: {}\n",
      "        policy_loss: -0.08390188217163086\n",
      "        total_loss: 435.8582763671875\n",
      "        vf_explained_var: 0.8466358184814453\n",
      "        vf_loss: 435.9234313964844\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01733333333334\n",
      "    ram_util_percent: 32.65693333333334\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.072789248941793\n",
      "    mean_env_wait_ms: 123.93182186274103\n",
      "    mean_inference_ms: 1.5785467125766512\n",
      "    mean_raw_obs_processing_ms: 8.009452142271964\n",
      "  time_since_restore: 38755.24848127365\n",
      "  time_this_iter_s: 525.473867893219\n",
      "  time_total_s: 38755.24848127365\n",
      "  timers:\n",
      "    learn_throughput: 351.422\n",
      "    learn_time_ms: 11382.315\n",
      "    load_throughput: 9999.06\n",
      "    load_time_ms: 400.038\n",
      "    sample_throughput: 7.653\n",
      "    sample_time_ms: 522680.675\n",
      "    update_time_ms: 3.515\n",
      "  timestamp: 1612509244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-23-03\n",
      "  done: false\n",
      "  episode_len_mean: 178.96\n",
      "  episode_reward_max: 118.39569765896039\n",
      "  episode_reward_mean: 47.79843310280524\n",
      "  episode_reward_min: -103.94364634513907\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 1874\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5062596201896667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012780861929059029\n",
      "        model: {}\n",
      "        policy_loss: -0.08638804405927658\n",
      "        total_loss: 281.5965270996094\n",
      "        vf_explained_var: 0.9166578054428101\n",
      "        vf_loss: 281.6634521484375\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.66142857142857\n",
      "    ram_util_percent: 32.80727272727273\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0728102539138026\n",
      "    mean_env_wait_ms: 123.90076297713568\n",
      "    mean_inference_ms: 1.5788976144653162\n",
      "    mean_raw_obs_processing_ms: 7.993933064589989\n",
      "  time_since_restore: 39294.24538350105\n",
      "  time_this_iter_s: 538.9969022274017\n",
      "  time_total_s: 39294.24538350105\n",
      "  timers:\n",
      "    learn_throughput: 348.486\n",
      "    learn_time_ms: 11478.218\n",
      "    load_throughput: 9966.947\n",
      "    load_time_ms: 401.326\n",
      "    sample_throughput: 7.662\n",
      "    sample_time_ms: 522036.051\n",
      "    update_time_ms: 3.488\n",
      "  timestamp: 1612509783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-31-53\n",
      "  done: false\n",
      "  episode_len_mean: 186.63\n",
      "  episode_reward_max: 118.39569765896039\n",
      "  episode_reward_mean: 47.256437370044914\n",
      "  episode_reward_min: -127.46632587874359\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1893\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5250278115272522\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012658757157623768\n",
      "        model: {}\n",
      "        policy_loss: -0.08076878637075424\n",
      "        total_loss: 545.3077392578125\n",
      "        vf_explained_var: 0.7670041918754578\n",
      "        vf_loss: 545.3692626953125\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.80132450331126\n",
      "    ram_util_percent: 32.846490066225165\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07283146936013794\n",
      "    mean_env_wait_ms: 123.8729362802173\n",
      "    mean_inference_ms: 1.5792667690509707\n",
      "    mean_raw_obs_processing_ms: 7.97713606499519\n",
      "  time_since_restore: 39823.95700883865\n",
      "  time_this_iter_s: 529.7116253376007\n",
      "  time_total_s: 39823.95700883865\n",
      "  timers:\n",
      "    learn_throughput: 348.624\n",
      "    learn_time_ms: 11473.681\n",
      "    load_throughput: 9976.466\n",
      "    load_time_ms: 400.944\n",
      "    sample_throughput: 7.666\n",
      "    sample_time_ms: 521780.196\n",
      "    update_time_ms: 3.503\n",
      "  timestamp: 1612510313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-40-50\n",
      "  done: false\n",
      "  episode_len_mean: 179.12\n",
      "  episode_reward_max: 118.39569765896039\n",
      "  episode_reward_mean: 49.52407898569192\n",
      "  episode_reward_min: -127.46632587874359\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1918\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.474099338054657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012306790798902512\n",
      "        model: {}\n",
      "        policy_loss: -0.08296136558055878\n",
      "        total_loss: 425.8874816894531\n",
      "        vf_explained_var: 0.8516895174980164\n",
      "        vf_loss: 425.9517517089844\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.53759791122715\n",
      "    ram_util_percent: 32.92467362924281\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07286209351150517\n",
      "    mean_env_wait_ms: 123.83830523564095\n",
      "    mean_inference_ms: 1.5798586355158957\n",
      "    mean_raw_obs_processing_ms: 7.958971865221454\n",
      "  time_since_restore: 40360.61599731445\n",
      "  time_this_iter_s: 536.6589884757996\n",
      "  time_total_s: 40360.61599731445\n",
      "  timers:\n",
      "    learn_throughput: 352.462\n",
      "    learn_time_ms: 11348.735\n",
      "    load_throughput: 10064.002\n",
      "    load_time_ms: 397.456\n",
      "    sample_throughput: 7.663\n",
      "    sample_time_ms: 522000.844\n",
      "    update_time_ms: 3.439\n",
      "  timestamp: 1612510850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-49-49\n",
      "  done: false\n",
      "  episode_len_mean: 178.21\n",
      "  episode_reward_max: 118.39569765896039\n",
      "  episode_reward_mean: 53.868999478553654\n",
      "  episode_reward_min: -127.46632587874359\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1941\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46686258912086487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011688164435327053\n",
      "        model: {}\n",
      "        policy_loss: -0.08172876387834549\n",
      "        total_loss: 110.69264221191406\n",
      "        vf_explained_var: 0.94387286901474\n",
      "        vf_loss: 110.75662231445312\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.57854356306892\n",
      "    ram_util_percent: 32.97737321196359\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0728936081955383\n",
      "    mean_env_wait_ms: 123.8147181566923\n",
      "    mean_inference_ms: 1.580424073723362\n",
      "    mean_raw_obs_processing_ms: 7.943076108204407\n",
      "  time_since_restore: 40899.48089456558\n",
      "  time_this_iter_s: 538.8648972511292\n",
      "  time_total_s: 40899.48089456558\n",
      "  timers:\n",
      "    learn_throughput: 352.494\n",
      "    learn_time_ms: 11347.718\n",
      "    load_throughput: 10067.286\n",
      "    load_time_ms: 397.327\n",
      "    sample_throughput: 7.653\n",
      "    sample_time_ms: 522657.12\n",
      "    update_time_ms: 3.439\n",
      "  timestamp: 1612511389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_08-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 182.79\n",
      "  episode_reward_max: 118.39569765896039\n",
      "  episode_reward_mean: 60.26832604473905\n",
      "  episode_reward_min: -127.46632587874359\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1961\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4750191271305084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010060109198093414\n",
      "        model: {}\n",
      "        policy_loss: -0.08063112944364548\n",
      "        total_loss: 125.62024688720703\n",
      "        vf_explained_var: 0.9498123526573181\n",
      "        vf_loss: 125.68560028076172\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.63753280839896\n",
      "    ram_util_percent: 33.087532808398954\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07292249954222403\n",
      "    mean_env_wait_ms: 123.79720067874172\n",
      "    mean_inference_ms: 1.5808905479290545\n",
      "    mean_raw_obs_processing_ms: 7.9273957189935675\n",
      "  time_since_restore: 41433.42294549942\n",
      "  time_this_iter_s: 533.9420509338379\n",
      "  time_total_s: 41433.42294549942\n",
      "  timers:\n",
      "    learn_throughput: 352.616\n",
      "    learn_time_ms: 11343.792\n",
      "    load_throughput: 10036.377\n",
      "    load_time_ms: 398.55\n",
      "    sample_throughput: 7.661\n",
      "    sample_time_ms: 522093.91\n",
      "    update_time_ms: 3.425\n",
      "  timestamp: 1612511923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 181.82\n",
      "  episode_reward_max: 118.38912876359556\n",
      "  episode_reward_mean: 62.647309607000835\n",
      "  episode_reward_min: -103.97213286513127\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1982\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49877041578292847\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011498447507619858\n",
      "        model: {}\n",
      "        policy_loss: -0.0802617222070694\n",
      "        total_loss: 352.6072998046875\n",
      "        vf_explained_var: 0.8560786247253418\n",
      "        vf_loss: 352.6700134277344\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48891786179922\n",
      "    ram_util_percent: 33.16558018252933\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0729521406507327\n",
      "    mean_env_wait_ms: 123.78018807305457\n",
      "    mean_inference_ms: 1.5813529707145917\n",
      "    mean_raw_obs_processing_ms: 7.91162212921226\n",
      "  time_since_restore: 41970.53997516632\n",
      "  time_this_iter_s: 537.1170296669006\n",
      "  time_total_s: 41970.53997516632\n",
      "  timers:\n",
      "    learn_throughput: 352.693\n",
      "    learn_time_ms: 11341.307\n",
      "    load_throughput: 10072.214\n",
      "    load_time_ms: 397.132\n",
      "    sample_throughput: 7.655\n",
      "    sample_time_ms: 522561.275\n",
      "    update_time_ms: 3.413\n",
      "  timestamp: 1612512460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-16-38\n",
      "  done: false\n",
      "  episode_len_mean: 181.54\n",
      "  episode_reward_max: 118.38912876359556\n",
      "  episode_reward_mean: 58.54953910881307\n",
      "  episode_reward_min: -104.68929334198754\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2003\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4713744521141052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011099059134721756\n",
      "        model: {}\n",
      "        policy_loss: -0.0838550478219986\n",
      "        total_loss: 241.16725158691406\n",
      "        vf_explained_var: 0.8915536999702454\n",
      "        vf_loss: 241.23423767089844\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55215123859192\n",
      "    ram_util_percent: 33.24198174706649\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07298144742837044\n",
      "    mean_env_wait_ms: 123.76717707462707\n",
      "    mean_inference_ms: 1.5817649310897144\n",
      "    mean_raw_obs_processing_ms: 7.896228541663936\n",
      "  time_since_restore: 42508.31288123131\n",
      "  time_this_iter_s: 537.7729060649872\n",
      "  time_total_s: 42508.31288123131\n",
      "  timers:\n",
      "    learn_throughput: 348.89\n",
      "    learn_time_ms: 11464.928\n",
      "    load_throughput: 10095.836\n",
      "    load_time_ms: 396.203\n",
      "    sample_throughput: 7.657\n",
      "    sample_time_ms: 522388.113\n",
      "    update_time_ms: 3.475\n",
      "  timestamp: 1612512998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 186.92\n",
      "  episode_reward_max: 118.38912876359556\n",
      "  episode_reward_mean: 56.516868787340854\n",
      "  episode_reward_min: -104.68929334198754\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2025\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47435054183006287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013381204567849636\n",
      "        model: {}\n",
      "        policy_loss: -0.08035458624362946\n",
      "        total_loss: 799.3738403320312\n",
      "        vf_explained_var: 0.6947742700576782\n",
      "        vf_loss: 799.4338989257812\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.31070967741935\n",
      "    ram_util_percent: 33.32516129032258\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07301256136066903\n",
      "    mean_env_wait_ms: 123.75934055481862\n",
      "    mean_inference_ms: 1.5821615898338037\n",
      "    mean_raw_obs_processing_ms: 7.878434506181634\n",
      "  time_since_restore: 43051.57099699974\n",
      "  time_this_iter_s: 543.2581157684326\n",
      "  time_total_s: 43051.57099699974\n",
      "  timers:\n",
      "    learn_throughput: 352.546\n",
      "    learn_time_ms: 11346.026\n",
      "    load_throughput: 10176.48\n",
      "    load_time_ms: 393.063\n",
      "    sample_throughput: 7.642\n",
      "    sample_time_ms: 523414.713\n",
      "    update_time_ms: 3.432\n",
      "  timestamp: 1612513541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-34-40\n",
      "  done: false\n",
      "  episode_len_mean: 183.02\n",
      "  episode_reward_max: 118.38912876359556\n",
      "  episode_reward_mean: 52.33104041484045\n",
      "  episode_reward_min: -104.68929334198754\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2049\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4683196246623993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012449665926396847\n",
      "        model: {}\n",
      "        policy_loss: -0.08872149139642715\n",
      "        total_loss: 233.38943481445312\n",
      "        vf_explained_var: 0.9134837985038757\n",
      "        vf_loss: 233.4592742919922\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.7087012987013\n",
      "    ram_util_percent: 33.35246753246752\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0730443280154883\n",
      "    mean_env_wait_ms: 123.75152949488512\n",
      "    mean_inference_ms: 1.58258300465339\n",
      "    mean_raw_obs_processing_ms: 7.861518017364458\n",
      "  time_since_restore: 43590.72134971619\n",
      "  time_this_iter_s: 539.1503527164459\n",
      "  time_total_s: 43590.72134971619\n",
      "  timers:\n",
      "    learn_throughput: 352.547\n",
      "    learn_time_ms: 11346.012\n",
      "    load_throughput: 10209.497\n",
      "    load_time_ms: 391.792\n",
      "    sample_throughput: 7.631\n",
      "    sample_time_ms: 524186.633\n",
      "    update_time_ms: 3.422\n",
      "  timestamp: 1612514080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-43-43\n",
      "  done: false\n",
      "  episode_len_mean: 179.41\n",
      "  episode_reward_max: 118.39952585646184\n",
      "  episode_reward_mean: 48.300229702699696\n",
      "  episode_reward_min: -104.68929334198754\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2072\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47069963812828064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014109460636973381\n",
      "        model: {}\n",
      "        policy_loss: -0.09577256441116333\n",
      "        total_loss: 435.3192443847656\n",
      "        vf_explained_var: 0.8644813895225525\n",
      "        vf_loss: 435.3935241699219\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47235142118863\n",
      "    ram_util_percent: 33.40930232558139\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07307376385338028\n",
      "    mean_env_wait_ms: 123.74804464310259\n",
      "    mean_inference_ms: 1.5829841098636797\n",
      "    mean_raw_obs_processing_ms: 7.848583330616341\n",
      "  time_since_restore: 44132.87919163704\n",
      "  time_this_iter_s: 542.1578419208527\n",
      "  time_total_s: 44132.87919163704\n",
      "  timers:\n",
      "    learn_throughput: 352.516\n",
      "    learn_time_ms: 11347.014\n",
      "    load_throughput: 10202.807\n",
      "    load_time_ms: 392.049\n",
      "    sample_throughput: 7.607\n",
      "    sample_time_ms: 525855.659\n",
      "    update_time_ms: 3.397\n",
      "  timestamp: 1612514623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_09-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 165.53\n",
      "  episode_reward_max: 118.39952585646184\n",
      "  episode_reward_mean: 45.794819447490134\n",
      "  episode_reward_min: -106.68011168934471\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2101\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43637940287590027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01350040640681982\n",
      "        model: {}\n",
      "        policy_loss: -0.09329833835363388\n",
      "        total_loss: 647.7672119140625\n",
      "        vf_explained_var: 0.7851170897483826\n",
      "        vf_loss: 647.8400268554688\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.15417754569191\n",
      "    ram_util_percent: 33.49986945169713\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07310050388049698\n",
      "    mean_env_wait_ms: 123.73622660460843\n",
      "    mean_inference_ms: 1.5833470416518178\n",
      "    mean_raw_obs_processing_ms: 7.841971958682502\n",
      "  time_since_restore: 44669.73174738884\n",
      "  time_this_iter_s: 536.8525557518005\n",
      "  time_total_s: 44669.73174738884\n",
      "  timers:\n",
      "    learn_throughput: 355.638\n",
      "    learn_time_ms: 11247.386\n",
      "    load_throughput: 10219.253\n",
      "    load_time_ms: 391.418\n",
      "    sample_throughput: 7.608\n",
      "    sample_time_ms: 525742.594\n",
      "    update_time_ms: 3.4\n",
      "  timestamp: 1612515160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-01-35\n",
      "  done: false\n",
      "  episode_len_mean: 166.19\n",
      "  episode_reward_max: 118.39952585646184\n",
      "  episode_reward_mean: 47.47216913955871\n",
      "  episode_reward_min: -110.0385667633551\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2122\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.504813015460968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01282199565321207\n",
      "        model: {}\n",
      "        policy_loss: -0.0841396227478981\n",
      "        total_loss: 317.51995849609375\n",
      "        vf_explained_var: 0.8743911385536194\n",
      "        vf_loss: 317.58465576171875\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.71480996068152\n",
      "    ram_util_percent: 33.52228047182176\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07310948818985419\n",
      "    mean_env_wait_ms: 123.72407312995715\n",
      "    mean_inference_ms: 1.5835048873215822\n",
      "    mean_raw_obs_processing_ms: 7.837681735868208\n",
      "  time_since_restore: 45204.71982097626\n",
      "  time_this_iter_s: 534.9880735874176\n",
      "  time_total_s: 45204.71982097626\n",
      "  timers:\n",
      "    learn_throughput: 355.652\n",
      "    learn_time_ms: 11246.942\n",
      "    load_throughput: 10186.288\n",
      "    load_time_ms: 392.685\n",
      "    sample_throughput: 7.601\n",
      "    sample_time_ms: 526268.082\n",
      "    update_time_ms: 3.426\n",
      "  timestamp: 1612515695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 169.98\n",
      "  episode_reward_max: 118.39952585646184\n",
      "  episode_reward_mean: 45.01909461291011\n",
      "  episode_reward_min: -110.0385667633551\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2144\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4917673170566559\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016243910416960716\n",
      "        model: {}\n",
      "        policy_loss: -0.10276500880718231\n",
      "        total_loss: 698.1041259765625\n",
      "        vf_explained_var: 0.7730000019073486\n",
      "        vf_loss: 698.18212890625\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.05073041168659\n",
      "    ram_util_percent: 33.50756972111554\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0731145093384701\n",
      "    mean_env_wait_ms: 123.70508587138622\n",
      "    mean_inference_ms: 1.5835816493864439\n",
      "    mean_raw_obs_processing_ms: 7.831671235180192\n",
      "  time_since_restore: 45731.886502981186\n",
      "  time_this_iter_s: 527.1666820049286\n",
      "  time_total_s: 45731.886502981186\n",
      "  timers:\n",
      "    learn_throughput: 355.716\n",
      "    learn_time_ms: 11244.912\n",
      "    load_throughput: 10167.25\n",
      "    load_time_ms: 393.42\n",
      "    sample_throughput: 7.614\n",
      "    sample_time_ms: 525316.903\n",
      "    update_time_ms: 3.404\n",
      "  timestamp: 1612516222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 165.87\n",
      "  episode_reward_max: 118.39875978432302\n",
      "  episode_reward_mean: 49.11243322757423\n",
      "  episode_reward_min: -118.80019117388706\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2167\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4921671152114868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013242063112556934\n",
      "        model: {}\n",
      "        policy_loss: -0.09198800474405289\n",
      "        total_loss: 345.9383544921875\n",
      "        vf_explained_var: 0.8736403584480286\n",
      "        vf_loss: 346.0102844238281\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.93986842105264\n",
      "    ram_util_percent: 33.483289473684216\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07311359757598436\n",
      "    mean_env_wait_ms: 123.67958054903629\n",
      "    mean_inference_ms: 1.5835526818939918\n",
      "    mean_raw_obs_processing_ms: 7.825322211291081\n",
      "  time_since_restore: 46264.54206609726\n",
      "  time_this_iter_s: 532.6555631160736\n",
      "  time_total_s: 46264.54206609726\n",
      "  timers:\n",
      "    learn_throughput: 359.568\n",
      "    learn_time_ms: 11124.462\n",
      "    load_throughput: 10231.082\n",
      "    load_time_ms: 390.966\n",
      "    sample_throughput: 7.622\n",
      "    sample_time_ms: 524825.013\n",
      "    update_time_ms: 3.368\n",
      "  timestamp: 1612516755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 175.68\n",
      "  episode_reward_max: 118.39875978432302\n",
      "  episode_reward_mean: 44.983734830454665\n",
      "  episode_reward_min: -118.80019117388706\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2190\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49061694741249084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01270939689129591\n",
      "        model: {}\n",
      "        policy_loss: -0.09111148118972778\n",
      "        total_loss: 442.6617126464844\n",
      "        vf_explained_var: 0.857528805732727\n",
      "        vf_loss: 442.7335205078125\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.95776315789473\n",
      "    ram_util_percent: 33.55197368421053\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07310998563643412\n",
      "    mean_env_wait_ms: 123.65494697988262\n",
      "    mean_inference_ms: 1.5835050851244352\n",
      "    mean_raw_obs_processing_ms: 7.8151877920382296\n",
      "  time_since_restore: 46797.30516862869\n",
      "  time_this_iter_s: 532.7631025314331\n",
      "  time_total_s: 46797.30516862869\n",
      "  timers:\n",
      "    learn_throughput: 363.569\n",
      "    learn_time_ms: 11002.047\n",
      "    load_throughput: 10294.533\n",
      "    load_time_ms: 388.556\n",
      "    sample_throughput: 7.621\n",
      "    sample_time_ms: 524831.522\n",
      "    update_time_ms: 3.34\n",
      "  timestamp: 1612517288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-37-05\n",
      "  done: false\n",
      "  episode_len_mean: 177.4\n",
      "  episode_reward_max: 118.39330573874977\n",
      "  episode_reward_mean: 54.03081626591617\n",
      "  episode_reward_min: -118.80019117388706\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2214\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43843087553977966\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011277108453214169\n",
      "        model: {}\n",
      "        policy_loss: -0.07698755711317062\n",
      "        total_loss: 140.5449981689453\n",
      "        vf_explained_var: 0.9380444884300232\n",
      "        vf_loss: 140.6048583984375\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.52659713168188\n",
      "    ram_util_percent: 33.536114732724904\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07311131295298383\n",
      "    mean_env_wait_ms: 123.6291004714206\n",
      "    mean_inference_ms: 1.5835144047153449\n",
      "    mean_raw_obs_processing_ms: 7.804972430584581\n",
      "  time_since_restore: 47334.37443828583\n",
      "  time_this_iter_s: 537.069269657135\n",
      "  time_total_s: 47334.37443828583\n",
      "  timers:\n",
      "    learn_throughput: 363.697\n",
      "    learn_time_ms: 10998.158\n",
      "    load_throughput: 10381.76\n",
      "    load_time_ms: 385.291\n",
      "    sample_throughput: 7.621\n",
      "    sample_time_ms: 524835.21\n",
      "    update_time_ms: 3.333\n",
      "  timestamp: 1612517825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-46-00\n",
      "  done: false\n",
      "  episode_len_mean: 167.8\n",
      "  episode_reward_max: 118.39330573874977\n",
      "  episode_reward_mean: 58.5075748570565\n",
      "  episode_reward_min: -118.80019117388706\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2237\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4646144509315491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011272178031504154\n",
      "        model: {}\n",
      "        policy_loss: -0.08626303821802139\n",
      "        total_loss: 390.6555480957031\n",
      "        vf_explained_var: 0.8649099469184875\n",
      "        vf_loss: 390.7247619628906\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.13769633507853\n",
      "    ram_util_percent: 33.62054973821989\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07312122062187855\n",
      "    mean_env_wait_ms: 123.60765401247222\n",
      "    mean_inference_ms: 1.583691117185134\n",
      "    mean_raw_obs_processing_ms: 7.796761264695997\n",
      "  time_since_restore: 47869.60615777969\n",
      "  time_this_iter_s: 535.231719493866\n",
      "  time_total_s: 47869.60615777969\n",
      "  timers:\n",
      "    learn_throughput: 367.824\n",
      "    learn_time_ms: 10874.774\n",
      "    load_throughput: 10406.351\n",
      "    load_time_ms: 384.381\n",
      "    sample_throughput: 7.623\n",
      "    sample_time_ms: 524709.895\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1612518360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_10-54-47\n",
      "  done: false\n",
      "  episode_len_mean: 176.47\n",
      "  episode_reward_max: 118.39330573874977\n",
      "  episode_reward_mean: 54.522421512168485\n",
      "  episode_reward_min: -103.33691996267834\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2258\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49386706948280334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013150731101632118\n",
      "        model: {}\n",
      "        policy_loss: -0.09443396329879761\n",
      "        total_loss: 358.13323974609375\n",
      "        vf_explained_var: 0.8664000630378723\n",
      "        vf_loss: 358.2076721191406\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.95366178428762\n",
      "    ram_util_percent: 33.68615179760319\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07313347978496132\n",
      "    mean_env_wait_ms: 123.58747067991763\n",
      "    mean_inference_ms: 1.58394056153602\n",
      "    mean_raw_obs_processing_ms: 7.788115112830228\n",
      "  time_since_restore: 48396.16787457466\n",
      "  time_this_iter_s: 526.5617167949677\n",
      "  time_total_s: 48396.16787457466\n",
      "  timers:\n",
      "    learn_throughput: 367.777\n",
      "    learn_time_ms: 10876.143\n",
      "    load_throughput: 10462.311\n",
      "    load_time_ms: 382.325\n",
      "    sample_throughput: 7.648\n",
      "    sample_time_ms: 523043.186\n",
      "    update_time_ms: 3.278\n",
      "  timestamp: 1612518887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-03-36\n",
      "  done: false\n",
      "  episode_len_mean: 180.52\n",
      "  episode_reward_max: 118.39330573874977\n",
      "  episode_reward_mean: 60.48723542551229\n",
      "  episode_reward_min: -103.33691996267834\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2279\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46698805689811707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012117586098611355\n",
      "        model: {}\n",
      "        policy_loss: -0.0846501886844635\n",
      "        total_loss: 263.1509094238281\n",
      "        vf_explained_var: 0.8917548060417175\n",
      "        vf_loss: 263.2171630859375\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.1317880794702\n",
      "    ram_util_percent: 33.79205298013245\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07315151946888625\n",
      "    mean_env_wait_ms: 123.56571502942518\n",
      "    mean_inference_ms: 1.5842745095342496\n",
      "    mean_raw_obs_processing_ms: 7.778012452128267\n",
      "  time_since_restore: 48925.13585424423\n",
      "  time_this_iter_s: 528.9679796695709\n",
      "  time_total_s: 48925.13585424423\n",
      "  timers:\n",
      "    learn_throughput: 363.753\n",
      "    learn_time_ms: 10996.468\n",
      "    load_throughput: 10345.463\n",
      "    load_time_ms: 386.643\n",
      "    sample_throughput: 7.664\n",
      "    sample_time_ms: 521901.442\n",
      "    update_time_ms: 3.328\n",
      "  timestamp: 1612519416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-12-27\n",
      "  done: false\n",
      "  episode_len_mean: 177.29\n",
      "  episode_reward_max: 118.39330573874977\n",
      "  episode_reward_mean: 62.65284619949735\n",
      "  episode_reward_min: -103.33691996267834\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2302\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46299833059310913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010728234425187111\n",
      "        model: {}\n",
      "        policy_loss: -0.07661561667919159\n",
      "        total_loss: 246.79835510253906\n",
      "        vf_explained_var: 0.9073380827903748\n",
      "        vf_loss: 246.85867309570312\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.11003963011889\n",
      "    ram_util_percent: 33.74504623513871\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07317766887146947\n",
      "    mean_env_wait_ms: 123.53954403222966\n",
      "    mean_inference_ms: 1.584748641259624\n",
      "    mean_raw_obs_processing_ms: 7.766690957496687\n",
      "  time_since_restore: 49455.72194766998\n",
      "  time_this_iter_s: 530.5860934257507\n",
      "  time_total_s: 49455.72194766998\n",
      "  timers:\n",
      "    learn_throughput: 363.866\n",
      "    learn_time_ms: 10993.05\n",
      "    load_throughput: 10379.732\n",
      "    load_time_ms: 385.366\n",
      "    sample_throughput: 7.681\n",
      "    sample_time_ms: 520749.671\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1612519947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 179.99\n",
      "  episode_reward_max: 118.3918709184116\n",
      "  episode_reward_mean: 62.790753701482636\n",
      "  episode_reward_min: -100.99494360790918\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2325\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47042641043663025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011483408510684967\n",
      "        model: {}\n",
      "        policy_loss: -0.08123757690191269\n",
      "        total_loss: 139.9750518798828\n",
      "        vf_explained_var: 0.9238725900650024\n",
      "        vf_loss: 140.0388641357422\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.49844760672703\n",
      "    ram_util_percent: 33.85821474773609\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07320404312605432\n",
      "    mean_env_wait_ms: 123.51726500556335\n",
      "    mean_inference_ms: 1.5852471900181024\n",
      "    mean_raw_obs_processing_ms: 7.755209517836902\n",
      "  time_since_restore: 49997.22968506813\n",
      "  time_this_iter_s: 541.5077373981476\n",
      "  time_total_s: 49997.22968506813\n",
      "  timers:\n",
      "    learn_throughput: 363.871\n",
      "    learn_time_ms: 10992.901\n",
      "    load_throughput: 10301.748\n",
      "    load_time_ms: 388.284\n",
      "    sample_throughput: 7.674\n",
      "    sample_time_ms: 521210.45\n",
      "    update_time_ms: 3.366\n",
      "  timestamp: 1612520488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 178.5\n",
      "  episode_reward_max: 118.3918709184116\n",
      "  episode_reward_mean: 62.80350493787742\n",
      "  episode_reward_min: -103.254811073986\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2347\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4721044600009918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011461609974503517\n",
      "        model: {}\n",
      "        policy_loss: -0.07792910188436508\n",
      "        total_loss: 355.20513916015625\n",
      "        vf_explained_var: 0.8380382657051086\n",
      "        vf_loss: 355.26568603515625\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.92961992136304\n",
      "    ram_util_percent: 33.88872870249017\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07323060594193719\n",
      "    mean_env_wait_ms: 123.4983085955133\n",
      "    mean_inference_ms: 1.5857359268923539\n",
      "    mean_raw_obs_processing_ms: 7.744564944665362\n",
      "  time_since_restore: 50532.08799123764\n",
      "  time_this_iter_s: 534.8583061695099\n",
      "  time_total_s: 50532.08799123764\n",
      "  timers:\n",
      "    learn_throughput: 363.893\n",
      "    learn_time_ms: 10992.238\n",
      "    load_throughput: 10356.098\n",
      "    load_time_ms: 386.246\n",
      "    sample_throughput: 7.675\n",
      "    sample_time_ms: 521202.652\n",
      "    update_time_ms: 3.319\n",
      "  timestamp: 1612521023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-39-20\n",
      "  done: false\n",
      "  episode_len_mean: 175.42\n",
      "  episode_reward_max: 118.3918709184116\n",
      "  episode_reward_mean: 65.06325111804091\n",
      "  episode_reward_min: -103.254811073986\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2371\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4408249258995056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010391505435109138\n",
      "        model: {}\n",
      "        policy_loss: -0.07663564383983612\n",
      "        total_loss: 145.11001586914062\n",
      "        vf_explained_var: 0.9271512031555176\n",
      "        vf_loss: 145.17088317871094\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.01660130718953\n",
      "    ram_util_percent: 34.01803921568627\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07326034980619438\n",
      "    mean_env_wait_ms: 123.48185154536594\n",
      "    mean_inference_ms: 1.5862813796675856\n",
      "    mean_raw_obs_processing_ms: 7.736219919642743\n",
      "  time_since_restore: 51068.08253145218\n",
      "  time_this_iter_s: 535.9945402145386\n",
      "  time_total_s: 51068.08253145218\n",
      "  timers:\n",
      "    learn_throughput: 363.856\n",
      "    learn_time_ms: 10993.362\n",
      "    load_throughput: 10344.693\n",
      "    load_time_ms: 386.672\n",
      "    sample_throughput: 7.662\n",
      "    sample_time_ms: 522087.453\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1612521560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 178.77\n",
      "  episode_reward_max: 118.39232283191318\n",
      "  episode_reward_mean: 69.43194717300712\n",
      "  episode_reward_min: -103.254811073986\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2392\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45436814427375793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010390931740403175\n",
      "        model: {}\n",
      "        policy_loss: -0.06645601242780685\n",
      "        total_loss: 265.43096923828125\n",
      "        vf_explained_var: 0.8633635640144348\n",
      "        vf_loss: 265.4816589355469\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.08216145833333\n",
      "    ram_util_percent: 34.032031249999996\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07328459953196464\n",
      "    mean_env_wait_ms: 123.47312809999168\n",
      "    mean_inference_ms: 1.5867558002898812\n",
      "    mean_raw_obs_processing_ms: 7.728628165930267\n",
      "  time_since_restore: 51606.19145321846\n",
      "  time_this_iter_s: 538.1089217662811\n",
      "  time_total_s: 51606.19145321846\n",
      "  timers:\n",
      "    learn_throughput: 359.954\n",
      "    learn_time_ms: 11112.546\n",
      "    load_throughput: 10281.735\n",
      "    load_time_ms: 389.039\n",
      "    sample_throughput: 7.655\n",
      "    sample_time_ms: 522507.817\n",
      "    update_time_ms: 3.368\n",
      "  timestamp: 1612522098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_11-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 177.77\n",
      "  episode_reward_max: 118.39232283191318\n",
      "  episode_reward_mean: 63.207443176511916\n",
      "  episode_reward_min: -104.76577467583313\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2415\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49953269958496094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01077390369027853\n",
      "        model: {}\n",
      "        policy_loss: -0.08400951325893402\n",
      "        total_loss: 378.93011474609375\n",
      "        vf_explained_var: 0.863498866558075\n",
      "        vf_loss: 378.9977722167969\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91814671814672\n",
      "    ram_util_percent: 34.13397683397684\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07331278827994059\n",
      "    mean_env_wait_ms: 123.46794881491638\n",
      "    mean_inference_ms: 1.5872938648198556\n",
      "    mean_raw_obs_processing_ms: 7.720369224003334\n",
      "  time_since_restore: 52150.47817993164\n",
      "  time_this_iter_s: 544.2867267131805\n",
      "  time_total_s: 52150.47817993164\n",
      "  timers:\n",
      "    learn_throughput: 359.924\n",
      "    learn_time_ms: 11113.457\n",
      "    load_throughput: 10207.928\n",
      "    load_time_ms: 391.852\n",
      "    sample_throughput: 7.639\n",
      "    sample_time_ms: 523659.708\n",
      "    update_time_ms: 3.361\n",
      "  timestamp: 1612522642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-06-20\n",
      "  done: false\n",
      "  episode_len_mean: 177.23\n",
      "  episode_reward_max: 118.39608592787857\n",
      "  episode_reward_mean: 65.26199185845095\n",
      "  episode_reward_min: -104.76577467583313\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2438\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4656931459903717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01228333916515112\n",
      "        model: {}\n",
      "        policy_loss: -0.0844133049249649\n",
      "        total_loss: 407.1094665527344\n",
      "        vf_explained_var: 0.8447543978691101\n",
      "        vf_loss: 407.1751708984375\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.09440104166667\n",
      "    ram_util_percent: 34.31302083333333\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0733416643543396\n",
      "    mean_env_wait_ms: 123.46277111095898\n",
      "    mean_inference_ms: 1.587797345114812\n",
      "    mean_raw_obs_processing_ms: 7.712574616025298\n",
      "  time_since_restore: 52688.53365659714\n",
      "  time_this_iter_s: 538.0554766654968\n",
      "  time_total_s: 52688.53365659714\n",
      "  timers:\n",
      "    learn_throughput: 359.884\n",
      "    learn_time_ms: 11114.706\n",
      "    load_throughput: 10099.483\n",
      "    load_time_ms: 396.06\n",
      "    sample_throughput: 7.637\n",
      "    sample_time_ms: 523749.516\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1612523180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-15-16\n",
      "  done: false\n",
      "  episode_len_mean: 176.48\n",
      "  episode_reward_max: 118.39608592787857\n",
      "  episode_reward_mean: 71.56799153199923\n",
      "  episode_reward_min: -104.76577467583313\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2462\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4029724895954132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010297095403075218\n",
      "        model: {}\n",
      "        policy_loss: -0.07316962629556656\n",
      "        total_loss: 37.18146514892578\n",
      "        vf_explained_var: 0.975229024887085\n",
      "        vf_loss: 37.23899459838867\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.30497382198953\n",
      "    ram_util_percent: 34.43782722513089\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07336988217481542\n",
      "    mean_env_wait_ms: 123.45728626078485\n",
      "    mean_inference_ms: 1.5882900632120576\n",
      "    mean_raw_obs_processing_ms: 7.705191226550535\n",
      "  time_since_restore: 53224.08219957352\n",
      "  time_this_iter_s: 535.5485429763794\n",
      "  time_total_s: 53224.08219957352\n",
      "  timers:\n",
      "    learn_throughput: 359.842\n",
      "    learn_time_ms: 11115.987\n",
      "    load_throughput: 10163.806\n",
      "    load_time_ms: 393.553\n",
      "    sample_throughput: 7.637\n",
      "    sample_time_ms: 523781.017\n",
      "    update_time_ms: 3.351\n",
      "  timestamp: 1612523716\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-24-15\n",
      "  done: false\n",
      "  episode_len_mean: 176.6\n",
      "  episode_reward_max: 118.39608592787857\n",
      "  episode_reward_mean: 69.63830210128823\n",
      "  episode_reward_min: -104.76577467583313\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2484\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44125160574913025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011542520485818386\n",
      "        model: {}\n",
      "        policy_loss: -0.08256565034389496\n",
      "        total_loss: 33.8189811706543\n",
      "        vf_explained_var: 0.9888178706169128\n",
      "        vf_loss: 33.8840217590332\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07568270481144\n",
      "    ram_util_percent: 34.48751625487647\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07339629704949498\n",
      "    mean_env_wait_ms: 123.45396895971498\n",
      "    mean_inference_ms: 1.588756269015302\n",
      "    mean_raw_obs_processing_ms: 7.698211201348513\n",
      "  time_since_restore: 53763.10483908653\n",
      "  time_this_iter_s: 539.0226395130157\n",
      "  time_total_s: 53763.10483908653\n",
      "  timers:\n",
      "    learn_throughput: 359.769\n",
      "    learn_time_ms: 11118.242\n",
      "    load_throughput: 10139.401\n",
      "    load_time_ms: 394.501\n",
      "    sample_throughput: 7.619\n",
      "    sample_time_ms: 525024.209\n",
      "    update_time_ms: 3.341\n",
      "  timestamp: 1612524255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-33-27\n",
      "  done: false\n",
      "  episode_len_mean: 168.27\n",
      "  episode_reward_max: 118.39608592787857\n",
      "  episode_reward_mean: 73.30014505998749\n",
      "  episode_reward_min: -105.12614933020622\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2511\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.428167462348938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014454951509833336\n",
      "        model: {}\n",
      "        policy_loss: -0.09028702229261398\n",
      "        total_loss: 605.248291015625\n",
      "        vf_explained_var: 0.778565526008606\n",
      "        vf_loss: 605.3165283203125\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.7876747141042\n",
      "    ram_util_percent: 34.52249047013977\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07342783438781166\n",
      "    mean_env_wait_ms: 123.45178133438768\n",
      "    mean_inference_ms: 1.5892867636596566\n",
      "    mean_raw_obs_processing_ms: 7.693873064652319\n",
      "  time_since_restore: 54314.459030628204\n",
      "  time_this_iter_s: 551.3541915416718\n",
      "  time_total_s: 54314.459030628204\n",
      "  timers:\n",
      "    learn_throughput: 359.696\n",
      "    learn_time_ms: 11120.498\n",
      "    load_throughput: 10245.237\n",
      "    load_time_ms: 390.425\n",
      "    sample_throughput: 7.586\n",
      "    sample_time_ms: 527264.443\n",
      "    update_time_ms: 3.38\n",
      "  timestamp: 1612524807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 160.04\n",
      "  episode_reward_max: 118.39002500193679\n",
      "  episode_reward_mean: 64.81813711120469\n",
      "  episode_reward_min: -105.12614933020622\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2537\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47492772340774536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018682101741433144\n",
      "        model: {}\n",
      "        policy_loss: -0.11153403669595718\n",
      "        total_loss: 1388.706298828125\n",
      "        vf_explained_var: 0.6788126826286316\n",
      "        vf_loss: 1388.7894287109375\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.60871261378414\n",
      "    ram_util_percent: 34.581664499349806\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07345136300061272\n",
      "    mean_env_wait_ms: 123.44707926778133\n",
      "    mean_inference_ms: 1.589732780545682\n",
      "    mean_raw_obs_processing_ms: 7.692230369364151\n",
      "  time_since_restore: 54853.193388938904\n",
      "  time_this_iter_s: 538.7343583106995\n",
      "  time_total_s: 54853.193388938904\n",
      "  timers:\n",
      "    learn_throughput: 359.579\n",
      "    learn_time_ms: 11124.112\n",
      "    load_throughput: 10229.425\n",
      "    load_time_ms: 391.029\n",
      "    sample_throughput: 7.575\n",
      "    sample_time_ms: 528073.752\n",
      "    update_time_ms: 3.44\n",
      "  timestamp: 1612525346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_12-51-30\n",
      "  done: false\n",
      "  episode_len_mean: 155.2\n",
      "  episode_reward_max: 118.39761442366546\n",
      "  episode_reward_mean: 43.49401718795762\n",
      "  episode_reward_min: -105.75514678694614\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2564\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.510195791721344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016079414635896683\n",
      "        model: {}\n",
      "        policy_loss: -0.10932323336601257\n",
      "        total_loss: 928.6104125976562\n",
      "        vf_explained_var: 0.7320625185966492\n",
      "        vf_loss: 928.6951904296875\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.51222651222652\n",
      "    ram_util_percent: 34.49498069498069\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07347553262842572\n",
      "    mean_env_wait_ms: 123.44598560141044\n",
      "    mean_inference_ms: 1.5901655510520405\n",
      "    mean_raw_obs_processing_ms: 7.69349016646597\n",
      "  time_since_restore: 55397.72618365288\n",
      "  time_this_iter_s: 544.532794713974\n",
      "  time_total_s: 55397.72618365288\n",
      "  timers:\n",
      "    learn_throughput: 359.463\n",
      "    learn_time_ms: 11127.701\n",
      "    load_throughput: 10290.948\n",
      "    load_time_ms: 388.691\n",
      "    sample_throughput: 7.57\n",
      "    sample_time_ms: 528374.293\n",
      "    update_time_ms: 3.466\n",
      "  timestamp: 1612525890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-00-32\n",
      "  done: false\n",
      "  episode_len_mean: 147.54\n",
      "  episode_reward_max: 118.39761442366546\n",
      "  episode_reward_mean: 24.36709690178275\n",
      "  episode_reward_min: -105.75514678694614\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2591\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5020962953567505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016460176557302475\n",
      "        model: {}\n",
      "        policy_loss: -0.11453396081924438\n",
      "        total_loss: 803.754638671875\n",
      "        vf_explained_var: 0.8052867650985718\n",
      "        vf_loss: 803.8442993164062\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.71578266494178\n",
      "    ram_util_percent: 34.59262613195343\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07349812112034199\n",
      "    mean_env_wait_ms: 123.44192992809727\n",
      "    mean_inference_ms: 1.5905145556777018\n",
      "    mean_raw_obs_processing_ms: 7.698345120409369\n",
      "  time_since_restore: 55939.362355709076\n",
      "  time_this_iter_s: 541.6361720561981\n",
      "  time_total_s: 55939.362355709076\n",
      "  timers:\n",
      "    learn_throughput: 359.46\n",
      "    learn_time_ms: 11127.79\n",
      "    load_throughput: 10321.092\n",
      "    load_time_ms: 387.556\n",
      "    sample_throughput: 7.561\n",
      "    sample_time_ms: 529051.417\n",
      "    update_time_ms: 3.466\n",
      "  timestamp: 1612526432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 153.38\n",
      "  episode_reward_max: 118.39761442366546\n",
      "  episode_reward_mean: 24.64420850131832\n",
      "  episode_reward_min: -105.75514678694614\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2615\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4695769250392914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012286829762160778\n",
      "        model: {}\n",
      "        policy_loss: -0.09180513769388199\n",
      "        total_loss: 282.4839782714844\n",
      "        vf_explained_var: 0.9158528447151184\n",
      "        vf_loss: 282.55706787109375\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.6015306122449\n",
      "    ram_util_percent: 34.75829081632653\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07351763419982275\n",
      "    mean_env_wait_ms: 123.44014801598607\n",
      "    mean_inference_ms: 1.590812914483688\n",
      "    mean_raw_obs_processing_ms: 7.700830813246182\n",
      "  time_since_restore: 56488.35331821442\n",
      "  time_this_iter_s: 548.9909625053406\n",
      "  time_total_s: 56488.35331821442\n",
      "  timers:\n",
      "    learn_throughput: 355.557\n",
      "    learn_time_ms: 11249.939\n",
      "    load_throughput: 10257.963\n",
      "    load_time_ms: 389.941\n",
      "    sample_throughput: 7.544\n",
      "    sample_time_ms: 530222.443\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1612526981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-18-43\n",
      "  done: false\n",
      "  episode_len_mean: 155.31\n",
      "  episode_reward_max: 118.39761442366546\n",
      "  episode_reward_mean: 30.999357187122563\n",
      "  episode_reward_min: -105.75514678694614\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2641\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4530840218067169\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01288775447756052\n",
      "        model: {}\n",
      "        policy_loss: -0.09852475672960281\n",
      "        total_loss: 172.53660583496094\n",
      "        vf_explained_var: 0.9489732384681702\n",
      "        vf_loss: 172.61557006835938\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.17629533678758\n",
      "    ram_util_percent: 34.85906735751295\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07354246665636033\n",
      "    mean_env_wait_ms: 123.43826412751535\n",
      "    mean_inference_ms: 1.59114796685809\n",
      "    mean_raw_obs_processing_ms: 7.703711929779576\n",
      "  time_since_restore: 57029.54025435448\n",
      "  time_this_iter_s: 541.1869361400604\n",
      "  time_total_s: 57029.54025435448\n",
      "  timers:\n",
      "    learn_throughput: 355.472\n",
      "    learn_time_ms: 11252.642\n",
      "    load_throughput: 10283.065\n",
      "    load_time_ms: 388.989\n",
      "    sample_throughput: 7.54\n",
      "    sample_time_ms: 530528.812\n",
      "    update_time_ms: 3.517\n",
      "  timestamp: 1612527523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 148.19\n",
      "  episode_reward_max: 118.39499602379124\n",
      "  episode_reward_mean: 31.282302654198165\n",
      "  episode_reward_min: -99.53432473050825\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2671\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49567660689353943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01347724162042141\n",
      "        model: {}\n",
      "        policy_loss: -0.09978564083576202\n",
      "        total_loss: 670.3734741210938\n",
      "        vf_explained_var: 0.8293397426605225\n",
      "        vf_loss: 670.4528198242188\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07997448979592\n",
      "    ram_util_percent: 34.93520408163265\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0735705471794638\n",
      "    mean_env_wait_ms: 123.43631189565362\n",
      "    mean_inference_ms: 1.5915633161829539\n",
      "    mean_raw_obs_processing_ms: 7.709349234432236\n",
      "  time_since_restore: 57578.452746629715\n",
      "  time_this_iter_s: 548.912492275238\n",
      "  time_total_s: 57578.452746629715\n",
      "  timers:\n",
      "    learn_throughput: 351.682\n",
      "    learn_time_ms: 11373.912\n",
      "    load_throughput: 10352.094\n",
      "    load_time_ms: 386.395\n",
      "    sample_throughput: 7.535\n",
      "    sample_time_ms: 530868.34\n",
      "    update_time_ms: 3.757\n",
      "  timestamp: 1612528072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 153.2\n",
      "  episode_reward_max: 118.39499602379124\n",
      "  episode_reward_mean: 39.779188607717515\n",
      "  episode_reward_min: -99.53432473050825\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2695\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4514172375202179\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009419534355401993\n",
      "        model: {}\n",
      "        policy_loss: -0.08280853927135468\n",
      "        total_loss: 363.3909606933594\n",
      "        vf_explained_var: 0.8470445275306702\n",
      "        vf_loss: 363.4595031738281\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.34032869785082\n",
      "    ram_util_percent: 35.17838179519595\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07359140296842412\n",
      "    mean_env_wait_ms: 123.44285908029377\n",
      "    mean_inference_ms: 1.591921200004955\n",
      "    mean_raw_obs_processing_ms: 7.71230260193332\n",
      "  time_since_restore: 58133.00604891777\n",
      "  time_this_iter_s: 554.5533022880554\n",
      "  time_total_s: 58133.00604891777\n",
      "  timers:\n",
      "    learn_throughput: 351.635\n",
      "    learn_time_ms: 11375.448\n",
      "    load_throughput: 10399.718\n",
      "    load_time_ms: 384.626\n",
      "    sample_throughput: 7.511\n",
      "    sample_time_ms: 532516.901\n",
      "    update_time_ms: 3.759\n",
      "  timestamp: 1612528626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-46-17\n",
      "  done: false\n",
      "  episode_len_mean: 152.7\n",
      "  episode_reward_max: 118.39499602379124\n",
      "  episode_reward_mean: 39.70487245539667\n",
      "  episode_reward_min: -105.42684537587338\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2719\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4496541917324066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01174901518970728\n",
      "        model: {}\n",
      "        policy_loss: -0.09148486703634262\n",
      "        total_loss: 129.99533081054688\n",
      "        vf_explained_var: 0.9561825394630432\n",
      "        vf_loss: 130.0689697265625\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48254777070063\n",
      "    ram_util_percent: 35.19108280254777\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07361295709373114\n",
      "    mean_env_wait_ms: 123.45155904001034\n",
      "    mean_inference_ms: 1.592303635359251\n",
      "    mean_raw_obs_processing_ms: 7.7150242654227394\n",
      "  time_since_restore: 58683.27928709984\n",
      "  time_this_iter_s: 550.2732381820679\n",
      "  time_total_s: 58683.27928709984\n",
      "  timers:\n",
      "    learn_throughput: 351.576\n",
      "    learn_time_ms: 11377.34\n",
      "    load_throughput: 10413.169\n",
      "    load_time_ms: 384.129\n",
      "    sample_throughput: 7.491\n",
      "    sample_time_ms: 533987.002\n",
      "    update_time_ms: 3.796\n",
      "  timestamp: 1612529177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_13-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 152.65\n",
      "  episode_reward_max: 118.38972302378619\n",
      "  episode_reward_mean: 43.99568414073358\n",
      "  episode_reward_min: -105.42684537587338\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2746\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42632046341896057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009339649230241776\n",
      "        model: {}\n",
      "        policy_loss: -0.07464350759983063\n",
      "        total_loss: 617.6470947265625\n",
      "        vf_explained_var: 0.8159834742546082\n",
      "        vf_loss: 617.7074584960938\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0476129032258\n",
      "    ram_util_percent: 35.27716129032258\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07363609555151292\n",
      "    mean_env_wait_ms: 123.46211897206959\n",
      "    mean_inference_ms: 1.5927238926501568\n",
      "    mean_raw_obs_processing_ms: 7.718046162488679\n",
      "  time_since_restore: 59225.9187169075\n",
      "  time_this_iter_s: 542.639429807663\n",
      "  time_total_s: 59225.9187169075\n",
      "  timers:\n",
      "    learn_throughput: 351.633\n",
      "    learn_time_ms: 11375.481\n",
      "    load_throughput: 10441.751\n",
      "    load_time_ms: 383.078\n",
      "    sample_throughput: 7.486\n",
      "    sample_time_ms: 534348.955\n",
      "    update_time_ms: 3.83\n",
      "  timestamp: 1612529720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 157.74\n",
      "  episode_reward_max: 118.38972302378619\n",
      "  episode_reward_mean: 60.75943465117197\n",
      "  episode_reward_min: -105.42684537587338\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2772\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40929967164993286\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009299209341406822\n",
      "        model: {}\n",
      "        policy_loss: -0.07331255078315735\n",
      "        total_loss: 64.7305679321289\n",
      "        vf_explained_var: 0.9752508401870728\n",
      "        vf_loss: 64.78975677490234\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.17109677419354\n",
      "    ram_util_percent: 35.38116129032258\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07365761598309549\n",
      "    mean_env_wait_ms: 123.47224262528492\n",
      "    mean_inference_ms: 1.5931228672309516\n",
      "    mean_raw_obs_processing_ms: 7.7185214620963425\n",
      "  time_since_restore: 59769.35050749779\n",
      "  time_this_iter_s: 543.4317905902863\n",
      "  time_total_s: 59769.35050749779\n",
      "  timers:\n",
      "    learn_throughput: 355.423\n",
      "    learn_time_ms: 11254.204\n",
      "    load_throughput: 10441.413\n",
      "    load_time_ms: 383.09\n",
      "    sample_throughput: 7.495\n",
      "    sample_time_ms: 533675.662\n",
      "    update_time_ms: 3.738\n",
      "  timestamp: 1612530263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-13-32\n",
      "  done: false\n",
      "  episode_len_mean: 160.56\n",
      "  episode_reward_max: 118.39997654795954\n",
      "  episode_reward_mean: 62.73993219457251\n",
      "  episode_reward_min: -105.42684537587338\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2795\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4482981860637665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01006897259503603\n",
      "        model: {}\n",
      "        policy_loss: -0.08057627826929092\n",
      "        total_loss: 79.91790771484375\n",
      "        vf_explained_var: 0.9702785611152649\n",
      "        vf_loss: 79.98319244384766\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.74201787994892\n",
      "    ram_util_percent: 35.3882503192848\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07367792020003192\n",
      "    mean_env_wait_ms: 123.47865633465719\n",
      "    mean_inference_ms: 1.5934622851239286\n",
      "    mean_raw_obs_processing_ms: 7.718260915199462\n",
      "  time_since_restore: 60317.79840660095\n",
      "  time_this_iter_s: 548.4478991031647\n",
      "  time_total_s: 60317.79840660095\n",
      "  timers:\n",
      "    learn_throughput: 355.413\n",
      "    learn_time_ms: 11254.512\n",
      "    load_throughput: 10416.043\n",
      "    load_time_ms: 384.023\n",
      "    sample_throughput: 7.482\n",
      "    sample_time_ms: 534646.357\n",
      "    update_time_ms: 3.709\n",
      "  timestamp: 1612530812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-22-37\n",
      "  done: false\n",
      "  episode_len_mean: 155.09\n",
      "  episode_reward_max: 118.39997654795954\n",
      "  episode_reward_mean: 64.88891235545474\n",
      "  episode_reward_min: -100.92859175629619\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2822\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43689316511154175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011276992969214916\n",
      "        model: {}\n",
      "        policy_loss: -0.09028776735067368\n",
      "        total_loss: 210.50767517089844\n",
      "        vf_explained_var: 0.9320527911186218\n",
      "        vf_loss: 210.58079528808594\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.2992297817715\n",
      "    ram_util_percent: 35.43581514762515\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07369985004822169\n",
      "    mean_env_wait_ms: 123.4811735035526\n",
      "    mean_inference_ms: 1.593826878799848\n",
      "    mean_raw_obs_processing_ms: 7.719942615224909\n",
      "  time_since_restore: 60863.296265125275\n",
      "  time_this_iter_s: 545.4978585243225\n",
      "  time_total_s: 60863.296265125275\n",
      "  timers:\n",
      "    learn_throughput: 355.473\n",
      "    learn_time_ms: 11252.626\n",
      "    load_throughput: 10362.907\n",
      "    load_time_ms: 385.992\n",
      "    sample_throughput: 7.48\n",
      "    sample_time_ms: 534745.277\n",
      "    update_time_ms: 3.677\n",
      "  timestamp: 1612531357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-31-41\n",
      "  done: false\n",
      "  episode_len_mean: 155.86\n",
      "  episode_reward_max: 118.39997654795954\n",
      "  episode_reward_mean: 69.01958932811293\n",
      "  episode_reward_min: -100.92859175629619\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2849\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4313434064388275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010203427635133266\n",
      "        model: {}\n",
      "        policy_loss: -0.08482225239276886\n",
      "        total_loss: 121.26283264160156\n",
      "        vf_explained_var: 0.9628395438194275\n",
      "        vf_loss: 121.33214569091797\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47806451612904\n",
      "    ram_util_percent: 35.533806451612904\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07371998323461908\n",
      "    mean_env_wait_ms: 123.48395072187469\n",
      "    mean_inference_ms: 1.5941950335322792\n",
      "    mean_raw_obs_processing_ms: 7.721632270277773\n",
      "  time_since_restore: 61406.53528046608\n",
      "  time_this_iter_s: 543.239015340805\n",
      "  time_total_s: 61406.53528046608\n",
      "  timers:\n",
      "    learn_throughput: 355.433\n",
      "    learn_time_ms: 11253.89\n",
      "    load_throughput: 10373.428\n",
      "    load_time_ms: 385.601\n",
      "    sample_throughput: 7.478\n",
      "    sample_time_ms: 534901.494\n",
      "    update_time_ms: 3.71\n",
      "  timestamp: 1612531901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 159.31\n",
      "  episode_reward_max: 118.39997654795954\n",
      "  episode_reward_mean: 62.75910748426091\n",
      "  episode_reward_min: -105.17029777075086\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2872\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4555119574069977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009635505266487598\n",
      "        model: {}\n",
      "        policy_loss: -0.08456302434206009\n",
      "        total_loss: 127.25492095947266\n",
      "        vf_explained_var: 0.9565321803092957\n",
      "        vf_loss: 127.32484436035156\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.82282471626733\n",
      "    ram_util_percent: 35.63934426229509\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07373917129472694\n",
      "    mean_env_wait_ms: 123.49331142262422\n",
      "    mean_inference_ms: 1.5944980749132327\n",
      "    mean_raw_obs_processing_ms: 7.7213721265899995\n",
      "  time_since_restore: 61961.75388097763\n",
      "  time_this_iter_s: 555.2186005115509\n",
      "  time_total_s: 61961.75388097763\n",
      "  timers:\n",
      "    learn_throughput: 355.371\n",
      "    learn_time_ms: 11255.856\n",
      "    load_throughput: 10342.24\n",
      "    load_time_ms: 386.763\n",
      "    sample_throughput: 7.469\n",
      "    sample_time_ms: 535520.727\n",
      "    update_time_ms: 3.681\n",
      "  timestamp: 1612532456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-49-58\n",
      "  done: false\n",
      "  episode_len_mean: 156.13\n",
      "  episode_reward_max: 118.39829112414093\n",
      "  episode_reward_mean: 60.37340062842288\n",
      "  episode_reward_min: -105.17029777075086\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 2896\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43210741877555847\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011653155088424683\n",
      "        model: {}\n",
      "        policy_loss: -0.09110958129167557\n",
      "        total_loss: 167.46995544433594\n",
      "        vf_explained_var: 0.9453206062316895\n",
      "        vf_loss: 167.5434112548828\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.64080310880829\n",
      "    ram_util_percent: 35.70647668393782\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07375866164213628\n",
      "    mean_env_wait_ms: 123.4985160016458\n",
      "    mean_inference_ms: 1.5948321480295282\n",
      "    mean_raw_obs_processing_ms: 7.721704768010804\n",
      "  time_since_restore: 62503.07709145546\n",
      "  time_this_iter_s: 541.323210477829\n",
      "  time_total_s: 62503.07709145546\n",
      "  timers:\n",
      "    learn_throughput: 359.147\n",
      "    learn_time_ms: 11137.511\n",
      "    load_throughput: 10394.963\n",
      "    load_time_ms: 384.802\n",
      "    sample_throughput: 7.467\n",
      "    sample_time_ms: 535656.201\n",
      "    update_time_ms: 3.661\n",
      "  timestamp: 1612532998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_14-59-02\n",
      "  done: false\n",
      "  episode_len_mean: 159.75\n",
      "  episode_reward_max: 118.39829112414093\n",
      "  episode_reward_mean: 62.08312986136233\n",
      "  episode_reward_min: -106.61622257819715\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2922\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4464707672595978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012250366620719433\n",
      "        model: {}\n",
      "        policy_loss: -0.09532641619443893\n",
      "        total_loss: 196.49148559570312\n",
      "        vf_explained_var: 0.9257437586784363\n",
      "        vf_loss: 196.56820678710938\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47593307593309\n",
      "    ram_util_percent: 35.733333333333334\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07377896479283122\n",
      "    mean_env_wait_ms: 123.50410466567175\n",
      "    mean_inference_ms: 1.5951867769776045\n",
      "    mean_raw_obs_processing_ms: 7.7214165837066\n",
      "  time_since_restore: 63047.41296958923\n",
      "  time_this_iter_s: 544.3358781337738\n",
      "  time_total_s: 63047.41296958923\n",
      "  timers:\n",
      "    learn_throughput: 362.991\n",
      "    learn_time_ms: 11019.545\n",
      "    load_throughput: 10433.879\n",
      "    load_time_ms: 383.367\n",
      "    sample_throughput: 7.472\n",
      "    sample_time_ms: 535322.309\n",
      "    update_time_ms: 3.427\n",
      "  timestamp: 1612533542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 167.84\n",
      "  episode_reward_max: 118.39829112414093\n",
      "  episode_reward_mean: 66.42188450934577\n",
      "  episode_reward_min: -106.61622257819715\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2944\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4301445484161377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009143443778157234\n",
      "        model: {}\n",
      "        policy_loss: -0.07237754762172699\n",
      "        total_loss: 370.1715087890625\n",
      "        vf_explained_var: 0.7425861358642578\n",
      "        vf_loss: 370.2300109863281\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10964467005077\n",
      "    ram_util_percent: 35.811421319796956\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07379746506716205\n",
      "    mean_env_wait_ms: 123.51592592851557\n",
      "    mean_inference_ms: 1.5954883927715235\n",
      "    mean_raw_obs_processing_ms: 7.7182696185776525\n",
      "  time_since_restore: 63599.6956987381\n",
      "  time_this_iter_s: 552.2827291488647\n",
      "  time_total_s: 63599.6956987381\n",
      "  timers:\n",
      "    learn_throughput: 362.911\n",
      "    learn_time_ms: 11021.986\n",
      "    load_throughput: 10415.214\n",
      "    load_time_ms: 384.054\n",
      "    sample_throughput: 7.475\n",
      "    sample_time_ms: 535093.755\n",
      "    update_time_ms: 3.461\n",
      "  timestamp: 1612534095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-17-21\n",
      "  done: false\n",
      "  episode_len_mean: 166.91\n",
      "  episode_reward_max: 118.39393077861043\n",
      "  episode_reward_mean: 68.61526888676761\n",
      "  episode_reward_min: -106.61622257819715\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2969\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44245895743370056\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01040991023182869\n",
      "        model: {}\n",
      "        policy_loss: -0.0845080241560936\n",
      "        total_loss: 504.7536926269531\n",
      "        vf_explained_var: 0.7994770407676697\n",
      "        vf_loss: 504.8222961425781\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.60294871794872\n",
      "    ram_util_percent: 35.88782051282051\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07381814935752642\n",
      "    mean_env_wait_ms: 123.52538116640424\n",
      "    mean_inference_ms: 1.5958428939440246\n",
      "    mean_raw_obs_processing_ms: 7.71560082900138\n",
      "  time_since_restore: 64146.135041713715\n",
      "  time_this_iter_s: 546.4393429756165\n",
      "  time_total_s: 64146.135041713715\n",
      "  timers:\n",
      "    learn_throughput: 363.017\n",
      "    learn_time_ms: 11018.769\n",
      "    load_throughput: 10447.359\n",
      "    load_time_ms: 382.872\n",
      "    sample_throughput: 7.481\n",
      "    sample_time_ms: 534716.184\n",
      "    update_time_ms: 3.45\n",
      "  timestamp: 1612534641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 168.28\n",
      "  episode_reward_max: 118.39393077861043\n",
      "  episode_reward_mean: 64.76755545347365\n",
      "  episode_reward_min: -106.61622257819715\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 2992\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4568873345851898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011911025270819664\n",
      "        model: {}\n",
      "        policy_loss: -0.08810447156429291\n",
      "        total_loss: 181.31687927246094\n",
      "        vf_explained_var: 0.9353740811347961\n",
      "        vf_loss: 181.3869171142578\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.28654822335025\n",
      "    ram_util_percent: 35.92322335025381\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07383504444837982\n",
      "    mean_env_wait_ms: 123.53830742701446\n",
      "    mean_inference_ms: 1.5961194338148423\n",
      "    mean_raw_obs_processing_ms: 7.712714267410461\n",
      "  time_since_restore: 64697.755410671234\n",
      "  time_this_iter_s: 551.6203689575195\n",
      "  time_total_s: 64697.755410671234\n",
      "  timers:\n",
      "    learn_throughput: 363.034\n",
      "    learn_time_ms: 11018.253\n",
      "    load_throughput: 10417.459\n",
      "    load_time_ms: 383.971\n",
      "    sample_throughput: 7.468\n",
      "    sample_time_ms: 535615.189\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1612535193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-35-39\n",
      "  done: false\n",
      "  episode_len_mean: 170.54\n",
      "  episode_reward_max: 118.39393077861043\n",
      "  episode_reward_mean: 67.42758898475361\n",
      "  episode_reward_min: -96.07721063961988\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3015\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4643767476081848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0111186932772398\n",
      "        model: {}\n",
      "        policy_loss: -0.09293357282876968\n",
      "        total_loss: 343.3838195800781\n",
      "        vf_explained_var: 0.8551475405693054\n",
      "        vf_loss: 343.45989990234375\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.80577663671374\n",
      "    ram_util_percent: 36.09178433889602\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07385402868890828\n",
      "    mean_env_wait_ms: 123.55388860848927\n",
      "    mean_inference_ms: 1.5963864626867896\n",
      "    mean_raw_obs_processing_ms: 7.708247168024567\n",
      "  time_since_restore: 65244.0644133091\n",
      "  time_this_iter_s: 546.3090026378632\n",
      "  time_total_s: 65244.0644133091\n",
      "  timers:\n",
      "    learn_throughput: 363.065\n",
      "    learn_time_ms: 11017.297\n",
      "    load_throughput: 10415.812\n",
      "    load_time_ms: 384.032\n",
      "    sample_throughput: 7.464\n",
      "    sample_time_ms: 535906.007\n",
      "    update_time_ms: 3.646\n",
      "  timestamp: 1612535739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 165.32\n",
      "  episode_reward_max: 118.3981982012666\n",
      "  episode_reward_mean: 60.910713910049765\n",
      "  episode_reward_min: -104.31480838893714\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3041\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42718705534935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011103106662631035\n",
      "        model: {}\n",
      "        policy_loss: -0.0929199606180191\n",
      "        total_loss: 279.90399169921875\n",
      "        vf_explained_var: 0.9122319221496582\n",
      "        vf_loss: 279.9800720214844\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.80216010165185\n",
      "    ram_util_percent: 36.177001270648034\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.073875925243989\n",
      "    mean_env_wait_ms: 123.56988664862234\n",
      "    mean_inference_ms: 1.5966815974569863\n",
      "    mean_raw_obs_processing_ms: 7.705271802136495\n",
      "  time_since_restore: 65795.32003331184\n",
      "  time_this_iter_s: 551.2556200027466\n",
      "  time_total_s: 65795.32003331184\n",
      "  timers:\n",
      "    learn_throughput: 363.08\n",
      "    learn_time_ms: 11016.852\n",
      "    load_throughput: 10466.173\n",
      "    load_time_ms: 382.184\n",
      "    sample_throughput: 7.46\n",
      "    sample_time_ms: 536190.802\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1612536291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_15-53-54\n",
      "  done: false\n",
      "  episode_len_mean: 164.81\n",
      "  episode_reward_max: 118.3981982012666\n",
      "  episode_reward_mean: 58.482039282639995\n",
      "  episode_reward_min: -104.31480838893714\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3064\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4688795804977417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01179638784378767\n",
      "        model: {}\n",
      "        policy_loss: -0.09119810163974762\n",
      "        total_loss: 641.0614013671875\n",
      "        vf_explained_var: 0.7268823385238647\n",
      "        vf_loss: 641.1345825195312\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0869677419355\n",
      "    ram_util_percent: 36.2403870967742\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07389332285120735\n",
      "    mean_env_wait_ms: 123.58194908550817\n",
      "    mean_inference_ms: 1.596930263465146\n",
      "    mean_raw_obs_processing_ms: 7.702112245248307\n",
      "  time_since_restore: 66338.54445242882\n",
      "  time_this_iter_s: 543.2244191169739\n",
      "  time_total_s: 66338.54445242882\n",
      "  timers:\n",
      "    learn_throughput: 363.147\n",
      "    learn_time_ms: 11014.83\n",
      "    load_throughput: 10584.241\n",
      "    load_time_ms: 377.92\n",
      "    sample_throughput: 7.463\n",
      "    sample_time_ms: 535968.259\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1612536834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 168.55\n",
      "  episode_reward_max: 118.3981982012666\n",
      "  episode_reward_mean: 66.93127175280115\n",
      "  episode_reward_min: -104.31480838893714\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3087\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4359348714351654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010986492969095707\n",
      "        model: {}\n",
      "        policy_loss: -0.08692305535078049\n",
      "        total_loss: 201.26083374023438\n",
      "        vf_explained_var: 0.914314866065979\n",
      "        vf_loss: 201.33106994628906\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.83554140127389\n",
      "    ram_util_percent: 36.26140127388535\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07391177429306586\n",
      "    mean_env_wait_ms: 123.5937976225673\n",
      "    mean_inference_ms: 1.5971941811370614\n",
      "    mean_raw_obs_processing_ms: 7.6987489534722\n",
      "  time_since_restore: 66888.27560138702\n",
      "  time_this_iter_s: 549.7311489582062\n",
      "  time_total_s: 66888.27560138702\n",
      "  timers:\n",
      "    learn_throughput: 363.199\n",
      "    learn_time_ms: 11013.256\n",
      "    load_throughput: 10563.138\n",
      "    load_time_ms: 378.675\n",
      "    sample_throughput: 7.454\n",
      "    sample_time_ms: 536622.275\n",
      "    update_time_ms: 3.605\n",
      "  timestamp: 1612537384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 172.41\n",
      "  episode_reward_max: 118.3981982012666\n",
      "  episode_reward_mean: 72.81178550125237\n",
      "  episode_reward_min: -104.31480838893714\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3108\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44484618306159973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009742040187120438\n",
      "        model: {}\n",
      "        policy_loss: -0.07670141011476517\n",
      "        total_loss: 163.8535919189453\n",
      "        vf_explained_var: 0.8910155296325684\n",
      "        vf_loss: 163.9154815673828\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.67946768060837\n",
      "    ram_util_percent: 36.28593155893536\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07392756181700517\n",
      "    mean_env_wait_ms: 123.60711233657327\n",
      "    mean_inference_ms: 1.5974328036096943\n",
      "    mean_raw_obs_processing_ms: 7.694726055686101\n",
      "  time_since_restore: 67440.91092586517\n",
      "  time_this_iter_s: 552.6353244781494\n",
      "  time_total_s: 67440.91092586517\n",
      "  timers:\n",
      "    learn_throughput: 363.23\n",
      "    learn_time_ms: 11012.303\n",
      "    load_throughput: 10563.388\n",
      "    load_time_ms: 378.666\n",
      "    sample_throughput: 7.458\n",
      "    sample_time_ms: 536365.099\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1612537937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-21-27\n",
      "  done: false\n",
      "  episode_len_mean: 171.7\n",
      "  episode_reward_max: 118.38412592147529\n",
      "  episode_reward_mean: 72.69223832427096\n",
      "  episode_reward_min: -106.1712669792479\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3134\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4613892138004303\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012181523256003857\n",
      "        model: {}\n",
      "        policy_loss: -0.09250757843255997\n",
      "        total_loss: 680.6489868164062\n",
      "        vf_explained_var: 0.766514241695404\n",
      "        vf_loss: 680.7230834960938\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.26471337579618\n",
      "    ram_util_percent: 36.4192356687898\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0739460105711843\n",
      "    mean_env_wait_ms: 123.62252188902994\n",
      "    mean_inference_ms: 1.597740732512246\n",
      "    mean_raw_obs_processing_ms: 7.690126373656141\n",
      "  time_since_restore: 67991.09452533722\n",
      "  time_this_iter_s: 550.1835994720459\n",
      "  time_total_s: 67991.09452533722\n",
      "  timers:\n",
      "    learn_throughput: 359.345\n",
      "    learn_time_ms: 11131.357\n",
      "    load_throughput: 10505.837\n",
      "    load_time_ms: 380.741\n",
      "    sample_throughput: 7.447\n",
      "    sample_time_ms: 537127.454\n",
      "    update_time_ms: 3.663\n",
      "  timestamp: 1612538487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 173.52\n",
      "  episode_reward_max: 118.38412592147529\n",
      "  episode_reward_mean: 72.77219772627005\n",
      "  episode_reward_min: -106.1712669792479\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3156\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4543769955635071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010645396076142788\n",
      "        model: {}\n",
      "        policy_loss: -0.08419772982597351\n",
      "        total_loss: 382.41168212890625\n",
      "        vf_explained_var: 0.8175969123840332\n",
      "        vf_loss: 382.479736328125\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.85500633713562\n",
      "    ram_util_percent: 36.48111533586819\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0739624139481256\n",
      "    mean_env_wait_ms: 123.63977486770897\n",
      "    mean_inference_ms: 1.5980031955588672\n",
      "    mean_raw_obs_processing_ms: 7.685241951966309\n",
      "  time_since_restore: 68544.16131901741\n",
      "  time_this_iter_s: 553.066793680191\n",
      "  time_total_s: 68544.16131901741\n",
      "  timers:\n",
      "    learn_throughput: 359.564\n",
      "    learn_time_ms: 11124.589\n",
      "    load_throughput: 10395.588\n",
      "    load_time_ms: 384.779\n",
      "    sample_throughput: 7.435\n",
      "    sample_time_ms: 538001.608\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1612539041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 170.41\n",
      "  episode_reward_max: 118.38223935371855\n",
      "  episode_reward_mean: 72.97300591411735\n",
      "  episode_reward_min: -106.1712669792479\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3180\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4552662670612335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012583695352077484\n",
      "        model: {}\n",
      "        policy_loss: -0.10088962316513062\n",
      "        total_loss: 579.8086547851562\n",
      "        vf_explained_var: 0.7793418169021606\n",
      "        vf_loss: 579.890380859375\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.10165605095541\n",
      "    ram_util_percent: 36.51070063694268\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07398073880151018\n",
      "    mean_env_wait_ms: 123.65886365013124\n",
      "    mean_inference_ms: 1.5982915131835147\n",
      "    mean_raw_obs_processing_ms: 7.680565520308246\n",
      "  time_since_restore: 69094.08769249916\n",
      "  time_this_iter_s: 549.9263734817505\n",
      "  time_total_s: 69094.08769249916\n",
      "  timers:\n",
      "    learn_throughput: 359.663\n",
      "    learn_time_ms: 11121.525\n",
      "    load_throughput: 10450.762\n",
      "    load_time_ms: 382.747\n",
      "    sample_throughput: 7.438\n",
      "    sample_time_ms: 537770.809\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1612539591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-48-59\n",
      "  done: false\n",
      "  episode_len_mean: 165.45\n",
      "  episode_reward_max: 118.38589683516037\n",
      "  episode_reward_mean: 64.70058899978534\n",
      "  episode_reward_min: -106.1712669792479\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3206\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44746413826942444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012148977257311344\n",
      "        model: {}\n",
      "        policy_loss: -0.09393349289894104\n",
      "        total_loss: 451.075439453125\n",
      "        vf_explained_var: 0.8445163369178772\n",
      "        vf_loss: 451.1509094238281\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.52685421994885\n",
      "    ram_util_percent: 36.64079283887469\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07399990221071633\n",
      "    mean_env_wait_ms: 123.67477593968499\n",
      "    mean_inference_ms: 1.5986045131098825\n",
      "    mean_raw_obs_processing_ms: 7.678296282107278\n",
      "  time_since_restore: 69642.04183673859\n",
      "  time_this_iter_s: 547.9541442394257\n",
      "  time_total_s: 69642.04183673859\n",
      "  timers:\n",
      "    learn_throughput: 359.761\n",
      "    learn_time_ms: 11118.498\n",
      "    load_throughput: 10371.042\n",
      "    load_time_ms: 385.689\n",
      "    sample_throughput: 7.436\n",
      "    sample_time_ms: 537920.354\n",
      "    update_time_ms: 3.604\n",
      "  timestamp: 1612540139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_16-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 165.3\n",
      "  episode_reward_max: 118.38758184671698\n",
      "  episode_reward_mean: 69.01738470322785\n",
      "  episode_reward_min: -106.1712669792479\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3229\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4345180094242096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009561547078192234\n",
      "        model: {}\n",
      "        policy_loss: -0.07463575899600983\n",
      "        total_loss: 175.87632751464844\n",
      "        vf_explained_var: 0.9106888175010681\n",
      "        vf_loss: 175.93643188476562\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.00126582278482\n",
      "    ram_util_percent: 36.73303797468354\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07401633364975989\n",
      "    mean_env_wait_ms: 123.69074624896953\n",
      "    mean_inference_ms: 1.5988608805173647\n",
      "    mean_raw_obs_processing_ms: 7.675469401063338\n",
      "  time_since_restore: 70195.40217638016\n",
      "  time_this_iter_s: 553.360339641571\n",
      "  time_total_s: 70195.40217638016\n",
      "  timers:\n",
      "    learn_throughput: 359.841\n",
      "    learn_time_ms: 11116.012\n",
      "    load_throughput: 10420.234\n",
      "    load_time_ms: 383.869\n",
      "    sample_throughput: 7.434\n",
      "    sample_time_ms: 538096.396\n",
      "    update_time_ms: 3.411\n",
      "  timestamp: 1612540692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 160.14\n",
      "  episode_reward_max: 118.38758184671698\n",
      "  episode_reward_mean: 71.31251783772366\n",
      "  episode_reward_min: -99.2009149163234\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3256\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4157487452030182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011544783599674702\n",
      "        model: {}\n",
      "        policy_loss: -0.08671996742486954\n",
      "        total_loss: 289.4321594238281\n",
      "        vf_explained_var: 0.8593843579292297\n",
      "        vf_loss: 289.5013427734375\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.67557544757034\n",
      "    ram_util_percent: 36.80882352941177\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07403552830882282\n",
      "    mean_env_wait_ms: 123.70420525575304\n",
      "    mean_inference_ms: 1.5991809124962038\n",
      "    mean_raw_obs_processing_ms: 7.675060268472621\n",
      "  time_since_restore: 70743.14888811111\n",
      "  time_this_iter_s: 547.746711730957\n",
      "  time_total_s: 70743.14888811111\n",
      "  timers:\n",
      "    learn_throughput: 359.88\n",
      "    learn_time_ms: 11114.812\n",
      "    load_throughput: 10412.175\n",
      "    load_time_ms: 384.166\n",
      "    sample_throughput: 7.432\n",
      "    sample_time_ms: 538239.701\n",
      "    update_time_ms: 3.411\n",
      "  timestamp: 1612541240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 159.52\n",
      "  episode_reward_max: 118.38758184671698\n",
      "  episode_reward_mean: 73.35797321980081\n",
      "  episode_reward_min: -99.2009149163234\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3280\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4477018117904663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012937062419950962\n",
      "        model: {}\n",
      "        policy_loss: -0.08505003154277802\n",
      "        total_loss: 728.245849609375\n",
      "        vf_explained_var: 0.6325822472572327\n",
      "        vf_loss: 728.311279296875\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.36670902160103\n",
      "    ram_util_percent: 36.90152477763659\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07405065244458847\n",
      "    mean_env_wait_ms: 123.71539221972515\n",
      "    mean_inference_ms: 1.599437449172545\n",
      "    mean_raw_obs_processing_ms: 7.675070944809567\n",
      "  time_since_restore: 71294.29447340965\n",
      "  time_this_iter_s: 551.1455852985382\n",
      "  time_total_s: 71294.29447340965\n",
      "  timers:\n",
      "    learn_throughput: 359.946\n",
      "    learn_time_ms: 11112.773\n",
      "    load_throughput: 10374.587\n",
      "    load_time_ms: 385.558\n",
      "    sample_throughput: 7.432\n",
      "    sample_time_ms: 538229.091\n",
      "    update_time_ms: 3.416\n",
      "  timestamp: 1612541792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-25-50\n",
      "  done: false\n",
      "  episode_len_mean: 163.59\n",
      "  episode_reward_max: 118.38758184671698\n",
      "  episode_reward_mean: 77.65058518097486\n",
      "  episode_reward_min: -99.2009149163234\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3304\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4347648024559021\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011194107122719288\n",
      "        model: {}\n",
      "        policy_loss: -0.08610609173774719\n",
      "        total_loss: 268.7384948730469\n",
      "        vf_explained_var: 0.8530004024505615\n",
      "        vf_loss: 268.80755615234375\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.32123115577889\n",
      "    ram_util_percent: 36.96369346733668\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0740659905142996\n",
      "    mean_env_wait_ms: 123.73138866546145\n",
      "    mean_inference_ms: 1.5996877122844964\n",
      "    mean_raw_obs_processing_ms: 7.674064133156589\n",
      "  time_since_restore: 71852.19196653366\n",
      "  time_this_iter_s: 557.8974931240082\n",
      "  time_total_s: 71852.19196653366\n",
      "  timers:\n",
      "    learn_throughput: 355.887\n",
      "    learn_time_ms: 11239.516\n",
      "    load_throughput: 10243.472\n",
      "    load_time_ms: 390.493\n",
      "    sample_throughput: 7.413\n",
      "    sample_time_ms: 539563.379\n",
      "    update_time_ms: 3.466\n",
      "  timestamp: 1612542350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 167.22\n",
      "  episode_reward_max: 118.39216508752872\n",
      "  episode_reward_mean: 79.64854508256079\n",
      "  episode_reward_min: -97.42165838491559\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3325\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44456398487091064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01147162914276123\n",
      "        model: {}\n",
      "        policy_loss: -0.08386582136154175\n",
      "        total_loss: 235.33810424804688\n",
      "        vf_explained_var: 0.8224122524261475\n",
      "        vf_loss: 235.40455627441406\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.61388535031847\n",
      "    ram_util_percent: 37.02203821656051\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07408012396287068\n",
      "    mean_env_wait_ms: 123.7451945158028\n",
      "    mean_inference_ms: 1.599906027419862\n",
      "    mean_raw_obs_processing_ms: 7.672114148378878\n",
      "  time_since_restore: 72401.98120546341\n",
      "  time_this_iter_s: 549.7892389297485\n",
      "  time_total_s: 72401.98120546341\n",
      "  timers:\n",
      "    learn_throughput: 355.962\n",
      "    learn_time_ms: 11237.141\n",
      "    load_throughput: 10290.324\n",
      "    load_time_ms: 388.715\n",
      "    sample_throughput: 7.413\n",
      "    sample_time_ms: 539566.987\n",
      "    update_time_ms: 3.732\n",
      "  timestamp: 1612542900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 174.02\n",
      "  episode_reward_max: 118.3988778595599\n",
      "  episode_reward_mean: 79.6503588796897\n",
      "  episode_reward_min: -98.58772843718992\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3348\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4205785095691681\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01255876012146473\n",
      "        model: {}\n",
      "        policy_loss: -0.08491669595241547\n",
      "        total_loss: 577.0393676757812\n",
      "        vf_explained_var: 0.7533905506134033\n",
      "        vf_loss: 577.105224609375\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.98876404494382\n",
      "    ram_util_percent: 37.00212234706616\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07409468699755112\n",
      "    mean_env_wait_ms: 123.76769822743896\n",
      "    mean_inference_ms: 1.6001216816303105\n",
      "    mean_raw_obs_processing_ms: 7.668208319002229\n",
      "  time_since_restore: 72963.35838723183\n",
      "  time_this_iter_s: 561.3771817684174\n",
      "  time_total_s: 72963.35838723183\n",
      "  timers:\n",
      "    learn_throughput: 359.883\n",
      "    learn_time_ms: 11114.723\n",
      "    load_throughput: 10360.139\n",
      "    load_time_ms: 386.095\n",
      "    sample_throughput: 7.4\n",
      "    sample_time_ms: 540567.319\n",
      "    update_time_ms: 3.679\n",
      "  timestamp: 1612543461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_17-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 174.72\n",
      "  episode_reward_max: 118.3988778595599\n",
      "  episode_reward_mean: 79.42030723896697\n",
      "  episode_reward_min: -98.89269798243448\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3371\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44044801592826843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012471041642129421\n",
      "        model: {}\n",
      "        policy_loss: -0.09028154611587524\n",
      "        total_loss: 720.9268188476562\n",
      "        vf_explained_var: 0.6553348302841187\n",
      "        vf_loss: 720.9981079101562\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.48310214375788\n",
      "    ram_util_percent: 37.08877679697352\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07411032605580313\n",
      "    mean_env_wait_ms: 123.7933313344388\n",
      "    mean_inference_ms: 1.6003521936732659\n",
      "    mean_raw_obs_processing_ms: 7.6634840470100585\n",
      "  time_since_restore: 73518.63030385971\n",
      "  time_this_iter_s: 555.2719166278839\n",
      "  time_total_s: 73518.63030385971\n",
      "  timers:\n",
      "    learn_throughput: 363.877\n",
      "    learn_time_ms: 10992.732\n",
      "    load_throughput: 10324.201\n",
      "    load_time_ms: 387.439\n",
      "    sample_throughput: 7.391\n",
      "    sample_time_ms: 541199.669\n",
      "    update_time_ms: 3.611\n",
      "  timestamp: 1612544017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 178.11\n",
      "  episode_reward_max: 118.3988778595599\n",
      "  episode_reward_mean: 87.63651969757257\n",
      "  episode_reward_min: -98.89269798243448\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3394\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43004146218299866\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008486988954246044\n",
      "        model: {}\n",
      "        policy_loss: -0.06376770883798599\n",
      "        total_loss: 162.5463104248047\n",
      "        vf_explained_var: 0.8426206707954407\n",
      "        vf_loss: 162.59718322753906\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.94302176696543\n",
      "    ram_util_percent: 37.16081946222791\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07412648159168465\n",
      "    mean_env_wait_ms: 123.81639877663866\n",
      "    mean_inference_ms: 1.6005777546294704\n",
      "    mean_raw_obs_processing_ms: 7.658104247005087\n",
      "  time_since_restore: 74065.86624193192\n",
      "  time_this_iter_s: 547.2359380722046\n",
      "  time_total_s: 74065.86624193192\n",
      "  timers:\n",
      "    learn_throughput: 363.715\n",
      "    learn_time_ms: 10997.61\n",
      "    load_throughput: 10403.717\n",
      "    load_time_ms: 384.478\n",
      "    sample_throughput: 7.399\n",
      "    sample_time_ms: 540615.535\n",
      "    update_time_ms: 3.597\n",
      "  timestamp: 1612544564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 178.23\n",
      "  episode_reward_max: 118.3988778595599\n",
      "  episode_reward_mean: 83.39888696434213\n",
      "  episode_reward_min: -99.49796862583413\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3416\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4433595538139343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012938636355102062\n",
      "        model: {}\n",
      "        policy_loss: -0.09350971132516861\n",
      "        total_loss: 343.8197021484375\n",
      "        vf_explained_var: 0.8425376415252686\n",
      "        vf_loss: 343.8935852050781\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.84968152866242\n",
      "    ram_util_percent: 37.19541401273885\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07414133099296266\n",
      "    mean_env_wait_ms: 123.83764097349913\n",
      "    mean_inference_ms: 1.6007932755058931\n",
      "    mean_raw_obs_processing_ms: 7.652910835258187\n",
      "  time_since_restore: 74616.44479370117\n",
      "  time_this_iter_s: 550.5785517692566\n",
      "  time_total_s: 74616.44479370117\n",
      "  timers:\n",
      "    learn_throughput: 363.654\n",
      "    learn_time_ms: 10999.463\n",
      "    load_throughput: 10391.0\n",
      "    load_time_ms: 384.949\n",
      "    sample_throughput: 7.398\n",
      "    sample_time_ms: 540675.633\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1612545115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-21-06\n",
      "  done: false\n",
      "  episode_len_mean: 172.77\n",
      "  episode_reward_max: 118.38885610800675\n",
      "  episode_reward_mean: 85.34105902595334\n",
      "  episode_reward_min: -105.40853923024936\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3441\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44118568301200867\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0112985260784626\n",
      "        model: {}\n",
      "        policy_loss: -0.08578947186470032\n",
      "        total_loss: 385.3487243652344\n",
      "        vf_explained_var: 0.7971364259719849\n",
      "        vf_loss: 385.4173278808594\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.15063613231553\n",
      "    ram_util_percent: 37.38040712468193\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415779311852931\n",
      "    mean_env_wait_ms: 123.85650876588664\n",
      "    mean_inference_ms: 1.6010370854585587\n",
      "    mean_raw_obs_processing_ms: 7.648801278548879\n",
      "  time_since_restore: 75167.19957113266\n",
      "  time_this_iter_s: 550.754777431488\n",
      "  time_total_s: 75167.19957113266\n",
      "  timers:\n",
      "    learn_throughput: 363.615\n",
      "    learn_time_ms: 11000.66\n",
      "    load_throughput: 10429.523\n",
      "    load_time_ms: 383.527\n",
      "    sample_throughput: 7.394\n",
      "    sample_time_ms: 540958.089\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1612545666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-30-04\n",
      "  done: false\n",
      "  episode_len_mean: 171.54\n",
      "  episode_reward_max: 118.38885610800675\n",
      "  episode_reward_mean: 80.84804013233253\n",
      "  episode_reward_min: -105.40853923024936\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3464\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4634040594100952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010463912039995193\n",
      "        model: {}\n",
      "        policy_loss: -0.08335606008768082\n",
      "        total_loss: 454.3860778808594\n",
      "        vf_explained_var: 0.7073298692703247\n",
      "        vf_loss: 454.4534912109375\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.62408854166667\n",
      "    ram_util_percent: 37.418749999999996\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417392647252172\n",
      "    mean_env_wait_ms: 123.86528293609204\n",
      "    mean_inference_ms: 1.6012533163125013\n",
      "    mean_raw_obs_processing_ms: 7.645016582694038\n",
      "  time_since_restore: 75705.20835757256\n",
      "  time_this_iter_s: 538.0087864398956\n",
      "  time_total_s: 75705.20835757256\n",
      "  timers:\n",
      "    learn_throughput: 363.526\n",
      "    learn_time_ms: 11003.351\n",
      "    load_throughput: 10439.402\n",
      "    load_time_ms: 383.164\n",
      "    sample_throughput: 7.415\n",
      "    sample_time_ms: 539421.347\n",
      "    update_time_ms: 3.598\n",
      "  timestamp: 1612546204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-39-23\n",
      "  done: false\n",
      "  episode_len_mean: 174.4\n",
      "  episode_reward_max: 118.39688129370964\n",
      "  episode_reward_mean: 83.0926535937148\n",
      "  episode_reward_min: -105.40853923024936\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3486\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4437454044818878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012058843858540058\n",
      "        model: {}\n",
      "        policy_loss: -0.08465130627155304\n",
      "        total_loss: 687.2110595703125\n",
      "        vf_explained_var: 0.5712448358535767\n",
      "        vf_loss: 687.2774047851562\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.67380952380952\n",
      "    ram_util_percent: 37.49799498746867\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07418862103012361\n",
      "    mean_env_wait_ms: 123.8773626109076\n",
      "    mean_inference_ms: 1.6014669185971098\n",
      "    mean_raw_obs_processing_ms: 7.641177937255296\n",
      "  time_since_restore: 76263.90670013428\n",
      "  time_this_iter_s: 558.6983425617218\n",
      "  time_total_s: 76263.90670013428\n",
      "  timers:\n",
      "    learn_throughput: 363.562\n",
      "    learn_time_ms: 11002.242\n",
      "    load_throughput: 10483.241\n",
      "    load_time_ms: 381.561\n",
      "    sample_throughput: 7.4\n",
      "    sample_time_ms: 540522.278\n",
      "    update_time_ms: 3.625\n",
      "  timestamp: 1612546763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-48-33\n",
      "  done: false\n",
      "  episode_len_mean: 175.85\n",
      "  episode_reward_max: 118.39688129370964\n",
      "  episode_reward_mean: 85.24599627388852\n",
      "  episode_reward_min: -105.40853923024936\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3507\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4088135361671448\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00897451676428318\n",
      "        model: {}\n",
      "        policy_loss: -0.0673583447933197\n",
      "        total_loss: 234.69325256347656\n",
      "        vf_explained_var: 0.7627097368240356\n",
      "        vf_loss: 234.74697875976562\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.20892857142857\n",
      "    ram_util_percent: 37.60765306122449\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07420219892401457\n",
      "    mean_env_wait_ms: 123.88963957347427\n",
      "    mean_inference_ms: 1.6016724890693148\n",
      "    mean_raw_obs_processing_ms: 7.6369799904351385\n",
      "  time_since_restore: 76813.89093136787\n",
      "  time_this_iter_s: 549.9842312335968\n",
      "  time_total_s: 76813.89093136787\n",
      "  timers:\n",
      "    learn_throughput: 363.534\n",
      "    learn_time_ms: 11003.09\n",
      "    load_throughput: 10444.106\n",
      "    load_time_ms: 382.991\n",
      "    sample_throughput: 7.402\n",
      "    sample_time_ms: 540400.344\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1612547313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_18-57-43\n",
      "  done: false\n",
      "  episode_len_mean: 181.17\n",
      "  episode_reward_max: 118.39688129370964\n",
      "  episode_reward_mean: 95.92989597645523\n",
      "  episode_reward_min: -102.47169675020142\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3529\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4231834411621094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007945266552269459\n",
      "        model: {}\n",
      "        policy_loss: -0.06313847750425339\n",
      "        total_loss: 197.12643432617188\n",
      "        vf_explained_var: 0.8007545471191406\n",
      "        vf_loss: 197.17752075195312\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.1115923566879\n",
      "    ram_util_percent: 37.65566878980892\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421692183861744\n",
      "    mean_env_wait_ms: 123.90299618924162\n",
      "    mean_inference_ms: 1.6018814166951236\n",
      "    mean_raw_obs_processing_ms: 7.63173424323064\n",
      "  time_since_restore: 77363.83865237236\n",
      "  time_this_iter_s: 549.9477210044861\n",
      "  time_total_s: 77363.83865237236\n",
      "  timers:\n",
      "    learn_throughput: 367.717\n",
      "    learn_time_ms: 10877.932\n",
      "    load_throughput: 10493.8\n",
      "    load_time_ms: 381.177\n",
      "    sample_throughput: 7.411\n",
      "    sample_time_ms: 539731.901\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1612547863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-06-55\n",
      "  done: false\n",
      "  episode_len_mean: 179.27\n",
      "  episode_reward_max: 118.39688129370964\n",
      "  episode_reward_mean: 94.04478793287231\n",
      "  episode_reward_min: -102.47169675020142\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3552\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40497133135795593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009016490541398525\n",
      "        model: {}\n",
      "        policy_loss: -0.06876073032617569\n",
      "        total_loss: 429.4163818359375\n",
      "        vf_explained_var: 0.6888920664787292\n",
      "        vf_loss: 429.4714660644531\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.23748411689962\n",
      "    ram_util_percent: 37.73570520965693\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07423069756441256\n",
      "    mean_env_wait_ms: 123.92026165621098\n",
      "    mean_inference_ms: 1.6020784786065525\n",
      "    mean_raw_obs_processing_ms: 7.6258884020921665\n",
      "  time_since_restore: 77915.18640828133\n",
      "  time_this_iter_s: 551.3477559089661\n",
      "  time_total_s: 77915.18640828133\n",
      "  timers:\n",
      "    learn_throughput: 367.643\n",
      "    learn_time_ms: 10880.129\n",
      "    load_throughput: 10413.912\n",
      "    load_time_ms: 384.102\n",
      "    sample_throughput: 7.409\n",
      "    sample_time_ms: 539889.239\n",
      "    update_time_ms: 3.289\n",
      "  timestamp: 1612548415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-16-15\n",
      "  done: false\n",
      "  episode_len_mean: 184.8\n",
      "  episode_reward_max: 118.39416594531232\n",
      "  episode_reward_mean: 102.50895575843892\n",
      "  episode_reward_min: -98.55013596054269\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3573\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4047042429447174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013987275771796703\n",
      "        model: {}\n",
      "        policy_loss: -0.08442830294370651\n",
      "        total_loss: 2.6668102741241455\n",
      "        vf_explained_var: 0.994769811630249\n",
      "        vf_loss: 2.7299954891204834\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.99125\n",
      "    ram_util_percent: 35.08775000000001\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0742421819412325\n",
      "    mean_env_wait_ms: 123.94120617887427\n",
      "    mean_inference_ms: 1.6022455709894223\n",
      "    mean_raw_obs_processing_ms: 7.620028927730093\n",
      "  time_since_restore: 78475.37015748024\n",
      "  time_this_iter_s: 560.1837491989136\n",
      "  time_total_s: 78475.37015748024\n",
      "  timers:\n",
      "    learn_throughput: 367.634\n",
      "    learn_time_ms: 10880.378\n",
      "    load_throughput: 10424.12\n",
      "    load_time_ms: 383.725\n",
      "    sample_throughput: 7.411\n",
      "    sample_time_ms: 539772.354\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1612548975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 182.68\n",
      "  episode_reward_max: 118.39416594531232\n",
      "  episode_reward_mean: 104.52290814274271\n",
      "  episode_reward_min: -95.10231038926125\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3595\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45872074365615845\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010546006262302399\n",
      "        model: {}\n",
      "        policy_loss: -0.07344182580709457\n",
      "        total_loss: 323.6219177246094\n",
      "        vf_explained_var: 0.7573205232620239\n",
      "        vf_loss: 323.67938232421875\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.21352405721716\n",
      "    ram_util_percent: 32.74421326397919\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07425143877896005\n",
      "    mean_env_wait_ms: 123.9560890209409\n",
      "    mean_inference_ms: 1.602375482505863\n",
      "    mean_raw_obs_processing_ms: 7.614094169714232\n",
      "  time_since_restore: 79014.01163697243\n",
      "  time_this_iter_s: 538.6414794921875\n",
      "  time_total_s: 79014.01163697243\n",
      "  timers:\n",
      "    learn_throughput: 363.305\n",
      "    learn_time_ms: 11010.027\n",
      "    load_throughput: 10477.484\n",
      "    load_time_ms: 381.771\n",
      "    sample_throughput: 7.435\n",
      "    sample_time_ms: 537976.914\n",
      "    update_time_ms: 3.301\n",
      "  timestamp: 1612549514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 177.69\n",
      "  episode_reward_max: 118.368110907273\n",
      "  episode_reward_mean: 97.913123007117\n",
      "  episode_reward_min: -107.16600264852504\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3620\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4387730062007904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012467202730476856\n",
      "        model: {}\n",
      "        policy_loss: -0.09214158356189728\n",
      "        total_loss: 668.3999633789062\n",
      "        vf_explained_var: 0.6735032200813293\n",
      "        vf_loss: 668.4732055664062\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.64231770833334\n",
      "    ram_util_percent: 32.84466145833334\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07425495042220548\n",
      "    mean_env_wait_ms: 123.96549083167173\n",
      "    mean_inference_ms: 1.6024187471937845\n",
      "    mean_raw_obs_processing_ms: 7.609445554481276\n",
      "  time_since_restore: 79551.9704053402\n",
      "  time_this_iter_s: 537.9587683677673\n",
      "  time_total_s: 79551.9704053402\n",
      "  timers:\n",
      "    learn_throughput: 363.477\n",
      "    learn_time_ms: 11004.837\n",
      "    load_throughput: 10364.28\n",
      "    load_time_ms: 385.941\n",
      "    sample_throughput: 7.448\n",
      "    sample_time_ms: 537050.714\n",
      "    update_time_ms: 3.3\n",
      "  timestamp: 1612550052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 177.55\n",
      "  episode_reward_max: 118.36896058723761\n",
      "  episode_reward_mean: 97.84793983089313\n",
      "  episode_reward_min: -107.16600264852504\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3643\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.432574987411499\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009630763903260231\n",
      "        model: {}\n",
      "        policy_loss: -0.0695473700761795\n",
      "        total_loss: 92.17274475097656\n",
      "        vf_explained_var: 0.909010112285614\n",
      "        vf_loss: 92.22764587402344\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.9262812089356\n",
      "    ram_util_percent: 32.8819973718791\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.074254319671669\n",
      "    mean_env_wait_ms: 123.9666510328489\n",
      "    mean_inference_ms: 1.6023850090782787\n",
      "    mean_raw_obs_processing_ms: 7.605506674305125\n",
      "  time_since_restore: 80085.06455945969\n",
      "  time_this_iter_s: 533.0941541194916\n",
      "  time_total_s: 80085.06455945969\n",
      "  timers:\n",
      "    learn_throughput: 363.464\n",
      "    learn_time_ms: 11005.22\n",
      "    load_throughput: 10321.219\n",
      "    load_time_ms: 387.551\n",
      "    sample_throughput: 7.472\n",
      "    sample_time_ms: 535304.975\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1612550585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_19-51-56\n",
      "  done: false\n",
      "  episode_len_mean: 168.24\n",
      "  episode_reward_max: 118.3947577976576\n",
      "  episode_reward_mean: 89.41535398348633\n",
      "  episode_reward_min: -107.16600264852504\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3668\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4166856110095978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011992297135293484\n",
      "        model: {}\n",
      "        policy_loss: -0.0880388468503952\n",
      "        total_loss: 726.1949462890625\n",
      "        vf_explained_var: 0.6026613712310791\n",
      "        vf_loss: 726.2649536132812\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.35568783068783\n",
      "    ram_util_percent: 32.92724867724867\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07425056887516353\n",
      "    mean_env_wait_ms: 123.95472331496265\n",
      "    mean_inference_ms: 1.6022859809679124\n",
      "    mean_raw_obs_processing_ms: 7.602918728972502\n",
      "  time_since_restore: 80615.2869758606\n",
      "  time_this_iter_s: 530.2224164009094\n",
      "  time_total_s: 80615.2869758606\n",
      "  timers:\n",
      "    learn_throughput: 363.275\n",
      "    learn_time_ms: 11010.955\n",
      "    load_throughput: 10303.933\n",
      "    load_time_ms: 388.201\n",
      "    sample_throughput: 7.501\n",
      "    sample_time_ms: 533242.421\n",
      "    update_time_ms: 3.309\n",
      "  timestamp: 1612551116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 163.28\n",
      "  episode_reward_max: 118.3947577976576\n",
      "  episode_reward_mean: 69.74634999749635\n",
      "  episode_reward_min: -109.51884397580608\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3693\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4970381259918213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017992058768868446\n",
      "        model: {}\n",
      "        policy_loss: -0.12171321362257004\n",
      "        total_loss: 1336.9246826171875\n",
      "        vf_explained_var: 0.5758603811264038\n",
      "        vf_loss: 1337.0189208984375\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61634241245136\n",
      "    ram_util_percent: 32.921400778210106\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07424397398915351\n",
      "    mean_env_wait_ms: 123.93927139316405\n",
      "    mean_inference_ms: 1.6021462229420376\n",
      "    mean_raw_obs_processing_ms: 7.602174113981209\n",
      "  time_since_restore: 81155.34913897514\n",
      "  time_this_iter_s: 540.0621631145477\n",
      "  time_total_s: 81155.34913897514\n",
      "  timers:\n",
      "    learn_throughput: 363.298\n",
      "    learn_time_ms: 11010.237\n",
      "    load_throughput: 10255.419\n",
      "    load_time_ms: 390.038\n",
      "    sample_throughput: 7.498\n",
      "    sample_time_ms: 533446.426\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1612551656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 159.9\n",
      "  episode_reward_max: 118.3947577976576\n",
      "  episode_reward_mean: 60.89453504410072\n",
      "  episode_reward_min: -109.5861422587938\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3720\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43981850147247314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016921525821089745\n",
      "        model: {}\n",
      "        policy_loss: -0.12046623229980469\n",
      "        total_loss: 820.6953125\n",
      "        vf_explained_var: 0.7555510997772217\n",
      "        vf_loss: 820.789794921875\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.85845161290322\n",
      "    ram_util_percent: 32.93225806451612\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0742388218319861\n",
      "    mean_env_wait_ms: 123.92405707332924\n",
      "    mean_inference_ms: 1.6020177850357025\n",
      "    mean_raw_obs_processing_ms: 7.602799312385494\n",
      "  time_since_restore: 81698.43557310104\n",
      "  time_this_iter_s: 543.0864341259003\n",
      "  time_total_s: 81698.43557310104\n",
      "  timers:\n",
      "    learn_throughput: 363.199\n",
      "    learn_time_ms: 11013.236\n",
      "    load_throughput: 10095.47\n",
      "    load_time_ms: 396.217\n",
      "    sample_throughput: 7.521\n",
      "    sample_time_ms: 531874.371\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1612552199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-18-57\n",
      "  done: false\n",
      "  episode_len_mean: 158.45\n",
      "  episode_reward_max: 118.3947577976576\n",
      "  episode_reward_mean: 50.25818375117635\n",
      "  episode_reward_min: -109.5861422587938\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 3744\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45139965415000916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014370901510119438\n",
      "        model: {}\n",
      "        policy_loss: -0.09852424263954163\n",
      "        total_loss: 601.2747192382812\n",
      "        vf_explained_var: 0.754544198513031\n",
      "        vf_loss: 601.3514404296875\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.61419270833333\n",
      "    ram_util_percent: 32.97174479166666\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07423418176753824\n",
      "    mean_env_wait_ms: 123.91176883249973\n",
      "    mean_inference_ms: 1.601919356540302\n",
      "    mean_raw_obs_processing_ms: 7.603764346172668\n",
      "  time_since_restore: 82236.26952886581\n",
      "  time_this_iter_s: 537.8339557647705\n",
      "  time_total_s: 82236.26952886581\n",
      "  timers:\n",
      "    learn_throughput: 356.268\n",
      "    learn_time_ms: 11227.498\n",
      "    load_throughput: 9996.882\n",
      "    load_time_ms: 400.125\n",
      "    sample_throughput: 7.541\n",
      "    sample_time_ms: 530439.425\n",
      "    update_time_ms: 3.331\n",
      "  timestamp: 1612552737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-28-14\n",
      "  done: false\n",
      "  episode_len_mean: 162.71\n",
      "  episode_reward_max: 118.38744627026932\n",
      "  episode_reward_mean: 47.83944075728092\n",
      "  episode_reward_min: -109.77606519202311\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3767\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42525285482406616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012654083780944347\n",
      "        model: {}\n",
      "        policy_loss: -0.08936700224876404\n",
      "        total_loss: 243.78933715820312\n",
      "        vf_explained_var: 0.8709100484848022\n",
      "        vf_loss: 243.8594512939453\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.96863979848867\n",
      "    ram_util_percent: 32.99382871536524\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07422978508691984\n",
      "    mean_env_wait_ms: 123.91074760986248\n",
      "    mean_inference_ms: 1.6018611205884248\n",
      "    mean_raw_obs_processing_ms: 7.60361798808872\n",
      "  time_since_restore: 82792.1963903904\n",
      "  time_this_iter_s: 555.9268615245819\n",
      "  time_total_s: 82792.1963903904\n",
      "  timers:\n",
      "    learn_throughput: 356.192\n",
      "    learn_time_ms: 11229.897\n",
      "    load_throughput: 9953.652\n",
      "    load_time_ms: 401.863\n",
      "    sample_throughput: 7.532\n",
      "    sample_time_ms: 531035.692\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1612553294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-37-15\n",
      "  done: false\n",
      "  episode_len_mean: 163.06\n",
      "  episode_reward_max: 118.38744627026932\n",
      "  episode_reward_mean: 56.45690780463366\n",
      "  episode_reward_min: -109.77606519202311\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3790\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44616422057151794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014726038090884686\n",
      "        model: {}\n",
      "        policy_loss: -0.09930548816919327\n",
      "        total_loss: 594.713134765625\n",
      "        vf_explained_var: 0.7364388108253479\n",
      "        vf_loss: 594.7899780273438\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.07746113989637\n",
      "    ram_util_percent: 33.04067357512954\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07422707438920505\n",
      "    mean_env_wait_ms: 123.91094181660337\n",
      "    mean_inference_ms: 1.6018365792549065\n",
      "    mean_raw_obs_processing_ms: 7.602606586139731\n",
      "  time_since_restore: 83333.42088317871\n",
      "  time_this_iter_s: 541.2244927883148\n",
      "  time_total_s: 83333.42088317871\n",
      "  timers:\n",
      "    learn_throughput: 356.016\n",
      "    learn_time_ms: 11235.463\n",
      "    load_throughput: 9921.745\n",
      "    load_time_ms: 403.155\n",
      "    sample_throughput: 7.547\n",
      "    sample_time_ms: 530012.113\n",
      "    update_time_ms: 3.324\n",
      "  timestamp: 1612553835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 170.39\n",
      "  episode_reward_max: 118.38744627026932\n",
      "  episode_reward_mean: 60.96667630197983\n",
      "  episode_reward_min: -109.77606519202311\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3813\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4642556309700012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013698128052055836\n",
      "        model: {}\n",
      "        policy_loss: -0.10759066045284271\n",
      "        total_loss: 692.0045166015625\n",
      "        vf_explained_var: 0.7172709703445435\n",
      "        vf_loss: 692.0914916992188\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.81920614596672\n",
      "    ram_util_percent: 32.971062740076825\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0742249416308402\n",
      "    mean_env_wait_ms: 123.91421238108164\n",
      "    mean_inference_ms: 1.6018344501230106\n",
      "    mean_raw_obs_processing_ms: 7.599943344584268\n",
      "  time_since_restore: 83880.28850626945\n",
      "  time_this_iter_s: 546.867623090744\n",
      "  time_total_s: 83880.28850626945\n",
      "  timers:\n",
      "    learn_throughput: 355.97\n",
      "    learn_time_ms: 11236.888\n",
      "    load_throughput: 9883.051\n",
      "    load_time_ms: 404.733\n",
      "    sample_throughput: 7.566\n",
      "    sample_time_ms: 528676.661\n",
      "    update_time_ms: 3.331\n",
      "  timestamp: 1612554382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_20-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 171.48\n",
      "  episode_reward_max: 118.38744627026932\n",
      "  episode_reward_mean: 64.71629623403965\n",
      "  episode_reward_min: -109.8947801834444\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3836\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46162301301956177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014638403430581093\n",
      "        model: {}\n",
      "        policy_loss: -0.10716770589351654\n",
      "        total_loss: 616.3295288085938\n",
      "        vf_explained_var: 0.756269097328186\n",
      "        vf_loss: 616.4144897460938\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.87056555269923\n",
      "    ram_util_percent: 32.92943444730077\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07422179091916373\n",
      "    mean_env_wait_ms: 123.92061177684944\n",
      "    mean_inference_ms: 1.6018571677393094\n",
      "    mean_raw_obs_processing_ms: 7.596677218149598\n",
      "  time_since_restore: 84425.11103105545\n",
      "  time_this_iter_s: 544.8225247859955\n",
      "  time_total_s: 84425.11103105545\n",
      "  timers:\n",
      "    learn_throughput: 360.034\n",
      "    learn_time_ms: 11110.065\n",
      "    load_throughput: 9859.273\n",
      "    load_time_ms: 405.709\n",
      "    sample_throughput: 7.555\n",
      "    sample_time_ms: 529425.303\n",
      "    update_time_ms: 3.316\n",
      "  timestamp: 1612554927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 176.31\n",
      "  episode_reward_max: 118.39558251149289\n",
      "  episode_reward_mean: 60.19990696167705\n",
      "  episode_reward_min: -130.83901650606902\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3858\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4644749164581299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013407884165644646\n",
      "        model: {}\n",
      "        policy_loss: -0.10247442126274109\n",
      "        total_loss: 521.523681640625\n",
      "        vf_explained_var: 0.7713882327079773\n",
      "        vf_loss: 521.6057739257812\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0430412371134\n",
      "    ram_util_percent: 32.93582474226804\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0742190463863997\n",
      "    mean_env_wait_ms: 123.92484635136535\n",
      "    mean_inference_ms: 1.6018881638440519\n",
      "    mean_raw_obs_processing_ms: 7.593083855401201\n",
      "  time_since_restore: 84968.9373011589\n",
      "  time_this_iter_s: 543.8262701034546\n",
      "  time_total_s: 84968.9373011589\n",
      "  timers:\n",
      "    learn_throughput: 356.009\n",
      "    learn_time_ms: 11235.655\n",
      "    load_throughput: 9902.949\n",
      "    load_time_ms: 403.92\n",
      "    sample_throughput: 7.549\n",
      "    sample_time_ms: 529881.853\n",
      "    update_time_ms: 3.334\n",
      "  timestamp: 1612555471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-13-31\n",
      "  done: false\n",
      "  episode_len_mean: 180.52\n",
      "  episode_reward_max: 118.39660706992181\n",
      "  episode_reward_mean: 66.6776309016837\n",
      "  episode_reward_min: -130.83901650606902\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 3879\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.413097083568573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01111940760165453\n",
      "        model: {}\n",
      "        policy_loss: -0.07937850058078766\n",
      "        total_loss: 172.9703826904297\n",
      "        vf_explained_var: 0.8596677184104919\n",
      "        vf_loss: 173.03289794921875\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9348051948052\n",
      "    ram_util_percent: 32.976493506493505\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421634983061332\n",
      "    mean_env_wait_ms: 123.92714916758123\n",
      "    mean_inference_ms: 1.6019150686579622\n",
      "    mean_raw_obs_processing_ms: 7.58885150927842\n",
      "  time_since_restore: 85508.20851373672\n",
      "  time_this_iter_s: 539.2712125778198\n",
      "  time_total_s: 85508.20851373672\n",
      "  timers:\n",
      "    learn_throughput: 356.024\n",
      "    learn_time_ms: 11235.198\n",
      "    load_throughput: 9869.36\n",
      "    load_time_ms: 405.295\n",
      "    sample_throughput: 7.54\n",
      "    sample_time_ms: 530496.096\n",
      "    update_time_ms: 3.351\n",
      "  timestamp: 1612556011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 179.09\n",
      "  episode_reward_max: 118.39660706992181\n",
      "  episode_reward_mean: 81.68621928203734\n",
      "  episode_reward_min: -130.83901650606902\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3902\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4230906665325165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013441582210361958\n",
      "        model: {}\n",
      "        policy_loss: -0.08769556879997253\n",
      "        total_loss: 28.610193252563477\n",
      "        vf_explained_var: 0.9749789834022522\n",
      "        vf_loss: 28.67747688293457\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.80735107731306\n",
      "    ram_util_percent: 33.007858048162234\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421422397240052\n",
      "    mean_env_wait_ms: 123.93226441165052\n",
      "    mean_inference_ms: 1.6019717500759185\n",
      "    mean_raw_obs_processing_ms: 7.58418798648689\n",
      "  time_since_restore: 86060.91312623024\n",
      "  time_this_iter_s: 552.704612493515\n",
      "  time_total_s: 86060.91312623024\n",
      "  timers:\n",
      "    learn_throughput: 352.258\n",
      "    learn_time_ms: 11355.327\n",
      "    load_throughput: 9741.245\n",
      "    load_time_ms: 410.625\n",
      "    sample_throughput: 7.51\n",
      "    sample_time_ms: 532614.775\n",
      "    update_time_ms: 3.378\n",
      "  timestamp: 1612556563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-31-54\n",
      "  done: false\n",
      "  episode_len_mean: 179.76\n",
      "  episode_reward_max: 118.39660706992181\n",
      "  episode_reward_mean: 90.29068826796511\n",
      "  episode_reward_min: -130.83901650606902\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 3924\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4123307168483734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008981037884950638\n",
      "        model: {}\n",
      "        policy_loss: -0.06645353138446808\n",
      "        total_loss: 245.4762420654297\n",
      "        vf_explained_var: 0.79758620262146\n",
      "        vf_loss: 245.52903747558594\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.88751592356688\n",
      "    ram_util_percent: 32.97898089171974\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421271892347138\n",
      "    mean_env_wait_ms: 123.93850827473312\n",
      "    mean_inference_ms: 1.6020267995391482\n",
      "    mean_raw_obs_processing_ms: 7.5793943820753\n",
      "  time_since_restore: 86611.06447577477\n",
      "  time_this_iter_s: 550.1513495445251\n",
      "  time_total_s: 86611.06447577477\n",
      "  timers:\n",
      "    learn_throughput: 348.424\n",
      "    learn_time_ms: 11480.265\n",
      "    load_throughput: 9598.985\n",
      "    load_time_ms: 416.711\n",
      "    sample_throughput: 7.498\n",
      "    sample_time_ms: 533488.206\n",
      "    update_time_ms: 3.387\n",
      "  timestamp: 1612557114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 179.3\n",
      "  episode_reward_max: 118.39660706992181\n",
      "  episode_reward_mean: 97.01146125281625\n",
      "  episode_reward_min: -130.83901650606902\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3947\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4032095670700073\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007117121946066618\n",
      "        model: {}\n",
      "        policy_loss: -0.060569629073143005\n",
      "        total_loss: 181.7837677001953\n",
      "        vf_explained_var: 0.836219847202301\n",
      "        vf_loss: 181.8335418701172\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.87477592829705\n",
      "    ram_util_percent: 32.96542893725992\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421186773042754\n",
      "    mean_env_wait_ms: 123.94596805440634\n",
      "    mean_inference_ms: 1.6020833634031784\n",
      "    mean_raw_obs_processing_ms: 7.574614418142879\n",
      "  time_since_restore: 87158.07657146454\n",
      "  time_this_iter_s: 547.0120956897736\n",
      "  time_total_s: 87158.07657146454\n",
      "  timers:\n",
      "    learn_throughput: 348.391\n",
      "    learn_time_ms: 11481.36\n",
      "    load_throughput: 9663.872\n",
      "    load_time_ms: 413.913\n",
      "    sample_throughput: 7.492\n",
      "    sample_time_ms: 533881.523\n",
      "    update_time_ms: 3.394\n",
      "  timestamp: 1612557661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-50-05\n",
      "  done: false\n",
      "  episode_len_mean: 179.13\n",
      "  episode_reward_max: 118.39660706992181\n",
      "  episode_reward_mean: 99.46279161157561\n",
      "  episode_reward_min: -109.83968403111263\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3970\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.439169317483902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011329482309520245\n",
      "        model: {}\n",
      "        policy_loss: -0.07038860768079758\n",
      "        total_loss: 424.8155517578125\n",
      "        vf_explained_var: 0.7250011563301086\n",
      "        vf_loss: 424.8688049316406\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.08453608247423\n",
      "    ram_util_percent: 32.99987113402062\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07420970081372766\n",
      "    mean_env_wait_ms: 123.95375851158697\n",
      "    mean_inference_ms: 1.6021289277226396\n",
      "    mean_raw_obs_processing_ms: 7.570598030822109\n",
      "  time_since_restore: 87701.80701684952\n",
      "  time_this_iter_s: 543.7304453849792\n",
      "  time_total_s: 87701.80701684952\n",
      "  timers:\n",
      "    learn_throughput: 354.989\n",
      "    learn_time_ms: 11267.957\n",
      "    load_throughput: 9805.987\n",
      "    load_time_ms: 407.914\n",
      "    sample_throughput: 7.481\n",
      "    sample_time_ms: 534690.906\n",
      "    update_time_ms: 3.351\n",
      "  timestamp: 1612558205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_21-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 174.56\n",
      "  episode_reward_max: 118.38704610492795\n",
      "  episode_reward_mean: 93.00765191719768\n",
      "  episode_reward_min: -109.83968403111263\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 3993\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42071273922920227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010607176460325718\n",
      "        model: {}\n",
      "        policy_loss: -0.0776035487651825\n",
      "        total_loss: 629.7680053710938\n",
      "        vf_explained_var: 0.5722531080245972\n",
      "        vf_loss: 629.8295288085938\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.10864516129031\n",
      "    ram_util_percent: 33.0658064516129\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07420691551655528\n",
      "    mean_env_wait_ms: 123.95998421679442\n",
      "    mean_inference_ms: 1.6021527404599891\n",
      "    mean_raw_obs_processing_ms: 7.567094189021517\n",
      "  time_since_restore: 88245.05001306534\n",
      "  time_this_iter_s: 543.2429962158203\n",
      "  time_total_s: 88245.05001306534\n",
      "  timers:\n",
      "    learn_throughput: 355.019\n",
      "    learn_time_ms: 11267.009\n",
      "    load_throughput: 9843.158\n",
      "    load_time_ms: 406.374\n",
      "    sample_throughput: 7.499\n",
      "    sample_time_ms: 533422.589\n",
      "    update_time_ms: 3.599\n",
      "  timestamp: 1612558748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-08-14\n",
      "  done: false\n",
      "  episode_len_mean: 174.8\n",
      "  episode_reward_max: 118.38704610492795\n",
      "  episode_reward_mean: 86.54748403226091\n",
      "  episode_reward_min: -109.83968403111263\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4016\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43552714586257935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013469327241182327\n",
      "        model: {}\n",
      "        policy_loss: -0.08960948139429092\n",
      "        total_loss: 546.148193359375\n",
      "        vf_explained_var: 0.7074467539787292\n",
      "        vf_loss: 546.21728515625\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.05848329048843\n",
      "    ram_util_percent: 33.108740359897176\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07420377629449551\n",
      "    mean_env_wait_ms: 123.96420934696029\n",
      "    mean_inference_ms: 1.6021702931144326\n",
      "    mean_raw_obs_processing_ms: 7.563855845546282\n",
      "  time_since_restore: 88790.11874270439\n",
      "  time_this_iter_s: 545.0687296390533\n",
      "  time_total_s: 88790.11874270439\n",
      "  timers:\n",
      "    learn_throughput: 355.121\n",
      "    learn_time_ms: 11263.764\n",
      "    load_throughput: 9898.206\n",
      "    load_time_ms: 404.114\n",
      "    sample_throughput: 7.493\n",
      "    sample_time_ms: 533814.991\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1612559294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-17-14\n",
      "  done: false\n",
      "  episode_len_mean: 170.58\n",
      "  episode_reward_max: 118.35688092087982\n",
      "  episode_reward_mean: 82.36774540606184\n",
      "  episode_reward_min: -110.17706820030782\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4041\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4271816313266754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012455505318939686\n",
      "        model: {}\n",
      "        policy_loss: -0.08868199586868286\n",
      "        total_loss: 424.1562805175781\n",
      "        vf_explained_var: 0.7479289770126343\n",
      "        vf_loss: 424.2261047363281\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.34046692607004\n",
      "    ram_util_percent: 33.14669260700389\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0741995265020147\n",
      "    mean_env_wait_ms: 123.9645750024154\n",
      "    mean_inference_ms: 1.6021853024434092\n",
      "    mean_raw_obs_processing_ms: 7.561491098898228\n",
      "  time_since_restore: 89330.08315038681\n",
      "  time_this_iter_s: 539.9644076824188\n",
      "  time_total_s: 89330.08315038681\n",
      "  timers:\n",
      "    learn_throughput: 355.151\n",
      "    learn_time_ms: 11262.804\n",
      "    load_throughput: 9795.173\n",
      "    load_time_ms: 408.364\n",
      "    sample_throughput: 7.503\n",
      "    sample_time_ms: 533119.77\n",
      "    update_time_ms: 3.584\n",
      "  timestamp: 1612559834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 167.65\n",
      "  episode_reward_max: 118.37134051316642\n",
      "  episode_reward_mean: 89.02573535986308\n",
      "  episode_reward_min: -110.17706820030782\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4066\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.39397504925727844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008642108179628849\n",
      "        model: {}\n",
      "        policy_loss: -0.05929189175367355\n",
      "        total_loss: 69.00140380859375\n",
      "        vf_explained_var: 0.9209516644477844\n",
      "        vf_loss: 69.04756927490234\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4292587776333\n",
      "    ram_util_percent: 33.10299089726918\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0741965001964425\n",
      "    mean_env_wait_ms: 123.96177056255382\n",
      "    mean_inference_ms: 1.6021985538201762\n",
      "    mean_raw_obs_processing_ms: 7.560054206660386\n",
      "  time_since_restore: 89868.84523010254\n",
      "  time_this_iter_s: 538.7620797157288\n",
      "  time_total_s: 89868.84523010254\n",
      "  timers:\n",
      "    learn_throughput: 355.262\n",
      "    learn_time_ms: 11259.286\n",
      "    load_throughput: 9723.434\n",
      "    load_time_ms: 411.377\n",
      "    sample_throughput: 7.512\n",
      "    sample_time_ms: 532513.243\n",
      "    update_time_ms: 3.597\n",
      "  timestamp: 1612560373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 167.37\n",
      "  episode_reward_max: 118.39721682265817\n",
      "  episode_reward_mean: 93.106578513241\n",
      "  episode_reward_min: -110.23004539395069\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4089\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44625091552734375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0100972019135952\n",
      "        model: {}\n",
      "        policy_loss: -0.07345553487539291\n",
      "        total_loss: 216.7339324951172\n",
      "        vf_explained_var: 0.8446842432022095\n",
      "        vf_loss: 216.7920379638672\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.32181818181819\n",
      "    ram_util_percent: 33.042857142857144\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0741941947009224\n",
      "    mean_env_wait_ms: 123.95775177781371\n",
      "    mean_inference_ms: 1.602220620308847\n",
      "    mean_raw_obs_processing_ms: 7.558880943330247\n",
      "  time_since_restore: 90408.30646562576\n",
      "  time_this_iter_s: 539.4612355232239\n",
      "  time_total_s: 90408.30646562576\n",
      "  timers:\n",
      "    learn_throughput: 359.143\n",
      "    learn_time_ms: 11137.636\n",
      "    load_throughput: 9725.591\n",
      "    load_time_ms: 411.286\n",
      "    sample_throughput: 7.516\n",
      "    sample_time_ms: 532204.442\n",
      "    update_time_ms: 3.583\n",
      "  timestamp: 1612560913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 166.17\n",
      "  episode_reward_max: 118.39721682265817\n",
      "  episode_reward_mean: 97.4182817715313\n",
      "  episode_reward_min: -110.23004539395069\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 4113\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.41627371311187744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009740103967487812\n",
      "        model: {}\n",
      "        policy_loss: -0.07682600617408752\n",
      "        total_loss: 289.1815185546875\n",
      "        vf_explained_var: 0.8221221566200256\n",
      "        vf_loss: 289.2434997558594\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.32357512953368\n",
      "    ram_util_percent: 33.08056994818653\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07419086332965509\n",
      "    mean_env_wait_ms: 123.95178009385114\n",
      "    mean_inference_ms: 1.6022422286098654\n",
      "    mean_raw_obs_processing_ms: 7.55797323011693\n",
      "  time_since_restore: 90948.85899448395\n",
      "  time_this_iter_s: 540.5525288581848\n",
      "  time_total_s: 90948.85899448395\n",
      "  timers:\n",
      "    learn_throughput: 359.213\n",
      "    learn_time_ms: 11135.462\n",
      "    load_throughput: 9828.63\n",
      "    load_time_ms: 406.974\n",
      "    sample_throughput: 7.514\n",
      "    sample_time_ms: 532340.779\n",
      "    update_time_ms: 3.578\n",
      "  timestamp: 1612561453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_22-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 167.18\n",
      "  episode_reward_max: 118.39721682265817\n",
      "  episode_reward_mean: 97.50348861468204\n",
      "  episode_reward_min: -110.23004539395069\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4135\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40218329429626465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014065968804061413\n",
      "        model: {}\n",
      "        policy_loss: -0.09010020643472672\n",
      "        total_loss: 342.2845458984375\n",
      "        vf_explained_var: 0.7853641510009766\n",
      "        vf_loss: 342.3533020019531\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.06954838709677\n",
      "    ram_util_percent: 33.060645161290324\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07418834163925181\n",
      "    mean_env_wait_ms: 123.94816485058593\n",
      "    mean_inference_ms: 1.602261283421344\n",
      "    mean_raw_obs_processing_ms: 7.556078438615191\n",
      "  time_since_restore: 91491.73087620735\n",
      "  time_this_iter_s: 542.8718817234039\n",
      "  time_total_s: 91491.73087620735\n",
      "  timers:\n",
      "    learn_throughput: 363.208\n",
      "    learn_time_ms: 11012.987\n",
      "    load_throughput: 9946.522\n",
      "    load_time_ms: 402.151\n",
      "    sample_throughput: 7.526\n",
      "    sample_time_ms: 531488.993\n",
      "    update_time_ms: 3.532\n",
      "  timestamp: 1612561996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 175.22\n",
      "  episode_reward_max: 118.39721682265817\n",
      "  episode_reward_mean: 91.1735450709618\n",
      "  episode_reward_min: -110.23004539395069\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4158\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45351287722587585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013766942545771599\n",
      "        model: {}\n",
      "        policy_loss: -0.09410598874092102\n",
      "        total_loss: 517.308837890625\n",
      "        vf_explained_var: 0.7639977931976318\n",
      "        vf_loss: 517.3820190429688\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.77395171537485\n",
      "    ram_util_percent: 33.076365946632784\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.074185589296274\n",
      "    mean_env_wait_ms: 123.94921149626992\n",
      "    mean_inference_ms: 1.6022963363371372\n",
      "    mean_raw_obs_processing_ms: 7.553429944842955\n",
      "  time_since_restore: 92043.4406850338\n",
      "  time_this_iter_s: 551.7098088264465\n",
      "  time_total_s: 92043.4406850338\n",
      "  timers:\n",
      "    learn_throughput: 367.437\n",
      "    learn_time_ms: 10886.231\n",
      "    load_throughput: 9951.361\n",
      "    load_time_ms: 401.955\n",
      "    sample_throughput: 7.522\n",
      "    sample_time_ms: 531775.814\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1612562548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-11-32\n",
      "  done: false\n",
      "  episode_len_mean: 179.36\n",
      "  episode_reward_max: 118.39721682265817\n",
      "  episode_reward_mean: 89.20067237813277\n",
      "  episode_reward_min: -104.10603813149308\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4179\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.41214075684547424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01066150888800621\n",
      "        model: {}\n",
      "        policy_loss: -0.07349267601966858\n",
      "        total_loss: 297.12811279296875\n",
      "        vf_explained_var: 0.7930014729499817\n",
      "        vf_loss: 297.1854248046875\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.225\n",
      "    ram_util_percent: 33.14252577319588\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07418240093461925\n",
      "    mean_env_wait_ms: 123.9517415865211\n",
      "    mean_inference_ms: 1.6023239547789194\n",
      "    mean_raw_obs_processing_ms: 7.550049859996149\n",
      "  time_since_restore: 92586.44476246834\n",
      "  time_this_iter_s: 543.0040774345398\n",
      "  time_total_s: 92586.44476246834\n",
      "  timers:\n",
      "    learn_throughput: 363.508\n",
      "    learn_time_ms: 11003.896\n",
      "    load_throughput: 9869.429\n",
      "    load_time_ms: 405.292\n",
      "    sample_throughput: 7.529\n",
      "    sample_time_ms: 531248.772\n",
      "    update_time_ms: 3.566\n",
      "  timestamp: 1612563092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 180.74\n",
      "  episode_reward_max: 118.38616192368198\n",
      "  episode_reward_mean: 93.52260766025698\n",
      "  episode_reward_min: -104.10603813149308\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4200\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4035082757472992\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009633541107177734\n",
      "        model: {}\n",
      "        policy_loss: -0.0667622983455658\n",
      "        total_loss: 265.7696838378906\n",
      "        vf_explained_var: 0.7476822733879089\n",
      "        vf_loss: 265.8218688964844\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.30772251308899\n",
      "    ram_util_percent: 33.11230366492147\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417964538731664\n",
      "    mean_env_wait_ms: 123.9538476814756\n",
      "    mean_inference_ms: 1.602353629851191\n",
      "    mean_raw_obs_processing_ms: 7.5456846006294835\n",
      "  time_since_restore: 93122.27306509018\n",
      "  time_this_iter_s: 535.8283026218414\n",
      "  time_total_s: 93122.27306509018\n",
      "  timers:\n",
      "    learn_throughput: 363.566\n",
      "    learn_time_ms: 11002.139\n",
      "    load_throughput: 9812.247\n",
      "    load_time_ms: 407.654\n",
      "    sample_throughput: 7.541\n",
      "    sample_time_ms: 530459.763\n",
      "    update_time_ms: 3.578\n",
      "  timestamp: 1612563628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-29-28\n",
      "  done: false\n",
      "  episode_len_mean: 183.63\n",
      "  episode_reward_max: 118.38616192368198\n",
      "  episode_reward_mean: 95.42120086889565\n",
      "  episode_reward_min: -104.10603813149308\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4222\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.41053903102874756\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009529002010822296\n",
      "        model: {}\n",
      "        policy_loss: -0.06727268546819687\n",
      "        total_loss: 138.30288696289062\n",
      "        vf_explained_var: 0.8545386791229248\n",
      "        vf_loss: 138.35569763183594\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.22940414507772\n",
      "    ram_util_percent: 33.14339378238342\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417777652496409\n",
      "    mean_env_wait_ms: 123.95591648814566\n",
      "    mean_inference_ms: 1.6023904385168557\n",
      "    mean_raw_obs_processing_ms: 7.540786589406749\n",
      "  time_since_restore: 93662.63874363899\n",
      "  time_this_iter_s: 540.3656785488129\n",
      "  time_total_s: 93662.63874363899\n",
      "  timers:\n",
      "    learn_throughput: 363.639\n",
      "    learn_time_ms: 10999.925\n",
      "    load_throughput: 9844.713\n",
      "    load_time_ms: 406.309\n",
      "    sample_throughput: 7.545\n",
      "    sample_time_ms: 530177.777\n",
      "    update_time_ms: 3.333\n",
      "  timestamp: 1612564168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 184.2\n",
      "  episode_reward_max: 118.36593943126906\n",
      "  episode_reward_mean: 99.70495360093706\n",
      "  episode_reward_min: -103.00470255291852\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4244\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40692970156669617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010006925091147423\n",
      "        model: {}\n",
      "        policy_loss: -0.06850401312112808\n",
      "        total_loss: 46.91301345825195\n",
      "        vf_explained_var: 0.9568142294883728\n",
      "        vf_loss: 46.96632385253906\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33675324675325\n",
      "    ram_util_percent: 33.137012987012994\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417531475790502\n",
      "    mean_env_wait_ms: 123.95594598867854\n",
      "    mean_inference_ms: 1.6024218266632897\n",
      "    mean_raw_obs_processing_ms: 7.53580846774851\n",
      "  time_since_restore: 94202.36893177032\n",
      "  time_this_iter_s: 539.7301881313324\n",
      "  time_total_s: 94202.36893177032\n",
      "  timers:\n",
      "    learn_throughput: 363.665\n",
      "    learn_time_ms: 10999.124\n",
      "    load_throughput: 9883.363\n",
      "    load_time_ms: 404.721\n",
      "    sample_throughput: 7.552\n",
      "    sample_time_ms: 529646.488\n",
      "    update_time_ms: 3.339\n",
      "  timestamp: 1612564708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 182.71\n",
      "  episode_reward_max: 118.36593943126906\n",
      "  episode_reward_mean: 101.87495826271774\n",
      "  episode_reward_min: -107.78012687548268\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4267\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38610339164733887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007468639872968197\n",
      "        model: {}\n",
      "        policy_loss: -0.06216759234666824\n",
      "        total_loss: 232.3528594970703\n",
      "        vf_explained_var: 0.7953667044639587\n",
      "        vf_loss: 232.40367126464844\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.95255754475704\n",
      "    ram_util_percent: 33.14130434782609\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417282805381242\n",
      "    mean_env_wait_ms: 123.95585002557647\n",
      "    mean_inference_ms: 1.602457600794189\n",
      "    mean_raw_obs_processing_ms: 7.530848393486115\n",
      "  time_since_restore: 94750.35914158821\n",
      "  time_this_iter_s: 547.9902098178864\n",
      "  time_total_s: 94750.35914158821\n",
      "  timers:\n",
      "    learn_throughput: 363.632\n",
      "    learn_time_ms: 11000.133\n",
      "    load_throughput: 10061.004\n",
      "    load_time_ms: 397.575\n",
      "    sample_throughput: 7.541\n",
      "    sample_time_ms: 530457.007\n",
      "    update_time_ms: 3.35\n",
      "  timestamp: 1612565257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-05_23-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 180.69\n",
      "  episode_reward_max: 118.39129248015783\n",
      "  episode_reward_mean: 108.26968762067528\n",
      "  episode_reward_min: -107.78012687548268\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4289\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36386293172836304\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01457921601831913\n",
      "        model: {}\n",
      "        policy_loss: -0.08947481215000153\n",
      "        total_loss: 3.408121109008789\n",
      "        vf_explained_var: 0.9948753118515015\n",
      "        vf_loss: 3.475454330444336\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.03764404609474\n",
      "    ram_util_percent: 33.18284250960307\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07417068436982581\n",
      "    mean_env_wait_ms: 123.95728309168406\n",
      "    mean_inference_ms: 1.6024986433283983\n",
      "    mean_raw_obs_processing_ms: 7.526623511692621\n",
      "  time_since_restore: 95296.7922706604\n",
      "  time_this_iter_s: 546.4331290721893\n",
      "  time_total_s: 95296.7922706604\n",
      "  timers:\n",
      "    learn_throughput: 359.622\n",
      "    learn_time_ms: 11122.78\n",
      "    load_throughput: 10077.277\n",
      "    load_time_ms: 396.933\n",
      "    sample_throughput: 7.532\n",
      "    sample_time_ms: 531098.629\n",
      "    update_time_ms: 3.56\n",
      "  timestamp: 1612565803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 179.32\n",
      "  episode_reward_max: 118.39129248015783\n",
      "  episode_reward_mean: 104.03626363407055\n",
      "  episode_reward_min: -107.78012687548268\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4311\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40089643001556396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007796469144523144\n",
      "        model: {}\n",
      "        policy_loss: -0.06298409402370453\n",
      "        total_loss: 252.48275756835938\n",
      "        vf_explained_var: 0.7821725606918335\n",
      "        vf_loss: 252.5338897705078\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91730769230769\n",
      "    ram_util_percent: 33.12358974358975\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07416746069440038\n",
      "    mean_env_wait_ms: 123.96135370230891\n",
      "    mean_inference_ms: 1.6025310913127788\n",
      "    mean_raw_obs_processing_ms: 7.522785889879144\n",
      "  time_since_restore: 95843.9361937046\n",
      "  time_this_iter_s: 547.1439230442047\n",
      "  time_total_s: 95843.9361937046\n",
      "  timers:\n",
      "    learn_throughput: 359.595\n",
      "    learn_time_ms: 11123.615\n",
      "    load_throughput: 10132.773\n",
      "    load_time_ms: 394.759\n",
      "    sample_throughput: 7.521\n",
      "    sample_time_ms: 531866.551\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1612566351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-14-52\n",
      "  done: false\n",
      "  episode_len_mean: 177.98\n",
      "  episode_reward_max: 118.39129248015783\n",
      "  episode_reward_mean: 99.99533593221992\n",
      "  episode_reward_min: -107.78012687548268\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4333\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40840664505958557\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010183082893490791\n",
      "        model: {}\n",
      "        policy_loss: -0.07672832906246185\n",
      "        total_loss: 473.2088928222656\n",
      "        vf_explained_var: 0.7051126956939697\n",
      "        vf_loss: 473.2701110839844\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.1130659767141\n",
      "    ram_util_percent: 33.0849935316947\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07416454956173275\n",
      "    mean_env_wait_ms: 123.96583183785339\n",
      "    mean_inference_ms: 1.6025660991041923\n",
      "    mean_raw_obs_processing_ms: 7.518913912978453\n",
      "  time_since_restore: 96385.13417005539\n",
      "  time_this_iter_s: 541.1979763507843\n",
      "  time_total_s: 96385.13417005539\n",
      "  timers:\n",
      "    learn_throughput: 359.531\n",
      "    learn_time_ms: 11125.6\n",
      "    load_throughput: 10048.418\n",
      "    load_time_ms: 398.073\n",
      "    sample_throughput: 7.52\n",
      "    sample_time_ms: 531926.897\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1612566892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-24-00\n",
      "  done: false\n",
      "  episode_len_mean: 180.48\n",
      "  episode_reward_max: 118.39129248015783\n",
      "  episode_reward_mean: 97.83063563214839\n",
      "  episode_reward_min: -107.78012687548268\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4355\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4142143428325653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012023290619254112\n",
      "        model: {}\n",
      "        policy_loss: -0.07838746160268784\n",
      "        total_loss: 298.3892517089844\n",
      "        vf_explained_var: 0.799001157283783\n",
      "        vf_loss: 298.4493103027344\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.85877080665813\n",
      "    ram_util_percent: 33.12586427656851\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07416234644417644\n",
      "    mean_env_wait_ms: 123.97160850529156\n",
      "    mean_inference_ms: 1.602600792928398\n",
      "    mean_raw_obs_processing_ms: 7.514900165633321\n",
      "  time_since_restore: 96932.7414598465\n",
      "  time_this_iter_s: 547.6072897911072\n",
      "  time_total_s: 96932.7414598465\n",
      "  timers:\n",
      "    learn_throughput: 359.538\n",
      "    learn_time_ms: 11125.401\n",
      "    load_throughput: 10084.143\n",
      "    load_time_ms: 396.662\n",
      "    sample_throughput: 7.513\n",
      "    sample_time_ms: 532403.738\n",
      "    update_time_ms: 3.771\n",
      "  timestamp: 1612567440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-33-08\n",
      "  done: false\n",
      "  episode_len_mean: 183.23\n",
      "  episode_reward_max: 118.39113007217415\n",
      "  episode_reward_mean: 95.71669662555927\n",
      "  episode_reward_min: -105.82406515735704\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4375\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42678016424179077\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00887107290327549\n",
      "        model: {}\n",
      "        policy_loss: -0.06920554488897324\n",
      "        total_loss: 159.2644805908203\n",
      "        vf_explained_var: 0.8569036722183228\n",
      "        vf_loss: 159.32020568847656\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.69833759590793\n",
      "    ram_util_percent: 33.12007672634271\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07416008150544938\n",
      "    mean_env_wait_ms: 123.9779784705138\n",
      "    mean_inference_ms: 1.6026252304546045\n",
      "    mean_raw_obs_processing_ms: 7.510385156954173\n",
      "  time_since_restore: 97480.22124838829\n",
      "  time_this_iter_s: 547.4797885417938\n",
      "  time_total_s: 97480.22124838829\n",
      "  timers:\n",
      "    learn_throughput: 359.466\n",
      "    learn_time_ms: 11127.629\n",
      "    load_throughput: 10238.129\n",
      "    load_time_ms: 390.696\n",
      "    sample_throughput: 7.519\n",
      "    sample_time_ms: 531984.461\n",
      "    update_time_ms: 3.752\n",
      "  timestamp: 1612567988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 181.69\n",
      "  episode_reward_max: 118.39113007217415\n",
      "  episode_reward_mean: 95.71646208101299\n",
      "  episode_reward_min: -105.82406515735704\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4398\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3828734755516052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008302934467792511\n",
      "        model: {}\n",
      "        policy_loss: -0.053676120936870575\n",
      "        total_loss: 77.59911346435547\n",
      "        vf_explained_var: 0.9097941517829895\n",
      "        vf_loss: 77.64018249511719\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88226221079692\n",
      "    ram_util_percent: 33.19473007712082\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415860718891172\n",
      "    mean_env_wait_ms: 123.98462969069766\n",
      "    mean_inference_ms: 1.602661898243847\n",
      "    mean_raw_obs_processing_ms: 7.505603880297497\n",
      "  time_since_restore: 98025.47537374496\n",
      "  time_this_iter_s: 545.2541253566742\n",
      "  time_total_s: 98025.47537374496\n",
      "  timers:\n",
      "    learn_throughput: 363.317\n",
      "    learn_time_ms: 11009.659\n",
      "    load_throughput: 10332.665\n",
      "    load_time_ms: 387.122\n",
      "    sample_throughput: 7.514\n",
      "    sample_time_ms: 532334.8\n",
      "    update_time_ms: 3.714\n",
      "  timestamp: 1612568533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_00-51-24\n",
      "  done: false\n",
      "  episode_len_mean: 184.44\n",
      "  episode_reward_max: 118.3993486073445\n",
      "  episode_reward_mean: 100.01484703586505\n",
      "  episode_reward_min: -105.82406515735704\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4419\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38607579469680786\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016155041754245758\n",
      "        model: {}\n",
      "        policy_loss: -0.08660035580396652\n",
      "        total_loss: 2.9771108627319336\n",
      "        vf_explained_var: 0.995143711566925\n",
      "        vf_loss: 3.0391764640808105\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56543837357052\n",
      "    ram_util_percent: 33.19212198221092\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415749356117024\n",
      "    mean_env_wait_ms: 123.99287735729395\n",
      "    mean_inference_ms: 1.602692763188569\n",
      "    mean_raw_obs_processing_ms: 7.500862625036421\n",
      "  time_since_restore: 98576.58061265945\n",
      "  time_this_iter_s: 551.1052389144897\n",
      "  time_total_s: 98576.58061265945\n",
      "  timers:\n",
      "    learn_throughput: 363.198\n",
      "    learn_time_ms: 11013.289\n",
      "    load_throughput: 10417.461\n",
      "    load_time_ms: 383.971\n",
      "    sample_throughput: 7.493\n",
      "    sample_time_ms: 533862.564\n",
      "    update_time_ms: 3.75\n",
      "  timestamp: 1612569084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_01-00-31\n",
      "  done: false\n",
      "  episode_len_mean: 186.64\n",
      "  episode_reward_max: 118.3993486073445\n",
      "  episode_reward_mean: 104.22415761796276\n",
      "  episode_reward_min: -105.82406515735704\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4440\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3993775546550751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01507883332669735\n",
      "        model: {}\n",
      "        policy_loss: -0.08904433995485306\n",
      "        total_loss: 2.409423828125\n",
      "        vf_explained_var: 0.9958893656730652\n",
      "        vf_loss: 2.47556734085083\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.92692307692307\n",
      "    ram_util_percent: 33.22628205128205\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415566538442564\n",
      "    mean_env_wait_ms: 124.00219297778372\n",
      "    mean_inference_ms: 1.602724502428847\n",
      "    mean_raw_obs_processing_ms: 7.4958436240432595\n",
      "  time_since_restore: 99122.85630607605\n",
      "  time_this_iter_s: 546.2756934165955\n",
      "  time_total_s: 99122.85630607605\n",
      "  timers:\n",
      "    learn_throughput: 363.159\n",
      "    learn_time_ms: 11014.465\n",
      "    load_throughput: 10323.751\n",
      "    load_time_ms: 387.456\n",
      "    sample_throughput: 7.484\n",
      "    sample_time_ms: 534448.034\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1612569631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_01-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 184.25\n",
      "  episode_reward_max: 118.3993486073445\n",
      "  episode_reward_mean: 104.18812705348927\n",
      "  episode_reward_min: -100.41709033022047\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4463\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43593791127204895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011433245614171028\n",
      "        model: {}\n",
      "        policy_loss: -0.08665183186531067\n",
      "        total_loss: 610.10546875\n",
      "        vf_explained_var: 0.6348351240158081\n",
      "        vf_loss: 610.1746826171875\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.38107049608355\n",
      "    ram_util_percent: 33.26266318537859\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415386985988849\n",
      "    mean_env_wait_ms: 124.00829311605739\n",
      "    mean_inference_ms: 1.6027612668627813\n",
      "    mean_raw_obs_processing_ms: 7.490995944298884\n",
      "  time_since_restore: 99659.6314406395\n",
      "  time_this_iter_s: 536.775134563446\n",
      "  time_total_s: 99659.6314406395\n",
      "  timers:\n",
      "    learn_throughput: 363.145\n",
      "    learn_time_ms: 11014.879\n",
      "    load_throughput: 10300.032\n",
      "    load_time_ms: 388.348\n",
      "    sample_throughput: 7.489\n",
      "    sample_time_ms: 534152.338\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1612570168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_01-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 182.31\n",
      "  episode_reward_max: 118.3993486073445\n",
      "  episode_reward_mean: 106.09162800398462\n",
      "  episode_reward_min: -107.23341964900744\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4486\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3980315923690796\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00807199440896511\n",
      "        model: {}\n",
      "        policy_loss: -0.06412962079048157\n",
      "        total_loss: 306.4832458496094\n",
      "        vf_explained_var: 0.7558860182762146\n",
      "        vf_loss: 306.53509521484375\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.27950713359273\n",
      "    ram_util_percent: 33.298054474708174\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07415249099927756\n",
      "    mean_env_wait_ms: 124.01143350085938\n",
      "    mean_inference_ms: 1.6027971220376385\n",
      "    mean_raw_obs_processing_ms: 7.487044420146661\n",
      "  time_since_restore: 100199.96264648438\n",
      "  time_this_iter_s: 540.3312058448792\n",
      "  time_total_s: 100199.96264648438\n",
      "  timers:\n",
      "    learn_throughput: 363.118\n",
      "    learn_time_ms: 11015.69\n",
      "    load_throughput: 10253.774\n",
      "    load_time_ms: 390.1\n",
      "    sample_throughput: 7.499\n",
      "    sample_time_ms: 533379.779\n",
      "    update_time_ms: 3.739\n",
      "  timestamp: 1612570709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 729ff_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_729ff_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-06_01-27-30\n",
      "  done: true\n",
      "  episode_len_mean: 183.12\n",
      "  episode_reward_max: 118.392521472361\n",
      "  episode_reward_mean: 103.99728850448247\n",
      "  episode_reward_min: -107.23341964900744\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 4508\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37125101685523987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004925148095935583\n",
      "        model: {}\n",
      "        policy_loss: -0.04343706741929054\n",
      "        total_loss: 141.06622314453125\n",
      "        vf_explained_var: 0.8348520994186401\n",
      "        vf_loss: 141.10214233398438\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.22031047865458\n",
      "    ram_util_percent: 33.292238033635186\n",
      "  pid: 1844151\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07414980868745127\n",
      "    mean_env_wait_ms: 124.01237025036986\n",
      "    mean_inference_ms: 1.6028263803073435\n",
      "    mean_raw_obs_processing_ms: 7.483317317501047\n",
      "  time_since_restore: 100741.15248703957\n",
      "  time_this_iter_s: 541.189840555191\n",
      "  time_total_s: 100741.15248703957\n",
      "  timers:\n",
      "    learn_throughput: 367.006\n",
      "    learn_time_ms: 10899.006\n",
      "    load_throughput: 10373.857\n",
      "    load_time_ms: 385.585\n",
      "    sample_throughput: 7.505\n",
      "    sample_time_ms: 532976.621\n",
      "    update_time_ms: 3.522\n",
      "  timestamp: 1612571250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 729ff_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         565.498</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -101.99</td><td style=\"text-align: right;\">            -89.5748</td><td style=\"text-align: right;\">            -141.709</td><td style=\"text-align: right;\">           121.121</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1124.09</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-99.3593</td><td style=\"text-align: right;\">            -87.5874</td><td style=\"text-align: right;\">            -141.709</td><td style=\"text-align: right;\">           107.797</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1678.23</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-100.038</td><td style=\"text-align: right;\">            -86.2166</td><td style=\"text-align: right;\">            -149.801</td><td style=\"text-align: right;\">            111.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          2232.6</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-98.2889</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -149.801</td><td style=\"text-align: right;\">             121.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         2787.55</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-100.459</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -149.801</td><td style=\"text-align: right;\">            132.67</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         3343.35</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-96.7037</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -148.252</td><td style=\"text-align: right;\">            138.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">          3891.2</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-93.5811</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -152.392</td><td style=\"text-align: right;\">            147.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          4438.2</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-82.5458</td><td style=\"text-align: right;\">             118.214</td><td style=\"text-align: right;\">            -152.392</td><td style=\"text-align: right;\">            130.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         4991.76</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-86.9565</td><td style=\"text-align: right;\">             118.214</td><td style=\"text-align: right;\">            -152.392</td><td style=\"text-align: right;\">            145.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5548.37</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -84.159</td><td style=\"text-align: right;\">             118.214</td><td style=\"text-align: right;\">            -152.392</td><td style=\"text-align: right;\">            152.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         6104.52</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-85.5154</td><td style=\"text-align: right;\">             118.214</td><td style=\"text-align: right;\">            -147.046</td><td style=\"text-align: right;\">            153.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         6657.75</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-89.0971</td><td style=\"text-align: right;\">             118.331</td><td style=\"text-align: right;\">            -147.046</td><td style=\"text-align: right;\">            157.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         7211.81</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-83.9799</td><td style=\"text-align: right;\">             118.331</td><td style=\"text-align: right;\">            -145.194</td><td style=\"text-align: right;\">            144.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         7765.06</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-78.8631</td><td style=\"text-align: right;\">             118.331</td><td style=\"text-align: right;\">            -140.628</td><td style=\"text-align: right;\">            142.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         8314.46</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-74.5136</td><td style=\"text-align: right;\">             118.331</td><td style=\"text-align: right;\">            -130.103</td><td style=\"text-align: right;\">            138.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         8866.24</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-75.6243</td><td style=\"text-align: right;\">             118.328</td><td style=\"text-align: right;\">            -130.103</td><td style=\"text-align: right;\">            141.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          9418.6</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-71.8264</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -128.919</td><td style=\"text-align: right;\">            149.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         9970.53</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-76.5133</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -128.919</td><td style=\"text-align: right;\">            155.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         10520.4</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-66.6899</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -128.784</td><td style=\"text-align: right;\">             146.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         11073.8</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-61.0208</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -128.784</td><td style=\"text-align: right;\">            133.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         11620.2</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-57.2546</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -128.784</td><td style=\"text-align: right;\">             144.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         12173.5</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -52.014</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -123.187</td><td style=\"text-align: right;\">            135.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         12717.2</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-52.4424</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -123.187</td><td style=\"text-align: right;\">             143.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         13270.6</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-57.3276</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -125.378</td><td style=\"text-align: right;\">            151.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         13822.8</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-53.3088</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">            -125.378</td><td style=\"text-align: right;\">            150.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         14366.2</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -42.619</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -125.378</td><td style=\"text-align: right;\">            152.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         14905.9</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-37.3489</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">              -122.9</td><td style=\"text-align: right;\">             150.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         15449.5</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-31.7103</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -121.19</td><td style=\"text-align: right;\">            138.38</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         15995.7</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -38.824</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -121.19</td><td style=\"text-align: right;\">            149.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         16538.3</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-34.7954</td><td style=\"text-align: right;\">             118.379</td><td style=\"text-align: right;\">             -121.19</td><td style=\"text-align: right;\">            151.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">           17082</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-41.3116</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">             -121.19</td><td style=\"text-align: right;\">            154.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         17626.1</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -36.919</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -120.705</td><td style=\"text-align: right;\">            162.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         18172.8</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-28.3697</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -120.764</td><td style=\"text-align: right;\">            164.08</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         18717.5</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-25.3317</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -121.496</td><td style=\"text-align: right;\">            156.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         19263.4</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-30.0883</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -121.496</td><td style=\"text-align: right;\">            158.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         19805.6</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-27.7179</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -121.496</td><td style=\"text-align: right;\">            152.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         20358.5</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-25.6339</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -121.496</td><td style=\"text-align: right;\">            156.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         20911.3</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">-23.3272</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -119.529</td><td style=\"text-align: right;\">            163.01</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         21458.4</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-20.4826</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -119.353</td><td style=\"text-align: right;\">            161.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         22014.7</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-28.1516</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -123.912</td><td style=\"text-align: right;\">             174.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         22554.7</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-32.9804</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -123.912</td><td style=\"text-align: right;\">            162.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         23100.7</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-40.9162</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -123.912</td><td style=\"text-align: right;\">            161.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         23646.7</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-45.4642</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -123.912</td><td style=\"text-align: right;\">             161.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         24192.7</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">-31.6837</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -119.153</td><td style=\"text-align: right;\">            159.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         24735.4</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-24.8946</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -119.153</td><td style=\"text-align: right;\">             159.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         25278.1</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-21.6019</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -111.633</td><td style=\"text-align: right;\">            145.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         25822.2</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -19.716</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.991</td><td style=\"text-align: right;\">            144.01</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         26365.7</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-15.0626</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -105.854</td><td style=\"text-align: right;\">            143.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         26906.7</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-23.7595</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -105.854</td><td style=\"text-align: right;\">            141.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         27449.1</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-6.75795</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.854</td><td style=\"text-align: right;\">            148.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         27982.1</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-2.27254</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.145</td><td style=\"text-align: right;\">            151.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         28525.9</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> 2.03628</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.145</td><td style=\"text-align: right;\">            159.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">           29068</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">0.115944</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.145</td><td style=\"text-align: right;\">            162.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         29613.2</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-16.6579</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -103.328</td><td style=\"text-align: right;\">            154.84</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         30156.6</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-29.8633</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -114.558</td><td style=\"text-align: right;\">            146.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         30702.8</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-21.9536</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -114.558</td><td style=\"text-align: right;\">            151.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">           31241</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-15.9512</td><td style=\"text-align: right;\">             118.372</td><td style=\"text-align: right;\">            -127.337</td><td style=\"text-align: right;\">            157.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         31781.1</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-9.97389</td><td style=\"text-align: right;\">             118.372</td><td style=\"text-align: right;\">            -128.003</td><td style=\"text-align: right;\">            172.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         32326.9</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">-12.4658</td><td style=\"text-align: right;\">             118.372</td><td style=\"text-align: right;\">            -128.003</td><td style=\"text-align: right;\">            179.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">           32872</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -9.7329</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -128.003</td><td style=\"text-align: right;\">            171.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">           33409</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-1.49448</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -128.003</td><td style=\"text-align: right;\">            180.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         33953.5</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 5.21581</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -122.833</td><td style=\"text-align: right;\">             173.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         34485.8</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> 16.2285</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -122.833</td><td style=\"text-align: right;\">            172.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         35021.5</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  14.027</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -122.833</td><td style=\"text-align: right;\">               174</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         35553.8</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">   6.078</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -113.897</td><td style=\"text-align: right;\">            177.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         36093.4</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> 16.1918</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -113.897</td><td style=\"text-align: right;\">            173.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         36625.9</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> 15.9686</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -114.829</td><td style=\"text-align: right;\">            164.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         37164.2</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> 34.7553</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -114.829</td><td style=\"text-align: right;\">            163.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         37698.4</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> 43.0166</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -114.829</td><td style=\"text-align: right;\">            164.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         38229.8</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> 51.4064</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -114.829</td><td style=\"text-align: right;\">            170.01</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         38755.2</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> 57.9419</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -103.525</td><td style=\"text-align: right;\">            171.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         39294.2</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> 47.7984</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -103.944</td><td style=\"text-align: right;\">            178.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">           39824</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> 47.2564</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -127.466</td><td style=\"text-align: right;\">            186.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         40360.6</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> 49.5241</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -127.466</td><td style=\"text-align: right;\">            179.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         40899.5</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  53.869</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -127.466</td><td style=\"text-align: right;\">            178.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         41433.4</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> 60.2683</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -127.466</td><td style=\"text-align: right;\">            182.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         41970.5</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> 62.6473</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -103.972</td><td style=\"text-align: right;\">            181.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         42508.3</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> 58.5495</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -104.689</td><td style=\"text-align: right;\">            181.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         43051.6</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> 56.5169</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -104.689</td><td style=\"text-align: right;\">            186.92</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         43590.7</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  52.331</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -104.689</td><td style=\"text-align: right;\">            183.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         44132.9</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> 48.3002</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -104.689</td><td style=\"text-align: right;\">            179.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         44669.7</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> 45.7948</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">             -106.68</td><td style=\"text-align: right;\">            165.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         45204.7</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> 47.4722</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -110.039</td><td style=\"text-align: right;\">            166.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         45731.9</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 45.0191</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -110.039</td><td style=\"text-align: right;\">            169.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         46264.5</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> 49.1124</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">              -118.8</td><td style=\"text-align: right;\">            165.87</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         46797.3</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 44.9837</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">              -118.8</td><td style=\"text-align: right;\">            175.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         47334.4</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> 54.0308</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">              -118.8</td><td style=\"text-align: right;\">             177.4</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         47869.6</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> 58.5076</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">              -118.8</td><td style=\"text-align: right;\">             167.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         48396.2</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 54.5224</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -103.337</td><td style=\"text-align: right;\">            176.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         48925.1</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 60.4872</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -103.337</td><td style=\"text-align: right;\">            180.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         49455.7</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> 62.6528</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -103.337</td><td style=\"text-align: right;\">            177.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         49997.2</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> 62.7908</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -100.995</td><td style=\"text-align: right;\">            179.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         50532.1</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 62.8035</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -103.255</td><td style=\"text-align: right;\">             178.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         51068.1</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> 65.0633</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -103.255</td><td style=\"text-align: right;\">            175.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         51606.2</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 69.4319</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -103.255</td><td style=\"text-align: right;\">            178.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         52150.5</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 63.2074</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -104.766</td><td style=\"text-align: right;\">            177.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         52688.5</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">  65.262</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -104.766</td><td style=\"text-align: right;\">            177.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         53224.1</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  71.568</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -104.766</td><td style=\"text-align: right;\">            176.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         53763.1</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 69.6383</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -104.766</td><td style=\"text-align: right;\">             176.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         54314.5</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 73.3001</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.126</td><td style=\"text-align: right;\">            168.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         54853.2</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> 64.8181</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -105.126</td><td style=\"text-align: right;\">            160.04</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         55397.7</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">  43.494</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.755</td><td style=\"text-align: right;\">             155.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         55939.4</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> 24.3671</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.755</td><td style=\"text-align: right;\">            147.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         56488.4</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> 24.6442</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.755</td><td style=\"text-align: right;\">            153.38</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         57029.5</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> 30.9994</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.755</td><td style=\"text-align: right;\">            155.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         57578.5</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 31.2823</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -99.5343</td><td style=\"text-align: right;\">            148.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">           58133</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> 39.7792</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -99.5343</td><td style=\"text-align: right;\">             153.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         58683.3</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> 39.7049</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -105.427</td><td style=\"text-align: right;\">             152.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         59225.9</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> 43.9957</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -105.427</td><td style=\"text-align: right;\">            152.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         59769.4</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\"> 60.7594</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -105.427</td><td style=\"text-align: right;\">            157.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         60317.8</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> 62.7399</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.427</td><td style=\"text-align: right;\">            160.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         60863.3</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> 64.8889</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -100.929</td><td style=\"text-align: right;\">            155.09</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         61406.5</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\"> 69.0196</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -100.929</td><td style=\"text-align: right;\">            155.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         61961.8</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\"> 62.7591</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">             -105.17</td><td style=\"text-align: right;\">            159.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         62503.1</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> 60.3734</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -105.17</td><td style=\"text-align: right;\">            156.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         63047.4</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> 62.0831</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.616</td><td style=\"text-align: right;\">            159.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         63599.7</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> 66.4219</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.616</td><td style=\"text-align: right;\">            167.84</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         64146.1</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> 68.6153</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -106.616</td><td style=\"text-align: right;\">            166.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         64697.8</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\"> 64.7676</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -106.616</td><td style=\"text-align: right;\">            168.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         65244.1</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\"> 67.4276</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -96.0772</td><td style=\"text-align: right;\">            170.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         65795.3</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> 60.9107</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -104.315</td><td style=\"text-align: right;\">            165.32</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         66338.5</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">  58.482</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -104.315</td><td style=\"text-align: right;\">            164.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         66888.3</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> 66.9313</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -104.315</td><td style=\"text-align: right;\">            168.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         67440.9</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> 72.8118</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -104.315</td><td style=\"text-align: right;\">            172.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         67991.1</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\"> 72.6922</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -106.171</td><td style=\"text-align: right;\">             171.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         68544.2</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\"> 72.7722</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -106.171</td><td style=\"text-align: right;\">            173.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         69094.1</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">  72.973</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -106.171</td><td style=\"text-align: right;\">            170.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">           69642</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> 64.7006</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -106.171</td><td style=\"text-align: right;\">            165.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         70195.4</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> 69.0174</td><td style=\"text-align: right;\">             118.388</td><td style=\"text-align: right;\">            -106.171</td><td style=\"text-align: right;\">             165.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         70743.1</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\"> 71.3125</td><td style=\"text-align: right;\">             118.388</td><td style=\"text-align: right;\">            -99.2009</td><td style=\"text-align: right;\">            160.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         71294.3</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">  73.358</td><td style=\"text-align: right;\">             118.388</td><td style=\"text-align: right;\">            -99.2009</td><td style=\"text-align: right;\">            159.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         71852.2</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> 77.6506</td><td style=\"text-align: right;\">             118.388</td><td style=\"text-align: right;\">            -99.2009</td><td style=\"text-align: right;\">            163.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">           72402</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\"> 79.6485</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -97.4217</td><td style=\"text-align: right;\">            167.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         72963.4</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> 79.6504</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -98.5877</td><td style=\"text-align: right;\">            174.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         73518.6</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> 79.4203</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -98.8927</td><td style=\"text-align: right;\">            174.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         74065.9</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\"> 87.6365</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -98.8927</td><td style=\"text-align: right;\">            178.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         74616.4</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> 83.3989</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">             -99.498</td><td style=\"text-align: right;\">            178.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         75167.2</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> 85.3411</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -105.409</td><td style=\"text-align: right;\">            172.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         75705.2</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">  80.848</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -105.409</td><td style=\"text-align: right;\">            171.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         76263.9</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\"> 83.0927</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -105.409</td><td style=\"text-align: right;\">             174.4</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         76813.9</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">  85.246</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -105.409</td><td style=\"text-align: right;\">            175.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         77363.8</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 95.9299</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -102.472</td><td style=\"text-align: right;\">            181.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         77915.2</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> 94.0448</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -102.472</td><td style=\"text-align: right;\">            179.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         78475.4</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> 102.509</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -98.5501</td><td style=\"text-align: right;\">             184.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">           79014</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> 104.523</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -95.1023</td><td style=\"text-align: right;\">            182.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">           79552</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> 97.9131</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -107.166</td><td style=\"text-align: right;\">            177.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         80085.1</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> 97.8479</td><td style=\"text-align: right;\">             118.369</td><td style=\"text-align: right;\">            -107.166</td><td style=\"text-align: right;\">            177.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         80615.3</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\"> 89.4154</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -107.166</td><td style=\"text-align: right;\">            168.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         81155.3</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> 69.7463</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -109.519</td><td style=\"text-align: right;\">            163.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         81698.4</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> 60.8945</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -109.586</td><td style=\"text-align: right;\">             159.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         82236.3</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> 50.2582</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -109.586</td><td style=\"text-align: right;\">            158.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         82792.2</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\"> 47.8394</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -109.776</td><td style=\"text-align: right;\">            162.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         83333.4</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\"> 56.4569</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -109.776</td><td style=\"text-align: right;\">            163.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         83880.3</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> 60.9667</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -109.776</td><td style=\"text-align: right;\">            170.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         84425.1</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> 64.7163</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -109.895</td><td style=\"text-align: right;\">            171.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         84968.9</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\"> 60.1999</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -130.839</td><td style=\"text-align: right;\">            176.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         85508.2</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\"> 66.6776</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -130.839</td><td style=\"text-align: right;\">            180.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         86060.9</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> 81.6862</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -130.839</td><td style=\"text-align: right;\">            179.09</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         86611.1</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\"> 90.2907</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -130.839</td><td style=\"text-align: right;\">            179.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         87158.1</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> 97.0115</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -130.839</td><td style=\"text-align: right;\">             179.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         87701.8</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\"> 99.4628</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -109.84</td><td style=\"text-align: right;\">            179.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         88245.1</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\"> 93.0077</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">             -109.84</td><td style=\"text-align: right;\">            174.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         88790.1</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\"> 86.5475</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">             -109.84</td><td style=\"text-align: right;\">             174.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         89330.1</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> 82.3677</td><td style=\"text-align: right;\">             118.357</td><td style=\"text-align: right;\">            -110.177</td><td style=\"text-align: right;\">            170.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         89868.8</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> 89.0257</td><td style=\"text-align: right;\">             118.371</td><td style=\"text-align: right;\">            -110.177</td><td style=\"text-align: right;\">            167.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         90408.3</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\"> 93.1066</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -110.23</td><td style=\"text-align: right;\">            167.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         90948.9</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> 97.4183</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -110.23</td><td style=\"text-align: right;\">            166.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         91491.7</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> 97.5035</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -110.23</td><td style=\"text-align: right;\">            167.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         92043.4</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> 91.1735</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -110.23</td><td style=\"text-align: right;\">            175.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         92586.4</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\"> 89.2007</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -104.106</td><td style=\"text-align: right;\">            179.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         93122.3</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> 93.5226</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -104.106</td><td style=\"text-align: right;\">            180.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         93662.6</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> 95.4212</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -104.106</td><td style=\"text-align: right;\">            183.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         94202.4</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">  99.705</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -103.005</td><td style=\"text-align: right;\">             184.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         94750.4</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\"> 101.875</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">             -107.78</td><td style=\"text-align: right;\">            182.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         95296.8</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">  108.27</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -107.78</td><td style=\"text-align: right;\">            180.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         95843.9</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\"> 104.036</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -107.78</td><td style=\"text-align: right;\">            179.32</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         96385.1</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> 99.9953</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -107.78</td><td style=\"text-align: right;\">            177.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         96932.7</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\"> 97.8306</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -107.78</td><td style=\"text-align: right;\">            180.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         97480.2</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\"> 95.7167</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.824</td><td style=\"text-align: right;\">            183.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         98025.5</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\"> 95.7165</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.824</td><td style=\"text-align: right;\">            181.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         98576.6</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\"> 100.015</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -105.824</td><td style=\"text-align: right;\">            184.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         99122.9</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\"> 104.224</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -105.824</td><td style=\"text-align: right;\">            186.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         99659.6</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\"> 104.188</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -100.417</td><td style=\"text-align: right;\">            184.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">          100200</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\"> 106.092</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.233</td><td style=\"text-align: right;\">            182.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>RUNNING </td><td>192.168.178.60:1844151</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">          100741</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\"> 103.997</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -107.233</td><td style=\"text-align: right;\">            183.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/15.23 GiB heap, 0.0/5.22 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-04_21-27-48<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_729ff_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">          100741</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\"> 103.997</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -107.233</td><td style=\"text-align: right;\">            183.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/'\n",
    "                                              'PPO_2021-02-03_22-07-13/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_c992f_00000_0_2021-02-03_22-07-13/' \\\n",
    "                  'checkpoint_71/checkpoint-71')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-02-04_21-27-48/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_729ff_00000_0_2021-02-04_21-27-48/' \\\n",
    "                  'checkpoint_5/checkpoint-5'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m [ERROR] [1612594630.650413, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m [WARN] [1612594630.653457, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m [WARN] [1612594630.654350, 0.000000]: END Init ControllersConnection\n",
      "2021-02-06 07:57:17,148\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-06 07:57:17,292\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-04_21-27-48/PPO_ScoutingDiscreteTask_729ff_00000_0_2021-02-04_21-27-48/checkpoint_5/checkpoint-5\n",
      "2021-02-06 07:57:17,293\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2787.55278468132, '_episodes_total': 158}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1844146)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "-110.20786768587367"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}