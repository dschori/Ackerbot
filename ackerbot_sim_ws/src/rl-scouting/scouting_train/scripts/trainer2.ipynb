{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1611727512.596370, 0.001000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1611727512.604854, 0.032000]: Start Init ControllersConnection\n",
      "[WARN] [1611727512.606105, 0.032000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 8.0, (12,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2628748416900635\n",
      "7.99\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(20):\n",
    "    #obs, reward, done, _ = env.step(1)\n",
    "    obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.31133652 2.18991303 2.19176579 2.31844544 3.04565477 5.49906635\n",
      " 2.58944726 1.55777252 1.29146457 1.26287484 7.24449328 7.99      ]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 07:05:16,366\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-01-27_07-05-15_875136_2534000/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-01-27_07-05-15_875136_2534000/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-01-27_07-05-15_875136_2534000',\n 'metrics_export_port': 62359,\n 'node_id': 'dad1d6c56212a0f07ce49f7e02f7203259f40839'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [128, 128, ],\n",
    "        #\"fcnet_hiddens\": tune.grid_search([[64, 64, ], [128, 128, ], [256, 256, ]])\n",
    "    }\n",
    "    #\"model\": {\n",
    "    #    \"dim\": 40,\n",
    "    #    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [512, [5, 16], 1]]\n",
    "    #}\n",
    "    #\"model\": {\n",
    "    #    \"use_lstm\": True,\n",
    "    #    # Max seq len for training the LSTM, defaults to 20.\n",
    "    #    \"max_seq_len\": 20,\n",
    "    #    # Size of the LSTM cell.\n",
    "    #    \"lstm_cell_size\": 256\n",
    "    #}\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 2500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True, restore=restorepath)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m 2021-01-27 07:05:21,147\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m 2021-01-27 07:05:21,147\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m [ERROR] [1611727524.276263, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m [WARN] [1611727524.279164, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m [WARN] [1611727524.279987, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m 2021-01-27 07:05:29,681\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m 2021-01-27 07:05:29,757\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-01-27_07-05-17/PPO_ScoutingDiscreteTask_a156d_00000_0_2021-01-27_07-05-18/tmptqvtbtkfrestore_from_object/checkpoint-100\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m 2021-01-27 07:05:29,757\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 15133.142075777054, '_episodes_total': 1335}\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m 2021-01-27 07:05:30,908\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2534259)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=2534265)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-08-11\n",
      "  done: false\n",
      "  episode_len_mean: 487.375\n",
      "  episode_reward_max: 116.02267329880763\n",
      "  episode_reward_mean: -67.3726155448889\n",
      "  episode_reward_min: -107.9129123412074\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1343\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8472505807876587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010873379185795784\n",
      "        model: {}\n",
      "        policy_loss: -0.01301891915500164\n",
      "        total_loss: 460.656982421875\n",
      "        vf_explained_var: 0.3018514811992645\n",
      "        vf_loss: 460.66790771484375\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.45689655172414\n",
      "    ram_util_percent: 34.37801724137931\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1068435231318208\n",
      "    mean_env_wait_ms: 36.327646601590416\n",
      "    mean_inference_ms: 1.0538644058887316\n",
      "    mean_raw_obs_processing_ms: 2.432392645227823\n",
      "  time_since_restore: 162.23648619651794\n",
      "  time_this_iter_s: 162.23648619651794\n",
      "  time_total_s: 15295.378561973572\n",
      "  timers:\n",
      "    learn_throughput: 1791.3\n",
      "    learn_time_ms: 2233.015\n",
      "    load_throughput: 107963.577\n",
      "    load_time_ms: 37.05\n",
      "    sample_throughput: 25.03\n",
      "    sample_time_ms: 159808.047\n",
      "    update_time_ms: 1.576\n",
      "  timestamp: 1611727691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-10-43\n",
      "  done: false\n",
      "  episode_len_mean: 500.4\n",
      "  episode_reward_max: 116.02267329880763\n",
      "  episode_reward_mean: -79.45056228405413\n",
      "  episode_reward_min: -107.9129123412074\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1350\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8033241629600525\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01766476407647133\n",
      "        model: {}\n",
      "        policy_loss: -0.020540114492177963\n",
      "        total_loss: 394.7581787109375\n",
      "        vf_explained_var: 0.3262329399585724\n",
      "        vf_loss: 394.7752380371094\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.17695852534563\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10571982187070876\n",
      "    mean_env_wait_ms: 35.815186790097094\n",
      "    mean_inference_ms: 1.0410347222029432\n",
      "    mean_raw_obs_processing_ms: 2.365735026733289\n",
      "  time_since_restore: 314.03821849823\n",
      "  time_this_iter_s: 151.80173230171204\n",
      "  time_total_s: 15447.180294275284\n",
      "  timers:\n",
      "    learn_throughput: 1863.275\n",
      "    learn_time_ms: 2146.758\n",
      "    load_throughput: 161133.461\n",
      "    load_time_ms: 24.824\n",
      "    sample_throughput: 25.858\n",
      "    sample_time_ms: 154689.506\n",
      "    update_time_ms: 1.602\n",
      "  timestamp: 1611727843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 518.1739130434783\n",
      "  episode_reward_max: 116.02267329880763\n",
      "  episode_reward_mean: -85.6071016288835\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1358\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8224250674247742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006291175726801157\n",
      "        model: {}\n",
      "        policy_loss: -0.012154254131019115\n",
      "        total_loss: 404.787841796875\n",
      "        vf_explained_var: 0.2724483907222748\n",
      "        vf_loss: 404.7987060546875\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.11330275229358\n",
      "    ram_util_percent: 34.41055045871559\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10484373574832154\n",
      "    mean_env_wait_ms: 35.47650988565937\n",
      "    mean_inference_ms: 1.032116581478581\n",
      "    mean_raw_obs_processing_ms: 2.3533555673007043\n",
      "  time_since_restore: 466.6147360801697\n",
      "  time_this_iter_s: 152.5765175819397\n",
      "  time_total_s: 15599.756811857224\n",
      "  timers:\n",
      "    learn_throughput: 1884.738\n",
      "    learn_time_ms: 2122.311\n",
      "    load_throughput: 178565.238\n",
      "    load_time_ms: 22.401\n",
      "    sample_throughput: 26.103\n",
      "    sample_time_ms: 153239.669\n",
      "    update_time_ms: 1.536\n",
      "  timestamp: 1611727996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 435.6666666666667\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -83.45712035533741\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1371\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.828904926776886\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007383215706795454\n",
      "        model: {}\n",
      "        policy_loss: -0.013818862847983837\n",
      "        total_loss: 568.78076171875\n",
      "        vf_explained_var: 0.36101511120796204\n",
      "        vf_loss: 568.7930908203125\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.08008849557521\n",
      "    ram_util_percent: 34.400884955752204\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10409294430733053\n",
      "    mean_env_wait_ms: 35.17893194483835\n",
      "    mean_inference_ms: 1.0244824383181965\n",
      "    mean_raw_obs_processing_ms: 2.478951060558451\n",
      "  time_since_restore: 625.2190926074982\n",
      "  time_this_iter_s: 158.6043565273285\n",
      "  time_total_s: 15758.361168384552\n",
      "  timers:\n",
      "    learn_throughput: 1824.311\n",
      "    learn_time_ms: 2192.608\n",
      "    load_throughput: 202458.334\n",
      "    load_time_ms: 19.757\n",
      "    sample_throughput: 25.984\n",
      "    sample_time_ms: 153943.064\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1611728155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-18-31\n",
      "  done: false\n",
      "  episode_len_mean: 424.70212765957444\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -82.22791347249643\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1382\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7597966194152832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014901837334036827\n",
      "        model: {}\n",
      "        policy_loss: -0.021846989169716835\n",
      "        total_loss: 408.8172607421875\n",
      "        vf_explained_var: 0.5030110478401184\n",
      "        vf_loss: 408.83612060546875\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.11255605381166\n",
      "    ram_util_percent: 34.41928251121075\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1037586496885516\n",
      "    mean_env_wait_ms: 35.02721909001306\n",
      "    mean_inference_ms: 1.0205305754372123\n",
      "    mean_raw_obs_processing_ms: 2.5582797590442423\n",
      "  time_since_restore: 781.6160471439362\n",
      "  time_this_iter_s: 156.396954536438\n",
      "  time_total_s: 15914.75812292099\n",
      "  timers:\n",
      "    learn_throughput: 1779.88\n",
      "    learn_time_ms: 2247.343\n",
      "    load_throughput: 205211.324\n",
      "    load_time_ms: 19.492\n",
      "    sample_throughput: 25.99\n",
      "    sample_time_ms: 153905.69\n",
      "    update_time_ms: 1.594\n",
      "  timestamp: 1611728311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-21-05\n",
      "  done: false\n",
      "  episode_len_mean: 427.05357142857144\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -84.22431855688697\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1391\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7721710205078125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009973268024623394\n",
      "        model: {}\n",
      "        policy_loss: -0.017651572823524475\n",
      "        total_loss: 378.8121032714844\n",
      "        vf_explained_var: 0.4431336224079132\n",
      "        vf_loss: 378.8277893066406\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.1140909090909\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10355279255898413\n",
      "    mean_env_wait_ms: 34.936869139302345\n",
      "    mean_inference_ms: 1.0182785607614868\n",
      "    mean_raw_obs_processing_ms: 2.597240049393296\n",
      "  time_since_restore: 935.6964583396912\n",
      "  time_this_iter_s: 154.080411195755\n",
      "  time_total_s: 16068.838534116745\n",
      "  timers:\n",
      "    learn_throughput: 1800.495\n",
      "    learn_time_ms: 2221.612\n",
      "    load_throughput: 224838.895\n",
      "    load_time_ms: 17.791\n",
      "    sample_throughput: 26.047\n",
      "    sample_time_ms: 153566.702\n",
      "    update_time_ms: 1.534\n",
      "  timestamp: 1611728465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-23-46\n",
      "  done: false\n",
      "  episode_len_mean: 408.1470588235294\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -82.9849881452224\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1403\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8104385733604431\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011221079155802727\n",
      "        model: {}\n",
      "        policy_loss: -0.019183482974767685\n",
      "        total_loss: 380.220703125\n",
      "        vf_explained_var: 0.6216200590133667\n",
      "        vf_loss: 380.23773193359375\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.44672489082971\n",
      "    ram_util_percent: 34.444541484716154\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10335903533367763\n",
      "    mean_env_wait_ms: 34.86328685015877\n",
      "    mean_inference_ms: 1.0163869143800515\n",
      "    mean_raw_obs_processing_ms: 2.6518475308627374\n",
      "  time_since_restore: 1096.0072782039642\n",
      "  time_this_iter_s: 160.31081986427307\n",
      "  time_total_s: 16229.149353981018\n",
      "  timers:\n",
      "    learn_throughput: 1816.119\n",
      "    learn_time_ms: 2202.499\n",
      "    load_throughput: 224341.034\n",
      "    load_time_ms: 17.83\n",
      "    sample_throughput: 25.938\n",
      "    sample_time_ms: 154212.637\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1611728626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-26-19\n",
      "  done: false\n",
      "  episode_len_mean: 413.7763157894737\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -84.19575972390551\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1411\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7486122250556946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010863181203603745\n",
      "        model: {}\n",
      "        policy_loss: -0.018180513754487038\n",
      "        total_loss: 179.8787384033203\n",
      "        vf_explained_var: 0.7092308402061462\n",
      "        vf_loss: 179.89471435546875\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.09036697247706\n",
      "    ram_util_percent: 34.39999999999999\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10325447526300895\n",
      "    mean_env_wait_ms: 34.82310291802094\n",
      "    mean_inference_ms: 1.0152200170292973\n",
      "    mean_raw_obs_processing_ms: 2.6720195480312263\n",
      "  time_since_restore: 1249.127892255783\n",
      "  time_this_iter_s: 153.12061405181885\n",
      "  time_total_s: 16382.269968032837\n",
      "  timers:\n",
      "    learn_throughput: 1828.518\n",
      "    learn_time_ms: 2187.563\n",
      "    load_throughput: 224012.444\n",
      "    load_time_ms: 17.856\n",
      "    sample_throughput: 26.009\n",
      "    sample_time_ms: 153795.808\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1611728779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-28-56\n",
      "  done: false\n",
      "  episode_len_mean: 406.28409090909093\n",
      "  episode_reward_max: 116.17339750458677\n",
      "  episode_reward_mean: -80.77188462806167\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1423\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8428670763969421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0064690434373915195\n",
      "        model: {}\n",
      "        policy_loss: -0.011457937769591808\n",
      "        total_loss: 725.2473754882812\n",
      "        vf_explained_var: 0.3395184278488159\n",
      "        vf_loss: 725.2574462890625\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.07333333333334\n",
      "    ram_util_percent: 34.473333333333336\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10313930080315453\n",
      "    mean_env_wait_ms: 34.771089516421824\n",
      "    mean_inference_ms: 1.0137533041797726\n",
      "    mean_raw_obs_processing_ms: 2.7058705344464085\n",
      "  time_since_restore: 1406.4003038406372\n",
      "  time_this_iter_s: 157.27241158485413\n",
      "  time_total_s: 16539.54237961769\n",
      "  timers:\n",
      "    learn_throughput: 1841.664\n",
      "    learn_time_ms: 2171.949\n",
      "    load_throughput: 225215.332\n",
      "    load_time_ms: 17.761\n",
      "    sample_throughput: 25.984\n",
      "    sample_time_ms: 153939.473\n",
      "    update_time_ms: 1.468\n",
      "  timestamp: 1611728936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-31-28\n",
      "  done: false\n",
      "  episode_len_mean: 413.6770833333333\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -79.41253961339957\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1431\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7861567139625549\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012277934700250626\n",
      "        model: {}\n",
      "        policy_loss: -0.02105632796883583\n",
      "        total_loss: 459.860595703125\n",
      "        vf_explained_var: 0.4291813373565674\n",
      "        vf_loss: 459.879150390625\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.28341013824885\n",
      "    ram_util_percent: 34.411059907834094\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10306805125450591\n",
      "    mean_env_wait_ms: 34.74020095870399\n",
      "    mean_inference_ms: 1.0128627692042322\n",
      "    mean_raw_obs_processing_ms: 2.719378782586612\n",
      "  time_since_restore: 1558.7178919315338\n",
      "  time_this_iter_s: 152.3175880908966\n",
      "  time_total_s: 16691.859967708588\n",
      "  timers:\n",
      "    learn_throughput: 1851.961\n",
      "    learn_time_ms: 2159.873\n",
      "    load_throughput: 226538.211\n",
      "    load_time_ms: 17.657\n",
      "    sample_throughput: 26.049\n",
      "    sample_time_ms: 153558.477\n",
      "    update_time_ms: 1.491\n",
      "  timestamp: 1611729088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-34-03\n",
      "  done: false\n",
      "  episode_len_mean: 411.41\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -81.88499335410056\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8623210787773132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020873093977570534\n",
      "        model: {}\n",
      "        policy_loss: -0.02054908499121666\n",
      "        total_loss: 423.7340393066406\n",
      "        vf_explained_var: 0.3646005392074585\n",
      "        vf_loss: 423.7503967285156\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.07511312217194\n",
      "    ram_util_percent: 34.47285067873303\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10280028262482205\n",
      "    mean_env_wait_ms: 34.62811005312308\n",
      "    mean_inference_ms: 1.009863533553428\n",
      "    mean_raw_obs_processing_ms: 2.745778517230939\n",
      "  time_since_restore: 1713.1624374389648\n",
      "  time_this_iter_s: 154.44454550743103\n",
      "  time_total_s: 16846.30451321602\n",
      "  timers:\n",
      "    learn_throughput: 1832.869\n",
      "    learn_time_ms: 2182.371\n",
      "    load_throughput: 263386.443\n",
      "    load_time_ms: 15.187\n",
      "    sample_throughput: 26.185\n",
      "    sample_time_ms: 152760.046\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1611729243\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 398.78\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -80.34484874927007\n",
      "  episode_reward_min: -110.54727537035956\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1451\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7975403666496277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009781939908862114\n",
      "        model: {}\n",
      "        policy_loss: -0.021023765206336975\n",
      "        total_loss: 626.7958374023438\n",
      "        vf_explained_var: 0.28790488839149475\n",
      "        vf_loss: 626.8139038085938\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.11531531531534\n",
      "    ram_util_percent: 34.49009009009009\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10248985723465534\n",
      "    mean_env_wait_ms: 34.50216811200718\n",
      "    mean_inference_ms: 1.0064227990273624\n",
      "    mean_raw_obs_processing_ms: 2.806956617467933\n",
      "  time_since_restore: 1869.083083152771\n",
      "  time_this_iter_s: 155.92064571380615\n",
      "  time_total_s: 17002.225158929825\n",
      "  timers:\n",
      "    learn_throughput: 1833.246\n",
      "    learn_time_ms: 2181.922\n",
      "    load_throughput: 251416.001\n",
      "    load_time_ms: 15.91\n",
      "    sample_throughput: 26.114\n",
      "    sample_time_ms: 153173.546\n",
      "    update_time_ms: 1.491\n",
      "  timestamp: 1611729399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 390.91\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -82.10940199663901\n",
      "  episode_reward_min: -108.3024699705839\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7753998637199402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007837162353098392\n",
      "        model: {}\n",
      "        policy_loss: -0.019353078678250313\n",
      "        total_loss: 258.5098876953125\n",
      "        vf_explained_var: 0.6267247796058655\n",
      "        vf_loss: 258.5268859863281\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9524886877828\n",
      "    ram_util_percent: 34.508144796380094\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10240458693856759\n",
      "    mean_env_wait_ms: 34.46097525241311\n",
      "    mean_inference_ms: 1.0053493763212686\n",
      "    mean_raw_obs_processing_ms: 2.8482013072661596\n",
      "  time_since_restore: 2023.4712767601013\n",
      "  time_this_iter_s: 154.38819360733032\n",
      "  time_total_s: 17156.613352537155\n",
      "  timers:\n",
      "    learn_throughput: 1833.347\n",
      "    learn_time_ms: 2181.802\n",
      "    load_throughput: 263042.454\n",
      "    load_time_ms: 15.207\n",
      "    sample_throughput: 26.083\n",
      "    sample_time_ms: 153358.8\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1611729553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-41-48\n",
      "  done: false\n",
      "  episode_len_mean: 405.59\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -82.06336027130746\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1470\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7952349781990051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014881663024425507\n",
      "        model: {}\n",
      "        policy_loss: -0.024798741564154625\n",
      "        total_loss: 444.9472351074219\n",
      "        vf_explained_var: 0.37630021572113037\n",
      "        vf_loss: 444.9675598144531\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.15927601809955\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10233704482757677\n",
      "    mean_env_wait_ms: 34.42824433332715\n",
      "    mean_inference_ms: 1.0044353508814867\n",
      "    mean_raw_obs_processing_ms: 2.8660856696806083\n",
      "  time_since_restore: 2178.537932395935\n",
      "  time_this_iter_s: 155.06665563583374\n",
      "  time_total_s: 17311.68000817299\n",
      "  timers:\n",
      "    learn_throughput: 1861.082\n",
      "    learn_time_ms: 2149.288\n",
      "    load_throughput: 266698.714\n",
      "    load_time_ms: 14.998\n",
      "    sample_throughput: 26.137\n",
      "    sample_time_ms: 153040.091\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1611729708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.64\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -81.47125684811218\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1481\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8858565092086792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008608429692685604\n",
      "        model: {}\n",
      "        policy_loss: -0.01532662846148014\n",
      "        total_loss: 317.6376037597656\n",
      "        vf_explained_var: 0.625571608543396\n",
      "        vf_loss: 317.6503601074219\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.10580357142855\n",
      "    ram_util_percent: 34.51026785714286\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10227476721541454\n",
      "    mean_env_wait_ms: 34.404405248751054\n",
      "    mean_inference_ms: 1.0036973110549992\n",
      "    mean_raw_obs_processing_ms: 2.8768901653927412\n",
      "  time_since_restore: 2335.5240211486816\n",
      "  time_this_iter_s: 156.98608875274658\n",
      "  time_total_s: 17468.666096925735\n",
      "  timers:\n",
      "    learn_throughput: 1896.18\n",
      "    learn_time_ms: 2109.505\n",
      "    load_throughput: 266929.122\n",
      "    load_time_ms: 14.985\n",
      "    sample_throughput: 26.119\n",
      "    sample_time_ms: 153142.531\n",
      "    update_time_ms: 1.417\n",
      "  timestamp: 1611729866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-46-59\n",
      "  done: false\n",
      "  episode_len_mean: 404.47\n",
      "  episode_reward_max: 116.98697938528146\n",
      "  episode_reward_mean: -81.3744299645585\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1490\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8667002320289612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011203563772141933\n",
      "        model: {}\n",
      "        policy_loss: -0.01960889808833599\n",
      "        total_loss: 515.9444580078125\n",
      "        vf_explained_var: 0.2298804670572281\n",
      "        vf_loss: 515.9607543945312\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.03013698630136\n",
      "    ram_util_percent: 34.42739726027396\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10223839370553554\n",
      "    mean_env_wait_ms: 34.389581654109676\n",
      "    mean_inference_ms: 1.0031724278875143\n",
      "    mean_raw_obs_processing_ms: 2.8849702271933593\n",
      "  time_since_restore: 2489.164921283722\n",
      "  time_this_iter_s: 153.64090013504028\n",
      "  time_total_s: 17622.306997060776\n",
      "  timers:\n",
      "    learn_throughput: 1895.702\n",
      "    learn_time_ms: 2110.037\n",
      "    load_throughput: 251652.452\n",
      "    load_time_ms: 15.895\n",
      "    sample_throughput: 26.127\n",
      "    sample_time_ms: 153096.497\n",
      "    update_time_ms: 1.419\n",
      "  timestamp: 1611730019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 416.29\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -76.83667539978181\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9308325052261353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009271007031202316\n",
      "        model: {}\n",
      "        policy_loss: -0.018106991425156593\n",
      "        total_loss: 533.8634033203125\n",
      "        vf_explained_var: 0.4656963050365448\n",
      "        vf_loss: 533.878662109375\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.48354978354978\n",
      "    ram_util_percent: 34.42251082251082\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221835917922842\n",
      "    mean_env_wait_ms: 34.376638716840304\n",
      "    mean_inference_ms: 1.0026907421319995\n",
      "    mean_raw_obs_processing_ms: 2.885211077488583\n",
      "  time_since_restore: 2650.6289699077606\n",
      "  time_this_iter_s: 161.4640486240387\n",
      "  time_total_s: 17783.771045684814\n",
      "  timers:\n",
      "    learn_throughput: 1881.44\n",
      "    learn_time_ms: 2126.031\n",
      "    load_throughput: 265248.653\n",
      "    load_time_ms: 15.08\n",
      "    sample_throughput: 26.11\n",
      "    sample_time_ms: 153195.901\n",
      "    update_time_ms: 1.431\n",
      "  timestamp: 1611730181\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-52-26\n",
      "  done: false\n",
      "  episode_len_mean: 388.21\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -76.59827930678317\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1515\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9592689871788025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0090778898447752\n",
      "        model: {}\n",
      "        policy_loss: -0.018016083166003227\n",
      "        total_loss: 414.12127685546875\n",
      "        vf_explained_var: 0.5890504717826843\n",
      "        vf_loss: 414.1365661621094\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.72033898305085\n",
      "    ram_util_percent: 34.49745762711864\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221218763801987\n",
      "    mean_env_wait_ms: 34.3682707595282\n",
      "    mean_inference_ms: 1.0024031378297655\n",
      "    mean_raw_obs_processing_ms: 2.9011386298567015\n",
      "  time_since_restore: 2815.7543127536774\n",
      "  time_this_iter_s: 165.12534284591675\n",
      "  time_total_s: 17948.89638853073\n",
      "  timers:\n",
      "    learn_throughput: 1848.426\n",
      "    learn_time_ms: 2164.003\n",
      "    load_throughput: 279038.007\n",
      "    load_time_ms: 14.335\n",
      "    sample_throughput: 25.914\n",
      "    sample_time_ms: 154355.93\n",
      "    update_time_ms: 1.45\n",
      "  timestamp: 1611730346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 365.36\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -79.62369195395499\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1530\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8275261521339417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012055273167788982\n",
      "        model: {}\n",
      "        policy_loss: -0.025053584948182106\n",
      "        total_loss: 579.7545166015625\n",
      "        vf_explained_var: 0.4577372670173645\n",
      "        vf_loss: 579.7760620117188\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58008658008659\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10221066628398191\n",
      "    mean_env_wait_ms: 34.369344249042086\n",
      "    mean_inference_ms: 1.0024116895237172\n",
      "    mean_raw_obs_processing_ms: 2.9251657800043716\n",
      "  time_since_restore: 2978.1960723400116\n",
      "  time_this_iter_s: 162.44175958633423\n",
      "  time_total_s: 18111.338148117065\n",
      "  timers:\n",
      "    learn_throughput: 1844.595\n",
      "    learn_time_ms: 2168.497\n",
      "    load_throughput: 277019.058\n",
      "    load_time_ms: 14.439\n",
      "    sample_throughput: 25.829\n",
      "    sample_time_ms: 154867.581\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1611730508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_07-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 350.28\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -73.0843616386339\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1543\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9245057106018066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009567026980221272\n",
      "        model: {}\n",
      "        policy_loss: -0.017994167283177376\n",
      "        total_loss: 495.921875\n",
      "        vf_explained_var: 0.6359312534332275\n",
      "        vf_loss: 495.9370422363281\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.39649122807018\n",
      "    ram_util_percent: 34.50482456140351\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10222715044070427\n",
      "    mean_env_wait_ms: 34.37661604026442\n",
      "    mean_inference_ms: 1.0026142297092309\n",
      "    mean_raw_obs_processing_ms: 2.9551773573149513\n",
      "  time_since_restore: 3137.6919796466827\n",
      "  time_this_iter_s: 159.49590730667114\n",
      "  time_total_s: 18270.834055423737\n",
      "  timers:\n",
      "    learn_throughput: 1842.165\n",
      "    learn_time_ms: 2171.359\n",
      "    load_throughput: 287693.424\n",
      "    load_time_ms: 13.904\n",
      "    sample_throughput: 25.71\n",
      "    sample_time_ms: 155581.817\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1611730668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-00-34\n",
      "  done: false\n",
      "  episode_len_mean: 325.71\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -64.1897752447449\n",
      "  episode_reward_min: -110.87064400911335\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1557\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.933979332447052\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012966923415660858\n",
      "        model: {}\n",
      "        policy_loss: -0.025958312675356865\n",
      "        total_loss: 490.1916198730469\n",
      "        vf_explained_var: 0.6686280369758606\n",
      "        vf_loss: 490.2137451171875\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.98354430379746\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10226139422549047\n",
      "    mean_env_wait_ms: 34.396062217878054\n",
      "    mean_inference_ms: 1.003018781843298\n",
      "    mean_raw_obs_processing_ms: 2.9917420109870263\n",
      "  time_since_restore: 3303.474905729294\n",
      "  time_this_iter_s: 165.78292608261108\n",
      "  time_total_s: 18436.616981506348\n",
      "  timers:\n",
      "    learn_throughput: 1870.67\n",
      "    learn_time_ms: 2138.271\n",
      "    load_throughput: 294019.376\n",
      "    load_time_ms: 13.605\n",
      "    sample_throughput: 25.518\n",
      "    sample_time_ms: 156751.379\n",
      "    update_time_ms: 1.42\n",
      "  timestamp: 1611730834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 322.24\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -55.7759508405985\n",
      "  episode_reward_min: -109.41346214901597\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1569\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9399060010910034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015438152477145195\n",
      "        model: {}\n",
      "        policy_loss: -0.026842257007956505\n",
      "        total_loss: 433.0733337402344\n",
      "        vf_explained_var: 0.6494691371917725\n",
      "        vf_loss: 433.0955810546875\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.51194690265486\n",
      "    ram_util_percent: 34.510619469026544\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10229693186755386\n",
      "    mean_env_wait_ms: 34.414119956954444\n",
      "    mean_inference_ms: 1.0033325977140672\n",
      "    mean_raw_obs_processing_ms: 3.025557569727576\n",
      "  time_since_restore: 3461.8368723392487\n",
      "  time_this_iter_s: 158.36196660995483\n",
      "  time_total_s: 18594.978948116302\n",
      "  timers:\n",
      "    learn_throughput: 1865.157\n",
      "    learn_time_ms: 2144.591\n",
      "    load_throughput: 313669.041\n",
      "    load_time_ms: 12.752\n",
      "    sample_throughput: 25.48\n",
      "    sample_time_ms: 156988.409\n",
      "    update_time_ms: 1.384\n",
      "  timestamp: 1611730992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 323.46\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -57.791177480934465\n",
      "  episode_reward_min: -109.41346214901597\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8590400815010071\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01062469556927681\n",
      "        model: {}\n",
      "        policy_loss: -0.021595535799860954\n",
      "        total_loss: 448.21966552734375\n",
      "        vf_explained_var: 0.457763671875\n",
      "        vf_loss: 448.2380676269531\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.29615384615384\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10234435731894553\n",
      "    mean_env_wait_ms: 34.4384164245238\n",
      "    mean_inference_ms: 1.0038588444062866\n",
      "    mean_raw_obs_processing_ms: 3.0540362890557766\n",
      "  time_since_restore: 3625.729873418808\n",
      "  time_this_iter_s: 163.89300107955933\n",
      "  time_total_s: 18758.87194919586\n",
      "  timers:\n",
      "    learn_throughput: 1850.254\n",
      "    learn_time_ms: 2161.865\n",
      "    load_throughput: 316419.304\n",
      "    load_time_ms: 12.641\n",
      "    sample_throughput: 25.329\n",
      "    sample_time_ms: 157920.259\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1611731156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-08-36\n",
      "  done: false\n",
      "  episode_len_mean: 308.77\n",
      "  episode_reward_max: 117.53087998357763\n",
      "  episode_reward_mean: -57.68659269756216\n",
      "  episode_reward_min: -109.41346214901597\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1592\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8556486964225769\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01398246455937624\n",
      "        model: {}\n",
      "        policy_loss: -0.020912982523441315\n",
      "        total_loss: 313.44464111328125\n",
      "        vf_explained_var: 0.5931130051612854\n",
      "        vf_loss: 313.4613342285156\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.56929824561405\n",
      "    ram_util_percent: 34.5140350877193\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10239327722957361\n",
      "    mean_env_wait_ms: 34.46594741618505\n",
      "    mean_inference_ms: 1.0044637444987738\n",
      "    mean_raw_obs_processing_ms: 3.0880610925562495\n",
      "  time_since_restore: 3785.3731207847595\n",
      "  time_this_iter_s: 159.64324736595154\n",
      "  time_total_s: 18918.515196561813\n",
      "  timers:\n",
      "    learn_throughput: 1846.984\n",
      "    learn_time_ms: 2165.693\n",
      "    load_throughput: 300674.497\n",
      "    load_time_ms: 13.303\n",
      "    sample_throughput: 25.257\n",
      "    sample_time_ms: 158373.027\n",
      "    update_time_ms: 1.452\n",
      "  timestamp: 1611731316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-11-20\n",
      "  done: false\n",
      "  episode_len_mean: 296.35\n",
      "  episode_reward_max: 118.54537689241238\n",
      "  episode_reward_mean: -55.598424105043414\n",
      "  episode_reward_min: -109.41346214901597\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1608\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9321532249450684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008564368821680546\n",
      "        model: {}\n",
      "        policy_loss: -0.018582649528980255\n",
      "        total_loss: 437.2475280761719\n",
      "        vf_explained_var: 0.7142554521560669\n",
      "        vf_loss: 437.2635192871094\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.35622317596567\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10243393141140916\n",
      "    mean_env_wait_ms: 34.48925055079272\n",
      "    mean_inference_ms: 1.0050070191335934\n",
      "    mean_raw_obs_processing_ms: 3.135707063274793\n",
      "  time_since_restore: 3948.9466576576233\n",
      "  time_this_iter_s: 163.57353687286377\n",
      "  time_total_s: 19082.088733434677\n",
      "  timers:\n",
      "    learn_throughput: 1842.967\n",
      "    learn_time_ms: 2170.413\n",
      "    load_throughput: 322396.972\n",
      "    load_time_ms: 12.407\n",
      "    sample_throughput: 25.153\n",
      "    sample_time_ms: 159025.056\n",
      "    update_time_ms: 1.433\n",
      "  timestamp: 1611731480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-13-54\n",
      "  done: false\n",
      "  episode_len_mean: 312.21\n",
      "  episode_reward_max: 118.54537689241238\n",
      "  episode_reward_mean: -55.498471428427045\n",
      "  episode_reward_min: -109.41346214901597\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1616\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8053603172302246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010093598626554012\n",
      "        model: {}\n",
      "        policy_loss: -0.018419377505779266\n",
      "        total_loss: 203.03146362304688\n",
      "        vf_explained_var: 0.7313950061798096\n",
      "        vf_loss: 203.0468292236328\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.6398190045249\n",
      "    ram_util_percent: 34.510407239819\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10244940466430431\n",
      "    mean_env_wait_ms: 34.49926513432834\n",
      "    mean_inference_ms: 1.0052581627235928\n",
      "    mean_raw_obs_processing_ms: 3.152910247985911\n",
      "  time_since_restore: 4103.499359369278\n",
      "  time_this_iter_s: 154.55270171165466\n",
      "  time_total_s: 19236.64143514633\n",
      "  timers:\n",
      "    learn_throughput: 1846.489\n",
      "    learn_time_ms: 2166.274\n",
      "    load_throughput: 320945.166\n",
      "    load_time_ms: 12.463\n",
      "    sample_throughput: 25.139\n",
      "    sample_time_ms: 159116.71\n",
      "    update_time_ms: 1.463\n",
      "  timestamp: 1611731634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 326.6\n",
      "  episode_reward_max: 118.54537689241238\n",
      "  episode_reward_mean: -52.82754925078665\n",
      "  episode_reward_min: -102.12679694383992\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1627\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7850744724273682\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012243428267538548\n",
      "        model: {}\n",
      "        policy_loss: -0.02215499058365822\n",
      "        total_loss: 142.92578125\n",
      "        vf_explained_var: 0.8256109356880188\n",
      "        vf_loss: 142.94424438476562\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.60533333333332\n",
      "    ram_util_percent: 34.5\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10246992402739424\n",
      "    mean_env_wait_ms: 34.51200431570793\n",
      "    mean_inference_ms: 1.0056141399897274\n",
      "    mean_raw_obs_processing_ms: 3.169883055219014\n",
      "  time_since_restore: 4261.229256153107\n",
      "  time_this_iter_s: 157.72989678382874\n",
      "  time_total_s: 19394.37133193016\n",
      "  timers:\n",
      "    learn_throughput: 1862.561\n",
      "    learn_time_ms: 2147.581\n",
      "    load_throughput: 319905.423\n",
      "    load_time_ms: 12.504\n",
      "    sample_throughput: 25.195\n",
      "    sample_time_ms: 158762.846\n",
      "    update_time_ms: 1.453\n",
      "  timestamp: 1611731792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: a156d_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a156d_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-27_08-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 331.52\n",
      "  episode_reward_max: 118.54537689241238\n",
      "  episode_reward_mean: -55.04186966924943\n",
      "  episode_reward_min: -102.12679694383992\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1638\n",
      "  experiment_id: 8b40e97173b34a5286d39a888fffb61b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8457293510437012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008418182842433453\n",
      "        model: {}\n",
      "        policy_loss: -0.015756234526634216\n",
      "        total_loss: 140.08570861816406\n",
      "        vf_explained_var: 0.8205282092094421\n",
      "        vf_loss: 140.09893798828125\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.2151515151515\n",
      "    ram_util_percent: 34.52207792207792\n",
      "  pid: 2534259\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10249794119479531\n",
      "    mean_env_wait_ms: 34.528403670497134\n",
      "    mean_inference_ms: 1.0060785561667445\n",
      "    mean_raw_obs_processing_ms: 3.184127848850247\n",
      "  time_since_restore: 4423.142803430557\n",
      "  time_this_iter_s: 161.91354727745056\n",
      "  time_total_s: 19556.28487920761\n",
      "  timers:\n",
      "    learn_throughput: 1879.603\n",
      "    learn_time_ms: 2128.109\n",
      "    load_throughput: 320956.832\n",
      "    load_time_ms: 12.463\n",
      "    sample_throughput: 25.242\n",
      "    sample_time_ms: 158463.844\n",
      "    update_time_ms: 1.444\n",
      "  timestamp: 1611731954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: a156d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         15295.4</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">-67.3726</td><td style=\"text-align: right;\">             116.023</td><td style=\"text-align: right;\">            -107.913</td><td style=\"text-align: right;\">           487.375</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         15447.2</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">-79.4506</td><td style=\"text-align: right;\">             116.023</td><td style=\"text-align: right;\">            -107.913</td><td style=\"text-align: right;\">             500.4</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         15599.8</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">-85.6071</td><td style=\"text-align: right;\">             116.023</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           518.174</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         15758.4</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">-83.4571</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           435.667</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         15914.8</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">-82.2279</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           424.702</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         16068.8</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">-84.2243</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           427.054</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         16229.1</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> -82.985</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           408.147</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         16382.3</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">-84.1958</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           413.776</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         16539.5</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">-80.7719</td><td style=\"text-align: right;\">             116.173</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           406.284</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         16691.9</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">-79.4125</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">           413.677</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         16846.3</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> -81.885</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">            411.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         17002.2</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">-80.3448</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.547</td><td style=\"text-align: right;\">            398.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         17156.6</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">-82.1094</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -108.302</td><td style=\"text-align: right;\">            390.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         17311.7</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">-82.0634</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            405.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         17468.7</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">-81.4713</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            400.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         17622.3</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">-81.3744</td><td style=\"text-align: right;\">             116.987</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            404.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         17783.8</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">-76.8367</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            416.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         17948.9</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">-76.5983</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            388.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         18111.3</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-79.6237</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            365.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         18270.8</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">-73.0844</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            350.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         18436.6</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">-64.1898</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -110.871</td><td style=\"text-align: right;\">            325.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">           18595</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\"> -55.776</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -109.413</td><td style=\"text-align: right;\">            322.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         18758.9</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">-57.7912</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -109.413</td><td style=\"text-align: right;\">            323.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         18918.5</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">-57.6866</td><td style=\"text-align: right;\">             117.531</td><td style=\"text-align: right;\">            -109.413</td><td style=\"text-align: right;\">            308.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         19082.1</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">-55.5984</td><td style=\"text-align: right;\">             118.545</td><td style=\"text-align: right;\">            -109.413</td><td style=\"text-align: right;\">            296.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         19236.6</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">-55.4985</td><td style=\"text-align: right;\">             118.545</td><td style=\"text-align: right;\">            -109.413</td><td style=\"text-align: right;\">            312.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         19394.4</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">-52.8275</td><td style=\"text-align: right;\">             118.545</td><td style=\"text-align: right;\">            -102.127</td><td style=\"text-align: right;\">             326.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.96 GiB heap, 0.0/4.79 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-27_07-05-17<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a156d_00000</td><td>RUNNING </td><td>192.168.178.60:2534259</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         19556.3</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">-55.0419</td><td style=\"text-align: right;\">             118.545</td><td style=\"text-align: right;\">            -102.127</td><td style=\"text-align: right;\">            331.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/PPO_2021-01-26_21-34-32/PPO_ScoutingDiscreteTask_e5473_00000_0_2021-01-26_21-34-32/checkpoint_100/checkpoint-100')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/dschori/ray_results/PPO_2021-01-27_06-42-22/PPO_ScoutingDiscreteTask_6db9c_00000_0_2021-01-27_06-42-22/checkpoint_1/checkpoint-1'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 06:47:08,421\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-01-27 06:47:08,422\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m [ERROR] [1611726431.556651, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m [WARN] [1611726431.559983, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m [WARN] [1611726431.560941, 0.000000]: END Init ControllersConnection\n",
      "2021-01-27 06:47:17,369\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-01-27 06:47:17,453\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-01-27_06-42-22/PPO_ScoutingDiscreteTask_6db9c_00000_0_2021-01-27_06-42-22/checkpoint_1/checkpoint-1\n",
      "2021-01-27 06:47:17,454\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 239.04536890983582, '_episodes_total': 80}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2483286)\u001B[0m None\n"
     ]
    },
    {
     "data": {
      "text/plain": "-93.38296034454262"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)\n",
    "\n",
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}