{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ray\n",
    "from gym.spaces import Discrete, Tuple\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.models import ModelCatalog, ModelV2\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.utils import get_filter_config\n",
    "from ray.rllib.utils import override\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1611917370.253310, 58.378000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1611917370.260268, 0.010000]: Start Init ControllersConnection\n",
      "[WARN] [1611917370.261849, 0.010000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Box(0.0, 4.0, (18,), float32), Box(-10.0, 10.0, (2,), float32), Box(-1.0, 1.0, (2,), float32))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.,  1.])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "plt.figure()\n",
    "thetas = np.linspace(-2.44, 2.44, 18)\n",
    "thetas = np.linspace(-3.14, 3.14, 18)\n",
    "plt.polar(thetas, obs[0])\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978138\n",
      "3.99\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(40):\n",
    "    #obs, reward, done, _ = env.step(1)\n",
    "    obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs[0].min())\n",
    "print(obs[0].max())\n",
    "\n",
    "#plt.imshow(obs[0].squeeze())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#print(obs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "class CNNPlusFCConcatModel(TFModelV2):\n",
    "    \"\"\"TFModelV2 concat'ing CNN outputs to flat input(s), followed by FC(s).\n",
    "\n",
    "    Note: This model should be used for complex (Dict or Tuple) observation\n",
    "    spaces that have one or more image components.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        # TODO: (sven) Support Dicts as well.\n",
    "        assert isinstance(obs_space.original_space, (Tuple)), \\\n",
    "            \"`obs_space.original_space` must be Tuple!\"\n",
    "\n",
    "        super().__init__(obs_space, action_space, num_outputs, model_config,\n",
    "                         name)\n",
    "\n",
    "        # Build the CNN(s) given obs_space's image components.\n",
    "        self.cnns = {}\n",
    "        concat_size = 0\n",
    "        for i, component in enumerate(obs_space.original_space):\n",
    "            # Image space.\n",
    "            if len(component.shape) == 3:\n",
    "                config = {\n",
    "                    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [5, 16], 1]],\n",
    "                    \"conv_activation\": model_config.get(\"conv_activation\"),\n",
    "                }\n",
    "                cnn = ModelCatalog.get_model_v2(\n",
    "                    component,\n",
    "                    action_space,\n",
    "                    num_outputs=None,\n",
    "                    model_config=config,\n",
    "                    framework=\"tf\",\n",
    "                    name=\"cnn_{}\".format(i))\n",
    "                concat_size += cnn.num_outputs\n",
    "                self.cnns[i] = cnn\n",
    "                self.register_variables(cnn.base_model.variables)\n",
    "            # Discrete inputs -> One-hot encode.\n",
    "            elif isinstance(component, Discrete):\n",
    "                concat_size += component.n\n",
    "            # TODO: (sven) Multidiscrete (see e.g. our auto-LSTM wrappers).\n",
    "            # Everything else (1D Box).\n",
    "            else:\n",
    "                assert len(component.shape) == 1, \\\n",
    "                    \"Only input Box 1D or 3D spaces allowed!\"\n",
    "                concat_size += component.shape[-1]\n",
    "\n",
    "        self.logits_and_value_model = None\n",
    "        self._value_out = None\n",
    "        if num_outputs:\n",
    "            # Action-distribution head.\n",
    "            concat_layer = tf.keras.layers.Input((concat_size, ))\n",
    "            logits_layer = tf.keras.layers.Dense(\n",
    "                num_outputs,\n",
    "                activation=tf.keras.activations.linear,\n",
    "                name=\"logits\")(concat_layer)\n",
    "\n",
    "            # Create the value branch model.\n",
    "            value_layer = tf.keras.layers.Dense(\n",
    "                1,\n",
    "                name=\"value_out\",\n",
    "                activation=None,\n",
    "                kernel_initializer=normc_initializer(0.01))(concat_layer)\n",
    "            self.logits_and_value_model = tf.keras.models.Model(\n",
    "                concat_layer, [logits_layer, value_layer])\n",
    "        else:\n",
    "            self.num_outputs = concat_size\n",
    "        self.register_variables(self.logits_and_value_model.variables)\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        # Push image observations through our CNNs.\n",
    "        outs = []\n",
    "        for i, component in enumerate(input_dict[\"obs\"]):\n",
    "            if i in self.cnns:\n",
    "                cnn_out, _ = self.cnns[i]({\"obs\": component})\n",
    "                outs.append(cnn_out)\n",
    "            else:\n",
    "                outs.append(component)\n",
    "        # Concat all outputs and the non-image inputs.\n",
    "        out = tf.concat(outs, axis=1)\n",
    "        if not self.logits_and_value_model:\n",
    "            return out, []\n",
    "\n",
    "        # Value branch.\n",
    "        logits, values = self.logits_and_value_model(out)\n",
    "        self._value_out = tf.reshape(values, [-1])\n",
    "        return logits, []\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "\n",
    "ModelCatalog.register_custom_model(\"CNNPlusFCConcatModel\", CNNPlusFCConcatModel)\n",
    "\n",
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"model\": {\n",
    "        #\"custom_model\": \"CNNPlusFCConcatModel\",\n",
    "        #\"conv_filters\": [[16, [4, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [5, 16], 1]],\n",
    "        \"fcnet_hiddens\": [256, ],\n",
    "        #\"use_lstm\": True,\n",
    "        #\"lstm_cell_size\": 256,\n",
    "        #\"fcnet_hiddens\": tune.grid_search([[64, 64, ], [128, 128, ], [256, 256, ]])\n",
    "    }\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 4500,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-29 11:49:34,808\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-01-29_11-49-34_215985_3018947/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-01-29_11-49-34_215985_3018947/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-01-29_11-49-34_215985_3018947',\n 'metrics_export_port': 52338,\n 'node_id': '7286ffcc103f78465baf7fc2b9eddbbe230a80d5'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True, restore=restorepath)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m 2021-01-29 11:49:38,834\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m 2021-01-29 11:49:38,834\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m [ERROR] [1611917382.236710, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m [WARN] [1611917382.239425, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m [WARN] [1611917382.240412, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m 2021-01-29 11:49:47,233\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m 2021-01-29 11:49:47,292\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-01-29_11-49-35/PPO_ScoutingDiscreteTask_ad561_00000_0_2021-01-29_11-49-35/tmpc1zezpperestore_from_object/checkpoint-375\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m 2021-01-29 11:49:47,292\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 375, '_timesteps_total': None, '_time_total': 58624.67053723335, '_episodes_total': 3501}\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m 2021-01-29 11:49:48,441\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3019164)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=3019162)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 408.77777777777777\n",
      "  episode_reward_max: 108.7250733902892\n",
      "  episode_reward_mean: -56.65036288117798\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3510\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5126635432243347\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005027469713240862\n",
      "        model: {}\n",
      "        policy_loss: -0.008928177878260612\n",
      "        total_loss: 421.05218505859375\n",
      "        vf_explained_var: 0.5495344400405884\n",
      "        vf_loss: 421.06011962890625\n",
      "    num_steps_sampled: 1504000\n",
      "    num_steps_trained: 1504000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89453781512606\n",
      "    ram_util_percent: 34.209243697478975\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0668202481011217\n",
      "    mean_env_wait_ms: 37.20622925542647\n",
      "    mean_inference_ms: 0.935225151860991\n",
      "    mean_raw_obs_processing_ms: 2.718857543762253\n",
      "  time_since_restore: 166.1136875152588\n",
      "  time_this_iter_s: 166.1136875152588\n",
      "  time_total_s: 58790.78422474861\n",
      "  timers:\n",
      "    learn_throughput: 1886.125\n",
      "    learn_time_ms: 2120.75\n",
      "    load_throughput: 96151.116\n",
      "    load_time_ms: 41.601\n",
      "    sample_throughput: 24.416\n",
      "    sample_time_ms: 163828.514\n",
      "    update_time_ms: 1.155\n",
      "  timestamp: 1611917553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1504000\n",
      "  training_iteration: 376\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 462.125\n",
      "  episode_reward_max: 108.7250733902892\n",
      "  episode_reward_mean: -51.300273712128266\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3517\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.602975070476532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005922781303524971\n",
      "        model: {}\n",
      "        policy_loss: -0.011822396889328957\n",
      "        total_loss: 283.7127685546875\n",
      "        vf_explained_var: 0.6743267178535461\n",
      "        vf_loss: 283.7234191894531\n",
      "    num_steps_sampled: 1508000\n",
      "    num_steps_trained: 1508000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8913043478261\n",
      "    ram_util_percent: 34.54434782608695\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06686614138876333\n",
      "    mean_env_wait_ms: 37.06523143369494\n",
      "    mean_inference_ms: 0.93390563146515\n",
      "    mean_raw_obs_processing_ms: 2.6057035192125735\n",
      "  time_since_restore: 327.1969802379608\n",
      "  time_this_iter_s: 161.08329272270203\n",
      "  time_total_s: 58951.86751747131\n",
      "  timers:\n",
      "    learn_throughput: 1993.821\n",
      "    learn_time_ms: 2006.198\n",
      "    load_throughput: 151101.849\n",
      "    load_time_ms: 26.472\n",
      "    sample_throughput: 24.774\n",
      "    sample_time_ms: 161460.668\n",
      "    update_time_ms: 1.163\n",
      "  timestamp: 1611917714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1508000\n",
      "  training_iteration: 377\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_11-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 471.36\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -35.90593720528497\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3526\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6575131416320801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008030934259295464\n",
      "        model: {}\n",
      "        policy_loss: -0.009590727277100086\n",
      "        total_loss: 172.46066284179688\n",
      "        vf_explained_var: 0.8619052171707153\n",
      "        vf_loss: 172.46865844726562\n",
      "    num_steps_sampled: 1512000\n",
      "    num_steps_trained: 1512000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.42961373390557\n",
      "    ram_util_percent: 34.808154506437766\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06685280621046673\n",
      "    mean_env_wait_ms: 36.97239727710008\n",
      "    mean_inference_ms: 0.9328826964976956\n",
      "    mean_raw_obs_processing_ms: 2.5886653341538493\n",
      "  time_since_restore: 490.86979389190674\n",
      "  time_this_iter_s: 163.67281365394592\n",
      "  time_total_s: 59115.54033112526\n",
      "  timers:\n",
      "    learn_throughput: 2030.21\n",
      "    learn_time_ms: 1970.239\n",
      "    load_throughput: 190976.433\n",
      "    load_time_ms: 20.945\n",
      "    sample_throughput: 24.763\n",
      "    sample_time_ms: 161530.273\n",
      "    update_time_ms: 1.211\n",
      "  timestamp: 1611917878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1512000\n",
      "  training_iteration: 378\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 454.2\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -48.24931209198933\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3536\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49008074402809143\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00923119392246008\n",
      "        model: {}\n",
      "        policy_loss: -0.013906476087868214\n",
      "        total_loss: 528.7544555664062\n",
      "        vf_explained_var: 0.43555229902267456\n",
      "        vf_loss: 528.7665405273438\n",
      "    num_steps_sampled: 1516000\n",
      "    num_steps_trained: 1516000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.72179487179488\n",
      "    ram_util_percent: 34.957264957264954\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06678825296097461\n",
      "    mean_env_wait_ms: 36.89843001079395\n",
      "    mean_inference_ms: 0.9320151463581362\n",
      "    mean_raw_obs_processing_ms: 2.6125495023285916\n",
      "  time_since_restore: 654.7558615207672\n",
      "  time_this_iter_s: 163.88606762886047\n",
      "  time_total_s: 59279.42639875412\n",
      "  timers:\n",
      "    learn_throughput: 2041.884\n",
      "    learn_time_ms: 1958.975\n",
      "    load_throughput: 192896.993\n",
      "    load_time_ms: 20.736\n",
      "    sample_throughput: 24.753\n",
      "    sample_time_ms: 161599.106\n",
      "    update_time_ms: 1.228\n",
      "  timestamp: 1611918042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1516000\n",
      "  training_iteration: 379\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-03-23\n",
      "  done: false\n",
      "  episode_len_mean: 470.23809523809524\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -42.44479074251493\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3543\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5742988586425781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0037296349182724953\n",
      "        model: {}\n",
      "        policy_loss: -0.009002340957522392\n",
      "        total_loss: 258.40716552734375\n",
      "        vf_explained_var: 0.70762038230896\n",
      "        vf_loss: 258.4153747558594\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.91304347826086\n",
      "    ram_util_percent: 35.183913043478256\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06673669779681049\n",
      "    mean_env_wait_ms: 36.86151496880089\n",
      "    mean_inference_ms: 0.9315433290836527\n",
      "    mean_raw_obs_processing_ms: 2.605778859741398\n",
      "  time_since_restore: 815.6072916984558\n",
      "  time_this_iter_s: 160.8514301776886\n",
      "  time_total_s: 59440.27782893181\n",
      "  timers:\n",
      "    learn_throughput: 2052.222\n",
      "    learn_time_ms: 1949.107\n",
      "    load_throughput: 194643.47\n",
      "    load_time_ms: 20.55\n",
      "    sample_throughput: 24.838\n",
      "    sample_time_ms: 161044.941\n",
      "    update_time_ms: 1.206\n",
      "  timestamp: 1611918203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 380\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 479.0204081632653\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -37.91707947279339\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3550\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5281596779823303\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011514516547322273\n",
      "        model: {}\n",
      "        policy_loss: -0.010842515155673027\n",
      "        total_loss: 220.93704223632812\n",
      "        vf_explained_var: 0.7097970247268677\n",
      "        vf_loss: 220.9467315673828\n",
      "    num_steps_sampled: 1524000\n",
      "    num_steps_trained: 1524000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.28034934497815\n",
      "    ram_util_percent: 35.149781659388644\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06669775079515396\n",
      "    mean_env_wait_ms: 36.83120753979019\n",
      "    mean_inference_ms: 0.9312560976849824\n",
      "    mean_raw_obs_processing_ms: 2.590303926140753\n",
      "  time_since_restore: 976.4759576320648\n",
      "  time_this_iter_s: 160.868665933609\n",
      "  time_total_s: 59601.14649486542\n",
      "  timers:\n",
      "    learn_throughput: 2017.284\n",
      "    learn_time_ms: 1982.864\n",
      "    load_throughput: 194633.534\n",
      "    load_time_ms: 20.551\n",
      "    sample_throughput: 24.901\n",
      "    sample_time_ms: 160635.854\n",
      "    update_time_ms: 1.249\n",
      "  timestamp: 1611918364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1524000\n",
      "  training_iteration: 381\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-08-47\n",
      "  done: false\n",
      "  episode_len_mean: 476.0689655172414\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -36.956959489563275\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3559\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5708848237991333\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016378648579120636\n",
      "        model: {}\n",
      "        policy_loss: -0.021451866254210472\n",
      "        total_loss: 327.9744567871094\n",
      "        vf_explained_var: 0.6975752115249634\n",
      "        vf_loss: 327.9942932128906\n",
      "    num_steps_sampled: 1528000\n",
      "    num_steps_trained: 1528000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58974358974359\n",
      "    ram_util_percent: 35.21923076923077\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06666632151059579\n",
      "    mean_env_wait_ms: 36.8022177851122\n",
      "    mean_inference_ms: 0.9311650896374795\n",
      "    mean_raw_obs_processing_ms: 2.581431051862537\n",
      "  time_since_restore: 1140.0228340625763\n",
      "  time_this_iter_s: 163.54687643051147\n",
      "  time_total_s: 59764.69337129593\n",
      "  timers:\n",
      "    learn_throughput: 2030.891\n",
      "    learn_time_ms: 1969.579\n",
      "    load_throughput: 194776.212\n",
      "    load_time_ms: 20.536\n",
      "    sample_throughput: 24.882\n",
      "    sample_time_ms: 160761.773\n",
      "    update_time_ms: 1.262\n",
      "  timestamp: 1611918527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1528000\n",
      "  training_iteration: 382\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-11-28\n",
      "  done: false\n",
      "  episode_len_mean: 477.3731343283582\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -20.879335197732996\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3568\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6475222706794739\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01151668094098568\n",
      "        model: {}\n",
      "        policy_loss: -0.014042322523891926\n",
      "        total_loss: 37.28622055053711\n",
      "        vf_explained_var: 0.9575998783111572\n",
      "        vf_loss: 37.299110412597656\n",
      "    num_steps_sampled: 1532000\n",
      "    num_steps_trained: 1532000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.05109170305678\n",
      "    ram_util_percent: 35.16244541484716\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06663374287062036\n",
      "    mean_env_wait_ms: 36.77014128693238\n",
      "    mean_inference_ms: 0.9308988213446328\n",
      "    mean_raw_obs_processing_ms: 2.5779201308863726\n",
      "  time_since_restore: 1300.8274507522583\n",
      "  time_this_iter_s: 160.804616689682\n",
      "  time_total_s: 59925.49798798561\n",
      "  timers:\n",
      "    learn_throughput: 2043.29\n",
      "    learn_time_ms: 1957.627\n",
      "    load_throughput: 206306.628\n",
      "    load_time_ms: 19.389\n",
      "    sample_throughput: 24.919\n",
      "    sample_time_ms: 160519.285\n",
      "    update_time_ms: 1.267\n",
      "  timestamp: 1611918688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1532000\n",
      "  training_iteration: 383\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-14-10\n",
      "  done: false\n",
      "  episode_len_mean: 471.6578947368421\n",
      "  episode_reward_max: 118.3585956615665\n",
      "  episode_reward_mean: -22.20037096267648\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3577\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5947646498680115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012579433619976044\n",
      "        model: {}\n",
      "        policy_loss: -0.01984924077987671\n",
      "        total_loss: 361.0892028808594\n",
      "        vf_explained_var: 0.6505353450775146\n",
      "        vf_loss: 361.1078186035156\n",
      "    num_steps_sampled: 1536000\n",
      "    num_steps_trained: 1536000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.3194805194805\n",
      "    ram_util_percent: 35.02813852813853\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06658899533407693\n",
      "    mean_env_wait_ms: 36.73999658530014\n",
      "    mean_inference_ms: 0.930389058254265\n",
      "    mean_raw_obs_processing_ms: 2.577531746531348\n",
      "  time_since_restore: 1462.613778591156\n",
      "  time_this_iter_s: 161.7863278388977\n",
      "  time_total_s: 60087.28431582451\n",
      "  timers:\n",
      "    learn_throughput: 2008.593\n",
      "    learn_time_ms: 1991.444\n",
      "    load_throughput: 209146.728\n",
      "    load_time_ms: 19.125\n",
      "    sample_throughput: 24.939\n",
      "    sample_time_ms: 160393.657\n",
      "    update_time_ms: 1.259\n",
      "  timestamp: 1611918850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1536000\n",
      "  training_iteration: 384\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 477.0120481927711\n",
      "  episode_reward_max: 118.36803062063662\n",
      "  episode_reward_mean: -20.653508846297647\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3584\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.699796736240387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01133742742240429\n",
      "        model: {}\n",
      "        policy_loss: -0.018283668905496597\n",
      "        total_loss: 880.5572509765625\n",
      "        vf_explained_var: 0.09986253082752228\n",
      "        vf_loss: 880.574462890625\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.11008771929824\n",
      "    ram_util_percent: 35.026315789473685\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06656092640482236\n",
      "    mean_env_wait_ms: 36.71884388050909\n",
      "    mean_inference_ms: 0.930088163077683\n",
      "    mean_raw_obs_processing_ms: 2.5741566699447733\n",
      "  time_since_restore: 1622.569831609726\n",
      "  time_this_iter_s: 159.95605301856995\n",
      "  time_total_s: 60247.24036884308\n",
      "  timers:\n",
      "    learn_throughput: 2008.212\n",
      "    learn_time_ms: 1991.822\n",
      "    load_throughput: 220065.716\n",
      "    load_time_ms: 18.176\n",
      "    sample_throughput: 24.978\n",
      "    sample_time_ms: 160139.731\n",
      "    update_time_ms: 1.243\n",
      "  timestamp: 1611919010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 385\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-19-30\n",
      "  done: false\n",
      "  episode_len_mean: 484.6111111111111\n",
      "  episode_reward_max: 118.39136489124645\n",
      "  episode_reward_mean: -17.490632731959018\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3591\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6963111162185669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008534240536391735\n",
      "        model: {}\n",
      "        policy_loss: -0.012176177464425564\n",
      "        total_loss: 287.0588073730469\n",
      "        vf_explained_var: 0.6384162902832031\n",
      "        vf_loss: 287.07012939453125\n",
      "    num_steps_sampled: 1544000\n",
      "    num_steps_trained: 1544000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.84148471615721\n",
      "    ram_util_percent: 35.10829694323145\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06652970522158132\n",
      "    mean_env_wait_ms: 36.700411895176565\n",
      "    mean_inference_ms: 0.9297528855184265\n",
      "    mean_raw_obs_processing_ms: 2.568884030763708\n",
      "  time_since_restore: 1782.9569292068481\n",
      "  time_this_iter_s: 160.3870975971222\n",
      "  time_total_s: 60407.6274664402\n",
      "  timers:\n",
      "    learn_throughput: 2031.563\n",
      "    learn_time_ms: 1968.927\n",
      "    load_throughput: 265008.996\n",
      "    load_time_ms: 15.094\n",
      "    sample_throughput: 25.063\n",
      "    sample_time_ms: 159595.717\n",
      "    update_time_ms: 1.293\n",
      "  timestamp: 1611919170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1544000\n",
      "  training_iteration: 386\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-22-09\n",
      "  done: false\n",
      "  episode_len_mean: 492.319587628866\n",
      "  episode_reward_max: 118.39136489124645\n",
      "  episode_reward_mean: -19.511525961202885\n",
      "  episode_reward_min: -122.09440700998536\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3598\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6172552704811096\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00943772867321968\n",
      "        model: {}\n",
      "        policy_loss: -0.012548735365271568\n",
      "        total_loss: 311.32989501953125\n",
      "        vf_explained_var: 0.6458215117454529\n",
      "        vf_loss: 311.3414611816406\n",
      "    num_steps_sampled: 1548000\n",
      "    num_steps_trained: 1548000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44096916299559\n",
      "    ram_util_percent: 35.116299559471365\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06650102722579317\n",
      "    mean_env_wait_ms: 36.68222400067816\n",
      "    mean_inference_ms: 0.9293979006270446\n",
      "    mean_raw_obs_processing_ms: 2.562560420987739\n",
      "  time_since_restore: 1942.0331687927246\n",
      "  time_this_iter_s: 159.07623958587646\n",
      "  time_total_s: 60566.70370602608\n",
      "  timers:\n",
      "    learn_throughput: 2035.898\n",
      "    learn_time_ms: 1964.735\n",
      "    load_throughput: 246894.412\n",
      "    load_time_ms: 16.201\n",
      "    sample_throughput: 25.094\n",
      "    sample_time_ms: 159397.902\n",
      "    update_time_ms: 1.302\n",
      "  timestamp: 1611919329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1548000\n",
      "  training_iteration: 387\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-24-49\n",
      "  done: false\n",
      "  episode_len_mean: 493.71\n",
      "  episode_reward_max: 118.39136489124645\n",
      "  episode_reward_mean: -19.746294714981246\n",
      "  episode_reward_min: -119.03771501015515\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3607\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6775614023208618\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011380302719771862\n",
      "        model: {}\n",
      "        policy_loss: -0.015845634043216705\n",
      "        total_loss: 277.404541015625\n",
      "        vf_explained_var: 0.6353030800819397\n",
      "        vf_loss: 277.41925048828125\n",
      "    num_steps_sampled: 1552000\n",
      "    num_steps_trained: 1552000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.29342105263157\n",
      "    ram_util_percent: 35.12149122807018\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0664463588254452\n",
      "    mean_env_wait_ms: 36.62456890878435\n",
      "    mean_inference_ms: 0.9285818868803226\n",
      "    mean_raw_obs_processing_ms: 2.5476128188050713\n",
      "  time_since_restore: 2101.727314710617\n",
      "  time_this_iter_s: 159.69414591789246\n",
      "  time_total_s: 60726.39785194397\n",
      "  timers:\n",
      "    learn_throughput: 2039.37\n",
      "    learn_time_ms: 1961.39\n",
      "    load_throughput: 247141.361\n",
      "    load_time_ms: 16.185\n",
      "    sample_throughput: 25.157\n",
      "    sample_time_ms: 159003.535\n",
      "    update_time_ms: 1.293\n",
      "  timestamp: 1611919489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1552000\n",
      "  training_iteration: 388\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-27-28\n",
      "  done: false\n",
      "  episode_len_mean: 489.65\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -16.958601203114902\n",
      "  episode_reward_min: -119.03771501015515\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3616\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6921464204788208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00625722948461771\n",
      "        model: {}\n",
      "        policy_loss: -0.00840199738740921\n",
      "        total_loss: 273.7772521972656\n",
      "        vf_explained_var: 0.7070876359939575\n",
      "        vf_loss: 273.7850036621094\n",
      "    num_steps_sampled: 1556000\n",
      "    num_steps_trained: 1556000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.06008771929825\n",
      "    ram_util_percent: 35.106578947368426\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06637573725375905\n",
      "    mean_env_wait_ms: 36.56531108148906\n",
      "    mean_inference_ms: 0.9277668333766849\n",
      "    mean_raw_obs_processing_ms: 2.5450257791520827\n",
      "  time_since_restore: 2260.9830360412598\n",
      "  time_this_iter_s: 159.2557213306427\n",
      "  time_total_s: 60885.65357327461\n",
      "  timers:\n",
      "    learn_throughput: 2045.848\n",
      "    learn_time_ms: 1955.18\n",
      "    load_throughput: 264261.394\n",
      "    load_time_ms: 15.137\n",
      "    sample_throughput: 25.228\n",
      "    sample_time_ms: 158551.379\n",
      "    update_time_ms: 1.273\n",
      "  timestamp: 1611919648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1556000\n",
      "  training_iteration: 389\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 481.88\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -16.637141329884244\n",
      "  episode_reward_min: -119.03771501015515\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3625\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7243395447731018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011219815351068974\n",
      "        model: {}\n",
      "        policy_loss: -0.01681743562221527\n",
      "        total_loss: 194.5528564453125\n",
      "        vf_explained_var: 0.8114016652107239\n",
      "        vf_loss: 194.56854248046875\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.0442060085837\n",
      "    ram_util_percent: 35.13175965665237\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06631322582745856\n",
      "    mean_env_wait_ms: 36.52380544521415\n",
      "    mean_inference_ms: 0.9271908435023355\n",
      "    mean_raw_obs_processing_ms: 2.5438226455632056\n",
      "  time_since_restore: 2424.7218074798584\n",
      "  time_this_iter_s: 163.73877143859863\n",
      "  time_total_s: 61049.39234471321\n",
      "  timers:\n",
      "    learn_throughput: 2049.728\n",
      "    learn_time_ms: 1951.479\n",
      "    load_throughput: 268381.35\n",
      "    load_time_ms: 14.904\n",
      "    sample_throughput: 25.182\n",
      "    sample_time_ms: 158843.417\n",
      "    update_time_ms: 1.275\n",
      "  timestamp: 1611919812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 390\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 498.85\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -12.68480407915515\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3633\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6936590075492859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00544301001355052\n",
      "        model: {}\n",
      "        policy_loss: -0.00840270146727562\n",
      "        total_loss: 247.56300354003906\n",
      "        vf_explained_var: 0.7323237061500549\n",
      "        vf_loss: 247.5708770751953\n",
      "    num_steps_sampled: 1564000\n",
      "    num_steps_trained: 1564000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44405286343613\n",
      "    ram_util_percent: 35.11013215859032\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06627414622408727\n",
      "    mean_env_wait_ms: 36.49087420523706\n",
      "    mean_inference_ms: 0.9267958778411497\n",
      "    mean_raw_obs_processing_ms: 2.5335796022052843\n",
      "  time_since_restore: 2583.3787891864777\n",
      "  time_this_iter_s: 158.65698170661926\n",
      "  time_total_s: 61208.04932641983\n",
      "  timers:\n",
      "    learn_throughput: 2081.459\n",
      "    learn_time_ms: 1921.729\n",
      "    load_throughput: 282065.777\n",
      "    load_time_ms: 14.181\n",
      "    sample_throughput: 25.212\n",
      "    sample_time_ms: 158654.126\n",
      "    update_time_ms: 1.242\n",
      "  timestamp: 1611919971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1564000\n",
      "  training_iteration: 391\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-35-33\n",
      "  done: false\n",
      "  episode_len_mean: 487.72\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -14.967356027341209\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3642\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5943208932876587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00816230196505785\n",
      "        model: {}\n",
      "        policy_loss: -0.016226015985012054\n",
      "        total_loss: 368.7047424316406\n",
      "        vf_explained_var: 0.6131348609924316\n",
      "        vf_loss: 368.7201232910156\n",
      "    num_steps_sampled: 1568000\n",
      "    num_steps_trained: 1568000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.04415584415585\n",
      "    ram_util_percent: 35.10000000000001\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06624366713991896\n",
      "    mean_env_wait_ms: 36.4560245303193\n",
      "    mean_inference_ms: 0.9263483327712706\n",
      "    mean_raw_obs_processing_ms: 2.5280716645615224\n",
      "  time_since_restore: 2745.2274866104126\n",
      "  time_this_iter_s: 161.84869742393494\n",
      "  time_total_s: 61369.898023843765\n",
      "  timers:\n",
      "    learn_throughput: 2049.121\n",
      "    learn_time_ms: 1952.056\n",
      "    load_throughput: 299819.614\n",
      "    load_time_ms: 13.341\n",
      "    sample_throughput: 25.244\n",
      "    sample_time_ms: 158454.48\n",
      "    update_time_ms: 1.237\n",
      "  timestamp: 1611920133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1568000\n",
      "  training_iteration: 392\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-38-19\n",
      "  done: false\n",
      "  episode_len_mean: 490.99\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -15.21966311457826\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3649\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6337136030197144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006452178116887808\n",
      "        model: {}\n",
      "        policy_loss: -0.01210953202098608\n",
      "        total_loss: 211.8726043701172\n",
      "        vf_explained_var: 0.7445133924484253\n",
      "        vf_loss: 211.88400268554688\n",
      "    num_steps_sampled: 1572000\n",
      "    num_steps_trained: 1572000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.45907172995781\n",
      "    ram_util_percent: 35.20464135021096\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06623157608218042\n",
      "    mean_env_wait_ms: 36.437377933437126\n",
      "    mean_inference_ms: 0.9261006987547783\n",
      "    mean_raw_obs_processing_ms: 2.5292258173934217\n",
      "  time_since_restore: 2911.5772428512573\n",
      "  time_this_iter_s: 166.34975624084473\n",
      "  time_total_s: 61536.24778008461\n",
      "  timers:\n",
      "    learn_throughput: 2036.568\n",
      "    learn_time_ms: 1964.089\n",
      "    load_throughput: 286438.457\n",
      "    load_time_ms: 13.965\n",
      "    sample_throughput: 25.158\n",
      "    sample_time_ms: 158995.283\n",
      "    update_time_ms: 1.237\n",
      "  timestamp: 1611920299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1572000\n",
      "  training_iteration: 393\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 476.18\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -14.888441480970847\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 3661\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6417617201805115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0065925284288823605\n",
      "        model: {}\n",
      "        policy_loss: -0.012706398032605648\n",
      "        total_loss: 330.237548828125\n",
      "        vf_explained_var: 0.755355179309845\n",
      "        vf_loss: 330.2496643066406\n",
      "    num_steps_sampled: 1576000\n",
      "    num_steps_trained: 1576000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.1593625498008\n",
      "    ram_util_percent: 35.14183266932271\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06622657387824356\n",
      "    mean_env_wait_ms: 36.423081303971166\n",
      "    mean_inference_ms: 0.9258298709086614\n",
      "    mean_raw_obs_processing_ms: 2.535097910801275\n",
      "  time_since_restore: 3087.440803050995\n",
      "  time_this_iter_s: 175.86356019973755\n",
      "  time_total_s: 61712.11134028435\n",
      "  timers:\n",
      "    learn_throughput: 2067.215\n",
      "    learn_time_ms: 1934.97\n",
      "    load_throughput: 295240.975\n",
      "    load_time_ms: 13.548\n",
      "    sample_throughput: 24.932\n",
      "    sample_time_ms: 160434.795\n",
      "    update_time_ms: 1.28\n",
      "  timestamp: 1611920475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1576000\n",
      "  training_iteration: 394\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 468.19\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -25.109422348511956\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3670\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6065617799758911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007677478715777397\n",
      "        model: {}\n",
      "        policy_loss: -0.010460512712597847\n",
      "        total_loss: 280.3624267578125\n",
      "        vf_explained_var: 0.7228060960769653\n",
      "        vf_loss: 280.37213134765625\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.13694779116466\n",
      "    ram_util_percent: 35.21164658634538\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06624842780069412\n",
      "    mean_env_wait_ms: 36.431605779966965\n",
      "    mean_inference_ms: 0.926071664307635\n",
      "    mean_raw_obs_processing_ms: 2.5378714423664617\n",
      "  time_since_restore: 3261.667008638382\n",
      "  time_this_iter_s: 174.22620558738708\n",
      "  time_total_s: 61886.337545871735\n",
      "  timers:\n",
      "    learn_throughput: 2069.555\n",
      "    learn_time_ms: 1932.783\n",
      "    load_throughput: 277199.85\n",
      "    load_time_ms: 14.43\n",
      "    sample_throughput: 24.712\n",
      "    sample_time_ms: 161861.874\n",
      "    update_time_ms: 1.312\n",
      "  timestamp: 1611920649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 395\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-46-58\n",
      "  done: false\n",
      "  episode_len_mean: 470.9\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -25.661471288589883\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3678\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6357277035713196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0070866248570382595\n",
      "        model: {}\n",
      "        policy_loss: -0.02074248716235161\n",
      "        total_loss: 316.87225341796875\n",
      "        vf_explained_var: 0.6789147257804871\n",
      "        vf_loss: 316.89227294921875\n",
      "    num_steps_sampled: 1584000\n",
      "    num_steps_trained: 1584000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.33625\n",
      "    ram_util_percent: 35.08500000000001\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06628877482261425\n",
      "    mean_env_wait_ms: 36.447545983162854\n",
      "    mean_inference_ms: 0.9266185420814504\n",
      "    mean_raw_obs_processing_ms: 2.539014712904969\n",
      "  time_since_restore: 3429.8529777526855\n",
      "  time_this_iter_s: 168.1859691143036\n",
      "  time_total_s: 62054.52351498604\n",
      "  timers:\n",
      "    learn_throughput: 2060.733\n",
      "    learn_time_ms: 1941.056\n",
      "    load_throughput: 279002.277\n",
      "    load_time_ms: 14.337\n",
      "    sample_throughput: 24.596\n",
      "    sample_time_ms: 162628.647\n",
      "    update_time_ms: 1.302\n",
      "  timestamp: 1611920818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1584000\n",
      "  training_iteration: 396\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-49-48\n",
      "  done: false\n",
      "  episode_len_mean: 476.79\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -28.20986102663207\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3685\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6661216020584106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007450159173458815\n",
      "        model: {}\n",
      "        policy_loss: -0.00882885605096817\n",
      "        total_loss: 829.763916015625\n",
      "        vf_explained_var: 0.36722737550735474\n",
      "        vf_loss: 829.77197265625\n",
      "    num_steps_sampled: 1588000\n",
      "    num_steps_trained: 1588000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.64814814814814\n",
      "    ram_util_percent: 35.19711934156378\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06633112735266945\n",
      "    mean_env_wait_ms: 36.46988779357082\n",
      "    mean_inference_ms: 0.9272110618778149\n",
      "    mean_raw_obs_processing_ms: 2.5412732940195357\n",
      "  time_since_restore: 3600.0856277942657\n",
      "  time_this_iter_s: 170.2326500415802\n",
      "  time_total_s: 62224.75616502762\n",
      "  timers:\n",
      "    learn_throughput: 2048.041\n",
      "    learn_time_ms: 1953.086\n",
      "    load_throughput: 305326.187\n",
      "    load_time_ms: 13.101\n",
      "    sample_throughput: 24.43\n",
      "    sample_time_ms: 163732.499\n",
      "    update_time_ms: 1.285\n",
      "  timestamp: 1611920988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1588000\n",
      "  training_iteration: 397\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 466.41\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -32.84038730982351\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3693\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6785175800323486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008273785002529621\n",
      "        model: {}\n",
      "        policy_loss: -0.00891658291220665\n",
      "        total_loss: 602.9743041992188\n",
      "        vf_explained_var: 0.37499022483825684\n",
      "        vf_loss: 602.9823608398438\n",
      "    num_steps_sampled: 1592000\n",
      "    num_steps_trained: 1592000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.74897959183673\n",
      "    ram_util_percent: 35.14979591836736\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06639233996716945\n",
      "    mean_env_wait_ms: 36.5037933923941\n",
      "    mean_inference_ms: 0.9281181702228511\n",
      "    mean_raw_obs_processing_ms: 2.546139123736854\n",
      "  time_since_restore: 3771.8211007118225\n",
      "  time_this_iter_s: 171.73547291755676\n",
      "  time_total_s: 62396.491637945175\n",
      "  timers:\n",
      "    learn_throughput: 2030.42\n",
      "    learn_time_ms: 1970.036\n",
      "    load_throughput: 302103.298\n",
      "    load_time_ms: 13.241\n",
      "    sample_throughput: 24.254\n",
      "    sample_time_ms: 164919.446\n",
      "    update_time_ms: 1.273\n",
      "  timestamp: 1611921160\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1592000\n",
      "  training_iteration: 398\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 464.1\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -32.51883908669357\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3701\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6482171416282654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008293001912534237\n",
      "        model: {}\n",
      "        policy_loss: -0.015918031334877014\n",
      "        total_loss: 512.6806640625\n",
      "        vf_explained_var: 0.41241559386253357\n",
      "        vf_loss: 512.6956787109375\n",
      "    num_steps_sampled: 1596000\n",
      "    num_steps_trained: 1596000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.89146341463416\n",
      "    ram_util_percent: 35.2239837398374\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06646281785796886\n",
      "    mean_env_wait_ms: 36.54802749218246\n",
      "    mean_inference_ms: 0.9292419933338586\n",
      "    mean_raw_obs_processing_ms: 2.55181931638478\n",
      "  time_since_restore: 3943.8607223033905\n",
      "  time_this_iter_s: 172.039621591568\n",
      "  time_total_s: 62568.53125953674\n",
      "  timers:\n",
      "    learn_throughput: 2011.417\n",
      "    learn_time_ms: 1988.648\n",
      "    load_throughput: 303586.388\n",
      "    load_time_ms: 13.176\n",
      "    sample_throughput: 24.071\n",
      "    sample_time_ms: 166177.571\n",
      "    update_time_ms: 1.3\n",
      "  timestamp: 1611921332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1596000\n",
      "  training_iteration: 399\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_12-58-27\n",
      "  done: false\n",
      "  episode_len_mean: 472.0\n",
      "  episode_reward_max: 118.39736516668\n",
      "  episode_reward_mean: -24.306536349272754\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3710\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6370170712471008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007024493999779224\n",
      "        model: {}\n",
      "        policy_loss: -0.014790262095630169\n",
      "        total_loss: 304.2123718261719\n",
      "        vf_explained_var: 0.7052041292190552\n",
      "        vf_loss: 304.22650146484375\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.63839999999999\n",
      "    ram_util_percent: 35.2584\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06655383099323732\n",
      "    mean_env_wait_ms: 36.611504174815444\n",
      "    mean_inference_ms: 0.9306901885163907\n",
      "    mean_raw_obs_processing_ms: 2.5573784967396525\n",
      "  time_since_restore: 4118.991362571716\n",
      "  time_this_iter_s: 175.1306402683258\n",
      "  time_total_s: 62743.66189980507\n",
      "  timers:\n",
      "    learn_throughput: 1994.857\n",
      "    learn_time_ms: 2005.157\n",
      "    load_throughput: 320924.907\n",
      "    load_time_ms: 12.464\n",
      "    sample_throughput: 23.909\n",
      "    sample_time_ms: 167300.983\n",
      "    update_time_ms: 1.303\n",
      "  timestamp: 1611921507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 400\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-01-21\n",
      "  done: false\n",
      "  episode_len_mean: 477.72\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -24.851361241241474\n",
      "  episode_reward_min: -123.1507286494041\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3717\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6439140439033508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008043251000344753\n",
      "        model: {}\n",
      "        policy_loss: -0.01041231956332922\n",
      "        total_loss: 203.7755584716797\n",
      "        vf_explained_var: 0.7331852316856384\n",
      "        vf_loss: 203.78514099121094\n",
      "    num_steps_sampled: 1604000\n",
      "    num_steps_trained: 1604000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.19072580645162\n",
      "    ram_util_percent: 35.35120967741935\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06662819611853993\n",
      "    mean_env_wait_ms: 36.67051174263483\n",
      "    mean_inference_ms: 0.93195704981991\n",
      "    mean_raw_obs_processing_ms: 2.5596739313317585\n",
      "  time_since_restore: 4293.123536348343\n",
      "  time_this_iter_s: 174.1321737766266\n",
      "  time_total_s: 62917.794073581696\n",
      "  timers:\n",
      "    learn_throughput: 1971.507\n",
      "    learn_time_ms: 2028.905\n",
      "    load_throughput: 328572.022\n",
      "    load_time_ms: 12.174\n",
      "    sample_throughput: 23.693\n",
      "    sample_time_ms: 168822.83\n",
      "    update_time_ms: 1.335\n",
      "  timestamp: 1611921681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1604000\n",
      "  training_iteration: 401\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 481.93\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -25.768771179392644\n",
      "  episode_reward_min: -126.27684070428293\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 3725\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6521176695823669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004968817811459303\n",
      "        model: {}\n",
      "        policy_loss: -0.01370023563504219\n",
      "        total_loss: 174.10316467285156\n",
      "        vf_explained_var: 0.8333144187927246\n",
      "        vf_loss: 174.1163787841797\n",
      "    num_steps_sampled: 1608000\n",
      "    num_steps_trained: 1608000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.09816176470588\n",
      "    ram_util_percent: 35.49632352941177\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06672781660622604\n",
      "    mean_env_wait_ms: 36.75466419127174\n",
      "    mean_inference_ms: 0.9336679798994864\n",
      "    mean_raw_obs_processing_ms: 2.5611272317083826\n",
      "  time_since_restore: 4483.286893606186\n",
      "  time_this_iter_s: 190.16335725784302\n",
      "  time_total_s: 63107.95743083954\n",
      "  timers:\n",
      "    learn_throughput: 1983.633\n",
      "    learn_time_ms: 2016.502\n",
      "    load_throughput: 346403.373\n",
      "    load_time_ms: 11.547\n",
      "    sample_throughput: 23.301\n",
      "    sample_time_ms: 171668.763\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1611921871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1608000\n",
      "  training_iteration: 402\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-07-24\n",
      "  done: false\n",
      "  episode_len_mean: 474.62\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -27.757472189738998\n",
      "  episode_reward_min: -126.27684070428293\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3734\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6845365166664124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015353797003626823\n",
      "        model: {}\n",
      "        policy_loss: -0.01787225343286991\n",
      "        total_loss: 407.4532775878906\n",
      "        vf_explained_var: 0.5691268444061279\n",
      "        vf_loss: 407.47039794921875\n",
      "    num_steps_sampled: 1612000\n",
      "    num_steps_trained: 1612000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.27642276422765\n",
      "    ram_util_percent: 35.18902439024389\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06683988438880605\n",
      "    mean_env_wait_ms: 36.857367657630554\n",
      "    mean_inference_ms: 0.9356412622832357\n",
      "    mean_raw_obs_processing_ms: 2.563768117842554\n",
      "  time_since_restore: 4655.659100055695\n",
      "  time_this_iter_s: 172.37220644950867\n",
      "  time_total_s: 63280.32963728905\n",
      "  timers:\n",
      "    learn_throughput: 1982.932\n",
      "    learn_time_ms: 2017.215\n",
      "    load_throughput: 370083.47\n",
      "    load_time_ms: 10.808\n",
      "    sample_throughput: 23.219\n",
      "    sample_time_ms: 172270.287\n",
      "    update_time_ms: 1.359\n",
      "  timestamp: 1611922044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1612000\n",
      "  training_iteration: 403\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 473.46\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -23.32200916434182\n",
      "  episode_reward_min: -126.27684070428293\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 3743\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6819548606872559\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011040162295103073\n",
      "        model: {}\n",
      "        policy_loss: -0.017621628940105438\n",
      "        total_loss: 364.9435119628906\n",
      "        vf_explained_var: 0.6918976306915283\n",
      "        vf_loss: 364.96063232421875\n",
      "    num_steps_sampled: 1616000\n",
      "    num_steps_trained: 1616000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.35021276595747\n",
      "    ram_util_percent: 35.135744680851076\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06694719075874855\n",
      "    mean_env_wait_ms: 36.958287475963026\n",
      "    mean_inference_ms: 0.9376553529454997\n",
      "    mean_raw_obs_processing_ms: 2.565864460415664\n",
      "  time_since_restore: 4820.24986577034\n",
      "  time_this_iter_s: 164.59076571464539\n",
      "  time_total_s: 63444.92040300369\n",
      "  timers:\n",
      "    learn_throughput: 1962.215\n",
      "    learn_time_ms: 2038.513\n",
      "    load_throughput: 377265.339\n",
      "    load_time_ms: 10.603\n",
      "    sample_throughput: 23.376\n",
      "    sample_time_ms: 171119.208\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1611922209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1616000\n",
      "  training_iteration: 404\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 470.2\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -27.304824883738732\n",
      "  episode_reward_min: -126.27684070428293\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 3750\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7236712574958801\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015928739681839943\n",
      "        model: {}\n",
      "        policy_loss: -0.01569059118628502\n",
      "        total_loss: 348.7811279296875\n",
      "        vf_explained_var: 0.4744885563850403\n",
      "        vf_loss: 348.7960205078125\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.59790794979078\n",
      "    ram_util_percent: 35.19121338912134\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0670247991098914\n",
      "    mean_env_wait_ms: 37.03118024903068\n",
      "    mean_inference_ms: 0.9391725120122908\n",
      "    mean_raw_obs_processing_ms: 2.567264512811195\n",
      "  time_since_restore: 4987.633087158203\n",
      "  time_this_iter_s: 167.38322138786316\n",
      "  time_total_s: 63612.303624391556\n",
      "  timers:\n",
      "    learn_throughput: 1878.47\n",
      "    learn_time_ms: 2129.392\n",
      "    load_throughput: 361545.205\n",
      "    load_time_ms: 11.064\n",
      "    sample_throughput: 23.482\n",
      "    sample_time_ms: 170341.558\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1611922376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 405\n",
      "  trial_id: ad561_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_ad561_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-29_13-15-51\n",
      "  done: false\n",
      "  episode_len_mean: 482.49\n",
      "  episode_reward_max: 118.3842254759681\n",
      "  episode_reward_mean: -31.509982701078847\n",
      "  episode_reward_min: -126.27684070428293\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 3760\n",
      "  experiment_id: 492b4cc13cdc43eca95b2d3c68f6668a\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6622835993766785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013309895060956478\n",
      "        model: {}\n",
      "        policy_loss: -0.02077639102935791\n",
      "        total_loss: 423.50604248046875\n",
      "        vf_explained_var: 0.5199650526046753\n",
      "        vf_loss: 423.52618408203125\n",
      "    num_steps_sampled: 1624000\n",
      "    num_steps_trained: 1624000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.51967871485944\n",
      "    ram_util_percent: 35.24538152610442\n",
      "  pid: 3019164\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06712804442071887\n",
      "    mean_env_wait_ms: 37.129140051548504\n",
      "    mean_inference_ms: 0.9412813359692928\n",
      "    mean_raw_obs_processing_ms: 2.5658893946089387\n",
      "  time_since_restore: 5162.090919494629\n",
      "  time_this_iter_s: 174.45783233642578\n",
      "  time_total_s: 63786.76145672798\n",
      "  timers:\n",
      "    learn_throughput: 1845.936\n",
      "    learn_time_ms: 2166.923\n",
      "    load_throughput: 338935.026\n",
      "    load_time_ms: 11.802\n",
      "    sample_throughput: 23.402\n",
      "    sample_time_ms: 170929.056\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1611922551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1624000\n",
      "  training_iteration: 406\n",
      "  trial_id: ad561_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         58790.8</td><td style=\"text-align: right;\">1504000</td><td style=\"text-align: right;\">-56.6504</td><td style=\"text-align: right;\">             108.725</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           408.778</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         58951.9</td><td style=\"text-align: right;\">1508000</td><td style=\"text-align: right;\">-51.3003</td><td style=\"text-align: right;\">             108.725</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           462.125</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         59115.5</td><td style=\"text-align: right;\">1512000</td><td style=\"text-align: right;\">-35.9059</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">            471.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         59279.4</td><td style=\"text-align: right;\">1516000</td><td style=\"text-align: right;\">-48.2493</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">             454.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         59440.3</td><td style=\"text-align: right;\">1520000</td><td style=\"text-align: right;\">-42.4448</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           470.238</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         59601.1</td><td style=\"text-align: right;\">1524000</td><td style=\"text-align: right;\">-37.9171</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">            479.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         59764.7</td><td style=\"text-align: right;\">1528000</td><td style=\"text-align: right;\"> -36.957</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           476.069</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         59925.5</td><td style=\"text-align: right;\">1532000</td><td style=\"text-align: right;\">-20.8793</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           477.373</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         60087.3</td><td style=\"text-align: right;\">1536000</td><td style=\"text-align: right;\">-22.2004</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           471.658</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         60247.2</td><td style=\"text-align: right;\">1540000</td><td style=\"text-align: right;\">-20.6535</td><td style=\"text-align: right;\">             118.368</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           477.012</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         60407.6</td><td style=\"text-align: right;\">1544000</td><td style=\"text-align: right;\">-17.4906</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">           484.611</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         60566.7</td><td style=\"text-align: right;\">1548000</td><td style=\"text-align: right;\">-19.5115</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -122.094</td><td style=\"text-align: right;\">            492.32</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         60726.4</td><td style=\"text-align: right;\">1552000</td><td style=\"text-align: right;\">-19.7463</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -119.038</td><td style=\"text-align: right;\">            493.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         60885.7</td><td style=\"text-align: right;\">1556000</td><td style=\"text-align: right;\">-16.9586</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -119.038</td><td style=\"text-align: right;\">            489.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         61049.4</td><td style=\"text-align: right;\">1560000</td><td style=\"text-align: right;\">-16.6371</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -119.038</td><td style=\"text-align: right;\">            481.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">           61208</td><td style=\"text-align: right;\">1564000</td><td style=\"text-align: right;\">-12.6848</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            498.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         61369.9</td><td style=\"text-align: right;\">1568000</td><td style=\"text-align: right;\">-14.9674</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            487.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         61536.2</td><td style=\"text-align: right;\">1572000</td><td style=\"text-align: right;\">-15.2197</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            490.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         61712.1</td><td style=\"text-align: right;\">1576000</td><td style=\"text-align: right;\">-14.8884</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            476.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         61886.3</td><td style=\"text-align: right;\">1580000</td><td style=\"text-align: right;\">-25.1094</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            468.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         62054.5</td><td style=\"text-align: right;\">1584000</td><td style=\"text-align: right;\">-25.6615</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">             470.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         62224.8</td><td style=\"text-align: right;\">1588000</td><td style=\"text-align: right;\">-28.2099</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            476.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         62396.5</td><td style=\"text-align: right;\">1592000</td><td style=\"text-align: right;\">-32.8404</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            466.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         62568.5</td><td style=\"text-align: right;\">1596000</td><td style=\"text-align: right;\">-32.5188</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">             464.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         62743.7</td><td style=\"text-align: right;\">1600000</td><td style=\"text-align: right;\">-24.3065</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">               472</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         62917.8</td><td style=\"text-align: right;\">1604000</td><td style=\"text-align: right;\">-24.8514</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -123.151</td><td style=\"text-align: right;\">            477.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">           63108</td><td style=\"text-align: right;\">1608000</td><td style=\"text-align: right;\">-25.7688</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -126.277</td><td style=\"text-align: right;\">            481.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         63280.3</td><td style=\"text-align: right;\">1612000</td><td style=\"text-align: right;\">-27.7575</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -126.277</td><td style=\"text-align: right;\">            474.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         63444.9</td><td style=\"text-align: right;\">1616000</td><td style=\"text-align: right;\"> -23.322</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -126.277</td><td style=\"text-align: right;\">            473.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         63612.3</td><td style=\"text-align: right;\">1620000</td><td style=\"text-align: right;\">-27.3048</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -126.277</td><td style=\"text-align: right;\">             470.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.77 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-29_11-49-35<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_ad561_00000</td><td>RUNNING </td><td>192.168.178.60:3019164</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         63786.8</td><td style=\"text-align: right;\">1624000</td><td style=\"text-align: right;\">  -31.51</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -126.277</td><td style=\"text-align: right;\">            482.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/PPO_2021-01-28_15-51-43/PPO_ScoutingDiscreteTask_56467_00000_0_2021-01-28_15-51-43/checkpoint_375/checkpoint-375')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-01-28_15-51-43/PPO_ScoutingDiscreteTask_56467_00000_0_2021-01-28_15-51-43/checkpoint_375/checkpoint-375'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m non-resource variables are not supported in the long term\n",
      "2021-01-29 08:46:24,136\tWARNING worker.py:1034 -- Failed to unpickle actor class 'RolloutWorker' for actor ID 88866c7d01000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    logger.exception(\"Failed to load actor class %s.\", class_name)\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n",
      "    from ray.rllib.env.base_env import BaseEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 1, in <module>\n",
      "    from ray.rllib.env.base_env import BaseEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/base_env.py\", line 3, in <module>\n",
      "    from ray.rllib.env.external_env import ExternalEnv\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/external_env.py\", line 7, in <module>\n",
      "    from ray.rllib.utils.annotations import PublicAPI\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/utils/__init__.py\", line 18, in <module>\n",
      "    from ray.tune.utils import merge_dicts, deep_update\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/__init__.py\", line 2, in <module>\n",
      "    from ray.tune.tune import run_experiments, run\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/tune.py\", line 10, in <module>\n",
      "    from ray.tune.ray_trial_executor import RayTrialExecutor\n",
      "  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 11, in <module>\n",
      "    from ray.exceptions import RayTimeoutError\n",
      "ImportError: cannot import name 'RayTimeoutError' from 'ray.exceptions' (/home/dschori/.local/lib/python3.8/site-packages/ray/exceptions.py)\n",
      "\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m 2021-01-29 08:46:24,130\tERROR function_manager.py:498 -- Failed to load actor class RolloutWorker.\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m Traceback (most recent call last):\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     logger.exception(\"Failed to load actor class %s.\", class_name)\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.base_env import BaseEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 1, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.base_env import BaseEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/base_env.py\", line 3, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.env.external_env import ExternalEnv\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/external_env.py\", line 7, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.rllib.utils.annotations import PublicAPI\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/utils/__init__.py\", line 18, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.utils import merge_dicts, deep_update\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/__init__.py\", line 2, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.tune import run_experiments, run\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/tune.py\", line 10, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.tune.ray_trial_executor import RayTrialExecutor\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m   File \"/home/dschori/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 11, in <module>\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m     from ray.exceptions import RayTimeoutError\n",
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m ImportError: cannot import name 'RayTimeoutError' from 'ray.exceptions' (/home/dschori/.local/lib/python3.8/site-packages/ray/exceptions.py)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=2023600)\u001B[0m None\n"
     ]
    },
    {
     "ename": "RayTaskError(AssertionError)",
     "evalue": "\u001B[36mray::RolloutWorker.foreach_policy()\u001B[39m (pid=2023600, ip=192.168.178.60)\n  File \"python/ray/_raylet.pyx\", line 422, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 442, in ray._raylet.execute_task\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 310, in deserialize_objects\n    except DeserializationError:\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 248, in _deserialize_object\n    # Check if the object should be returned as raw bytes.\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 226, in _deserialize_msgpack_data\n    python_objects = []\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 216, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n    from ray.rllib.env.base_env import BaseEnv\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 11, in <module>\n    from ray.rllib.env.policy_client import PolicyClient\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/policy_client.py\", line 13, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/__init__.py\", line 2, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 73, in <module>\n    class RolloutWorker(ParallelIteratorWorker):\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in RolloutWorker\n    @ray.method(num_return_vals=2)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/actor.py\", line 45, in method\nAssertionError",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRayTaskError(AssertionError)\u001B[0m              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-7650ad861f1f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-13-8712695bee7b>\u001B[0m in \u001B[0;36mload\u001B[0;34m(checkpoint_path, config)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;34m:\u001B[0m\u001B[0mparam\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mPath\u001B[0m \u001B[0mpointing\u001B[0m \u001B[0mto\u001B[0m \u001B[0mthe\u001B[0m \u001B[0magent\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0msaved\u001B[0m \u001B[0mcheckpoint\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0monly\u001B[0m \u001B[0mused\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mRLlib\u001B[0m \u001B[0magents\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \"\"\"\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0magent\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPPOTrainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m     \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrestore\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, env, logger_creator)\u001B[0m\n\u001B[1;32m    104\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_exec_impl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexecution_plan\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mworkers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    105\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mafter_init\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 106\u001B[0;31m                 \u001B[0mafter_init\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0moverride\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTrainer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, env, logger_creator)\u001B[0m\n\u001B[1;32m    463\u001B[0m             \u001B[0mtimestr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoday\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrftime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"%Y-%m-%d_%H-%M-%S\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m             logdir_prefix = \"{}_{}_{}\".format(self._name, self._env_id,\n\u001B[0;32m--> 465\u001B[0;31m                                               timestr)\n\u001B[0m\u001B[1;32m    466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    467\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mdefault_logger_creator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, config, logger_creator)\u001B[0m\n\u001B[1;32m     94\u001B[0m             \u001B[0mFileNotFoundError\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mdirectory\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfound\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m         \"\"\"\n\u001B[0;32m---> 96\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Path does not exist\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0misdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheckpoint_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36msetup\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    627\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"callbacks\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    628\u001B[0m         \u001B[0mlog_level\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"log_level\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 629\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mlog_level\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"WARN\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"ERROR\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    630\u001B[0m             logger.info(\"Current log_level is {}. For more information, \"\n\u001B[1;32m    631\u001B[0m                         \u001B[0;34m\"set 'log_level': 'INFO' / 'DEBUG' or use the -v and \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001B[0m in \u001B[0;36m_init\u001B[0;34m(self, config, env_creator)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         \u001B[0mArguments\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m             \u001B[0moverrides\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0muse\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mto\u001B[0m \u001B[0moverride\u001B[0m \u001B[0many\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    134\u001B[0m                 \u001B[0moriginally\u001B[0m \u001B[0mpassed\u001B[0m \u001B[0mto\u001B[0m \u001B[0mbuild_trainer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m         \"\"\"\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36m_make_workers\u001B[0;34m(self, env_creator, validate_env, policy_class, config, num_workers)\u001B[0m\n\u001B[1;32m    698\u001B[0m                 \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    699\u001B[0m             \u001B[0mpolicy\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mPolicy\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mto\u001B[0m \u001B[0muse\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcreating\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mpolicies\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 700\u001B[0;31m                 \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mworkers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    701\u001B[0m             \u001B[0mconfig\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mTrainer\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    702\u001B[0m             \u001B[0mnum_workers\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mNumber\u001B[0m \u001B[0mof\u001B[0m \u001B[0mremote\u001B[0m \u001B[0mrollout\u001B[0m \u001B[0mworkers\u001B[0m \u001B[0mto\u001B[0m \u001B[0mcreate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, logdir, _setup)\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mremote_workers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"ActorHandle\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;34m\"\"\"Return a list of remote rollout workers.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_remote_workers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msync_weights\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(object_refs, timeout)\u001B[0m\n\u001B[1;32m   1377\u001B[0m             \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprinter_thread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1378\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mworker\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logger_thread\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1379\u001B[0;31m             \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogger_thread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1380\u001B[0m         \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mthreads_stopped\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1381\u001B[0m         \u001B[0mworker\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_session_index\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRayTaskError(AssertionError)\u001B[0m: \u001B[36mray::RolloutWorker.foreach_policy()\u001B[39m (pid=2023600, ip=192.168.178.60)\n  File \"python/ray/_raylet.pyx\", line 422, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 442, in ray._raylet.execute_task\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 310, in deserialize_objects\n    except DeserializationError:\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 248, in _deserialize_object\n    # Check if the object should be returned as raw bytes.\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 226, in _deserialize_msgpack_data\n    python_objects = []\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/serialization.py\", line 216, in _deserialize_pickle5_data\n    obj = pickle.loads(in_band)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/__init__.py\", line 5, in <module>\n    from ray.rllib.env.base_env import BaseEnv\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/__init__.py\", line 11, in <module>\n    from ray.rllib.env.policy_client import PolicyClient\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/env/policy_client.py\", line 13, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/__init__.py\", line 2, in <module>\n    from ray.rllib.evaluation.rollout_worker import RolloutWorker\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 73, in <module>\n    class RolloutWorker(ParallelIteratorWorker):\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 607, in RolloutWorker\n    @ray.method(num_return_vals=2)\n  File \"/home/dschori/.local/lib/python3.8/site-packages/ray/actor.py\", line 45, in method\nAssertionError"
     ]
    }
   ],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', (array([3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,\n       3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99], dtype=float32), array([9.99      , 0.22813019]), array([-1.,  1.])))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mcheck_shape\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_obs_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontains\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     63\u001B[0m                     raise ValueError(\n",
      "\u001B[0;32m/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py\u001B[0m in \u001B[0;36mcontains\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    127\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Promote list to array for contains check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlow\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhigh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-d77bb593b827>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mepisode_reward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0magent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mepisode_reward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-8712695bee7b>\u001B[0m in \u001B[0;36mtest\u001B[0;34m(agent, env)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[0mobs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0magent\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompute_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m         \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0mepisode_reward\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36mcompute_action\u001B[0;34m(self, observation, state, prev_action, prev_reward, info, policy_id, full_fetch, explore)\u001B[0m\n\u001B[1;32m    816\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mstate\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    817\u001B[0m             \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 818\u001B[0;31m         preprocessed = self.workers.local_worker().preprocessors[\n\u001B[0m\u001B[1;32m    819\u001B[0m             policy_id].transform(observation)\n\u001B[1;32m    820\u001B[0m         filtered_obs = self.workers.local_worker().filters[policy_id](\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0moverride\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPreprocessor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensorType\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    169\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py\u001B[0m in \u001B[0;36mcheck_shape\u001B[0;34m(self, observation)\u001B[0m\n\u001B[1;32m     65\u001B[0m                         observation, self._obs_space)\n\u001B[1;32m     66\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m                 raise ValueError(\n\u001B[0m\u001B[1;32m     68\u001B[0m                     \u001B[0;34m\"Observation for a Box/MultiBinary/MultiDiscrete space \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m                     \"should be an np.array, not a Python list.\", observation)\n",
      "\u001B[0;31mValueError\u001B[0m: ('Observation for a Box/MultiBinary/MultiDiscrete space should be an np.array, not a Python list.', (array([3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,\n       3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99], dtype=float32), array([9.99      , 0.22813019]), array([-1.,  1.])))"
     ]
    }
   ],
   "source": [
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}