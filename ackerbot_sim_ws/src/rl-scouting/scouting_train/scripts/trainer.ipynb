{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import rospy\n",
    "import gym, ray\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask\n",
    "\n",
    "from ray.tune import grid_search\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.visionnet import VisionNetwork\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "from ray.rllib.agents.pg import PGTrainer\n",
    "from ray.rllib.agents.ddpg import DDPGTrainer\n",
    "from ray.rllib.agents.impala import ImpalaTrainer\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.tune.logger import pretty_print\n",
    "from matplotlib import pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.misc import normc_initializer\n",
    "from ray.rllib.models.utils import get_filter_config\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.view_requirement import ViewRequirement\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.typing import ModelConfigDict, TensorType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1611672724.763344, 36.720000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1611672724.783485, 0.063000]: Start Init ControllersConnection\n",
      "[WARN] [1611672724.784797, 0.064000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 8.0, (12,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(env.observation_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4625917673110962\n",
      "7.99\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(20):\n",
    "    #obs, reward, done, _ = env.step(1)\n",
    "    obs, reward, done, _ = env.step([0.6, ])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    obs, reward, done, _ = env.step([0.5, ])\n",
    "    #obs, reward, done, _ = env.step([0.3, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.40289545 2.39321256 0.30885416 0.17761202 0.13282748 0.13256712\n",
      " 0.14170636 0.18840335 0.37458807 2.56611633 5.4644917  7.99      ]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.40289545 2.39321256 0.30885416 0.17761202 0.13282748 0.13256712\n",
      " 0.14170636 0.18840335 0.37458807 2.56611633 5.4644917  7.99      ]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (12,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-5787f9d66695>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m12\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcmap\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'gray'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001B[0m in \u001B[0;36mimshow\u001B[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001B[0m\n\u001B[1;32m   2669\u001B[0m         \u001B[0mfilterrad\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4.0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimlim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeprecation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_deprecated_parameter\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2670\u001B[0m         resample=None, url=None, *, data=None, **kwargs):\n\u001B[0;32m-> 2671\u001B[0;31m     __ret = gca().imshow(\n\u001B[0m\u001B[1;32m   2672\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcmap\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcmap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maspect\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maspect\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2673\u001B[0m         \u001B[0minterpolation\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minterpolation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvmin\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mvmin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/__init__.py\u001B[0m in \u001B[0;36minner\u001B[0;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1599\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minner\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1600\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1601\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msanitize_sequence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1602\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1603\u001B[0m         \u001B[0mbound\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew_sig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/cbook/deprecation.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m                 \u001B[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m                 f\"should be pass as keyword, not positionally.\")\n\u001B[0;32m--> 369\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    370\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/cbook/deprecation.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    367\u001B[0m                 \u001B[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    368\u001B[0m                 f\"should be pass as keyword, not positionally.\")\n\u001B[0;32m--> 369\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    370\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    371\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36mimshow\u001B[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001B[0m\n\u001B[1;32m   5677\u001B[0m                               resample=resample, **kwargs)\n\u001B[1;32m   5678\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5679\u001B[0;31m         \u001B[0mim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5680\u001B[0m         \u001B[0mim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_alpha\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0malpha\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5681\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_clip_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001B[0m in \u001B[0;36mset_data\u001B[0;34m(self, A)\u001B[0m\n\u001B[1;32m    687\u001B[0m         if not (self._A.ndim == 2\n\u001B[1;32m    688\u001B[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001B[0;32m--> 689\u001B[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001B[0m\u001B[1;32m    690\u001B[0m                             .format(self._A.shape))\n\u001B[1;32m    691\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Invalid shape (12,) for image data"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 864x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(obs.squeeze(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(obs.squeeze(), cmap='jet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.colorbar.Colorbar at 0x7fbd68daddc0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_all = np.arange(-1.2, 1.2, 0.1) # entire range of x, both in and out of spec\n",
    "# mean = 0, stddev = 1, since Z-transform was calculated\n",
    "y2 = norm.pdf(x_all,0,2/2)\n",
    "\n",
    "y2 = np.interp(y2, (y2.min(), y2.max()), (0, 1))\n",
    "\n",
    "tmp = np.zeros((40, 24))\n",
    "for i in range(40):\n",
    "    tmp[i] = y2\n",
    "\n",
    "plt.imshow(tmp)\n",
    "plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.colorbar.Colorbar at 0x7fbd68cec190>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 576x576 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def twoD_Gaussian(x, y, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "    xo = float(xo)\n",
    "    yo = float(yo)\n",
    "    a = (np.cos(theta)**2)/(2*sigma_x**2) + (np.sin(theta)**2)/(2*sigma_y**2)\n",
    "    b = -(np.sin(2*theta))/(4*sigma_x**2) + (np.sin(2*theta))/(4*sigma_y**2)\n",
    "    c = (np.sin(theta)**2)/(2*sigma_x**2) + (np.cos(theta)**2)/(2*sigma_y**2)\n",
    "    g = offset + amplitude*np.exp( - (a*((x-xo)**2) + 2*b*(x-xo)*(y-yo)\n",
    "                            + c*((y-yo)**2)))\n",
    "    return g.ravel()\n",
    "\n",
    "\n",
    "x = np.linspace(0, 39, 40)\n",
    "y = np.linspace(0, 39, 40)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "amplitude = 1\n",
    "xo = 20\n",
    "yo = 20\n",
    "s_x = -5\n",
    "s_y = 10\n",
    "\n",
    "#create data\n",
    "data = twoD_Gaussian(x, y, amplitude, xo, yo, s_x, s_y, 0, 0)\n",
    "\n",
    "# plot twoD_Gaussian data generated above\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(data.reshape(40, 40), cmap='jet')\n",
    "plt.colorbar()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-26 15:52:12,481\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-01-26_15-52-11_924519_1707033/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-01-26_15-52-11_924519_1707033/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-01-26_15-52-11_924519_1707033',\n 'metrics_export_port': 47767,\n 'node_id': 'fbb9d705a2552ca853f836148cae734d74df159f'}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#ModelCatalog.register_custom_model(\n",
    "#    \"my_model\", VisionNetwork)\n",
    "\n",
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,  # parallelism\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [256, 128]\n",
    "    }\n",
    "    #\"model\": {\n",
    "    #    \"dim\": 40,\n",
    "    #    \"conv_filters\": [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [512, [5, 16], 1]]\n",
    "    #}\n",
    "    #\"model\": {\n",
    "    #    \"use_lstm\": True,\n",
    "    #    # Max seq len for training the LSTM, defaults to 20.\n",
    "    #    \"max_seq_len\": 20,\n",
    "    #    # Size of the LSTM cell.\n",
    "    #    \"lstm_cell_size\": 256\n",
    "    #}\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    #\"training_iteration\": 1,\n",
    "    \"timesteps_total\": 100000\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-25 21:38:42,098\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-01-25 21:38:42,099\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m [ERROR] [1611607125.367892, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m [WARN] [1611607125.370488, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m [WARN] [1611607125.371146, 0.000000]: END Init ControllersConnection\n",
      "2021-01-25 21:38:51,264\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(config=config)\n",
    "policy = trainer.get_policy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          3328        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          3328        observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 128)          32896       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 128)          32896       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 2)            258         fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            129         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 72,835\n",
      "Trainable params: 72,835\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(policy.model.base_model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m 2021-01-26 06:32:37,063\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m 2021-01-26 06:32:37,063\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m [ERROR] [1611639160.161236, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m [WARN] [1611639160.164323, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m [WARN] [1611639160.165244, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m 2021-01-26 06:32:45,776\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m 2021-01-26 06:32:46,889\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3113369)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=3113371)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 53.80821917808219\n",
      "  episode_reward_max: -92.79065921589513\n",
      "  episode_reward_mean: -98.30140472610407\n",
      "  episode_reward_min: -101.98332086546421\n",
      "  episodes_this_iter: 73\n",
      "  episodes_total: 73\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.392870545387268\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010542325675487518\n",
      "        model: {}\n",
      "        policy_loss: -0.010539914481341839\n",
      "        total_loss: 4583.1474609375\n",
      "        vf_explained_var: 1.4218591786629986e-05\n",
      "        vf_loss: 4583.15673828125\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.80060606060606\n",
      "    ram_util_percent: 32.39999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10756074533078765\n",
      "    mean_env_wait_ms: 34.95477044978639\n",
      "    mean_inference_ms: 0.973461330369007\n",
      "    mean_raw_obs_processing_ms: 20.900881072933213\n",
      "  time_since_restore: 230.59766364097595\n",
      "  time_this_iter_s: 230.59766364097595\n",
      "  time_total_s: 230.59766364097595\n",
      "  timers:\n",
      "    learn_throughput: 1605.934\n",
      "    learn_time_ms: 2490.762\n",
      "    load_throughput: 107072.666\n",
      "    load_time_ms: 37.358\n",
      "    sample_throughput: 17.554\n",
      "    sample_time_ms: 227863.768\n",
      "    update_time_ms: 1.438\n",
      "  timestamp: 1611639396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 51.92\n",
      "  episode_reward_max: -92.72863594991507\n",
      "  episode_reward_mean: -98.29015056768446\n",
      "  episode_reward_min: -101.25060041488409\n",
      "  episodes_this_iter: 79\n",
      "  episodes_total: 152\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3654240369796753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005023751873522997\n",
      "        model: {}\n",
      "        policy_loss: -0.00817113183438778\n",
      "        total_loss: 4063.3330078125\n",
      "        vf_explained_var: 9.017606316774618e-06\n",
      "        vf_loss: 4063.339599609375\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.65424242424243\n",
      "    ram_util_percent: 32.43333333333333\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10651463852250774\n",
      "    mean_env_wait_ms: 34.43397543409653\n",
      "    mean_inference_ms: 0.9595821145898273\n",
      "    mean_raw_obs_processing_ms: 21.597051440311887\n",
      "  time_since_restore: 462.332307100296\n",
      "  time_this_iter_s: 231.73464345932007\n",
      "  time_total_s: 462.332307100296\n",
      "  timers:\n",
      "    learn_throughput: 1724.733\n",
      "    learn_time_ms: 2319.199\n",
      "    load_throughput: 167976.251\n",
      "    load_time_ms: 23.813\n",
      "    sample_throughput: 17.494\n",
      "    sample_time_ms: 228647.961\n",
      "    update_time_ms: 1.65\n",
      "  timestamp: 1611639628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 53.49\n",
      "  episode_reward_max: -92.74437481597764\n",
      "  episode_reward_mean: -98.50906788285721\n",
      "  episode_reward_min: -102.51224634697436\n",
      "  episodes_this_iter: 75\n",
      "  episodes_total: 227\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3599610328674316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013620194047689438\n",
      "        model: {}\n",
      "        policy_loss: -0.012716265395283699\n",
      "        total_loss: 3220.51318359375\n",
      "        vf_explained_var: 2.8956321784789907e-06\n",
      "        vf_loss: 3220.523193359375\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.2944099378882\n",
      "    ram_util_percent: 32.44130434782608\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10573527098847717\n",
      "    mean_env_wait_ms: 34.02355332197937\n",
      "    mean_inference_ms: 0.9495679201762033\n",
      "    mean_raw_obs_processing_ms: 21.705849063334554\n",
      "  time_since_restore: 687.5508658885956\n",
      "  time_this_iter_s: 225.21855878829956\n",
      "  time_total_s: 687.5508658885956\n",
      "  timers:\n",
      "    learn_throughput: 1765.186\n",
      "    learn_time_ms: 2266.05\n",
      "    load_throughput: 202598.098\n",
      "    load_time_ms: 19.744\n",
      "    sample_throughput: 17.641\n",
      "    sample_time_ms: 226743.627\n",
      "    update_time_ms: 1.661\n",
      "  timestamp: 1611639853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-47-46\n",
      "  done: false\n",
      "  episode_len_mean: 58.94\n",
      "  episode_reward_max: -91.0478944807029\n",
      "  episode_reward_mean: -98.24042691524262\n",
      "  episode_reward_min: -102.10414264894784\n",
      "  episodes_this_iter: 64\n",
      "  episodes_total: 291\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3089243173599243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010872535407543182\n",
      "        model: {}\n",
      "        policy_loss: -0.011942238546907902\n",
      "        total_loss: 2439.185302734375\n",
      "        vf_explained_var: 1.318993099630461e-06\n",
      "        vf_loss: 2439.195068359375\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.71683168316832\n",
      "    ram_util_percent: 32.41485148514852\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10522861100753823\n",
      "    mean_env_wait_ms: 33.825931787119366\n",
      "    mean_inference_ms: 0.9451692545526369\n",
      "    mean_raw_obs_processing_ms: 21.136753089350215\n",
      "  time_since_restore: 900.1987652778625\n",
      "  time_this_iter_s: 212.64789938926697\n",
      "  time_total_s: 900.1987652778625\n",
      "  timers:\n",
      "    learn_throughput: 1790.686\n",
      "    learn_time_ms: 2233.781\n",
      "    load_throughput: 221843.817\n",
      "    load_time_ms: 18.031\n",
      "    sample_throughput: 17.965\n",
      "    sample_time_ms: 222654.9\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1611640066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 64.3\n",
      "  episode_reward_max: -84.46443295584008\n",
      "  episode_reward_mean: -98.51885004733491\n",
      "  episode_reward_min: -104.7437489375174\n",
      "  episodes_this_iter: 59\n",
      "  episodes_total: 350\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3401310443878174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018028177320957184\n",
      "        model: {}\n",
      "        policy_loss: -0.020117737352848053\n",
      "        total_loss: 2009.8673095703125\n",
      "        vf_explained_var: -1.4016704881214537e-06\n",
      "        vf_loss: 2009.8837890625\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.10813559322034\n",
      "    ram_util_percent: 32.435932203389825\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1047108138936765\n",
      "    mean_env_wait_ms: 33.68526800205728\n",
      "    mean_inference_ms: 0.9409856439292065\n",
      "    mean_raw_obs_processing_ms: 20.38016704267967\n",
      "  time_since_restore: 1106.9656314849854\n",
      "  time_this_iter_s: 206.7668662071228\n",
      "  time_total_s: 1106.9656314849854\n",
      "  timers:\n",
      "    learn_throughput: 1809.357\n",
      "    learn_time_ms: 2210.73\n",
      "    load_throughput: 234215.29\n",
      "    load_time_ms: 17.078\n",
      "    sample_throughput: 18.262\n",
      "    sample_time_ms: 219028.119\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1611640272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 76.73\n",
      "  episode_reward_max: -84.46443295584008\n",
      "  episode_reward_mean: -98.50133176105223\n",
      "  episode_reward_min: -105.04064844281673\n",
      "  episodes_this_iter: 46\n",
      "  episodes_total: 396\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2941516637802124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012573865242302418\n",
      "        model: {}\n",
      "        policy_loss: -0.016138242557644844\n",
      "        total_loss: 1366.3726806640625\n",
      "        vf_explained_var: 2.9590821668534772e-06\n",
      "        vf_loss: 1366.3861083984375\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.31788321167885\n",
      "    ram_util_percent: 32.44197080291971\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10440173051618905\n",
      "    mean_env_wait_ms: 33.593398382297394\n",
      "    mean_inference_ms: 0.9383770120214223\n",
      "    mean_raw_obs_processing_ms: 19.54024764377085\n",
      "  time_since_restore: 1298.8603959083557\n",
      "  time_this_iter_s: 191.89476442337036\n",
      "  time_total_s: 1298.8603959083557\n",
      "  timers:\n",
      "    learn_throughput: 1819.445\n",
      "    learn_time_ms: 2198.473\n",
      "    load_throughput: 234773.341\n",
      "    load_time_ms: 17.038\n",
      "    sample_throughput: 18.68\n",
      "    sample_time_ms: 214127.815\n",
      "    update_time_ms: 1.598\n",
      "  timestamp: 1611640464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_06-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 95.97\n",
      "  episode_reward_max: -82.43759058161413\n",
      "  episode_reward_mean: -97.90810469326782\n",
      "  episode_reward_min: -105.04064844281673\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 431\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.288836121559143\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017889894545078278\n",
      "        model: {}\n",
      "        policy_loss: -0.020601321011781693\n",
      "        total_loss: 1035.694091796875\n",
      "        vf_explained_var: 2.616836127344868e-06\n",
      "        vf_loss: 1035.7113037109375\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.66303501945526\n",
      "    ram_util_percent: 32.41984435797665\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10429773334785314\n",
      "    mean_env_wait_ms: 33.54657702810158\n",
      "    mean_inference_ms: 0.9374643405556614\n",
      "    mean_raw_obs_processing_ms: 18.701374076004683\n",
      "  time_since_restore: 1478.5882074832916\n",
      "  time_this_iter_s: 179.7278115749359\n",
      "  time_total_s: 1478.5882074832916\n",
      "  timers:\n",
      "    learn_throughput: 1826.848\n",
      "    learn_time_ms: 2189.564\n",
      "    load_throughput: 234906.864\n",
      "    load_time_ms: 17.028\n",
      "    sample_throughput: 19.149\n",
      "    sample_time_ms: 208890.384\n",
      "    update_time_ms: 1.548\n",
      "  timestamp: 1611640644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 109.45\n",
      "  episode_reward_max: -82.43759058161413\n",
      "  episode_reward_mean: -96.99871198262798\n",
      "  episode_reward_min: -105.04064844281673\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 463\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2862541675567627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02145250327885151\n",
      "        model: {}\n",
      "        policy_loss: -0.025007305666804314\n",
      "        total_loss: 880.82470703125\n",
      "        vf_explained_var: 0.1954440474510193\n",
      "        vf_loss: 880.8455200195312\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.16560000000001\n",
      "    ram_util_percent: 32.42439999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10413346364578287\n",
      "    mean_env_wait_ms: 33.495330092867874\n",
      "    mean_inference_ms: 0.9359600775662387\n",
      "    mean_raw_obs_processing_ms: 17.74708174041526\n",
      "  time_since_restore: 1654.179726600647\n",
      "  time_this_iter_s: 175.59151911735535\n",
      "  time_total_s: 1654.179726600647\n",
      "  timers:\n",
      "    learn_throughput: 1832.941\n",
      "    learn_time_ms: 2182.285\n",
      "    load_throughput: 234819.619\n",
      "    load_time_ms: 17.034\n",
      "    sample_throughput: 19.565\n",
      "    sample_time_ms: 204443.177\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1611640820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-03-14\n",
      "  done: false\n",
      "  episode_len_mean: 123.74\n",
      "  episode_reward_max: -82.43759058161413\n",
      "  episode_reward_mean: -96.05413018803118\n",
      "  episode_reward_min: -105.03958326750993\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 494\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2684499025344849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008641515858471394\n",
      "        model: {}\n",
      "        policy_loss: -0.016645235940814018\n",
      "        total_loss: 725.2891235351562\n",
      "        vf_explained_var: 0.28180140256881714\n",
      "        vf_loss: 725.3031616210938\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.88145161290322\n",
      "    ram_util_percent: 32.41048387096773\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10395417842253493\n",
      "    mean_env_wait_ms: 33.43866715655056\n",
      "    mean_inference_ms: 0.9342414954052828\n",
      "    mean_raw_obs_processing_ms: 16.770950000802372\n",
      "  time_since_restore: 1827.8494534492493\n",
      "  time_this_iter_s: 173.6697268486023\n",
      "  time_total_s: 1827.8494534492493\n",
      "  timers:\n",
      "    learn_throughput: 1838.079\n",
      "    learn_time_ms: 2176.186\n",
      "    load_throughput: 246724.974\n",
      "    load_time_ms: 16.212\n",
      "    sample_throughput: 19.923\n",
      "    sample_time_ms: 200773.124\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1611640994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-05-57\n",
      "  done: false\n",
      "  episode_len_mean: 140.36\n",
      "  episode_reward_max: 114.33053248153846\n",
      "  episode_reward_mean: -93.93280586554191\n",
      "  episode_reward_min: -106.94272712222335\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 515\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.250533103942871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01841295324265957\n",
      "        model: {}\n",
      "        policy_loss: -0.028191350400447845\n",
      "        total_loss: 485.5684814453125\n",
      "        vf_explained_var: 0.42757415771484375\n",
      "        vf_loss: 485.5910339355469\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.19785407725323\n",
      "    ram_util_percent: 32.399999999999984\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10382576577301511\n",
      "    mean_env_wait_ms: 33.403455102103706\n",
      "    mean_inference_ms: 0.9331006242639753\n",
      "    mean_raw_obs_processing_ms: 16.144939042490382\n",
      "  time_since_restore: 1990.8383116722107\n",
      "  time_this_iter_s: 162.98885822296143\n",
      "  time_total_s: 1990.8383116722107\n",
      "  timers:\n",
      "    learn_throughput: 1842.0\n",
      "    learn_time_ms: 2171.553\n",
      "    load_throughput: 247574.255\n",
      "    load_time_ms: 16.157\n",
      "    sample_throughput: 20.329\n",
      "    sample_time_ms: 196767.36\n",
      "    update_time_ms: 1.522\n",
      "  timestamp: 1611641157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-08-42\n",
      "  done: false\n",
      "  episode_len_mean: 148.28\n",
      "  episode_reward_max: 114.33053248153846\n",
      "  episode_reward_mean: -93.50025795648193\n",
      "  episode_reward_min: -106.94272712222335\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 538\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2201846837997437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009179970249533653\n",
      "        model: {}\n",
      "        policy_loss: -0.017588868737220764\n",
      "        total_loss: 474.705322265625\n",
      "        vf_explained_var: 0.4562579095363617\n",
      "        vf_loss: 474.7202453613281\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41991525423728\n",
      "    ram_util_percent: 32.417372881355924\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10366521105466898\n",
      "    mean_env_wait_ms: 33.36288466498724\n",
      "    mean_inference_ms: 0.931722507052553\n",
      "    mean_raw_obs_processing_ms: 15.389599122759892\n",
      "  time_since_restore: 2156.1243121623993\n",
      "  time_this_iter_s: 165.2860004901886\n",
      "  time_total_s: 2156.1243121623993\n",
      "  timers:\n",
      "    learn_throughput: 1842.193\n",
      "    learn_time_ms: 2171.325\n",
      "    load_throughput: 297805.248\n",
      "    load_time_ms: 13.432\n",
      "    sample_throughput: 21.025\n",
      "    sample_time_ms: 190245.311\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1611641322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 168.14\n",
      "  episode_reward_max: 114.33053248153846\n",
      "  episode_reward_mean: -93.36713704022387\n",
      "  episode_reward_min: -106.94272712222335\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 557\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2053077220916748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01521383598446846\n",
      "        model: {}\n",
      "        policy_loss: -0.023392783477902412\n",
      "        total_loss: 421.5831604003906\n",
      "        vf_explained_var: 0.45733681321144104\n",
      "        vf_loss: 421.6020202636719\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54890829694322\n",
      "    ram_util_percent: 32.39999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353284612220523\n",
      "    mean_env_wait_ms: 33.33359018993761\n",
      "    mean_inference_ms: 0.9306825701432803\n",
      "    mean_raw_obs_processing_ms: 14.768596683127294\n",
      "  time_since_restore: 2316.766434431076\n",
      "  time_this_iter_s: 160.64212226867676\n",
      "  time_total_s: 2316.766434431076\n",
      "  timers:\n",
      "    learn_throughput: 1844.718\n",
      "    learn_time_ms: 2168.353\n",
      "    load_throughput: 298658.768\n",
      "    load_time_ms: 13.393\n",
      "    sample_throughput: 21.841\n",
      "    sample_time_ms: 183140.619\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1611641483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-14-01\n",
      "  done: false\n",
      "  episode_len_mean: 186.16\n",
      "  episode_reward_max: 115.10655987040482\n",
      "  episode_reward_mean: -91.02067681228395\n",
      "  episode_reward_min: -106.94272712222335\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 573\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.222739815711975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023119589313864708\n",
      "        model: {}\n",
      "        policy_loss: -0.03259888291358948\n",
      "        total_loss: 484.3351745605469\n",
      "        vf_explained_var: 0.4819546043872833\n",
      "        vf_loss: 484.3608093261719\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.43495575221239\n",
      "    ram_util_percent: 32.43141592920354\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343759817438804\n",
      "    mean_env_wait_ms: 33.3161227372235\n",
      "    mean_inference_ms: 0.9299759543235976\n",
      "    mean_raw_obs_processing_ms: 14.222665096099439\n",
      "  time_since_restore: 2474.884371995926\n",
      "  time_this_iter_s: 158.11793756484985\n",
      "  time_total_s: 2474.884371995926\n",
      "  timers:\n",
      "    learn_throughput: 1847.528\n",
      "    learn_time_ms: 2165.055\n",
      "    load_throughput: 283188.919\n",
      "    load_time_ms: 14.125\n",
      "    sample_throughput: 22.672\n",
      "    sample_time_ms: 176431.296\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1611641641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-16-38\n",
      "  done: false\n",
      "  episode_len_mean: 206.86\n",
      "  episode_reward_max: 115.57508347770857\n",
      "  episode_reward_mean: -88.5530829360256\n",
      "  episode_reward_min: -106.94272712222335\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 589\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.191521167755127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005775300785899162\n",
      "        model: {}\n",
      "        policy_loss: -0.014868390746414661\n",
      "        total_loss: 460.977783203125\n",
      "        vf_explained_var: 0.4664916396141052\n",
      "        vf_loss: 460.989990234375\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45669642857142\n",
      "    ram_util_percent: 32.39999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10335308486247331\n",
      "    mean_env_wait_ms: 33.30020286330416\n",
      "    mean_inference_ms: 0.9293212699767901\n",
      "    mean_raw_obs_processing_ms: 13.636015934971097\n",
      "  time_since_restore: 2632.223410844803\n",
      "  time_this_iter_s: 157.33903884887695\n",
      "  time_total_s: 2632.223410844803\n",
      "  timers:\n",
      "    learn_throughput: 1849.157\n",
      "    learn_time_ms: 2163.147\n",
      "    load_throughput: 284842.861\n",
      "    load_time_ms: 14.043\n",
      "    sample_throughput: 23.405\n",
      "    sample_time_ms: 170901.127\n",
      "    update_time_ms: 1.509\n",
      "  timestamp: 1611641798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-19-21\n",
      "  done: false\n",
      "  episode_len_mean: 209.55\n",
      "  episode_reward_max: 117.45633361193114\n",
      "  episode_reward_mean: -85.76163289638176\n",
      "  episode_reward_min: -106.81708800661563\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 609\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1458139419555664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006676710210740566\n",
      "        model: {}\n",
      "        policy_loss: -0.013894694857299328\n",
      "        total_loss: 666.741455078125\n",
      "        vf_explained_var: 0.4603755474090576\n",
      "        vf_loss: 666.7523193359375\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.34612068965517\n",
      "    ram_util_percent: 32.41810344827586\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10328156897463767\n",
      "    mean_env_wait_ms: 33.28500759623055\n",
      "    mean_inference_ms: 0.9288038285549587\n",
      "    mean_raw_obs_processing_ms: 12.963618188377644\n",
      "  time_since_restore: 2794.872815847397\n",
      "  time_this_iter_s: 162.649405002594\n",
      "  time_total_s: 2794.872815847397\n",
      "  timers:\n",
      "    learn_throughput: 1849.84\n",
      "    learn_time_ms: 2162.349\n",
      "    load_throughput: 292355.771\n",
      "    load_time_ms: 13.682\n",
      "    sample_throughput: 24.025\n",
      "    sample_time_ms: 166490.169\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1611641961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-22-04\n",
      "  done: false\n",
      "  episode_len_mean: 215.82\n",
      "  episode_reward_max: 117.45633361193114\n",
      "  episode_reward_mean: -83.39650852334425\n",
      "  episode_reward_min: -104.49144178801775\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 629\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2726919651031494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004786422476172447\n",
      "        model: {}\n",
      "        policy_loss: -0.013536561280488968\n",
      "        total_loss: 412.5890808105469\n",
      "        vf_explained_var: 0.621060848236084\n",
      "        vf_loss: 412.6004638671875\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22532188841201\n",
      "    ram_util_percent: 32.40815450643776\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10324674195761041\n",
      "    mean_env_wait_ms: 33.27690550190983\n",
      "    mean_inference_ms: 0.9286156549815012\n",
      "    mean_raw_obs_processing_ms: 12.369942267631455\n",
      "  time_since_restore: 2957.9284229278564\n",
      "  time_this_iter_s: 163.0556070804596\n",
      "  time_total_s: 2957.9284229278564\n",
      "  timers:\n",
      "    learn_throughput: 1853.103\n",
      "    learn_time_ms: 2158.542\n",
      "    load_throughput: 305015.89\n",
      "    load_time_ms: 13.114\n",
      "    sample_throughput: 24.448\n",
      "    sample_time_ms: 163609.353\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1611642124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-24-43\n",
      "  done: false\n",
      "  episode_len_mean: 227.06\n",
      "  episode_reward_max: 117.45633361193114\n",
      "  episode_reward_mean: -80.64154264634\n",
      "  episode_reward_min: -104.49144178801775\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 645\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1925714015960693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015324356965720654\n",
      "        model: {}\n",
      "        policy_loss: -0.017483962699770927\n",
      "        total_loss: 220.3501739501953\n",
      "        vf_explained_var: 0.757356584072113\n",
      "        vf_loss: 220.36422729492188\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45840707964601\n",
      "    ram_util_percent: 32.406637168141586\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10324278403228931\n",
      "    mean_env_wait_ms: 33.27309480312621\n",
      "    mean_inference_ms: 0.9286369299872512\n",
      "    mean_raw_obs_processing_ms: 11.917879289943015\n",
      "  time_since_restore: 3116.4117035865784\n",
      "  time_this_iter_s: 158.48328065872192\n",
      "  time_total_s: 3116.4117035865784\n",
      "  timers:\n",
      "    learn_throughput: 1822.272\n",
      "    learn_time_ms: 2195.062\n",
      "    load_throughput: 315095.267\n",
      "    load_time_ms: 12.695\n",
      "    sample_throughput: 24.776\n",
      "    sample_time_ms: 161443.819\n",
      "    update_time_ms: 1.552\n",
      "  timestamp: 1611642283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-27-21\n",
      "  done: false\n",
      "  episode_len_mean: 224.13\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -78.49418844899964\n",
      "  episode_reward_min: -104.49144178801775\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 662\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1931580305099487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0063627855852246284\n",
      "        model: {}\n",
      "        policy_loss: -0.012864403426647186\n",
      "        total_loss: 493.87286376953125\n",
      "        vf_explained_var: 0.5761516094207764\n",
      "        vf_loss: 493.88421630859375\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35330396475771\n",
      "    ram_util_percent: 32.415418502202634\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10324837012690644\n",
      "    mean_env_wait_ms: 33.26903571488435\n",
      "    mean_inference_ms: 0.9287428875505462\n",
      "    mean_raw_obs_processing_ms: 11.483575102097648\n",
      "  time_since_restore: 3275.2309427261353\n",
      "  time_this_iter_s: 158.81923913955688\n",
      "  time_total_s: 3275.2309427261353\n",
      "  timers:\n",
      "    learn_throughput: 1821.349\n",
      "    learn_time_ms: 2196.175\n",
      "    load_throughput: 317495.345\n",
      "    load_time_ms: 12.599\n",
      "    sample_throughput: 25.037\n",
      "    sample_time_ms: 159766.001\n",
      "    update_time_ms: 1.573\n",
      "  timestamp: 1611642441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 227.17\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -73.78793095304296\n",
      "  episode_reward_min: -104.49144178801775\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 679\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.156752347946167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010474367998540401\n",
      "        model: {}\n",
      "        policy_loss: -0.02352888695895672\n",
      "        total_loss: 471.48187255859375\n",
      "        vf_explained_var: 0.6207888126373291\n",
      "        vf_loss: 471.5030517578125\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.65707964601769\n",
      "    ram_util_percent: 32.411504424778755\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10324325650282845\n",
      "    mean_env_wait_ms: 33.26347076287218\n",
      "    mean_inference_ms: 0.9287639655369627\n",
      "    mean_raw_obs_processing_ms: 11.113977877148596\n",
      "  time_since_restore: 3433.556585073471\n",
      "  time_this_iter_s: 158.32564234733582\n",
      "  time_total_s: 3433.556585073471\n",
      "  timers:\n",
      "    learn_throughput: 1820.595\n",
      "    learn_time_ms: 2197.083\n",
      "    load_throughput: 316979.467\n",
      "    load_time_ms: 12.619\n",
      "    sample_throughput: 25.279\n",
      "    sample_time_ms: 158231.152\n",
      "    update_time_ms: 1.564\n",
      "  timestamp: 1611642600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 233.22\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -69.88185806350445\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 693\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1017223596572876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008425870910286903\n",
      "        model: {}\n",
      "        policy_loss: -0.010299821384251118\n",
      "        total_loss: 504.6974182128906\n",
      "        vf_explained_var: 0.610211193561554\n",
      "        vf_loss: 504.7058410644531\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67194570135747\n",
      "    ram_util_percent: 32.41628959276018\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1032220063525745\n",
      "    mean_env_wait_ms: 33.258354851652754\n",
      "    mean_inference_ms: 0.9286863177478091\n",
      "    mean_raw_obs_processing_ms: 10.834588792440666\n",
      "  time_since_restore: 3588.420431137085\n",
      "  time_this_iter_s: 154.8638460636139\n",
      "  time_total_s: 3588.420431137085\n",
      "  timers:\n",
      "    learn_throughput: 1791.149\n",
      "    learn_time_ms: 2233.203\n",
      "    load_throughput: 309852.583\n",
      "    load_time_ms: 12.909\n",
      "    sample_throughput: 25.416\n",
      "    sample_time_ms: 157379.759\n",
      "    update_time_ms: 1.533\n",
      "  timestamp: 1611642755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-35-13\n",
      "  done: false\n",
      "  episode_len_mean: 237.48\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -72.46750328966371\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 710\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1050702333450317\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011658180505037308\n",
      "        model: {}\n",
      "        policy_loss: -0.02394612692296505\n",
      "        total_loss: 576.8265991210938\n",
      "        vf_explained_var: 0.4927895665168762\n",
      "        vf_loss: 576.847900390625\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.59601769911504\n",
      "    ram_util_percent: 32.39999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10317274183669883\n",
      "    mean_env_wait_ms: 33.25025603814256\n",
      "    mean_inference_ms: 0.9283748895624389\n",
      "    mean_raw_obs_processing_ms: 10.50869494223082\n",
      "  time_since_restore: 3746.231277704239\n",
      "  time_this_iter_s: 157.81084656715393\n",
      "  time_total_s: 3746.231277704239\n",
      "  timers:\n",
      "    learn_throughput: 1820.828\n",
      "    learn_time_ms: 2196.803\n",
      "    load_throughput: 310242.78\n",
      "    load_time_ms: 12.893\n",
      "    sample_throughput: 25.531\n",
      "    sample_time_ms: 156671.564\n",
      "    update_time_ms: 1.535\n",
      "  timestamp: 1611642913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-37-43\n",
      "  done: false\n",
      "  episode_len_mean: 256.83\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -69.86194866949309\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 720\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.004278540611267\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009328741580247879\n",
      "        model: {}\n",
      "        policy_loss: -0.013971791602671146\n",
      "        total_loss: 388.6033935546875\n",
      "        vf_explained_var: 0.5817276239395142\n",
      "        vf_loss: 388.615234375\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.89672897196262\n",
      "    ram_util_percent: 32.41168224299065\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10313547484860898\n",
      "    mean_env_wait_ms: 33.24374732175844\n",
      "    mean_inference_ms: 0.9281279045228689\n",
      "    mean_raw_obs_processing_ms: 10.320734910586127\n",
      "  time_since_restore: 3896.3128578662872\n",
      "  time_this_iter_s: 150.08158016204834\n",
      "  time_total_s: 3896.3128578662872\n",
      "  timers:\n",
      "    learn_throughput: 1816.045\n",
      "    learn_time_ms: 2202.589\n",
      "    load_throughput: 294441.985\n",
      "    load_time_ms: 13.585\n",
      "    sample_throughput: 25.706\n",
      "    sample_time_ms: 155607.281\n",
      "    update_time_ms: 1.549\n",
      "  timestamp: 1611643063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-40-14\n",
      "  done: false\n",
      "  episode_len_mean: 276.74\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -69.48057673884063\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 731\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.148352861404419\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017762476578354836\n",
      "        model: {}\n",
      "        policy_loss: -0.023236485198140144\n",
      "        total_loss: 306.259033203125\n",
      "        vf_explained_var: 0.5854785442352295\n",
      "        vf_loss: 306.27825927734375\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.02546296296296\n",
      "    ram_util_percent: 32.35972222222222\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10308541782485083\n",
      "    mean_env_wait_ms: 33.23554233343238\n",
      "    mean_inference_ms: 0.9277840763091652\n",
      "    mean_raw_obs_processing_ms: 10.092013091785013\n",
      "  time_since_restore: 4047.4483466148376\n",
      "  time_this_iter_s: 151.13548874855042\n",
      "  time_total_s: 4047.4483466148376\n",
      "  timers:\n",
      "    learn_throughput: 1815.104\n",
      "    learn_time_ms: 2203.731\n",
      "    load_throughput: 300638.937\n",
      "    load_time_ms: 13.305\n",
      "    sample_throughput: 25.821\n",
      "    sample_time_ms: 154909.874\n",
      "    update_time_ms: 1.569\n",
      "  timestamp: 1611643214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-42-47\n",
      "  done: false\n",
      "  episode_len_mean: 281.42\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -65.24188253688425\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 743\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1658909320831299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009899402037262917\n",
      "        model: {}\n",
      "        policy_loss: -0.018349338322877884\n",
      "        total_loss: 510.60272216796875\n",
      "        vf_explained_var: 0.5001085996627808\n",
      "        vf_loss: 510.618896484375\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.76175115207373\n",
      "    ram_util_percent: 32.36221198156682\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10302645122338759\n",
      "    mean_env_wait_ms: 33.22579168378787\n",
      "    mean_inference_ms: 0.9273944363840174\n",
      "    mean_raw_obs_processing_ms: 9.852672847492864\n",
      "  time_since_restore: 4199.948331594467\n",
      "  time_this_iter_s: 152.49998497962952\n",
      "  time_total_s: 4199.948331594467\n",
      "  timers:\n",
      "    learn_throughput: 1815.611\n",
      "    learn_time_ms: 2203.116\n",
      "    load_throughput: 289336.175\n",
      "    load_time_ms: 13.825\n",
      "    sample_throughput: 25.903\n",
      "    sample_time_ms: 154424.383\n",
      "    update_time_ms: 1.579\n",
      "  timestamp: 1611643367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 286.52\n",
      "  episode_reward_max: 118.75671576561935\n",
      "  episode_reward_mean: -64.81355124890304\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 757\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0936776399612427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009938281029462814\n",
      "        model: {}\n",
      "        policy_loss: -0.019140513613820076\n",
      "        total_loss: 628.4830932617188\n",
      "        vf_explained_var: 0.45389145612716675\n",
      "        vf_loss: 628.5000610351562\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.61261261261261\n",
      "    ram_util_percent: 32.322522522522526\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.102969140118615\n",
      "    mean_env_wait_ms: 33.21494966602215\n",
      "    mean_inference_ms: 0.9270062464481086\n",
      "    mean_raw_obs_processing_ms: 9.586109752250083\n",
      "  time_since_restore: 4355.325740337372\n",
      "  time_this_iter_s: 155.37740874290466\n",
      "  time_total_s: 4355.325740337372\n",
      "  timers:\n",
      "    learn_throughput: 1811.734\n",
      "    learn_time_ms: 2207.83\n",
      "    load_throughput: 288725.713\n",
      "    load_time_ms: 13.854\n",
      "    sample_throughput: 26.027\n",
      "    sample_time_ms: 153687.652\n",
      "    update_time_ms: 1.584\n",
      "  timestamp: 1611643522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-48-02\n",
      "  done: false\n",
      "  episode_len_mean: 287.52\n",
      "  episode_reward_max: 118.64088472977106\n",
      "  episode_reward_mean: -63.185614526640975\n",
      "  episode_reward_min: -107.18388107796906\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 776\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1406201124191284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012132972478866577\n",
      "        model: {}\n",
      "        policy_loss: -0.0196334607899189\n",
      "        total_loss: 787.6278076171875\n",
      "        vf_explained_var: 0.5383845567703247\n",
      "        vf_loss: 787.644775390625\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.39825327510916\n",
      "    ram_util_percent: 32.33275109170306\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.102896922467193\n",
      "    mean_env_wait_ms: 33.20031610984066\n",
      "    mean_inference_ms: 0.926526278648042\n",
      "    mean_raw_obs_processing_ms: 9.251244427129452\n",
      "  time_since_restore: 4515.518007278442\n",
      "  time_this_iter_s: 160.19226694107056\n",
      "  time_total_s: 4515.518007278442\n",
      "  timers:\n",
      "    learn_throughput: 1805.097\n",
      "    learn_time_ms: 2215.947\n",
      "    load_throughput: 270266.784\n",
      "    load_time_ms: 14.8\n",
      "    sample_throughput: 26.077\n",
      "    sample_time_ms: 153393.359\n",
      "    update_time_ms: 1.552\n",
      "  timestamp: 1611643682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 292.56\n",
      "  episode_reward_max: 118.64088472977106\n",
      "  episode_reward_mean: -58.77813337580503\n",
      "  episode_reward_min: -102.9262608856082\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 790\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1142932176589966\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008392446674406528\n",
      "        model: {}\n",
      "        policy_loss: -0.01658761315047741\n",
      "        total_loss: 690.0574951171875\n",
      "        vf_explained_var: 0.5176987051963806\n",
      "        vf_loss: 690.0722045898438\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.7660633484163\n",
      "    ram_util_percent: 32.33257918552036\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10285148285536198\n",
      "    mean_env_wait_ms: 33.19082963919127\n",
      "    mean_inference_ms: 0.9262377702511836\n",
      "    mean_raw_obs_processing_ms: 9.026176869908086\n",
      "  time_since_restore: 4670.352862358093\n",
      "  time_this_iter_s: 154.83485507965088\n",
      "  time_total_s: 4670.352862358093\n",
      "  timers:\n",
      "    learn_throughput: 1834.899\n",
      "    learn_time_ms: 2179.956\n",
      "    load_throughput: 263507.652\n",
      "    load_time_ms: 15.18\n",
      "    sample_throughput: 26.133\n",
      "    sample_time_ms: 153066.09\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1611643837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 299.81\n",
      "  episode_reward_max: 118.7234517623815\n",
      "  episode_reward_mean: -58.54761396044397\n",
      "  episode_reward_min: -102.9262608856082\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 801\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1704638004302979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012541662901639938\n",
      "        model: {}\n",
      "        policy_loss: -0.017536334693431854\n",
      "        total_loss: 474.8573303222656\n",
      "        vf_explained_var: 0.4584282636642456\n",
      "        vf_loss: 474.8721008300781\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88425925925925\n",
      "    ram_util_percent: 32.31712962962963\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1028261465401196\n",
      "    mean_env_wait_ms: 33.184360833605325\n",
      "    mean_inference_ms: 0.9260850759013798\n",
      "    mean_raw_obs_processing_ms: 8.855616112420757\n",
      "  time_since_restore: 4822.045641422272\n",
      "  time_this_iter_s: 151.69277906417847\n",
      "  time_total_s: 4822.045641422272\n",
      "  timers:\n",
      "    learn_throughput: 1834.853\n",
      "    learn_time_ms: 2180.012\n",
      "    load_throughput: 259168.823\n",
      "    load_time_ms: 15.434\n",
      "    sample_throughput: 26.255\n",
      "    sample_time_ms: 152353.713\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1611643989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 317.13\n",
      "  episode_reward_max: 118.7234517623815\n",
      "  episode_reward_mean: -57.8292674862489\n",
      "  episode_reward_min: -102.9262608856082\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 811\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1424000263214111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009511254727840424\n",
      "        model: {}\n",
      "        policy_loss: -0.010943262837827206\n",
      "        total_loss: 361.30841064453125\n",
      "        vf_explained_var: 0.6312668323516846\n",
      "        vf_loss: 361.3172302246094\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.93953488372092\n",
      "    ram_util_percent: 32.32000000000001\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10280285233345396\n",
      "    mean_env_wait_ms: 33.17851714223911\n",
      "    mean_inference_ms: 0.9259532037291529\n",
      "    mean_raw_obs_processing_ms: 8.691924250882607\n",
      "  time_since_restore: 4972.397373199463\n",
      "  time_this_iter_s: 150.35173177719116\n",
      "  time_total_s: 4972.397373199463\n",
      "  timers:\n",
      "    learn_throughput: 1836.454\n",
      "    learn_time_ms: 2178.11\n",
      "    load_throughput: 242901.284\n",
      "    load_time_ms: 16.468\n",
      "    sample_throughput: 26.393\n",
      "    sample_time_ms: 151555.424\n",
      "    update_time_ms: 1.532\n",
      "  timestamp: 1611644139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_07-58-13\n",
      "  done: false\n",
      "  episode_len_mean: 310.27\n",
      "  episode_reward_max: 118.7234517623815\n",
      "  episode_reward_mean: -55.95274491402999\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 823\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1213902235031128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013102014549076557\n",
      "        model: {}\n",
      "        policy_loss: -0.020091740414500237\n",
      "        total_loss: 535.51416015625\n",
      "        vf_explained_var: 0.5774534344673157\n",
      "        vf_loss: 535.5313110351562\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69132420091324\n",
      "    ram_util_percent: 32.31689497716896\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10278087117049912\n",
      "    mean_env_wait_ms: 33.17279831248772\n",
      "    mean_inference_ms: 0.9258420753299057\n",
      "    mean_raw_obs_processing_ms: 8.518644163313818\n",
      "  time_since_restore: 5125.521318435669\n",
      "  time_this_iter_s: 153.12394523620605\n",
      "  time_total_s: 5125.521318435669\n",
      "  timers:\n",
      "    learn_throughput: 1833.774\n",
      "    learn_time_ms: 2181.293\n",
      "    load_throughput: 244764.925\n",
      "    load_time_ms: 16.342\n",
      "    sample_throughput: 26.424\n",
      "    sample_time_ms: 151380.013\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1611644293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-00-46\n",
      "  done: false\n",
      "  episode_len_mean: 303.41\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -47.93960812923323\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 835\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1481765508651733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01228339597582817\n",
      "        model: {}\n",
      "        policy_loss: -0.018170272931456566\n",
      "        total_loss: 554.3026733398438\n",
      "        vf_explained_var: 0.5781749486923218\n",
      "        vf_loss: 554.318115234375\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.77660550458715\n",
      "    ram_util_percent: 32.34036697247706\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10276576742543497\n",
      "    mean_env_wait_ms: 33.16818877770651\n",
      "    mean_inference_ms: 0.925800842121447\n",
      "    mean_raw_obs_processing_ms: 8.362145585356437\n",
      "  time_since_restore: 5278.593601703644\n",
      "  time_this_iter_s: 153.07228326797485\n",
      "  time_total_s: 5278.593601703644\n",
      "  timers:\n",
      "    learn_throughput: 1834.105\n",
      "    learn_time_ms: 2180.9\n",
      "    load_throughput: 245721.006\n",
      "    load_time_ms: 16.279\n",
      "    sample_throughput: 26.507\n",
      "    sample_time_ms: 150906.304\n",
      "    update_time_ms: 1.593\n",
      "  timestamp: 1611644446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-03-16\n",
      "  done: false\n",
      "  episode_len_mean: 311.02\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -49.99443925925234\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 845\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0675050020217896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010514491237699986\n",
      "        model: {}\n",
      "        policy_loss: -0.014155833050608635\n",
      "        total_loss: 427.7077941894531\n",
      "        vf_explained_var: 0.6089208722114563\n",
      "        vf_loss: 427.7196044921875\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.00279069767441\n",
      "    ram_util_percent: 32.3739534883721\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10275290901922193\n",
      "    mean_env_wait_ms: 33.16433969671366\n",
      "    mean_inference_ms: 0.9257637436805461\n",
      "    mean_raw_obs_processing_ms: 8.236183324032243\n",
      "  time_since_restore: 5429.108399629593\n",
      "  time_this_iter_s: 150.5147979259491\n",
      "  time_total_s: 5429.108399629593\n",
      "  timers:\n",
      "    learn_throughput: 1804.467\n",
      "    learn_time_ms: 2216.721\n",
      "    load_throughput: 240436.699\n",
      "    load_time_ms: 16.636\n",
      "    sample_throughput: 26.506\n",
      "    sample_time_ms: 150912.047\n",
      "    update_time_ms: 1.564\n",
      "  timestamp: 1611644596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 324.85\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -47.65715203050837\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 856\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0498601198196411\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016863927245140076\n",
      "        model: {}\n",
      "        policy_loss: -0.030567592009902\n",
      "        total_loss: 429.6501159667969\n",
      "        vf_explained_var: 0.6105998754501343\n",
      "        vf_loss: 429.6768798828125\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.04861111111111\n",
      "    ram_util_percent: 32.319907407407406\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10273156701816472\n",
      "    mean_env_wait_ms: 33.15971547943178\n",
      "    mean_inference_ms: 0.9256736465551524\n",
      "    mean_raw_obs_processing_ms: 8.099937587470636\n",
      "  time_since_restore: 5580.233505487442\n",
      "  time_this_iter_s: 151.12510585784912\n",
      "  time_total_s: 5580.233505487442\n",
      "  timers:\n",
      "    learn_throughput: 1804.889\n",
      "    learn_time_ms: 2216.202\n",
      "    load_throughput: 235388.427\n",
      "    load_time_ms: 16.993\n",
      "    sample_throughput: 26.506\n",
      "    sample_time_ms: 150908.561\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1611644748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 333.89\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -51.37803115202889\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 869\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.096621036529541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014043443836271763\n",
      "        model: {}\n",
      "        policy_loss: -0.021863508969545364\n",
      "        total_loss: 497.8664855957031\n",
      "        vf_explained_var: 0.5532037615776062\n",
      "        vf_loss: 497.8852233886719\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.78173515981736\n",
      "    ram_util_percent: 32.31689497716896\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10270529762163473\n",
      "    mean_env_wait_ms: 33.154675717965326\n",
      "    mean_inference_ms: 0.9255782528267471\n",
      "    mean_raw_obs_processing_ms: 7.939534185429527\n",
      "  time_since_restore: 5733.498456954956\n",
      "  time_this_iter_s: 153.26495146751404\n",
      "  time_total_s: 5733.498456954956\n",
      "  timers:\n",
      "    learn_throughput: 1799.876\n",
      "    learn_time_ms: 2222.375\n",
      "    load_throughput: 233379.924\n",
      "    load_time_ms: 17.139\n",
      "    sample_throughput: 26.494\n",
      "    sample_time_ms: 150980.317\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1611644901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-10-53\n",
      "  done: false\n",
      "  episode_len_mean: 347.57\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -50.94272962556043\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 880\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.081229329109192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011005464941263199\n",
      "        model: {}\n",
      "        policy_loss: -0.021370386704802513\n",
      "        total_loss: 277.9183654785156\n",
      "        vf_explained_var: 0.7068093419075012\n",
      "        vf_loss: 277.93731689453125\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81712962962963\n",
      "    ram_util_percent: 32.33101851851851\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10268369014003335\n",
      "    mean_env_wait_ms: 33.150451012229645\n",
      "    mean_inference_ms: 0.9255000630322376\n",
      "    mean_raw_obs_processing_ms: 7.798670516147253\n",
      "  time_since_restore: 5885.291151046753\n",
      "  time_this_iter_s: 151.79269409179688\n",
      "  time_total_s: 5885.291151046753\n",
      "  timers:\n",
      "    learn_throughput: 1801.021\n",
      "    learn_time_ms: 2220.962\n",
      "    load_throughput: 223094.157\n",
      "    load_time_ms: 17.93\n",
      "    sample_throughput: 26.556\n",
      "    sample_time_ms: 150624.766\n",
      "    update_time_ms: 1.468\n",
      "  timestamp: 1611645053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 354.47\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -56.83284921306148\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1454741954803467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01447678916156292\n",
      "        model: {}\n",
      "        policy_loss: -0.025652531534433365\n",
      "        total_loss: 220.4522705078125\n",
      "        vf_explained_var: 0.7230652570724487\n",
      "        vf_loss: 220.47467041015625\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88878504672896\n",
      "    ram_util_percent: 32.3570093457944\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10266473207722263\n",
      "    mean_env_wait_ms: 33.14623051565443\n",
      "    mean_inference_ms: 0.9254415538606285\n",
      "    mean_raw_obs_processing_ms: 7.6693393107427115\n",
      "  time_since_restore: 6035.034322977066\n",
      "  time_this_iter_s: 149.7431719303131\n",
      "  time_total_s: 6035.034322977066\n",
      "  timers:\n",
      "    learn_throughput: 1806.25\n",
      "    learn_time_ms: 2214.533\n",
      "    load_throughput: 236390.71\n",
      "    load_time_ms: 16.921\n",
      "    sample_throughput: 26.74\n",
      "    sample_time_ms: 149586.332\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1611645203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-15-57\n",
      "  done: false\n",
      "  episode_len_mean: 352.34\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -52.62809099806778\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 903\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1016789674758911\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010883857496082783\n",
      "        model: {}\n",
      "        policy_loss: -0.02157546766102314\n",
      "        total_loss: 548.4397583007812\n",
      "        vf_explained_var: 0.5321381688117981\n",
      "        vf_loss: 548.4588623046875\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57318181818181\n",
      "    ram_util_percent: 32.32090909090909\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10263876766163901\n",
      "    mean_env_wait_ms: 33.14142530564171\n",
      "    mean_inference_ms: 0.9253715210006888\n",
      "    mean_raw_obs_processing_ms: 7.517171591040933\n",
      "  time_since_restore: 6189.219634056091\n",
      "  time_this_iter_s: 154.18531107902527\n",
      "  time_total_s: 6189.219634056091\n",
      "  timers:\n",
      "    learn_throughput: 1809.02\n",
      "    learn_time_ms: 2211.142\n",
      "    load_throughput: 245165.163\n",
      "    load_time_ms: 16.316\n",
      "    sample_throughput: 26.752\n",
      "    sample_time_ms: 149524.278\n",
      "    update_time_ms: 1.449\n",
      "  timestamp: 1611645357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 345.91\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -46.8032791687163\n",
      "  episode_reward_min: -105.18315928572414\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 914\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0860766172409058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01688079722225666\n",
      "        model: {}\n",
      "        policy_loss: -0.019763942807912827\n",
      "        total_loss: 565.9077758789062\n",
      "        vf_explained_var: 0.5366469621658325\n",
      "        vf_loss: 565.9237060546875\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.79308755760368\n",
      "    ram_util_percent: 32.38571428571428\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10261647473104882\n",
      "    mean_env_wait_ms: 33.13760242289939\n",
      "    mean_inference_ms: 0.9253099345306535\n",
      "    mean_raw_obs_processing_ms: 7.399083052991482\n",
      "  time_since_restore: 6340.879694700241\n",
      "  time_this_iter_s: 151.66006064414978\n",
      "  time_total_s: 6340.879694700241\n",
      "  timers:\n",
      "    learn_throughput: 1811.503\n",
      "    learn_time_ms: 2208.112\n",
      "    load_throughput: 243302.855\n",
      "    load_time_ms: 16.44\n",
      "    sample_throughput: 26.752\n",
      "    sample_time_ms: 149521.592\n",
      "    update_time_ms: 1.448\n",
      "  timestamp: 1611645509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 353.42\n",
      "  episode_reward_max: 118.77023685982311\n",
      "  episode_reward_mean: -42.535538916810104\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 927\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0419660806655884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014075874350965023\n",
      "        model: {}\n",
      "        policy_loss: -0.02075149305164814\n",
      "        total_loss: 570.377197265625\n",
      "        vf_explained_var: 0.5617857575416565\n",
      "        vf_loss: 570.3948364257812\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.80867579908676\n",
      "    ram_util_percent: 32.323287671232876\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10258809389861973\n",
      "    mean_env_wait_ms: 33.132825465570335\n",
      "    mean_inference_ms: 0.9252126527683108\n",
      "    mean_raw_obs_processing_ms: 7.269079245744802\n",
      "  time_since_restore: 6494.361586809158\n",
      "  time_this_iter_s: 153.48189210891724\n",
      "  time_total_s: 6494.361586809158\n",
      "  timers:\n",
      "    learn_throughput: 1807.794\n",
      "    learn_time_ms: 2212.642\n",
      "    load_throughput: 254504.877\n",
      "    load_time_ms: 15.717\n",
      "    sample_throughput: 26.697\n",
      "    sample_time_ms: 149827.362\n",
      "    update_time_ms: 1.436\n",
      "  timestamp: 1611645662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 358.2\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -50.30793204454164\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 936\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8698325157165527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013372773304581642\n",
      "        model: {}\n",
      "        policy_loss: -0.016621654853224754\n",
      "        total_loss: 239.15115356445312\n",
      "        vf_explained_var: 0.7416965365409851\n",
      "        vf_loss: 239.16476440429688\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.84953271028039\n",
      "    ram_util_percent: 32.435981308411215\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10256967701502212\n",
      "    mean_env_wait_ms: 33.12960138597482\n",
      "    mean_inference_ms: 0.9251406555226365\n",
      "    mean_raw_obs_processing_ms: 7.180028797066327\n",
      "  time_since_restore: 6644.076917409897\n",
      "  time_this_iter_s: 149.71533060073853\n",
      "  time_total_s: 6644.076917409897\n",
      "  timers:\n",
      "    learn_throughput: 1838.315\n",
      "    learn_time_ms: 2175.906\n",
      "    load_throughput: 251000.368\n",
      "    load_time_ms: 15.936\n",
      "    sample_throughput: 26.751\n",
      "    sample_time_ms: 149524.82\n",
      "    update_time_ms: 1.346\n",
      "  timestamp: 1611645812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 350.35\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -46.292747993100626\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 948\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0078401565551758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0105094313621521\n",
      "        model: {}\n",
      "        policy_loss: -0.016953708603978157\n",
      "        total_loss: 483.36492919921875\n",
      "        vf_explained_var: 0.6424568295478821\n",
      "        vf_loss: 483.37945556640625\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.9589861751152\n",
      "    ram_util_percent: 32.40967741935483\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10254391188389941\n",
      "    mean_env_wait_ms: 33.125616126698205\n",
      "    mean_inference_ms: 0.925009131874701\n",
      "    mean_raw_obs_processing_ms: 7.071871224748468\n",
      "  time_since_restore: 6796.319839000702\n",
      "  time_this_iter_s: 152.24292159080505\n",
      "  time_total_s: 6796.319839000702\n",
      "  timers:\n",
      "    learn_throughput: 1836.869\n",
      "    learn_time_ms: 2177.618\n",
      "    load_throughput: 236162.774\n",
      "    load_time_ms: 16.937\n",
      "    sample_throughput: 26.768\n",
      "    sample_time_ms: 149434.748\n",
      "    update_time_ms: 1.349\n",
      "  timestamp: 1611645964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-28-44\n",
      "  done: false\n",
      "  episode_len_mean: 351.76\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -46.34702135881484\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 959\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9828372001647949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00853454414755106\n",
      "        model: {}\n",
      "        policy_loss: -0.015975572168827057\n",
      "        total_loss: 331.9899597167969\n",
      "        vf_explained_var: 0.6927706003189087\n",
      "        vf_loss: 332.0040283203125\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.19912280701753\n",
      "    ram_util_percent: 32.61140350877193\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10253155593645472\n",
      "    mean_env_wait_ms: 33.12742397199631\n",
      "    mean_inference_ms: 0.9249702766325663\n",
      "    mean_raw_obs_processing_ms: 6.9779589623760705\n",
      "  time_since_restore: 6956.030846357346\n",
      "  time_this_iter_s: 159.71100735664368\n",
      "  time_total_s: 6956.030846357346\n",
      "  timers:\n",
      "    learn_throughput: 1831.543\n",
      "    learn_time_ms: 2183.951\n",
      "    load_throughput: 236686.85\n",
      "    load_time_ms: 16.9\n",
      "    sample_throughput: 26.605\n",
      "    sample_time_ms: 150349.084\n",
      "    update_time_ms: 1.379\n",
      "  timestamp: 1611646124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-31-29\n",
      "  done: false\n",
      "  episode_len_mean: 363.67\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -44.015639783525664\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 969\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9783917665481567\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010439109988510609\n",
      "        model: {}\n",
      "        policy_loss: -0.022235572338104248\n",
      "        total_loss: 375.1468505859375\n",
      "        vf_explained_var: 0.6698873043060303\n",
      "        vf_loss: 375.166748046875\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.25254237288135\n",
      "    ram_util_percent: 32.55169491525424\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10254037065049762\n",
      "    mean_env_wait_ms: 33.1373435233196\n",
      "    mean_inference_ms: 0.9250957346827583\n",
      "    mean_raw_obs_processing_ms: 6.892075096108122\n",
      "  time_since_restore: 7121.190330028534\n",
      "  time_this_iter_s: 165.15948367118835\n",
      "  time_total_s: 7121.190330028534\n",
      "  timers:\n",
      "    learn_throughput: 1795.121\n",
      "    learn_time_ms: 2228.262\n",
      "    load_throughput: 250412.936\n",
      "    load_time_ms: 15.974\n",
      "    sample_throughput: 26.367\n",
      "    sample_time_ms: 151706.88\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1611646289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-34-05\n",
      "  done: false\n",
      "  episode_len_mean: 361.23\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -37.73797618679121\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 980\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9958792924880981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017832282930612564\n",
      "        model: {}\n",
      "        policy_loss: -0.02589871920645237\n",
      "        total_loss: 425.7043762207031\n",
      "        vf_explained_var: 0.6349640488624573\n",
      "        vf_loss: 425.72625732421875\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.33333333333333\n",
      "    ram_util_percent: 32.48198198198198\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10255524569162347\n",
      "    mean_env_wait_ms: 33.15034719735831\n",
      "    mean_inference_ms: 0.9252693047149725\n",
      "    mean_raw_obs_processing_ms: 6.802264397093986\n",
      "  time_since_restore: 7276.5490210056305\n",
      "  time_this_iter_s: 155.35869097709656\n",
      "  time_total_s: 7276.5490210056305\n",
      "  timers:\n",
      "    learn_throughput: 1796.176\n",
      "    learn_time_ms: 2226.954\n",
      "    load_throughput: 250662.858\n",
      "    load_time_ms: 15.958\n",
      "    sample_throughput: 26.331\n",
      "    sample_time_ms: 151912.775\n",
      "    update_time_ms: 1.37\n",
      "  timestamp: 1611646445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 347.15\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -31.846236718996735\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 993\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0036076307296753\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011103285476565361\n",
      "        model: {}\n",
      "        policy_loss: -0.01977911777794361\n",
      "        total_loss: 486.3615417480469\n",
      "        vf_explained_var: 0.6641245484352112\n",
      "        vf_loss: 486.3788146972656\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.785\n",
      "    ram_util_percent: 32.485454545454544\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10257230579047842\n",
      "    mean_env_wait_ms: 33.16613556570607\n",
      "    mean_inference_ms: 0.9254531998840764\n",
      "    mean_raw_obs_processing_ms: 6.706820849703979\n",
      "  time_since_restore: 7430.562801122665\n",
      "  time_this_iter_s: 154.0137801170349\n",
      "  time_total_s: 7430.562801122665\n",
      "  timers:\n",
      "    learn_throughput: 1796.451\n",
      "    learn_time_ms: 2226.613\n",
      "    load_throughput: 251120.591\n",
      "    load_time_ms: 15.929\n",
      "    sample_throughput: 26.292\n",
      "    sample_time_ms: 152137.608\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1611646599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 357.29\n",
      "  episode_reward_max: 118.76851196751517\n",
      "  episode_reward_mean: -27.818434864050246\n",
      "  episode_reward_min: -102.18504956202507\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1003\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9723984599113464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017001669853925705\n",
      "        model: {}\n",
      "        policy_loss: -0.025394802913069725\n",
      "        total_loss: 585.0899047851562\n",
      "        vf_explained_var: 0.3918983042240143\n",
      "        vf_loss: 585.1114501953125\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03738317757009\n",
      "    ram_util_percent: 32.452336448598125\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10258436323920103\n",
      "    mean_env_wait_ms: 33.17754609757174\n",
      "    mean_inference_ms: 0.9255875129625896\n",
      "    mean_raw_obs_processing_ms: 6.632977818715095\n",
      "  time_since_restore: 7580.670441865921\n",
      "  time_this_iter_s: 150.10764074325562\n",
      "  time_total_s: 7580.670441865921\n",
      "  timers:\n",
      "    learn_throughput: 1793.688\n",
      "    learn_time_ms: 2230.042\n",
      "    load_throughput: 250119.132\n",
      "    load_time_ms: 15.992\n",
      "    sample_throughput: 26.286\n",
      "    sample_time_ms: 152170.859\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1611646749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 355.99\n",
      "  episode_reward_max: 118.73577310117737\n",
      "  episode_reward_mean: -33.83906431201729\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1015\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9120075106620789\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016814127564430237\n",
      "        model: {}\n",
      "        policy_loss: -0.01654995232820511\n",
      "        total_loss: 329.3504333496094\n",
      "        vf_explained_var: 0.6958175301551819\n",
      "        vf_loss: 329.3631591796875\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.80642201834864\n",
      "    ram_util_percent: 32.4348623853211\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10260043460895656\n",
      "    mean_env_wait_ms: 33.191163198559444\n",
      "    mean_inference_ms: 0.9257619788857224\n",
      "    mean_raw_obs_processing_ms: 6.550009917773554\n",
      "  time_since_restore: 7733.539612770081\n",
      "  time_this_iter_s: 152.86917090415955\n",
      "  time_total_s: 7733.539612770081\n",
      "  timers:\n",
      "    learn_throughput: 1791.765\n",
      "    learn_time_ms: 2232.436\n",
      "    load_throughput: 251646.413\n",
      "    load_time_ms: 15.895\n",
      "    sample_throughput: 26.309\n",
      "    sample_time_ms: 152040.4\n",
      "    update_time_ms: 1.445\n",
      "  timestamp: 1611646902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 359.23\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: -27.357820706890813\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1027\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1308541297912598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018053267151117325\n",
      "        model: {}\n",
      "        policy_loss: -0.020605016499757767\n",
      "        total_loss: 402.0783386230469\n",
      "        vf_explained_var: 0.7080810070037842\n",
      "        vf_loss: 402.0948486328125\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.91336405529954\n",
      "    ram_util_percent: 32.504608294930875\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10261755315191444\n",
      "    mean_env_wait_ms: 33.20457521567421\n",
      "    mean_inference_ms: 0.9259342796297423\n",
      "    mean_raw_obs_processing_ms: 6.469037845300263\n",
      "  time_since_restore: 7885.751955270767\n",
      "  time_this_iter_s: 152.21234250068665\n",
      "  time_total_s: 7885.751955270767\n",
      "  timers:\n",
      "    learn_throughput: 1791.5\n",
      "    learn_time_ms: 2232.766\n",
      "    load_throughput: 251959.331\n",
      "    load_time_ms: 15.876\n",
      "    sample_throughput: 26.299\n",
      "    sample_time_ms: 152097.258\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1611647054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 354.73\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: -15.231927689726856\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1037\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1687535047531128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015993492677807808\n",
      "        model: {}\n",
      "        policy_loss: -0.02995915338397026\n",
      "        total_loss: 499.1471862792969\n",
      "        vf_explained_var: 0.6121177077293396\n",
      "        vf_loss: 499.1734924316406\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85560747663551\n",
      "    ram_util_percent: 32.42289719626168\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10262992074265444\n",
      "    mean_env_wait_ms: 33.215173891727545\n",
      "    mean_inference_ms: 0.9260615124854223\n",
      "    mean_raw_obs_processing_ms: 6.406139568744468\n",
      "  time_since_restore: 8035.63875079155\n",
      "  time_this_iter_s: 149.88679552078247\n",
      "  time_total_s: 8035.63875079155\n",
      "  timers:\n",
      "    learn_throughput: 1790.912\n",
      "    learn_time_ms: 2233.499\n",
      "    load_throughput: 253578.974\n",
      "    load_time_ms: 15.774\n",
      "    sample_throughput: 26.361\n",
      "    sample_time_ms: 151739.335\n",
      "    update_time_ms: 1.453\n",
      "  timestamp: 1611647204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-49-15\n",
      "  done: false\n",
      "  episode_len_mean: 359.76\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: -8.974912223174922\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1048\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0995138883590698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00928142573684454\n",
      "        model: {}\n",
      "        policy_loss: -0.0161006860435009\n",
      "        total_loss: 747.875732421875\n",
      "        vf_explained_var: 0.4418198764324188\n",
      "        vf_loss: 747.8898315429688\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.12592592592591\n",
      "    ram_util_percent: 32.510185185185186\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10264232240871012\n",
      "    mean_env_wait_ms: 33.22655259786842\n",
      "    mean_inference_ms: 0.9262048300449318\n",
      "    mean_raw_obs_processing_ms: 6.3384636963546175\n",
      "  time_since_restore: 8186.613126039505\n",
      "  time_this_iter_s: 150.97437524795532\n",
      "  time_total_s: 8186.613126039505\n",
      "  timers:\n",
      "    learn_throughput: 1792.695\n",
      "    learn_time_ms: 2231.277\n",
      "    load_throughput: 268153.573\n",
      "    load_time_ms: 14.917\n",
      "    sample_throughput: 26.339\n",
      "    sample_time_ms: 151866.864\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1611647355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 356.53\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: -2.840801659544818\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1059\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0755348205566406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017175748944282532\n",
      "        model: {}\n",
      "        policy_loss: -0.020621903240680695\n",
      "        total_loss: 464.1728210449219\n",
      "        vf_explained_var: 0.6301822066307068\n",
      "        vf_loss: 464.1895446777344\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.97916666666667\n",
      "    ram_util_percent: 32.49675925925926\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10264395597236942\n",
      "    mean_env_wait_ms: 33.23263251755247\n",
      "    mean_inference_ms: 0.9262579330219025\n",
      "    mean_raw_obs_processing_ms: 6.273576884586731\n",
      "  time_since_restore: 8337.583903312683\n",
      "  time_this_iter_s: 150.9707772731781\n",
      "  time_total_s: 8337.583903312683\n",
      "  timers:\n",
      "    learn_throughput: 1791.009\n",
      "    learn_time_ms: 2233.378\n",
      "    load_throughput: 266450.083\n",
      "    load_time_ms: 15.012\n",
      "    sample_throughput: 26.361\n",
      "    sample_time_ms: 151741.245\n",
      "    update_time_ms: 1.461\n",
      "  timestamp: 1611647507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 350.24\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: 3.430892661080526\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1072\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1211085319519043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015127019956707954\n",
      "        model: {}\n",
      "        policy_loss: -0.026078693568706512\n",
      "        total_loss: 624.8971557617188\n",
      "        vf_explained_var: 0.6109411120414734\n",
      "        vf_loss: 624.919921875\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88767123287671\n",
      "    ram_util_percent: 32.45342465753424\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1026191976389101\n",
      "    mean_env_wait_ms: 33.22885101624541\n",
      "    mean_inference_ms: 0.9260911413847341\n",
      "    mean_raw_obs_processing_ms: 6.204485064658061\n",
      "  time_since_restore: 8491.360387086868\n",
      "  time_this_iter_s: 153.77648377418518\n",
      "  time_total_s: 8491.360387086868\n",
      "  timers:\n",
      "    learn_throughput: 1828.428\n",
      "    learn_time_ms: 2187.672\n",
      "    load_throughput: 258882.086\n",
      "    load_time_ms: 15.451\n",
      "    sample_throughput: 26.456\n",
      "    sample_time_ms: 151195.767\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1611647660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-56-51\n",
      "  done: false\n",
      "  episode_len_mean: 353.69\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: 5.733552928860558\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1083\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.103337287902832\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009438776411116123\n",
      "        model: {}\n",
      "        policy_loss: -0.020920367911458015\n",
      "        total_loss: 504.5531005859375\n",
      "        vf_explained_var: 0.5695392489433289\n",
      "        vf_loss: 504.5719299316406\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.04186046511627\n",
      "    ram_util_percent: 32.47860465116279\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1025928890987353\n",
      "    mean_env_wait_ms: 33.22353494927019\n",
      "    mean_inference_ms: 0.9259089069232829\n",
      "    mean_raw_obs_processing_ms: 6.148412085928491\n",
      "  time_since_restore: 8641.891979455948\n",
      "  time_this_iter_s: 150.5315923690796\n",
      "  time_total_s: 8641.891979455948\n",
      "  timers:\n",
      "    learn_throughput: 1866.418\n",
      "    learn_time_ms: 2143.142\n",
      "    load_throughput: 260830.474\n",
      "    load_time_ms: 15.336\n",
      "    sample_throughput: 26.706\n",
      "    sample_time_ms: 149781.715\n",
      "    update_time_ms: 1.494\n",
      "  timestamp: 1611647811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_08-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 366.44\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: 12.047777158752924\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1092\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9523311257362366\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010132466442883015\n",
      "        model: {}\n",
      "        policy_loss: -0.015418770723044872\n",
      "        total_loss: 572.8510131835938\n",
      "        vf_explained_var: 0.4510645568370819\n",
      "        vf_loss: 572.8640747070312\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.10707547169811\n",
      "    ram_util_percent: 32.50801886792453\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10257302655407781\n",
      "    mean_env_wait_ms: 33.218904352212746\n",
      "    mean_inference_ms: 0.9257616590671697\n",
      "    mean_raw_obs_processing_ms: 6.100898134936399\n",
      "  time_since_restore: 8790.433572292328\n",
      "  time_this_iter_s: 148.54159283638\n",
      "  time_total_s: 8790.433572292328\n",
      "  timers:\n",
      "    learn_throughput: 1869.927\n",
      "    learn_time_ms: 2139.121\n",
      "    load_throughput: 258432.665\n",
      "    load_time_ms: 15.478\n",
      "    sample_throughput: 26.826\n",
      "    sample_time_ms: 149107.854\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1611647960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 356.59\n",
      "  episode_reward_max: 118.75902232919171\n",
      "  episode_reward_mean: 14.095383420494862\n",
      "  episode_reward_min: -101.60978250004649\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1104\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0463279485702515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010332898236811161\n",
      "        model: {}\n",
      "        policy_loss: -0.017251240089535713\n",
      "        total_loss: 506.7823181152344\n",
      "        vf_explained_var: 0.6357517838478088\n",
      "        vf_loss: 506.7972412109375\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6972602739726\n",
      "    ram_util_percent: 32.42146118721461\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10254722072136602\n",
      "    mean_env_wait_ms: 33.213301627656556\n",
      "    mean_inference_ms: 0.9255560808260931\n",
      "    mean_raw_obs_processing_ms: 6.041227672181151\n",
      "  time_since_restore: 8943.414011955261\n",
      "  time_this_iter_s: 152.98043966293335\n",
      "  time_total_s: 8943.414011955261\n",
      "  timers:\n",
      "    learn_throughput: 1870.137\n",
      "    learn_time_ms: 2138.88\n",
      "    load_throughput: 271795.721\n",
      "    load_time_ms: 14.717\n",
      "    sample_throughput: 26.845\n",
      "    sample_time_ms: 149003.415\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1611648113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-04-27\n",
      "  done: false\n",
      "  episode_len_mean: 356.28\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 22.078303609346378\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1117\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1690340042114258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011018997058272362\n",
      "        model: {}\n",
      "        policy_loss: -0.02010728418827057\n",
      "        total_loss: 280.74530029296875\n",
      "        vf_explained_var: 0.8290309906005859\n",
      "        vf_loss: 280.7629089355469\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.92511415525114\n",
      "    ram_util_percent: 32.465753424657535\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1025199747855062\n",
      "    mean_env_wait_ms: 33.2075746331345\n",
      "    mean_inference_ms: 0.9253230497257516\n",
      "    mean_raw_obs_processing_ms: 5.980902901176826\n",
      "  time_since_restore: 9097.363844394684\n",
      "  time_this_iter_s: 153.9498324394226\n",
      "  time_total_s: 9097.363844394684\n",
      "  timers:\n",
      "    learn_throughput: 1870.268\n",
      "    learn_time_ms: 2138.731\n",
      "    load_throughput: 274093.178\n",
      "    load_time_ms: 14.594\n",
      "    sample_throughput: 26.776\n",
      "    sample_time_ms: 149385.573\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1611648267\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 356.5\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 13.596922347075449\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1128\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1477620601654053\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013118595816195011\n",
      "        model: {}\n",
      "        policy_loss: -0.02356838621199131\n",
      "        total_loss: 435.0660095214844\n",
      "        vf_explained_var: 0.7058153748512268\n",
      "        vf_loss: 435.086669921875\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.95092592592593\n",
      "    ram_util_percent: 32.464814814814815\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10249748000753053\n",
      "    mean_env_wait_ms: 33.202798558272846\n",
      "    mean_inference_ms: 0.9251237106347222\n",
      "    mean_raw_obs_processing_ms: 5.930825481432562\n",
      "  time_since_restore: 9248.09683728218\n",
      "  time_this_iter_s: 150.73299288749695\n",
      "  time_total_s: 9248.09683728218\n",
      "  timers:\n",
      "    learn_throughput: 1868.681\n",
      "    learn_time_ms: 2140.547\n",
      "    load_throughput: 257882.888\n",
      "    load_time_ms: 15.511\n",
      "    sample_throughput: 26.815\n",
      "    sample_time_ms: 149168.231\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1611648418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 356.05\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 13.772456741016107\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1138\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9712725877761841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012811719439923763\n",
      "        model: {}\n",
      "        policy_loss: -0.01853306218981743\n",
      "        total_loss: 380.0371398925781\n",
      "        vf_explained_var: 0.6630144715309143\n",
      "        vf_loss: 380.05279541015625\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.93411214953272\n",
      "    ram_util_percent: 32.43691588785046\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10247885465610866\n",
      "    mean_env_wait_ms: 33.198648441656225\n",
      "    mean_inference_ms: 0.9249675960410799\n",
      "    mean_raw_obs_processing_ms: 5.887111062240001\n",
      "  time_since_restore: 9398.02467417717\n",
      "  time_this_iter_s: 149.927836894989\n",
      "  time_total_s: 9398.02467417717\n",
      "  timers:\n",
      "    learn_throughput: 1867.276\n",
      "    learn_time_ms: 2142.158\n",
      "    load_throughput: 258735.563\n",
      "    load_time_ms: 15.46\n",
      "    sample_throughput: 26.857\n",
      "    sample_time_ms: 148937.133\n",
      "    update_time_ms: 1.463\n",
      "  timestamp: 1611648568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-12-00\n",
      "  done: false\n",
      "  episode_len_mean: 357.0\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 17.91108035640942\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9950202703475952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013992669060826302\n",
      "        model: {}\n",
      "        policy_loss: -0.01844724267721176\n",
      "        total_loss: 283.2976379394531\n",
      "        vf_explained_var: 0.8024027347564697\n",
      "        vf_loss: 283.31292724609375\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8105504587156\n",
      "    ram_util_percent: 32.49036697247707\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10245981228383955\n",
      "    mean_env_wait_ms: 33.19423107928491\n",
      "    mean_inference_ms: 0.9248104407047498\n",
      "    mean_raw_obs_processing_ms: 5.837591704216432\n",
      "  time_since_restore: 9550.7463824749\n",
      "  time_this_iter_s: 152.7217082977295\n",
      "  time_total_s: 9550.7463824749\n",
      "  timers:\n",
      "    learn_throughput: 1870.615\n",
      "    learn_time_ms: 2138.334\n",
      "    load_throughput: 257828.99\n",
      "    load_time_ms: 15.514\n",
      "    sample_throughput: 26.805\n",
      "    sample_time_ms: 149226.088\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1611648720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 349.33\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 15.665104506430534\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1162\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8881590962409973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013067208230495453\n",
      "        model: {}\n",
      "        policy_loss: -0.02410438470542431\n",
      "        total_loss: 471.3443603515625\n",
      "        vf_explained_var: 0.6756031513214111\n",
      "        vf_loss: 471.36541748046875\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98294930875576\n",
      "    ram_util_percent: 32.50967741935484\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10244332069479459\n",
      "    mean_env_wait_ms: 33.190007668861305\n",
      "    mean_inference_ms: 0.9246759273121867\n",
      "    mean_raw_obs_processing_ms: 5.790753332001285\n",
      "  time_since_restore: 9703.156224012375\n",
      "  time_this_iter_s: 152.4098415374756\n",
      "  time_total_s: 9703.156224012375\n",
      "  timers:\n",
      "    learn_throughput: 1867.003\n",
      "    learn_time_ms: 2142.471\n",
      "    load_throughput: 260128.411\n",
      "    load_time_ms: 15.377\n",
      "    sample_throughput: 26.78\n",
      "    sample_time_ms: 149365.512\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1611648873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 358.71\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 21.659743387704985\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1173\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9707887768745422\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01569896563887596\n",
      "        model: {}\n",
      "        policy_loss: -0.022332284599542618\n",
      "        total_loss: 177.3842010498047\n",
      "        vf_explained_var: 0.8468506932258606\n",
      "        vf_loss: 177.40298461914062\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.80414746543781\n",
      "    ram_util_percent: 32.43594470046083\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10242846698306224\n",
      "    mean_env_wait_ms: 33.186396185749956\n",
      "    mean_inference_ms: 0.9245623601541573\n",
      "    mean_raw_obs_processing_ms: 5.747584197849822\n",
      "  time_since_restore: 9855.065665960312\n",
      "  time_this_iter_s: 151.909441947937\n",
      "  time_total_s: 9855.065665960312\n",
      "  timers:\n",
      "    learn_throughput: 1870.388\n",
      "    learn_time_ms: 2138.594\n",
      "    load_throughput: 263188.529\n",
      "    load_time_ms: 15.198\n",
      "    sample_throughput: 26.762\n",
      "    sample_time_ms: 149463.616\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1611649025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 357.25\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 25.70021842295575\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1184\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8079593181610107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01348272804170847\n",
      "        model: {}\n",
      "        policy_loss: -0.02198789268732071\n",
      "        total_loss: 274.5471496582031\n",
      "        vf_explained_var: 0.7942025065422058\n",
      "        vf_loss: 274.5661315917969\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.04759825327511\n",
      "    ram_util_percent: 32.51877729257642\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10241933386085433\n",
      "    mean_env_wait_ms: 33.187095162476936\n",
      "    mean_inference_ms: 0.9245159505258336\n",
      "    mean_raw_obs_processing_ms: 5.70607043257134\n",
      "  time_since_restore: 10015.313643217087\n",
      "  time_this_iter_s: 160.2479772567749\n",
      "  time_total_s: 10015.313643217087\n",
      "  timers:\n",
      "    learn_throughput: 1862.018\n",
      "    learn_time_ms: 2148.207\n",
      "    load_throughput: 291824.869\n",
      "    load_time_ms: 13.707\n",
      "    sample_throughput: 26.649\n",
      "    sample_time_ms: 150099.442\n",
      "    update_time_ms: 1.444\n",
      "  timestamp: 1611649185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-22-22\n",
      "  done: false\n",
      "  episode_len_mean: 346.54\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 21.45831614239226\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1196\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8835800886154175\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01120420265942812\n",
      "        model: {}\n",
      "        policy_loss: -0.019640421494841576\n",
      "        total_loss: 473.99847412109375\n",
      "        vf_explained_var: 0.607223629951477\n",
      "        vf_loss: 474.0156555175781\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.03794642857143\n",
      "    ram_util_percent: 32.5\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10240949767777696\n",
      "    mean_env_wait_ms: 33.190054156543134\n",
      "    mean_inference_ms: 0.9244714891160625\n",
      "    mean_raw_obs_processing_ms: 5.665270980654952\n",
      "  time_since_restore: 10171.928128004074\n",
      "  time_this_iter_s: 156.6144847869873\n",
      "  time_total_s: 10171.928128004074\n",
      "  timers:\n",
      "    learn_throughput: 1851.665\n",
      "    learn_time_ms: 2160.217\n",
      "    load_throughput: 291449.219\n",
      "    load_time_ms: 13.725\n",
      "    sample_throughput: 26.544\n",
      "    sample_time_ms: 150693.744\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1611649342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 348.78\n",
      "  episode_reward_max: 118.7739339747565\n",
      "  episode_reward_mean: 23.56534798219207\n",
      "  episode_reward_min: -102.38447274306417\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1208\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8925991654396057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01722646690905094\n",
      "        model: {}\n",
      "        policy_loss: -0.024194827303290367\n",
      "        total_loss: 404.34259033203125\n",
      "        vf_explained_var: 0.7075451612472534\n",
      "        vf_loss: 404.3629455566406\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.0502262443439\n",
      "    ram_util_percent: 32.514027149321265\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10240249282582364\n",
      "    mean_env_wait_ms: 33.19402100353486\n",
      "    mean_inference_ms: 0.9244540827622728\n",
      "    mean_raw_obs_processing_ms: 5.625589149051474\n",
      "  time_since_restore: 10327.090544462204\n",
      "  time_this_iter_s: 155.16241645812988\n",
      "  time_total_s: 10327.090544462204\n",
      "  timers:\n",
      "    learn_throughput: 1848.399\n",
      "    learn_time_ms: 2164.035\n",
      "    load_throughput: 313081.359\n",
      "    load_time_ms: 12.776\n",
      "    sample_throughput: 26.428\n",
      "    sample_time_ms: 151353.877\n",
      "    update_time_ms: 1.426\n",
      "  timestamp: 1611649497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-27-30\n",
      "  done: false\n",
      "  episode_len_mean: 345.67\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 25.69250858399414\n",
      "  episode_reward_min: -100.78380984171629\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0726163387298584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011731479316949844\n",
      "        model: {}\n",
      "        policy_loss: -0.020082345232367516\n",
      "        total_loss: 427.76580810546875\n",
      "        vf_explained_var: 0.715103805065155\n",
      "        vf_loss: 427.78326416015625\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81009174311927\n",
      "    ram_util_percent: 32.50091743119266\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10239649152708602\n",
      "    mean_env_wait_ms: 33.19793572132296\n",
      "    mean_inference_ms: 0.9244399833569801\n",
      "    mean_raw_obs_processing_ms: 5.586489289894823\n",
      "  time_since_restore: 10479.580783367157\n",
      "  time_this_iter_s: 152.490238904953\n",
      "  time_total_s: 10479.580783367157\n",
      "  timers:\n",
      "    learn_throughput: 1846.244\n",
      "    learn_time_ms: 2166.561\n",
      "    load_throughput: 312269.035\n",
      "    load_time_ms: 12.809\n",
      "    sample_throughput: 26.437\n",
      "    sample_time_ms: 151302.839\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1611649650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 352.88\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 36.28410119636655\n",
      "  episode_reward_min: -100.78380984171629\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1229\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.927108645439148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021074021235108376\n",
      "        model: {}\n",
      "        policy_loss: -0.0317140631377697\n",
      "        total_loss: 160.10523986816406\n",
      "        vf_explained_var: 0.8220545053482056\n",
      "        vf_loss: 160.13221740722656\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03943661971832\n",
      "    ram_util_percent: 32.509389671361504\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10239069311949058\n",
      "    mean_env_wait_ms: 33.2009643614796\n",
      "    mean_inference_ms: 0.9244263876672255\n",
      "    mean_raw_obs_processing_ms: 5.556932652036111\n",
      "  time_since_restore: 10628.659356594086\n",
      "  time_this_iter_s: 149.0785732269287\n",
      "  time_total_s: 10628.659356594086\n",
      "  timers:\n",
      "    learn_throughput: 1845.422\n",
      "    learn_time_ms: 2167.526\n",
      "    load_throughput: 290775.883\n",
      "    load_time_ms: 13.756\n",
      "    sample_throughput: 26.522\n",
      "    sample_time_ms: 150816.273\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1611649799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-32-32\n",
      "  done: false\n",
      "  episode_len_mean: 353.33\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 34.1921826541341\n",
      "  episode_reward_min: -101.70905284045935\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1241\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9626656174659729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009832540526986122\n",
      "        model: {}\n",
      "        policy_loss: -0.019425658509135246\n",
      "        total_loss: 989.0792846679688\n",
      "        vf_explained_var: 0.3321571350097656\n",
      "        vf_loss: 989.095458984375\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.90366972477064\n",
      "    ram_util_percent: 32.5105504587156\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10238442206506226\n",
      "    mean_env_wait_ms: 33.20544495153122\n",
      "    mean_inference_ms: 0.9244018253712417\n",
      "    mean_raw_obs_processing_ms: 5.520328814701873\n",
      "  time_since_restore: 10781.64044380188\n",
      "  time_this_iter_s: 152.9810872077942\n",
      "  time_total_s: 10781.64044380188\n",
      "  timers:\n",
      "    learn_throughput: 1847.879\n",
      "    learn_time_ms: 2164.643\n",
      "    load_throughput: 312598.351\n",
      "    load_time_ms: 12.796\n",
      "    sample_throughput: 26.482\n",
      "    sample_time_ms: 151044.885\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1611649952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-35-03\n",
      "  done: false\n",
      "  episode_len_mean: 356.28\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 36.29459960694975\n",
      "  episode_reward_min: -101.70905284045935\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1251\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9044789671897888\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01013190858066082\n",
      "        model: {}\n",
      "        policy_loss: -0.022151468321681023\n",
      "        total_loss: 244.6677703857422\n",
      "        vf_explained_var: 0.7917694449424744\n",
      "        vf_loss: 244.68650817871094\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.90185185185184\n",
      "    ram_util_percent: 32.5\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1023797431801755\n",
      "    mean_env_wait_ms: 33.209260263966684\n",
      "    mean_inference_ms: 0.9243888854013178\n",
      "    mean_raw_obs_processing_ms: 5.489421793319204\n",
      "  time_since_restore: 10932.645654916763\n",
      "  time_this_iter_s: 151.00521111488342\n",
      "  time_total_s: 10932.645654916763\n",
      "  timers:\n",
      "    learn_throughput: 1844.098\n",
      "    learn_time_ms: 2169.082\n",
      "    load_throughput: 318899.671\n",
      "    load_time_ms: 12.543\n",
      "    sample_throughput: 26.464\n",
      "    sample_time_ms: 151147.221\n",
      "    update_time_ms: 1.4\n",
      "  timestamp: 1611650103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 360.91\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 40.4248939052464\n",
      "  episode_reward_min: -101.70905284045935\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1262\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.975516140460968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013118820264935493\n",
      "        model: {}\n",
      "        policy_loss: -0.023516977205872536\n",
      "        total_loss: 167.04705810546875\n",
      "        vf_explained_var: 0.8442301154136658\n",
      "        vf_loss: 167.06614685058594\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.925\n",
      "    ram_util_percent: 32.515740740740746\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10237455580792154\n",
      "    mean_env_wait_ms: 33.21343598236017\n",
      "    mean_inference_ms: 0.924369456036704\n",
      "    mean_raw_obs_processing_ms: 5.455646368720303\n",
      "  time_since_restore: 11084.079683303833\n",
      "  time_this_iter_s: 151.4340283870697\n",
      "  time_total_s: 11084.079683303833\n",
      "  timers:\n",
      "    learn_throughput: 1843.369\n",
      "    learn_time_ms: 2169.94\n",
      "    load_throughput: 299939.144\n",
      "    load_time_ms: 13.336\n",
      "    sample_throughput: 26.487\n",
      "    sample_time_ms: 151017.487\n",
      "    update_time_ms: 1.397\n",
      "  timestamp: 1611650255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 364.12\n",
      "  episode_reward_max: 118.76979886973245\n",
      "  episode_reward_mean: 40.56789279229835\n",
      "  episode_reward_min: -101.70905284045935\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1271\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8635329604148865\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012786051258444786\n",
      "        model: {}\n",
      "        policy_loss: -0.026324013248085976\n",
      "        total_loss: 322.8634033203125\n",
      "        vf_explained_var: 0.6775249242782593\n",
      "        vf_loss: 322.8855285644531\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.88317757009345\n",
      "    ram_util_percent: 32.53084112149532\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10237201220524295\n",
      "    mean_env_wait_ms: 33.21677731160955\n",
      "    mean_inference_ms: 0.9243621033685763\n",
      "    mean_raw_obs_processing_ms: 5.4281379964321355\n",
      "  time_since_restore: 11233.7017121315\n",
      "  time_this_iter_s: 149.62202882766724\n",
      "  time_total_s: 11233.7017121315\n",
      "  timers:\n",
      "    learn_throughput: 1844.553\n",
      "    learn_time_ms: 2168.547\n",
      "    load_throughput: 299616.684\n",
      "    load_time_ms: 13.35\n",
      "    sample_throughput: 26.536\n",
      "    sample_time_ms: 150740.898\n",
      "    update_time_ms: 1.386\n",
      "  timestamp: 1611650405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 367.26\n",
      "  episode_reward_max: 118.76880544128224\n",
      "  episode_reward_mean: 34.49722079050942\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1282\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9364966750144958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012575560249388218\n",
      "        model: {}\n",
      "        policy_loss: -0.025046078488230705\n",
      "        total_loss: 912.2933349609375\n",
      "        vf_explained_var: 0.32880041003227234\n",
      "        vf_loss: 912.314208984375\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67004608294931\n",
      "    ram_util_percent: 32.51981566820277\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10236874501874382\n",
      "    mean_env_wait_ms: 33.21801886566843\n",
      "    mean_inference_ms: 0.9243263664443113\n",
      "    mean_raw_obs_processing_ms: 5.394681406460859\n",
      "  time_since_restore: 11385.887085437775\n",
      "  time_this_iter_s: 152.1853733062744\n",
      "  time_total_s: 11385.887085437775\n",
      "  timers:\n",
      "    learn_throughput: 1843.258\n",
      "    learn_time_ms: 2170.071\n",
      "    load_throughput: 297155.934\n",
      "    load_time_ms: 13.461\n",
      "    sample_throughput: 26.532\n",
      "    sample_time_ms: 150763.591\n",
      "    update_time_ms: 1.403\n",
      "  timestamp: 1611650557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-45-09\n",
      "  done: false\n",
      "  episode_len_mean: 370.45\n",
      "  episode_reward_max: 118.76880544128224\n",
      "  episode_reward_mean: 40.35170277078899\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1293\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8652102947235107\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01267139334231615\n",
      "        model: {}\n",
      "        policy_loss: -0.022457273676991463\n",
      "        total_loss: 128.58694458007812\n",
      "        vf_explained_var: 0.8756281137466431\n",
      "        vf_loss: 128.6051025390625\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6940366972477\n",
      "    ram_util_percent: 32.518348623853214\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10236571182634278\n",
      "    mean_env_wait_ms: 33.217554896778395\n",
      "    mean_inference_ms: 0.924294012186538\n",
      "    mean_raw_obs_processing_ms: 5.361739060636616\n",
      "  time_since_restore: 11538.136422395706\n",
      "  time_this_iter_s: 152.24933695793152\n",
      "  time_total_s: 11538.136422395706\n",
      "  timers:\n",
      "    learn_throughput: 1853.051\n",
      "    learn_time_ms: 2158.602\n",
      "    load_throughput: 273462.35\n",
      "    load_time_ms: 14.627\n",
      "    sample_throughput: 26.671\n",
      "    sample_time_ms: 149975.87\n",
      "    update_time_ms: 1.408\n",
      "  timestamp: 1611650709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 368.81\n",
      "  episode_reward_max: 118.76880544128224\n",
      "  episode_reward_mean: 42.46412233091783\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1306\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8017573952674866\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011207477189600468\n",
      "        model: {}\n",
      "        policy_loss: -0.02345355600118637\n",
      "        total_loss: 613.0624389648438\n",
      "        vf_explained_var: 0.5959508419036865\n",
      "        vf_loss: 613.0820922851562\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.82237442922374\n",
      "    ram_util_percent: 32.52511415525114\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10236097431092935\n",
      "    mean_env_wait_ms: 33.21571981284913\n",
      "    mean_inference_ms: 0.9242513720617217\n",
      "    mean_raw_obs_processing_ms: 5.324122568168657\n",
      "  time_since_restore: 11691.868816614151\n",
      "  time_this_iter_s: 153.73239421844482\n",
      "  time_total_s: 11691.868816614151\n",
      "  timers:\n",
      "    learn_throughput: 1861.46\n",
      "    learn_time_ms: 2148.851\n",
      "    load_throughput: 257214.721\n",
      "    load_time_ms: 15.551\n",
      "    sample_throughput: 26.72\n",
      "    sample_time_ms: 149698.343\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1611650863\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-50-31\n",
      "  done: false\n",
      "  episode_len_mean: 370.2\n",
      "  episode_reward_max: 118.76880544128224\n",
      "  episode_reward_mean: 44.496593433884655\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1318\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0233381986618042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011173725128173828\n",
      "        model: {}\n",
      "        policy_loss: -0.023518983274698257\n",
      "        total_loss: 351.7256774902344\n",
      "        vf_explained_var: 0.7555978298187256\n",
      "        vf_loss: 351.7454528808594\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.79623430962344\n",
      "    ram_util_percent: 32.51841004184101\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1023670285003293\n",
      "    mean_env_wait_ms: 33.21969989218404\n",
      "    mean_inference_ms: 0.9243094120016923\n",
      "    mean_raw_obs_processing_ms: 5.290618446259825\n",
      "  time_since_restore: 11859.28439116478\n",
      "  time_this_iter_s: 167.41557455062866\n",
      "  time_total_s: 11859.28439116478\n",
      "  timers:\n",
      "    learn_throughput: 1855.697\n",
      "    learn_time_ms: 2155.524\n",
      "    load_throughput: 256237.329\n",
      "    load_time_ms: 15.611\n",
      "    sample_throughput: 26.505\n",
      "    sample_time_ms: 150915.824\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1611651031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 371.31\n",
      "  episode_reward_max: 118.76563102794825\n",
      "  episode_reward_mean: 42.42825181455848\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1326\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.844889760017395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009061288088560104\n",
      "        model: {}\n",
      "        policy_loss: -0.019236477091908455\n",
      "        total_loss: 289.33404541015625\n",
      "        vf_explained_var: 0.6797301769256592\n",
      "        vf_loss: 289.3502502441406\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.44162895927602\n",
      "    ram_util_percent: 32.57692307692308\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10237670597318996\n",
      "    mean_env_wait_ms: 33.22405918908078\n",
      "    mean_inference_ms: 0.9243970960091977\n",
      "    mean_raw_obs_processing_ms: 5.268275435133948\n",
      "  time_since_restore: 12014.203302383423\n",
      "  time_this_iter_s: 154.9189112186432\n",
      "  time_total_s: 12014.203302383423\n",
      "  timers:\n",
      "    learn_throughput: 1845.698\n",
      "    learn_time_ms: 2167.202\n",
      "    load_throughput: 246411.764\n",
      "    load_time_ms: 16.233\n",
      "    sample_throughput: 26.464\n",
      "    sample_time_ms: 151146.371\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1611651186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 368.26\n",
      "  episode_reward_max: 118.76563102794825\n",
      "  episode_reward_mean: 39.992842466121736\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1338\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0888103246688843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011024316772818565\n",
      "        model: {}\n",
      "        policy_loss: -0.02420652098953724\n",
      "        total_loss: 341.16796875\n",
      "        vf_explained_var: 0.7658804059028625\n",
      "        vf_loss: 341.1885070800781\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.70973451327434\n",
      "    ram_util_percent: 32.52256637168142\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10239604076559854\n",
      "    mean_env_wait_ms: 33.232266420227134\n",
      "    mean_inference_ms: 0.9245822356688206\n",
      "    mean_raw_obs_processing_ms: 5.236151965722557\n",
      "  time_since_restore: 12171.972368717194\n",
      "  time_this_iter_s: 157.76906633377075\n",
      "  time_total_s: 12171.972368717194\n",
      "  timers:\n",
      "    learn_throughput: 1847.092\n",
      "    learn_time_ms: 2165.566\n",
      "    load_throughput: 244423.666\n",
      "    load_time_ms: 16.365\n",
      "    sample_throughput: 26.313\n",
      "    sample_time_ms: 152016.904\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1611651344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_09-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 370.72\n",
      "  episode_reward_max: 118.76563102794825\n",
      "  episode_reward_mean: 42.02251537802888\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1348\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9252236485481262\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011288839392364025\n",
      "        model: {}\n",
      "        policy_loss: -0.025171706452965736\n",
      "        total_loss: 192.28982543945312\n",
      "        vf_explained_var: 0.8225991725921631\n",
      "        vf_loss: 192.31118774414062\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45509259259259\n",
      "    ram_util_percent: 32.57314814814816\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10241423878396023\n",
      "    mean_env_wait_ms: 33.23910342985196\n",
      "    mean_inference_ms: 0.9247414879064817\n",
      "    mean_raw_obs_processing_ms: 5.209772540674504\n",
      "  time_since_restore: 12323.18052816391\n",
      "  time_this_iter_s: 151.2081594467163\n",
      "  time_total_s: 12323.18052816391\n",
      "  timers:\n",
      "    learn_throughput: 1843.711\n",
      "    learn_time_ms: 2169.537\n",
      "    load_throughput: 230128.773\n",
      "    load_time_ms: 17.382\n",
      "    sample_throughput: 26.345\n",
      "    sample_time_ms: 151832.865\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1611651495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 372.88\n",
      "  episode_reward_max: 118.76491856081839\n",
      "  episode_reward_mean: 44.05181226574018\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1359\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8728565573692322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010470676235854626\n",
      "        model: {}\n",
      "        policy_loss: -0.01984899491071701\n",
      "        total_loss: 121.04429626464844\n",
      "        vf_explained_var: 0.8858823180198669\n",
      "        vf_loss: 121.06060028076172\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45412844036697\n",
      "    ram_util_percent: 32.515137614678906\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10243651591889451\n",
      "    mean_env_wait_ms: 33.24686764328799\n",
      "    mean_inference_ms: 0.9249306078198493\n",
      "    mean_raw_obs_processing_ms: 5.1816815452302185\n",
      "  time_since_restore: 12475.905906915665\n",
      "  time_this_iter_s: 152.72537875175476\n",
      "  time_total_s: 12475.905906915665\n",
      "  timers:\n",
      "    learn_throughput: 1815.6\n",
      "    learn_time_ms: 2203.128\n",
      "    load_throughput: 231976.218\n",
      "    load_time_ms: 17.243\n",
      "    sample_throughput: 26.321\n",
      "    sample_time_ms: 151968.939\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1611651648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 376.81\n",
      "  episode_reward_max: 118.76491856081839\n",
      "  episode_reward_mean: 41.80137232947049\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1368\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9426144361495972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011230939999222755\n",
      "        model: {}\n",
      "        policy_loss: -0.022593777626752853\n",
      "        total_loss: 84.62342834472656\n",
      "        vf_explained_var: 0.9129330515861511\n",
      "        vf_loss: 84.6422348022461\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.59953271028037\n",
      "    ram_util_percent: 32.610280373831785\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10245622814330663\n",
      "    mean_env_wait_ms: 33.25333166557931\n",
      "    mean_inference_ms: 0.9250862550931228\n",
      "    mean_raw_obs_processing_ms: 5.158938402599912\n",
      "  time_since_restore: 12625.946304321289\n",
      "  time_this_iter_s: 150.0403974056244\n",
      "  time_total_s: 12625.946304321289\n",
      "  timers:\n",
      "    learn_throughput: 1813.897\n",
      "    learn_time_ms: 2205.197\n",
      "    load_throughput: 233387.716\n",
      "    load_time_ms: 17.139\n",
      "    sample_throughput: 26.346\n",
      "    sample_time_ms: 151822.998\n",
      "    update_time_ms: 1.432\n",
      "  timestamp: 1611651798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-05-51\n",
      "  done: false\n",
      "  episode_len_mean: 371.29\n",
      "  episode_reward_max: 118.77500948179338\n",
      "  episode_reward_mean: 47.53269729437553\n",
      "  episode_reward_min: -101.79201457924843\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1379\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9776598811149597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010028684511780739\n",
      "        model: {}\n",
      "        policy_loss: -0.02327224239706993\n",
      "        total_loss: 160.1305694580078\n",
      "        vf_explained_var: 0.8577614426612854\n",
      "        vf_loss: 160.15045166015625\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38669724770642\n",
      "    ram_util_percent: 32.52614678899083\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10247903407095713\n",
      "    mean_env_wait_ms: 33.26131649137252\n",
      "    mean_inference_ms: 0.9252694512639575\n",
      "    mean_raw_obs_processing_ms: 5.132400388970032\n",
      "  time_since_restore: 12778.673343896866\n",
      "  time_this_iter_s: 152.72703957557678\n",
      "  time_total_s: 12778.673343896866\n",
      "  timers:\n",
      "    learn_throughput: 1812.09\n",
      "    learn_time_ms: 2207.396\n",
      "    load_throughput: 232773.77\n",
      "    load_time_ms: 17.184\n",
      "    sample_throughput: 26.293\n",
      "    sample_time_ms: 152131.247\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1611651951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-08-25\n",
      "  done: false\n",
      "  episode_len_mean: 362.85\n",
      "  episode_reward_max: 118.77500948179338\n",
      "  episode_reward_mean: 41.45055099850132\n",
      "  episode_reward_min: -101.06119776560412\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1392\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9735352396965027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012333495542407036\n",
      "        model: {}\n",
      "        policy_loss: -0.022645222023129463\n",
      "        total_loss: 588.8138427734375\n",
      "        vf_explained_var: 0.6224430203437805\n",
      "        vf_loss: 588.8323364257812\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.43181818181819\n",
      "    ram_util_percent: 32.601363636363644\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10250418947741491\n",
      "    mean_env_wait_ms: 33.27052855819633\n",
      "    mean_inference_ms: 0.9254743402167466\n",
      "    mean_raw_obs_processing_ms: 5.1028682568080015\n",
      "  time_since_restore: 12932.785816907883\n",
      "  time_this_iter_s: 154.11247301101685\n",
      "  time_total_s: 12932.785816907883\n",
      "  timers:\n",
      "    learn_throughput: 1810.767\n",
      "    learn_time_ms: 2209.008\n",
      "    load_throughput: 233944.544\n",
      "    load_time_ms: 17.098\n",
      "    sample_throughput: 26.26\n",
      "    sample_time_ms: 152325.33\n",
      "    update_time_ms: 1.427\n",
      "  timestamp: 1611652105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 373.56\n",
      "  episode_reward_max: 118.77500948179338\n",
      "  episode_reward_mean: 41.46558210966666\n",
      "  episode_reward_min: -101.06119776560412\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1401\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8857146501541138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009328230284154415\n",
      "        model: {}\n",
      "        policy_loss: -0.021806301549077034\n",
      "        total_loss: 175.53970336914062\n",
      "        vf_explained_var: 0.7844190001487732\n",
      "        vf_loss: 175.55841064453125\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45255813953489\n",
      "    ram_util_percent: 32.52930232558139\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10252222908823252\n",
      "    mean_env_wait_ms: 33.27711997458347\n",
      "    mean_inference_ms: 0.9256175012703138\n",
      "    mean_raw_obs_processing_ms: 5.082009852737787\n",
      "  time_since_restore: 13083.107358455658\n",
      "  time_this_iter_s: 150.32154154777527\n",
      "  time_total_s: 13083.107358455658\n",
      "  timers:\n",
      "    learn_throughput: 1810.843\n",
      "    learn_time_ms: 2208.916\n",
      "    load_throughput: 239460.368\n",
      "    load_time_ms: 16.704\n",
      "    sample_throughput: 26.293\n",
      "    sample_time_ms: 152130.184\n",
      "    update_time_ms: 1.44\n",
      "  timestamp: 1611652256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 380.21\n",
      "  episode_reward_max: 118.77500948179338\n",
      "  episode_reward_mean: 41.18385778439314\n",
      "  episode_reward_min: -101.16165179321766\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1412\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.26650071144104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010568513534963131\n",
      "        model: {}\n",
      "        policy_loss: -0.027435803785920143\n",
      "        total_loss: 460.7593078613281\n",
      "        vf_explained_var: 0.6330299973487854\n",
      "        vf_loss: 460.78326416015625\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.37660550458715\n",
      "    ram_util_percent: 32.6105504587156\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10254015586346898\n",
      "    mean_env_wait_ms: 33.282427055174736\n",
      "    mean_inference_ms: 0.925762169931742\n",
      "    mean_raw_obs_processing_ms: 5.055826643332438\n",
      "  time_since_restore: 13235.729707956314\n",
      "  time_this_iter_s: 152.62234950065613\n",
      "  time_total_s: 13235.729707956314\n",
      "  timers:\n",
      "    learn_throughput: 1812.042\n",
      "    learn_time_ms: 2207.454\n",
      "    load_throughput: 237688.121\n",
      "    load_time_ms: 16.829\n",
      "    sample_throughput: 26.313\n",
      "    sample_time_ms: 152018.87\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1611652408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-16-08\n",
      "  done: false\n",
      "  episode_len_mean: 370.24\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 38.78212254214691\n",
      "  episode_reward_min: -101.40083338085412\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1425\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9109860062599182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008862068876624107\n",
      "        model: {}\n",
      "        policy_loss: -0.022902218624949455\n",
      "        total_loss: 245.5772247314453\n",
      "        vf_explained_var: 0.823063313961029\n",
      "        vf_loss: 245.59713745117188\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.74692982456142\n",
      "    ram_util_percent: 32.666666666666664\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10255616308056709\n",
      "    mean_env_wait_ms: 33.286408888317446\n",
      "    mean_inference_ms: 0.9258879197428185\n",
      "    mean_raw_obs_processing_ms: 5.027075384036501\n",
      "  time_since_restore: 13395.27143740654\n",
      "  time_this_iter_s: 159.54172945022583\n",
      "  time_total_s: 13395.27143740654\n",
      "  timers:\n",
      "    learn_throughput: 1816.055\n",
      "    learn_time_ms: 2202.576\n",
      "    load_throughput: 224337.52\n",
      "    load_time_ms: 17.83\n",
      "    sample_throughput: 26.449\n",
      "    sample_time_ms: 151233.138\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1611652568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 367.11\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 36.333238999817176\n",
      "  episode_reward_min: -101.40083338085412\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1436\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0519092082977295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01174608338624239\n",
      "        model: {}\n",
      "        policy_loss: -0.029445940628647804\n",
      "        total_loss: 417.9153747558594\n",
      "        vf_explained_var: 0.7150412797927856\n",
      "        vf_loss: 417.94085693359375\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4591743119266\n",
      "    ram_util_percent: 32.71055045871559\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1025636650950879\n",
      "    mean_env_wait_ms: 33.28752243709264\n",
      "    mean_inference_ms: 0.9259382658919322\n",
      "    mean_raw_obs_processing_ms: 5.0045078025953815\n",
      "  time_since_restore: 13548.14879322052\n",
      "  time_this_iter_s: 152.8773558139801\n",
      "  time_total_s: 13548.14879322052\n",
      "  timers:\n",
      "    learn_throughput: 1797.342\n",
      "    learn_time_ms: 2225.509\n",
      "    load_throughput: 221688.015\n",
      "    load_time_ms: 18.043\n",
      "    sample_throughput: 26.49\n",
      "    sample_time_ms: 150999.279\n",
      "    update_time_ms: 1.428\n",
      "  timestamp: 1611652721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 371.76\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 36.449796405349474\n",
      "  episode_reward_min: -101.40083338085412\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1446\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0656569004058838\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012461607344448566\n",
      "        model: {}\n",
      "        policy_loss: -0.02956445701420307\n",
      "        total_loss: 235.47096252441406\n",
      "        vf_explained_var: 0.8075416684150696\n",
      "        vf_loss: 235.49630737304688\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25760368663595\n",
      "    ram_util_percent: 32.70092165898617\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10256941617229647\n",
      "    mean_env_wait_ms: 33.288488484002826\n",
      "    mean_inference_ms: 0.9259812630310199\n",
      "    mean_raw_obs_processing_ms: 4.9841652288323655\n",
      "  time_since_restore: 13699.70383810997\n",
      "  time_this_iter_s: 151.55504488945007\n",
      "  time_total_s: 13699.70383810997\n",
      "  timers:\n",
      "    learn_throughput: 1798.349\n",
      "    learn_time_ms: 2224.262\n",
      "    load_throughput: 236146.153\n",
      "    load_time_ms: 16.939\n",
      "    sample_throughput: 26.6\n",
      "    sample_time_ms: 150375.054\n",
      "    update_time_ms: 1.457\n",
      "  timestamp: 1611652873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-23-45\n",
      "  done: false\n",
      "  episode_len_mean: 371.97\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 34.2129769611133\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1456\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9552560448646545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018957342952489853\n",
      "        model: {}\n",
      "        policy_loss: -0.03362463414669037\n",
      "        total_loss: 251.01605224609375\n",
      "        vf_explained_var: 0.7793201208114624\n",
      "        vf_loss: 251.0432891845703\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.61898148148148\n",
      "    ram_util_percent: 32.72222222222222\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1025750603288806\n",
      "    mean_env_wait_ms: 33.28962161418904\n",
      "    mean_inference_ms: 0.9260216870733234\n",
      "    mean_raw_obs_processing_ms: 4.964015390240784\n",
      "  time_since_restore: 13851.301176309586\n",
      "  time_this_iter_s: 151.59733819961548\n",
      "  time_total_s: 13851.301176309586\n",
      "  timers:\n",
      "    learn_throughput: 1801.189\n",
      "    learn_time_ms: 2220.755\n",
      "    load_throughput: 236367.73\n",
      "    load_time_ms: 16.923\n",
      "    sample_throughput: 26.592\n",
      "    sample_time_ms: 150419.145\n",
      "    update_time_ms: 1.444\n",
      "  timestamp: 1611653025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-26-24\n",
      "  done: false\n",
      "  episode_len_mean: 363.77\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 34.10367416622431\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1467\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9587433338165283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009852919727563858\n",
      "        model: {}\n",
      "        policy_loss: -0.025286978110671043\n",
      "        total_loss: 103.57000732421875\n",
      "        vf_explained_var: 0.9065701365470886\n",
      "        vf_loss: 103.59196472167969\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36666666666666\n",
      "    ram_util_percent: 32.70921052631578\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10258343029198913\n",
      "    mean_env_wait_ms: 33.29293061656071\n",
      "    mean_inference_ms: 0.9260993319545672\n",
      "    mean_raw_obs_processing_ms: 4.942814591959392\n",
      "  time_since_restore: 14010.984052658081\n",
      "  time_this_iter_s: 159.68287634849548\n",
      "  time_total_s: 14010.984052658081\n",
      "  timers:\n",
      "    learn_throughput: 1796.408\n",
      "    learn_time_ms: 2226.665\n",
      "    load_throughput: 244196.688\n",
      "    load_time_ms: 16.38\n",
      "    sample_throughput: 26.471\n",
      "    sample_time_ms: 151106.89\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1611653184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 372.42\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 38.470415034662146\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1476\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0976799726486206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010393108241260052\n",
      "        model: {}\n",
      "        policy_loss: -0.022454846650362015\n",
      "        total_loss: 136.63677978515625\n",
      "        vf_explained_var: 0.846425473690033\n",
      "        vf_loss: 136.65573120117188\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.42131147540984\n",
      "    ram_util_percent: 32.82950819672132\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10259772195403816\n",
      "    mean_env_wait_ms: 33.300402538635815\n",
      "    mean_inference_ms: 0.9262346827120034\n",
      "    mean_raw_obs_processing_ms: 4.925819945896128\n",
      "  time_since_restore: 14181.931260347366\n",
      "  time_this_iter_s: 170.94720768928528\n",
      "  time_total_s: 14181.931260347366\n",
      "  timers:\n",
      "    learn_throughput: 1703.573\n",
      "    learn_time_ms: 2348.006\n",
      "    load_throughput: 268121.432\n",
      "    load_time_ms: 14.919\n",
      "    sample_throughput: 26.131\n",
      "    sample_time_ms: 153076.004\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1611653356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-32-10\n",
      "  done: false\n",
      "  episode_len_mean: 369.88\n",
      "  episode_reward_max: 118.77787108028426\n",
      "  episode_reward_mean: 34.15111881630433\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1490\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9552999138832092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015181593596935272\n",
      "        model: {}\n",
      "        policy_loss: -0.028111865743994713\n",
      "        total_loss: 504.00616455078125\n",
      "        vf_explained_var: 0.6987512111663818\n",
      "        vf_loss: 504.0291748046875\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.80803212851404\n",
      "    ram_util_percent: 34.54417670682731\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10263424391233905\n",
      "    mean_env_wait_ms: 33.318905483058764\n",
      "    mean_inference_ms: 0.926571530916732\n",
      "    mean_raw_obs_processing_ms: 4.900348157501545\n",
      "  time_since_restore: 14356.454943418503\n",
      "  time_this_iter_s: 174.52368307113647\n",
      "  time_total_s: 14356.454943418503\n",
      "  timers:\n",
      "    learn_throughput: 1675.074\n",
      "    learn_time_ms: 2387.954\n",
      "    load_throughput: 268714.92\n",
      "    load_time_ms: 14.886\n",
      "    sample_throughput: 25.771\n",
      "    sample_time_ms: 155212.247\n",
      "    update_time_ms: 1.455\n",
      "  timestamp: 1611653530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 356.97\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 29.832136301533918\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1501\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9858173131942749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012626835145056248\n",
      "        model: {}\n",
      "        policy_loss: -0.02585562691092491\n",
      "        total_loss: 569.6251831054688\n",
      "        vf_explained_var: 0.5233449339866638\n",
      "        vf_loss: 569.646728515625\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.40823045267491\n",
      "    ram_util_percent: 34.61234567901236\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10266972588655765\n",
      "    mean_env_wait_ms: 33.338283106090955\n",
      "    mean_inference_ms: 0.9269171915002574\n",
      "    mean_raw_obs_processing_ms: 4.881369253261572\n",
      "  time_since_restore: 14526.344021081924\n",
      "  time_this_iter_s: 169.88907766342163\n",
      "  time_total_s: 14526.344021081924\n",
      "  timers:\n",
      "    learn_throughput: 1644.814\n",
      "    learn_time_ms: 2431.886\n",
      "    load_throughput: 272347.676\n",
      "    load_time_ms: 14.687\n",
      "    sample_throughput: 25.52\n",
      "    sample_time_ms: 156739.694\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1611653700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 354.34\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 34.469094628837\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1514\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0236132144927979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011569319292902946\n",
      "        model: {}\n",
      "        policy_loss: -0.02692808024585247\n",
      "        total_loss: 545.4763793945312\n",
      "        vf_explained_var: 0.6545229554176331\n",
      "        vf_loss: 545.4993896484375\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.72804878048781\n",
      "    ram_util_percent: 34.5109756097561\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10271853978555132\n",
      "    mean_env_wait_ms: 33.36641765431026\n",
      "    mean_inference_ms: 0.9273943583278695\n",
      "    mean_raw_obs_processing_ms: 4.861227401438992\n",
      "  time_since_restore: 14698.824371099472\n",
      "  time_this_iter_s: 172.4803500175476\n",
      "  time_total_s: 14698.824371099472\n",
      "  timers:\n",
      "    learn_throughput: 1586.132\n",
      "    learn_time_ms: 2521.858\n",
      "    load_throughput: 287592.819\n",
      "    load_time_ms: 13.909\n",
      "    sample_throughput: 25.179\n",
      "    sample_time_ms: 158862.806\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1611653873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 355.42\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 34.65545533864068\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1526\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.953014612197876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00875948928296566\n",
      "        model: {}\n",
      "        policy_loss: -0.021468762308359146\n",
      "        total_loss: 217.9215850830078\n",
      "        vf_explained_var: 0.8273184299468994\n",
      "        vf_loss: 217.94007873535156\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.59297520661157\n",
      "    ram_util_percent: 34.54710743801653\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10276847168512511\n",
      "    mean_env_wait_ms: 33.3955180457761\n",
      "    mean_inference_ms: 0.9278916149955745\n",
      "    mean_raw_obs_processing_ms: 4.842542185597325\n",
      "  time_since_restore: 14868.047247171402\n",
      "  time_this_iter_s: 169.22287607192993\n",
      "  time_total_s: 14868.047247171402\n",
      "  timers:\n",
      "    learn_throughput: 1558.04\n",
      "    learn_time_ms: 2567.328\n",
      "    load_throughput: 310934.478\n",
      "    load_time_ms: 12.864\n",
      "    sample_throughput: 24.926\n",
      "    sample_time_ms: 160472.723\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1611654042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 367.27\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 41.29638276303197\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1535\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9213982820510864\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015725763514637947\n",
      "        model: {}\n",
      "        policy_loss: -0.032861821353435516\n",
      "        total_loss: 226.3183135986328\n",
      "        vf_explained_var: 0.7822593450546265\n",
      "        vf_loss: 226.34584045410156\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8294117647059\n",
      "    ram_util_percent: 34.625210084033625\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10281007399414165\n",
      "    mean_env_wait_ms: 33.42082298798464\n",
      "    mean_inference_ms: 0.9283200736440383\n",
      "    mean_raw_obs_processing_ms: 4.828176604326977\n",
      "  time_since_restore: 15034.969123363495\n",
      "  time_this_iter_s: 166.9218761920929\n",
      "  time_total_s: 15034.969123363495\n",
      "  timers:\n",
      "    learn_throughput: 1510.111\n",
      "    learn_time_ms: 2648.812\n",
      "    load_throughput: 341868.198\n",
      "    load_time_ms: 11.7\n",
      "    sample_throughput: 24.825\n",
      "    sample_time_ms: 161126.887\n",
      "    update_time_ms: 1.56\n",
      "  timestamp: 1611654209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-46-17\n",
      "  done: false\n",
      "  episode_len_mean: 361.88\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 41.22982724430159\n",
      "  episode_reward_min: -101.83503961365223\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1545\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1755112409591675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010365452617406845\n",
      "        model: {}\n",
      "        policy_loss: -0.023618442937731743\n",
      "        total_loss: 501.3316345214844\n",
      "        vf_explained_var: 0.5921698808670044\n",
      "        vf_loss: 501.35174560546875\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.59958158995816\n",
      "    ram_util_percent: 34.60209205020921\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1028612339981346\n",
      "    mean_env_wait_ms: 33.4525849621792\n",
      "    mean_inference_ms: 0.928862695989177\n",
      "    mean_raw_obs_processing_ms: 4.81240340740179\n",
      "  time_since_restore: 15202.346215724945\n",
      "  time_this_iter_s: 167.3770923614502\n",
      "  time_total_s: 15202.346215724945\n",
      "  timers:\n",
      "    learn_throughput: 1506.326\n",
      "    learn_time_ms: 2655.468\n",
      "    load_throughput: 373393.751\n",
      "    load_time_ms: 10.713\n",
      "    sample_throughput: 24.604\n",
      "    sample_time_ms: 162575.893\n",
      "    update_time_ms: 1.62\n",
      "  timestamp: 1611654377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-49-09\n",
      "  done: false\n",
      "  episode_len_mean: 358.96\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 35.13616422295314\n",
      "  episode_reward_min: -101.40157345924378\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1557\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1004743576049805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019195418804883957\n",
      "        model: {}\n",
      "        policy_loss: -0.03310481831431389\n",
      "        total_loss: 744.0559692382812\n",
      "        vf_explained_var: 0.5155086517333984\n",
      "        vf_loss: 744.0825805664062\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.57520325203252\n",
      "    ram_util_percent: 34.64227642276424\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10292933638054254\n",
      "    mean_env_wait_ms: 33.49530127492489\n",
      "    mean_inference_ms: 0.9295966569285831\n",
      "    mean_raw_obs_processing_ms: 4.795003220443279\n",
      "  time_since_restore: 15374.11601281166\n",
      "  time_this_iter_s: 171.7697970867157\n",
      "  time_total_s: 15374.11601281166\n",
      "  timers:\n",
      "    learn_throughput: 1478.225\n",
      "    learn_time_ms: 2705.948\n",
      "    load_throughput: 369571.508\n",
      "    load_time_ms: 10.823\n",
      "    sample_throughput: 24.309\n",
      "    sample_time_ms: 164546.091\n",
      "    update_time_ms: 1.645\n",
      "  timestamp: 1611654549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 347.44\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 26.824797494841373\n",
      "  episode_reward_min: -101.21350921652814\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1569\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.983540415763855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01297632697969675\n",
      "        model: {}\n",
      "        policy_loss: -0.026754233986139297\n",
      "        total_loss: 430.45196533203125\n",
      "        vf_explained_var: 0.700839638710022\n",
      "        vf_loss: 430.47430419921875\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.75140562248995\n",
      "    ram_util_percent: 34.699999999999996\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10299906317489717\n",
      "    mean_env_wait_ms: 33.54045382715014\n",
      "    mean_inference_ms: 0.9303562167953102\n",
      "    mean_raw_obs_processing_ms: 4.778972974017821\n",
      "  time_since_restore: 15548.635950565338\n",
      "  time_this_iter_s: 174.51993775367737\n",
      "  time_total_s: 15548.635950565338\n",
      "  timers:\n",
      "    learn_throughput: 1437.072\n",
      "    learn_time_ms: 2783.438\n",
      "    load_throughput: 393838.758\n",
      "    load_time_ms: 10.156\n",
      "    sample_throughput: 23.988\n",
      "    sample_time_ms: 166751.334\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1611654723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 343.76\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 22.86960508955703\n",
      "  episode_reward_min: -102.04484138066769\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1581\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2330639362335205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013739491812884808\n",
      "        model: {}\n",
      "        policy_loss: -0.02573249489068985\n",
      "        total_loss: 616.5890502929688\n",
      "        vf_explained_var: 0.5161970257759094\n",
      "        vf_loss: 616.610107421875\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.53497942386831\n",
      "    ram_util_percent: 34.69300411522633\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10306280133738385\n",
      "    mean_env_wait_ms: 33.58209321863236\n",
      "    mean_inference_ms: 0.9310684633588316\n",
      "    mean_raw_obs_processing_ms: 4.764321171551593\n",
      "  time_since_restore: 15718.946233272552\n",
      "  time_this_iter_s: 170.31028270721436\n",
      "  time_total_s: 15718.946233272552\n",
      "  timers:\n",
      "    learn_throughput: 1414.932\n",
      "    learn_time_ms: 2826.992\n",
      "    load_throughput: 385087.348\n",
      "    load_time_ms: 10.387\n",
      "    sample_throughput: 23.842\n",
      "    sample_time_ms: 167770.112\n",
      "    update_time_ms: 1.636\n",
      "  timestamp: 1611654894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_10-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 341.43\n",
      "  episode_reward_max: 118.78078758152746\n",
      "  episode_reward_mean: 22.95288523083812\n",
      "  episode_reward_min: -102.04484138066769\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1595\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0857709646224976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015481903217732906\n",
      "        model: {}\n",
      "        policy_loss: -0.03045138157904148\n",
      "        total_loss: 581.9627685546875\n",
      "        vf_explained_var: 0.6408640742301941\n",
      "        vf_loss: 581.9879760742188\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.59717741935485\n",
      "    ram_util_percent: 34.70927419354838\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10313278962701933\n",
      "    mean_env_wait_ms: 33.62983357266079\n",
      "    mean_inference_ms: 0.9318769794871443\n",
      "    mean_raw_obs_processing_ms: 4.747869985357395\n",
      "  time_since_restore: 15892.43068408966\n",
      "  time_this_iter_s: 173.48445081710815\n",
      "  time_total_s: 15892.43068408966\n",
      "  timers:\n",
      "    learn_throughput: 1455.369\n",
      "    learn_time_ms: 2748.443\n",
      "    load_throughput: 351554.83\n",
      "    load_time_ms: 11.378\n",
      "    sample_throughput: 23.795\n",
      "    sample_time_ms: 168101.607\n",
      "    update_time_ms: 1.623\n",
      "  timestamp: 1611655068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 345.38\n",
      "  episode_reward_max: 118.7777165723456\n",
      "  episode_reward_mean: 25.070508669211236\n",
      "  episode_reward_min: -102.04484138066769\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1605\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0106276273727417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01142198033630848\n",
      "        model: {}\n",
      "        policy_loss: -0.02696296200156212\n",
      "        total_loss: 458.00042724609375\n",
      "        vf_explained_var: 0.6243836283683777\n",
      "        vf_loss: 458.02362060546875\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.71464435146441\n",
      "    ram_util_percent: 34.700418410041834\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10318236150251775\n",
      "    mean_env_wait_ms: 33.6627420937959\n",
      "    mean_inference_ms: 0.9324349912383195\n",
      "    mean_raw_obs_processing_ms: 4.735897481185799\n",
      "  time_since_restore: 16059.834283590317\n",
      "  time_this_iter_s: 167.40359950065613\n",
      "  time_total_s: 16059.834283590317\n",
      "  timers:\n",
      "    learn_throughput: 1454.987\n",
      "    learn_time_ms: 2749.166\n",
      "    load_throughput: 333244.268\n",
      "    load_time_ms: 12.003\n",
      "    sample_throughput: 23.897\n",
      "    sample_time_ms: 167385.132\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1611655235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 346.56\n",
      "  episode_reward_max: 118.7777165723456\n",
      "  episode_reward_mean: 26.73336622629046\n",
      "  episode_reward_min: -104.3401238346696\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1619\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1841224431991577\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012969791889190674\n",
      "        model: {}\n",
      "        policy_loss: -0.027394311502575874\n",
      "        total_loss: 211.57949829101562\n",
      "        vf_explained_var: 0.8638415932655334\n",
      "        vf_loss: 211.6025390625\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.68995983935744\n",
      "    ram_util_percent: 34.71004016064257\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10325080442303741\n",
      "    mean_env_wait_ms: 33.708566045966\n",
      "    mean_inference_ms: 0.9332041742556815\n",
      "    mean_raw_obs_processing_ms: 4.720023812709505\n",
      "  time_since_restore: 16234.430533885956\n",
      "  time_this_iter_s: 174.59625029563904\n",
      "  time_total_s: 16234.430533885956\n",
      "  timers:\n",
      "    learn_throughput: 1433.917\n",
      "    learn_time_ms: 2789.561\n",
      "    load_throughput: 329362.838\n",
      "    load_time_ms: 12.145\n",
      "    sample_throughput: 23.836\n",
      "    sample_time_ms: 167814.045\n",
      "    update_time_ms: 1.742\n",
      "  timestamp: 1611655410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 319.66\n",
      "  episode_reward_max: 118.7777165723456\n",
      "  episode_reward_mean: 9.715250257311029\n",
      "  episode_reward_min: -104.3401238346696\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1635\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0272656679153442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008529739454388618\n",
      "        model: {}\n",
      "        policy_loss: -0.016132380813360214\n",
      "        total_loss: 602.8585815429688\n",
      "        vf_explained_var: 0.6777604222297668\n",
      "        vf_loss: 602.8717651367188\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.63426294820718\n",
      "    ram_util_percent: 34.7011952191235\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10332655957803968\n",
      "    mean_env_wait_ms: 33.759126304009186\n",
      "    mean_inference_ms: 0.9340295185979721\n",
      "    mean_raw_obs_processing_ms: 4.705435915101252\n",
      "  time_since_restore: 16410.008751630783\n",
      "  time_this_iter_s: 175.57821774482727\n",
      "  time_total_s: 16410.008751630783\n",
      "  timers:\n",
      "    learn_throughput: 1456.253\n",
      "    learn_time_ms: 2746.775\n",
      "    load_throughput: 330155.522\n",
      "    load_time_ms: 12.116\n",
      "    sample_throughput: 23.786\n",
      "    sample_time_ms: 168167.494\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1611655586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 321.62\n",
      "  episode_reward_max: 118.77118680926898\n",
      "  episode_reward_mean: -1.3026747150264242\n",
      "  episode_reward_min: -109.22612418312595\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1645\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3667999505996704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012600462883710861\n",
      "        model: {}\n",
      "        policy_loss: -0.030101118609309196\n",
      "        total_loss: 203.12374877929688\n",
      "        vf_explained_var: 0.7688450217247009\n",
      "        vf_loss: 203.14959716796875\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.44533898305086\n",
      "    ram_util_percent: 34.709322033898296\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10336933305656398\n",
      "    mean_env_wait_ms: 33.78794492993543\n",
      "    mean_inference_ms: 0.9344828133238718\n",
      "    mean_raw_obs_processing_ms: 4.6975314069370935\n",
      "  time_since_restore: 16575.133826494217\n",
      "  time_this_iter_s: 165.12507486343384\n",
      "  time_total_s: 16575.133826494217\n",
      "  timers:\n",
      "    learn_throughput: 1457.459\n",
      "    learn_time_ms: 2744.503\n",
      "    load_throughput: 330455.959\n",
      "    load_time_ms: 12.104\n",
      "    sample_throughput: 23.843\n",
      "    sample_time_ms: 167762.996\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1611655751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 313.49\n",
      "  episode_reward_max: 118.7670804163073\n",
      "  episode_reward_mean: -3.9645468033935067\n",
      "  episode_reward_min: -109.22612418312595\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1659\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.245244026184082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016935821622610092\n",
      "        model: {}\n",
      "        policy_loss: -0.029662998393177986\n",
      "        total_loss: 274.6111755371094\n",
      "        vf_explained_var: 0.8311789631843567\n",
      "        vf_loss: 274.63507080078125\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.74435483870967\n",
      "    ram_util_percent: 34.70887096774193\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342466870642325\n",
      "    mean_env_wait_ms: 33.826403628292965\n",
      "    mean_inference_ms: 0.9350653201077858\n",
      "    mean_raw_obs_processing_ms: 4.687694151510799\n",
      "  time_since_restore: 16749.08009505272\n",
      "  time_this_iter_s: 173.9462685585022\n",
      "  time_total_s: 16749.08009505272\n",
      "  timers:\n",
      "    learn_throughput: 1456.251\n",
      "    learn_time_ms: 2746.78\n",
      "    load_throughput: 328512.831\n",
      "    load_time_ms: 12.176\n",
      "    sample_throughput: 23.744\n",
      "    sample_time_ms: 168460.642\n",
      "    update_time_ms: 2.074\n",
      "  timestamp: 1611655925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-14-36\n",
      "  done: false\n",
      "  episode_len_mean: 319.31\n",
      "  episode_reward_max: 118.7670804163073\n",
      "  episode_reward_mean: -6.088383366621483\n",
      "  episode_reward_min: -109.22612418312595\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1664\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3186973333358765\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015201865695416927\n",
      "        model: {}\n",
      "        policy_loss: -0.031357571482658386\n",
      "        total_loss: 181.4580535888672\n",
      "        vf_explained_var: 0.6707220673561096\n",
      "        vf_loss: 181.4842987060547\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.92037037037036\n",
      "    ram_util_percent: 34.699999999999996\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034422537903716\n",
      "    mean_env_wait_ms: 33.838475995742826\n",
      "    mean_inference_ms: 0.9352490553791715\n",
      "    mean_raw_obs_processing_ms: 4.683191436606711\n",
      "  time_since_restore: 16900.12440252304\n",
      "  time_this_iter_s: 151.04430747032166\n",
      "  time_total_s: 16900.12440252304\n",
      "  timers:\n",
      "    learn_throughput: 1479.895\n",
      "    learn_time_ms: 2702.894\n",
      "    load_throughput: 314647.332\n",
      "    load_time_ms: 12.713\n",
      "    sample_throughput: 23.971\n",
      "    sample_time_ms: 166871.308\n",
      "    update_time_ms: 2.025\n",
      "  timestamp: 1611656076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-17-04\n",
      "  done: false\n",
      "  episode_len_mean: 336.8\n",
      "  episode_reward_max: 118.7670804163073\n",
      "  episode_reward_mean: -12.680000381027599\n",
      "  episode_reward_min: -115.3169590128636\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1667\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3871409893035889\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008077122271060944\n",
      "        model: {}\n",
      "        policy_loss: -0.017985103651881218\n",
      "        total_loss: 108.40515899658203\n",
      "        vf_explained_var: 0.6424000859260559\n",
      "        vf_loss: 108.42041015625\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.0308056872038\n",
      "    ram_util_percent: 34.66540284360189\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345288201997084\n",
      "    mean_env_wait_ms: 33.845791478000116\n",
      "    mean_inference_ms: 0.9353602016844113\n",
      "    mean_raw_obs_processing_ms: 4.679446555075625\n",
      "  time_since_restore: 17047.50602030754\n",
      "  time_this_iter_s: 147.38161778450012\n",
      "  time_total_s: 17047.50602030754\n",
      "  timers:\n",
      "    learn_throughput: 1497.618\n",
      "    learn_time_ms: 2670.908\n",
      "    load_throughput: 330293.317\n",
      "    load_time_ms: 12.11\n",
      "    sample_throughput: 24.321\n",
      "    sample_time_ms: 164468.508\n",
      "    update_time_ms: 1.989\n",
      "  timestamp: 1611656224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 355.92\n",
      "  episode_reward_max: 118.7670804163073\n",
      "  episode_reward_mean: -19.700030565174686\n",
      "  episode_reward_min: -115.55799223966619\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1682\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0349013805389404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016021834686398506\n",
      "        model: {}\n",
      "        policy_loss: -0.02638944797217846\n",
      "        total_loss: 417.05718994140625\n",
      "        vf_explained_var: 0.7768847346305847\n",
      "        vf_loss: 417.0780944824219\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.87379912663755\n",
      "    ram_util_percent: 34.70698689956331\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349574319380396\n",
      "    mean_env_wait_ms: 33.877746316559076\n",
      "    mean_inference_ms: 0.9358220553817523\n",
      "    mean_raw_obs_processing_ms: 4.662052557718262\n",
      "  time_since_restore: 17208.27213907242\n",
      "  time_this_iter_s: 160.76611876487732\n",
      "  time_total_s: 17208.27213907242\n",
      "  timers:\n",
      "    learn_throughput: 1531.438\n",
      "    learn_time_ms: 2611.925\n",
      "    load_throughput: 350062.826\n",
      "    load_time_ms: 11.427\n",
      "    sample_throughput: 24.516\n",
      "    sample_time_ms: 163162.004\n",
      "    update_time_ms: 1.968\n",
      "  timestamp: 1611656385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 369.98\n",
      "  episode_reward_max: 118.7670804163073\n",
      "  episode_reward_mean: -20.04984383746401\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1687\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.312638759613037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014850707724690437\n",
      "        model: {}\n",
      "        policy_loss: -0.02904949150979519\n",
      "        total_loss: 54.86962127685547\n",
      "        vf_explained_var: 0.8918694257736206\n",
      "        vf_loss: 54.893653869628906\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.31818181818181\n",
      "    ram_util_percent: 34.69999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10350609193213252\n",
      "    mean_env_wait_ms: 33.88616801207948\n",
      "    mean_inference_ms: 0.9359320000755968\n",
      "    mean_raw_obs_processing_ms: 4.655212501644287\n",
      "  time_since_restore: 17354.537994146347\n",
      "  time_this_iter_s: 146.26585507392883\n",
      "  time_total_s: 17354.537994146347\n",
      "  timers:\n",
      "    learn_throughput: 1586.771\n",
      "    learn_time_ms: 2520.843\n",
      "    load_throughput: 357988.021\n",
      "    load_time_ms: 11.174\n",
      "    sample_throughput: 24.868\n",
      "    sample_time_ms: 160851.878\n",
      "    update_time_ms: 1.955\n",
      "  timestamp: 1611656531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-24-36\n",
      "  done: false\n",
      "  episode_len_mean: 407.22\n",
      "  episode_reward_max: 118.76575464973723\n",
      "  episode_reward_mean: -26.74135882363856\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1692\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.352565050125122\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01061013899743557\n",
      "        model: {}\n",
      "        policy_loss: -0.025280658155679703\n",
      "        total_loss: 163.14456176757812\n",
      "        vf_explained_var: 0.6150372624397278\n",
      "        vf_loss: 163.16627502441406\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.67788461538461\n",
      "    ram_util_percent: 34.71009615384615\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10351548353946076\n",
      "    mean_env_wait_ms: 33.89422569475181\n",
      "    mean_inference_ms: 0.9360342088384406\n",
      "    mean_raw_obs_processing_ms: 4.646980692105607\n",
      "  time_since_restore: 17499.69329738617\n",
      "  time_this_iter_s: 145.1553032398224\n",
      "  time_total_s: 17499.69329738617\n",
      "  timers:\n",
      "    learn_throughput: 1615.191\n",
      "    learn_time_ms: 2476.488\n",
      "    load_throughput: 347247.247\n",
      "    load_time_ms: 11.519\n",
      "    sample_throughput: 25.306\n",
      "    sample_time_ms: 158067.829\n",
      "    update_time_ms: 1.952\n",
      "  timestamp: 1611656676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 401.03\n",
      "  episode_reward_max: 118.76575464973723\n",
      "  episode_reward_mean: -35.45200554309735\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1703\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.17959463596344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009504483081400394\n",
      "        model: {}\n",
      "        policy_loss: -0.01873711310327053\n",
      "        total_loss: 398.9951477050781\n",
      "        vf_explained_var: 0.6734710931777954\n",
      "        vf_loss: 399.0107421875\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81889400921659\n",
      "    ram_util_percent: 34.69999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353055229559092\n",
      "    mean_env_wait_ms: 33.90864799489493\n",
      "    mean_inference_ms: 0.9362078155719118\n",
      "    mean_raw_obs_processing_ms: 4.628931547281178\n",
      "  time_since_restore: 17652.060598134995\n",
      "  time_this_iter_s: 152.36730074882507\n",
      "  time_total_s: 17652.060598134995\n",
      "  timers:\n",
      "    learn_throughput: 1649.047\n",
      "    learn_time_ms: 2425.643\n",
      "    load_throughput: 334926.715\n",
      "    load_time_ms: 11.943\n",
      "    sample_throughput: 25.54\n",
      "    sample_time_ms: 156617.126\n",
      "    update_time_ms: 1.871\n",
      "  timestamp: 1611656829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-29-30\n",
      "  done: false\n",
      "  episode_len_mean: 419.5\n",
      "  episode_reward_max: 118.76575464973723\n",
      "  episode_reward_mean: -37.72380887569477\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1704\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3726789951324463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010975603014230728\n",
      "        model: {}\n",
      "        policy_loss: -0.02003464847803116\n",
      "        total_loss: 69.18069458007812\n",
      "        vf_explained_var: 0.36880117654800415\n",
      "        vf_loss: 69.197021484375\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.15841584158416\n",
      "    ram_util_percent: 34.70544554455445\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353161525820789\n",
      "    mean_env_wait_ms: 33.90980983341716\n",
      "    mean_inference_ms: 0.9362205710943293\n",
      "    mean_raw_obs_processing_ms: 4.626966621071453\n",
      "  time_since_restore: 17793.24266552925\n",
      "  time_this_iter_s: 141.1820673942566\n",
      "  time_total_s: 17793.24266552925\n",
      "  timers:\n",
      "    learn_throughput: 1708.344\n",
      "    learn_time_ms: 2341.449\n",
      "    load_throughput: 340716.395\n",
      "    load_time_ms: 11.74\n",
      "    sample_throughput: 26.082\n",
      "    sample_time_ms: 153362.338\n",
      "    update_time_ms: 1.841\n",
      "  timestamp: 1611656970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-32-00\n",
      "  done: false\n",
      "  episode_len_mean: 452.25\n",
      "  episode_reward_max: 118.76575464973723\n",
      "  episode_reward_mean: -50.22282981473729\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1713\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1591566801071167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014207715168595314\n",
      "        model: {}\n",
      "        policy_loss: -0.029958680272102356\n",
      "        total_loss: 529.0338134765625\n",
      "        vf_explained_var: 0.4414213001728058\n",
      "        vf_loss: 529.0590209960938\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.83457943925232\n",
      "    ram_util_percent: 34.64065420560748\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353655615082952\n",
      "    mean_env_wait_ms: 33.916732835480495\n",
      "    mean_inference_ms: 0.9362852884076864\n",
      "    mean_raw_obs_processing_ms: 4.608200246791972\n",
      "  time_since_restore: 17943.16499400139\n",
      "  time_this_iter_s: 149.92232847213745\n",
      "  time_total_s: 17943.16499400139\n",
      "  timers:\n",
      "    learn_throughput: 1741.575\n",
      "    learn_time_ms: 2296.772\n",
      "    load_throughput: 321454.95\n",
      "    load_time_ms: 12.443\n",
      "    sample_throughput: 26.517\n",
      "    sample_time_ms: 150846.615\n",
      "    update_time_ms: 1.821\n",
      "  timestamp: 1611657120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 454.26\n",
      "  episode_reward_max: 118.76575464973723\n",
      "  episode_reward_mean: -56.390465879363376\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1727\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9578030109405518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012486152350902557\n",
      "        model: {}\n",
      "        policy_loss: -0.022411206737160683\n",
      "        total_loss: 656.740234375\n",
      "        vf_explained_var: 0.5629804730415344\n",
      "        vf_loss: 656.7584228515625\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.50675675675676\n",
      "    ram_util_percent: 34.70855855855855\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353754249883047\n",
      "    mean_env_wait_ms: 33.92318357158017\n",
      "    mean_inference_ms: 0.9363240014952366\n",
      "    mean_raw_obs_processing_ms: 4.578537641950409\n",
      "  time_since_restore: 18098.379613876343\n",
      "  time_this_iter_s: 155.21461987495422\n",
      "  time_total_s: 18098.379613876343\n",
      "  timers:\n",
      "    learn_throughput: 1772.981\n",
      "    learn_time_ms: 2256.088\n",
      "    load_throughput: 282428.072\n",
      "    load_time_ms: 14.163\n",
      "    sample_throughput: 26.685\n",
      "    sample_time_ms: 149897.68\n",
      "    update_time_ms: 1.793\n",
      "  timestamp: 1611657276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 440.49\n",
      "  episode_reward_max: 118.7610686093656\n",
      "  episode_reward_mean: -51.74121674986181\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1743\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0235270261764526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014309939928352833\n",
      "        model: {}\n",
      "        policy_loss: -0.028488006442785263\n",
      "        total_loss: 600.6679077148438\n",
      "        vf_explained_var: 0.6951113343238831\n",
      "        vf_loss: 600.6914672851562\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54222222222222\n",
      "    ram_util_percent: 34.71066666666666\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10353018950434625\n",
      "    mean_env_wait_ms: 33.92509645883183\n",
      "    mean_inference_ms: 0.9362843208248012\n",
      "    mean_raw_obs_processing_ms: 4.5461699376641285\n",
      "  time_since_restore: 18256.118438005447\n",
      "  time_this_iter_s: 157.73882412910461\n",
      "  time_total_s: 18256.118438005447\n",
      "  timers:\n",
      "    learn_throughput: 1840.682\n",
      "    learn_time_ms: 2173.108\n",
      "    load_throughput: 275792.27\n",
      "    load_time_ms: 14.504\n",
      "    sample_throughput: 26.96\n",
      "    sample_time_ms: 148365.531\n",
      "    update_time_ms: 1.422\n",
      "  timestamp: 1611657434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 421.3\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -60.04452383085544\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1763\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1407603025436401\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015972847118973732\n",
      "        model: {}\n",
      "        policy_loss: -0.03210088238120079\n",
      "        total_loss: 499.6271057128906\n",
      "        vf_explained_var: 0.691259503364563\n",
      "        vf_loss: 499.65374755859375\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30344827586205\n",
      "    ram_util_percent: 34.69999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1035090702930432\n",
      "    mean_env_wait_ms: 33.91742731500803\n",
      "    mean_inference_ms: 0.9360931687715826\n",
      "    mean_raw_obs_processing_ms: 4.5116663141223725\n",
      "  time_since_restore: 18418.52237844467\n",
      "  time_this_iter_s: 162.40394043922424\n",
      "  time_total_s: 18418.52237844467\n",
      "  timers:\n",
      "    learn_throughput: 1840.749\n",
      "    learn_time_ms: 2173.029\n",
      "    load_throughput: 266533.473\n",
      "    load_time_ms: 15.007\n",
      "    sample_throughput: 26.756\n",
      "    sample_time_ms: 149501.701\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1611657596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 385.29\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -59.62616489493768\n",
      "  episode_reward_min: -116.20154686865901\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1773\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3272885084152222\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012837743386626244\n",
      "        model: {}\n",
      "        policy_loss: -0.027526648715138435\n",
      "        total_loss: 279.56732177734375\n",
      "        vf_explained_var: 0.7403489351272583\n",
      "        vf_loss: 279.59051513671875\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.73767441860464\n",
      "    ram_util_percent: 34.71023255813952\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349642377339624\n",
      "    mean_env_wait_ms: 33.911779323852905\n",
      "    mean_inference_ms: 0.9359784093392094\n",
      "    mean_raw_obs_processing_ms: 4.4987333383321015\n",
      "  time_since_restore: 18569.160359621048\n",
      "  time_this_iter_s: 150.63798117637634\n",
      "  time_total_s: 18569.160359621048\n",
      "  timers:\n",
      "    learn_throughput: 1855.589\n",
      "    learn_time_ms: 2155.65\n",
      "    load_throughput: 256155.172\n",
      "    load_time_ms: 15.616\n",
      "    sample_throughput: 26.694\n",
      "    sample_time_ms: 149845.056\n",
      "    update_time_ms: 1.402\n",
      "  timestamp: 1611657747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 352.16\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.11543271227794\n",
      "  episode_reward_min: -108.47083123553381\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1788\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0523898601531982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01978486031293869\n",
      "        model: {}\n",
      "        policy_loss: -0.02782646380364895\n",
      "        total_loss: 851.6702880859375\n",
      "        vf_explained_var: 0.42210474610328674\n",
      "        vf_loss: 851.6915283203125\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35535714285716\n",
      "    ram_util_percent: 34.699999999999996\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034775691515416\n",
      "    mean_env_wait_ms: 33.90263648778422\n",
      "    mean_inference_ms: 0.9358068255513341\n",
      "    mean_raw_obs_processing_ms: 4.481798037168694\n",
      "  time_since_restore: 18725.841832876205\n",
      "  time_this_iter_s: 156.68147325515747\n",
      "  time_total_s: 18725.841832876205\n",
      "  timers:\n",
      "    learn_throughput: 1870.748\n",
      "    learn_time_ms: 2138.183\n",
      "    load_throughput: 239982.377\n",
      "    load_time_ms: 16.668\n",
      "    sample_throughput: 26.765\n",
      "    sample_time_ms: 149449.691\n",
      "    update_time_ms: 1.418\n",
      "  timestamp: 1611657904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-47-30\n",
      "  done: false\n",
      "  episode_len_mean: 326.62\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -62.936832751845934\n",
      "  episode_reward_min: -108.47083123553381\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1794\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3061139583587646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010519583709537983\n",
      "        model: {}\n",
      "        policy_loss: -0.024338213726878166\n",
      "        total_loss: 162.25672912597656\n",
      "        vf_explained_var: 0.6907535195350647\n",
      "        vf_loss: 162.2775115966797\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.96746411483254\n",
      "    ram_util_percent: 34.71052631578947\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10347149031979753\n",
      "    mean_env_wait_ms: 33.89934269449773\n",
      "    mean_inference_ms: 0.9357488317387995\n",
      "    mean_raw_obs_processing_ms: 4.476474411727845\n",
      "  time_since_restore: 18872.269709825516\n",
      "  time_this_iter_s: 146.4278769493103\n",
      "  time_total_s: 18872.269709825516\n",
      "  timers:\n",
      "    learn_throughput: 1866.827\n",
      "    learn_time_ms: 2142.673\n",
      "    load_throughput: 230182.764\n",
      "    load_time_ms: 17.377\n",
      "    sample_throughput: 26.762\n",
      "    sample_time_ms: 149464.345\n",
      "    update_time_ms: 1.394\n",
      "  timestamp: 1611658050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-49-51\n",
      "  done: false\n",
      "  episode_len_mean: 326.62\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -62.936832751845934\n",
      "  episode_reward_min: -108.47083123553381\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1794\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3730233907699585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018084371462464333\n",
      "        model: {}\n",
      "        policy_loss: -0.028601430356502533\n",
      "        total_loss: 21.155241012573242\n",
      "        vf_explained_var: -0.7955664396286011\n",
      "        vf_loss: 21.1777400970459\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.214\n",
      "    ram_util_percent: 34.69999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10347149031979753\n",
      "    mean_env_wait_ms: 33.89934269449773\n",
      "    mean_inference_ms: 0.9357488317387996\n",
      "    mean_raw_obs_processing_ms: 4.476474411727846\n",
      "  time_since_restore: 19012.542157411575\n",
      "  time_this_iter_s: 140.27244758605957\n",
      "  time_total_s: 19012.542157411575\n",
      "  timers:\n",
      "    learn_throughput: 1841.292\n",
      "    learn_time_ms: 2172.387\n",
      "    load_throughput: 231788.731\n",
      "    load_time_ms: 17.257\n",
      "    sample_throughput: 26.856\n",
      "    sample_time_ms: 148942.98\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1611658191\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 404.45\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.30548131500022\n",
      "  episode_reward_min: -137.27903933340946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1796\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.423466444015503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009243288077414036\n",
      "        model: {}\n",
      "        policy_loss: -0.01843092031776905\n",
      "        total_loss: 73.66714477539062\n",
      "        vf_explained_var: 0.7618961334228516\n",
      "        vf_loss: 73.68245697021484\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03300492610838\n",
      "    ram_util_percent: 34.71527093596058\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346917366477441\n",
      "    mean_env_wait_ms: 33.89811485713851\n",
      "    mean_inference_ms: 0.9357270859460821\n",
      "    mean_raw_obs_processing_ms: 4.473507331857448\n",
      "  time_since_restore: 19154.582597494125\n",
      "  time_this_iter_s: 142.04044008255005\n",
      "  time_total_s: 19154.582597494125\n",
      "  timers:\n",
      "    learn_throughput: 1835.621\n",
      "    learn_time_ms: 2179.099\n",
      "    load_throughput: 235921.675\n",
      "    load_time_ms: 16.955\n",
      "    sample_throughput: 27.044\n",
      "    sample_time_ms: 147906.277\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1611658333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-54-34\n",
      "  done: false\n",
      "  episode_len_mean: 404.45\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.305481315000236\n",
      "  episode_reward_min: -137.27903933340946\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1796\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3879457712173462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011617770418524742\n",
      "        model: {}\n",
      "        policy_loss: -0.021700162440538406\n",
      "        total_loss: 3.8450210094451904\n",
      "        vf_explained_var: -0.06396979838609695\n",
      "        vf_loss: 3.862799644470215\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.99303482587064\n",
      "    ram_util_percent: 34.719402985074616\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346917366477441\n",
      "    mean_env_wait_ms: 33.89811485713851\n",
      "    mean_inference_ms: 0.935727085946082\n",
      "    mean_raw_obs_processing_ms: 4.473507331857447\n",
      "  time_since_restore: 19295.182229042053\n",
      "  time_this_iter_s: 140.59963154792786\n",
      "  time_total_s: 19295.182229042053\n",
      "  timers:\n",
      "    learn_throughput: 1833.77\n",
      "    learn_time_ms: 2181.299\n",
      "    load_throughput: 231280.997\n",
      "    load_time_ms: 17.295\n",
      "    sample_throughput: 27.054\n",
      "    sample_time_ms: 147850.617\n",
      "    update_time_ms: 1.453\n",
      "  timestamp: 1611658474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 404.45\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.305481315000236\n",
      "  episode_reward_min: -137.27903933340946\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1796\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.408698558807373\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014004315249621868\n",
      "        model: {}\n",
      "        policy_loss: -0.02614014223217964\n",
      "        total_loss: 1.2022194862365723\n",
      "        vf_explained_var: -0.26757556200027466\n",
      "        vf_loss: 1.2236332893371582\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.0425\n",
      "    ram_util_percent: 34.69999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346917366477441\n",
      "    mean_env_wait_ms: 33.89811485713851\n",
      "    mean_inference_ms: 0.935727085946082\n",
      "    mean_raw_obs_processing_ms: 4.473507331857447\n",
      "  time_since_restore: 19435.033222675323\n",
      "  time_this_iter_s: 139.85099363327026\n",
      "  time_total_s: 19435.033222675323\n",
      "  timers:\n",
      "    learn_throughput: 1833.56\n",
      "    learn_time_ms: 2181.548\n",
      "    load_throughput: 239934.329\n",
      "    load_time_ms: 16.671\n",
      "    sample_throughput: 27.24\n",
      "    sample_time_ms: 146843.587\n",
      "    update_time_ms: 1.482\n",
      "  timestamp: 1611658614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_11-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 532.18\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.885257392060794\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.338009238243103\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018745016306638718\n",
      "        model: {}\n",
      "        policy_loss: -0.039426833391189575\n",
      "        total_loss: 143.60079956054688\n",
      "        vf_explained_var: 0.7120417952537537\n",
      "        vf_loss: 143.6339111328125\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.09758454106282\n",
      "    ram_util_percent: 34.73285024154589\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346368957368329\n",
      "    mean_env_wait_ms: 33.89511801708806\n",
      "    mean_inference_ms: 0.9356769716518305\n",
      "    mean_raw_obs_processing_ms: 4.463763927252532\n",
      "  time_since_restore: 19579.471310853958\n",
      "  time_this_iter_s: 144.43808817863464\n",
      "  time_total_s: 19579.471310853958\n",
      "  timers:\n",
      "    learn_throughput: 1835.516\n",
      "    learn_time_ms: 2179.223\n",
      "    load_throughput: 246034.506\n",
      "    load_time_ms: 16.258\n",
      "    sample_throughput: 27.441\n",
      "    sample_time_ms: 145767.422\n",
      "    update_time_ms: 1.453\n",
      "  timestamp: 1611658759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-01-53\n",
      "  done: false\n",
      "  episode_len_mean: 480.44\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -59.49938501982675\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1813\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2875254154205322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017014216631650925\n",
      "        model: {}\n",
      "        policy_loss: -0.03405487537384033\n",
      "        total_loss: 743.3167724609375\n",
      "        vf_explained_var: 0.4700236916542053\n",
      "        vf_loss: 743.34521484375\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81454545454545\n",
      "    ram_util_percent: 34.70954545454545\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034466968707226\n",
      "    mean_env_wait_ms: 33.88579425717793\n",
      "    mean_inference_ms: 0.9355240842690494\n",
      "    mean_raw_obs_processing_ms: 4.436824349584054\n",
      "  time_since_restore: 19733.857666254044\n",
      "  time_this_iter_s: 154.38635540008545\n",
      "  time_total_s: 19733.857666254044\n",
      "  timers:\n",
      "    learn_throughput: 1837.174\n",
      "    learn_time_ms: 2177.257\n",
      "    load_throughput: 249762.047\n",
      "    load_time_ms: 16.015\n",
      "    sample_throughput: 27.504\n",
      "    sample_time_ms: 145435.916\n",
      "    update_time_ms: 1.457\n",
      "  timestamp: 1611658913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 493.21\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -61.441668159625486\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1822\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2806223630905151\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01654745079576969\n",
      "        model: {}\n",
      "        policy_loss: -0.02954564243555069\n",
      "        total_loss: 500.6758117675781\n",
      "        vf_explained_var: 0.4650649428367615\n",
      "        vf_loss: 500.6997375488281\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6364055299539\n",
      "    ram_util_percent: 34.783410138248854\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343756328225343\n",
      "    mean_env_wait_ms: 33.88005438247286\n",
      "    mean_inference_ms: 0.9354373912904115\n",
      "    mean_raw_obs_processing_ms: 4.41858411959131\n",
      "  time_since_restore: 19885.418077468872\n",
      "  time_this_iter_s: 151.5604112148285\n",
      "  time_total_s: 19885.418077468872\n",
      "  timers:\n",
      "    learn_throughput: 1833.955\n",
      "    learn_time_ms: 2181.079\n",
      "    load_throughput: 248362.225\n",
      "    load_time_ms: 16.106\n",
      "    sample_throughput: 27.712\n",
      "    sample_time_ms: 144344.097\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1611659065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-06-50\n",
      "  done: false\n",
      "  episode_len_mean: 501.28\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -58.995285615378634\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1827\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.209331750869751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013274254277348518\n",
      "        model: {}\n",
      "        policy_loss: -0.023738078773021698\n",
      "        total_loss: 638.5521850585938\n",
      "        vf_explained_var: 0.1445242315530777\n",
      "        vf_loss: 638.5714111328125\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.05072463768116\n",
      "    ram_util_percent: 34.710628019323664\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343210538994964\n",
      "    mean_env_wait_ms: 33.876596540283145\n",
      "    mean_inference_ms: 0.9353862798882854\n",
      "    mean_raw_obs_processing_ms: 4.407357811152588\n",
      "  time_since_restore: 20030.6085999012\n",
      "  time_this_iter_s: 145.19052243232727\n",
      "  time_total_s: 20030.6085999012\n",
      "  timers:\n",
      "    learn_throughput: 1835.774\n",
      "    learn_time_ms: 2178.917\n",
      "    load_throughput: 235566.57\n",
      "    load_time_ms: 16.98\n",
      "    sample_throughput: 27.817\n",
      "    sample_time_ms: 143798.319\n",
      "    update_time_ms: 1.446\n",
      "  timestamp: 1611659210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-09-11\n",
      "  done: false\n",
      "  episode_len_mean: 501.28\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -58.99528561537864\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 1827\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.274475336074829\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012145929969847202\n",
      "        model: {}\n",
      "        policy_loss: -0.02077038586139679\n",
      "        total_loss: 4.82819128036499\n",
      "        vf_explained_var: -0.06497955322265625\n",
      "        vf_loss: 4.844862937927246\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36\n",
      "    ram_util_percent: 34.86699999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343210538994958\n",
      "    mean_env_wait_ms: 33.876596540283145\n",
      "    mean_inference_ms: 0.9353862798882854\n",
      "    mean_raw_obs_processing_ms: 4.407357811152589\n",
      "  time_since_restore: 20170.807116746902\n",
      "  time_this_iter_s: 140.19851684570312\n",
      "  time_total_s: 20170.807116746902\n",
      "  timers:\n",
      "    learn_throughput: 1833.346\n",
      "    learn_time_ms: 2181.804\n",
      "    load_throughput: 234691.346\n",
      "    load_time_ms: 17.044\n",
      "    sample_throughput: 28.139\n",
      "    sample_time_ms: 142149.383\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1611659351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 586.96\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -61.379750532630084\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1830\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.273589849472046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012769754976034164\n",
      "        model: {}\n",
      "        policy_loss: -0.02626759000122547\n",
      "        total_loss: 144.03077697753906\n",
      "        vf_explained_var: 0.5737818479537964\n",
      "        vf_loss: 144.052734375\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.02634146341464\n",
      "    ram_util_percent: 34.89463414634146\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342903445113935\n",
      "    mean_env_wait_ms: 33.87449132040499\n",
      "    mean_inference_ms: 0.9353552303655703\n",
      "    mean_raw_obs_processing_ms: 4.398822449679566\n",
      "  time_since_restore: 20314.456833600998\n",
      "  time_this_iter_s: 143.64971685409546\n",
      "  time_total_s: 20314.456833600998\n",
      "  timers:\n",
      "    learn_throughput: 1833.647\n",
      "    learn_time_ms: 2181.445\n",
      "    load_throughput: 231769.518\n",
      "    load_time_ms: 17.259\n",
      "    sample_throughput: 28.195\n",
      "    sample_time_ms: 141870.757\n",
      "    update_time_ms: 1.457\n",
      "  timestamp: 1611659494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-13-56\n",
      "  done: false\n",
      "  episode_len_mean: 588.81\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -61.376226137678394\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1831\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2893083095550537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0059476494789123535\n",
      "        model: {}\n",
      "        policy_loss: -0.015756836161017418\n",
      "        total_loss: 31.325517654418945\n",
      "        vf_explained_var: 0.2003437876701355\n",
      "        vf_loss: 31.33926773071289\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03366336633663\n",
      "    ram_util_percent: 34.82821782178218\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034280393598337\n",
      "    mean_env_wait_ms: 33.87374815421588\n",
      "    mean_inference_ms: 0.9353450985621821\n",
      "    mean_raw_obs_processing_ms: 4.395683326415263\n",
      "  time_since_restore: 20455.78070950508\n",
      "  time_this_iter_s: 141.32387590408325\n",
      "  time_total_s: 20455.78070950508\n",
      "  timers:\n",
      "    learn_throughput: 1858.857\n",
      "    learn_time_ms: 2151.86\n",
      "    load_throughput: 240301.702\n",
      "    load_time_ms: 16.646\n",
      "    sample_throughput: 28.168\n",
      "    sample_time_ms: 142004.738\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1611659636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 649.58\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -63.38408146152749\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1837\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.278275966644287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014056333340704441\n",
      "        model: {}\n",
      "        policy_loss: -0.023901572450995445\n",
      "        total_loss: 371.6934814453125\n",
      "        vf_explained_var: 0.39670923352241516\n",
      "        vf_loss: 371.7126159667969\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98591549295774\n",
      "    ram_util_percent: 34.84272300469483\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342210490879376\n",
      "    mean_env_wait_ms: 33.869242461164596\n",
      "    mean_inference_ms: 0.9352837712540891\n",
      "    mean_raw_obs_processing_ms: 4.375771084327453\n",
      "  time_since_restore: 20604.950969219208\n",
      "  time_this_iter_s: 149.1702597141266\n",
      "  time_total_s: 20604.950969219208\n",
      "  timers:\n",
      "    learn_throughput: 1823.07\n",
      "    learn_time_ms: 2194.101\n",
      "    load_throughput: 250107.2\n",
      "    load_time_ms: 15.993\n",
      "    sample_throughput: 28.036\n",
      "    sample_time_ms: 142671.537\n",
      "    update_time_ms: 1.459\n",
      "  timestamp: 1611659785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 669.64\n",
      "  episode_reward_max: 118.77642820134858\n",
      "  episode_reward_mean: -60.88843713626849\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1848\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2267107963562012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01572686992585659\n",
      "        model: {}\n",
      "        policy_loss: -0.025833426043391228\n",
      "        total_loss: 450.7000427246094\n",
      "        vf_explained_var: 0.5915902853012085\n",
      "        vf_loss: 450.72052001953125\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.62845528455284\n",
      "    ram_util_percent: 35.019105691056915\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10341706784984733\n",
      "    mean_env_wait_ms: 33.8646111920376\n",
      "    mean_inference_ms: 0.9352328002940488\n",
      "    mean_raw_obs_processing_ms: 4.337980513702341\n",
      "  time_since_restore: 20776.582330942154\n",
      "  time_this_iter_s: 171.63136172294617\n",
      "  time_total_s: 20776.582330942154\n",
      "  timers:\n",
      "    learn_throughput: 1786.914\n",
      "    learn_time_ms: 2238.496\n",
      "    load_throughput: 251244.691\n",
      "    load_time_ms: 15.921\n",
      "    sample_throughput: 27.449\n",
      "    sample_time_ms: 145724.746\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1611659957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 684.37\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -58.1423820724264\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1858\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1739002466201782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015327204018831253\n",
      "        model: {}\n",
      "        policy_loss: -0.028049534186720848\n",
      "        total_loss: 804.0511474609375\n",
      "        vf_explained_var: 0.317577064037323\n",
      "        vf_loss: 804.0740356445312\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.15833333333333\n",
      "    ram_util_percent: 35.10958333333335\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034159955303207\n",
      "    mean_env_wait_ms: 33.86332358664268\n",
      "    mean_inference_ms: 0.9352312183119085\n",
      "    mean_raw_obs_processing_ms: 4.302136325061864\n",
      "  time_since_restore: 20944.857407808304\n",
      "  time_this_iter_s: 168.2750768661499\n",
      "  time_total_s: 20944.857407808304\n",
      "  timers:\n",
      "    learn_throughput: 1752.026\n",
      "    learn_time_ms: 2283.071\n",
      "    load_throughput: 252161.933\n",
      "    load_time_ms: 15.863\n",
      "    sample_throughput: 26.933\n",
      "    sample_time_ms: 148519.063\n",
      "    update_time_ms: 1.416\n",
      "  timestamp: 1611660126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-24-50\n",
      "  done: false\n",
      "  episode_len_mean: 685.7\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -55.634335541224516\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1869\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3149164915084839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014395557343959808\n",
      "        model: {}\n",
      "        policy_loss: -0.030932562425732613\n",
      "        total_loss: 477.331298828125\n",
      "        vf_explained_var: 0.5310102105140686\n",
      "        vf_loss: 477.35736083984375\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.5936170212766\n",
      "    ram_util_percent: 34.994468085106384\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10341936933051254\n",
      "    mean_env_wait_ms: 33.86413817376819\n",
      "    mean_inference_ms: 0.9352710019888295\n",
      "    mean_raw_obs_processing_ms: 4.262807451459409\n",
      "  time_since_restore: 21109.186017513275\n",
      "  time_this_iter_s: 164.3286097049713\n",
      "  time_total_s: 21109.186017513275\n",
      "  timers:\n",
      "    learn_throughput: 1740.064\n",
      "    learn_time_ms: 2298.765\n",
      "    load_throughput: 271033.912\n",
      "    load_time_ms: 14.758\n",
      "    sample_throughput: 26.579\n",
      "    sample_time_ms: 150494.097\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1611660290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-27-28\n",
      "  done: false\n",
      "  episode_len_mean: 705.6\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -57.01773888420128\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3268755674362183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01203976757824421\n",
      "        model: {}\n",
      "        policy_loss: -0.02859479933977127\n",
      "        total_loss: 490.9866943359375\n",
      "        vf_explained_var: 0.592624306678772\n",
      "        vf_loss: 491.01116943359375\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.24062500000001\n",
      "    ram_util_percent: 34.72589285714285\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034253436887116\n",
      "    mean_env_wait_ms: 33.86598938190096\n",
      "    mean_inference_ms: 0.9353350545598121\n",
      "    mean_raw_obs_processing_ms: 4.223503439167762\n",
      "  time_since_restore: 21266.37286067009\n",
      "  time_this_iter_s: 157.18684315681458\n",
      "  time_total_s: 21266.37286067009\n",
      "  timers:\n",
      "    learn_throughput: 1697.827\n",
      "    learn_time_ms: 2355.953\n",
      "    load_throughput: 282886.188\n",
      "    load_time_ms: 14.14\n",
      "    sample_throughput: 26.54\n",
      "    sample_time_ms: 150714.067\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1611660448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 707.35\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -48.746380939570415\n",
      "  episode_reward_min: -151.12186155045748\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1893\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0050137042999268\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014375440776348114\n",
      "        model: {}\n",
      "        policy_loss: -0.03270787373185158\n",
      "        total_loss: 371.69866943359375\n",
      "        vf_explained_var: 0.7660136222839355\n",
      "        vf_loss: 371.7265625\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.68230088495577\n",
      "    ram_util_percent: 34.80044247787612\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343479761313577\n",
      "    mean_env_wait_ms: 33.868793821972815\n",
      "    mean_inference_ms: 0.9354296976197825\n",
      "    mean_raw_obs_processing_ms: 4.1779720768537505\n",
      "  time_since_restore: 21424.070043087006\n",
      "  time_this_iter_s: 157.6971824169159\n",
      "  time_total_s: 21424.070043087006\n",
      "  timers:\n",
      "    learn_throughput: 1701.782\n",
      "    learn_time_ms: 2350.477\n",
      "    load_throughput: 305722.33\n",
      "    load_time_ms: 13.084\n",
      "    sample_throughput: 26.431\n",
      "    sample_time_ms: 151337.752\n",
      "    update_time_ms: 1.503\n",
      "  timestamp: 1611660605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-32-40\n",
      "  done: false\n",
      "  episode_len_mean: 499.5\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -49.49837920481363\n",
      "  episode_reward_min: -116.29335162126671\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1905\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1387838125228882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01849217712879181\n",
      "        model: {}\n",
      "        policy_loss: -0.03557901456952095\n",
      "        total_loss: 628.9572143554688\n",
      "        vf_explained_var: 0.4933522641658783\n",
      "        vf_loss: 628.986572265625\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.41\n",
      "    ram_util_percent: 34.7959090909091\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10344942168322951\n",
      "    mean_env_wait_ms: 33.87400197143668\n",
      "    mean_inference_ms: 0.9355651832662091\n",
      "    mean_raw_obs_processing_ms: 4.153308486846463\n",
      "  time_since_restore: 21578.23638033867\n",
      "  time_this_iter_s: 154.1663372516632\n",
      "  time_total_s: 21578.23638033867\n",
      "  timers:\n",
      "    learn_throughput: 1700.044\n",
      "    learn_time_ms: 2352.88\n",
      "    load_throughput: 307133.265\n",
      "    load_time_ms: 13.024\n",
      "    sample_throughput: 26.275\n",
      "    sample_time_ms: 152235.114\n",
      "    update_time_ms: 1.517\n",
      "  timestamp: 1611660760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 500.39\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -51.402682181112816\n",
      "  episode_reward_min: -116.29335162126671\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1915\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.056604266166687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014979859814047813\n",
      "        model: {}\n",
      "        policy_loss: -0.030508941039443016\n",
      "        total_loss: 227.09548950195312\n",
      "        vf_explained_var: 0.7249665856361389\n",
      "        vf_loss: 227.12094116210938\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5626728110599\n",
      "    ram_util_percent: 34.81428571428572\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346221609589211\n",
      "    mean_env_wait_ms: 33.878753349782635\n",
      "    mean_inference_ms: 0.935687483048045\n",
      "    mean_raw_obs_processing_ms: 4.135329168397069\n",
      "  time_since_restore: 21730.045053958893\n",
      "  time_this_iter_s: 151.808673620224\n",
      "  time_total_s: 21730.045053958893\n",
      "  timers:\n",
      "    learn_throughput: 1699.979\n",
      "    learn_time_ms: 2352.971\n",
      "    load_throughput: 320994.291\n",
      "    load_time_ms: 12.461\n",
      "    sample_throughput: 26.076\n",
      "    sample_time_ms: 153394.915\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1611660912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-37-44\n",
      "  done: false\n",
      "  episode_len_mean: 507.41\n",
      "  episode_reward_max: 118.7756020769313\n",
      "  episode_reward_mean: -47.262550639983736\n",
      "  episode_reward_min: -116.29335162126671\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1925\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2286628484725952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013214172795414925\n",
      "        model: {}\n",
      "        policy_loss: -0.026014946401119232\n",
      "        total_loss: 651.7749633789062\n",
      "        vf_explained_var: 0.4503859877586365\n",
      "        vf_loss: 651.7965698242188\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54470046082947\n",
      "    ram_util_percent: 34.81797235023042\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10347469018358384\n",
      "    mean_env_wait_ms: 33.88350934667682\n",
      "    mean_inference_ms: 0.9358067815629053\n",
      "    mean_raw_obs_processing_ms: 4.11817508920693\n",
      "  time_since_restore: 21881.9109916687\n",
      "  time_this_iter_s: 151.86593770980835\n",
      "  time_total_s: 21881.9109916687\n",
      "  timers:\n",
      "    learn_throughput: 1700.696\n",
      "    learn_time_ms: 2351.979\n",
      "    load_throughput: 322043.608\n",
      "    load_time_ms: 12.421\n",
      "    sample_throughput: 25.937\n",
      "    sample_time_ms: 154217.284\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1611661064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 364.02\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -48.58785644449234\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1935\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.188533902168274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014198148623108864\n",
      "        model: {}\n",
      "        policy_loss: -0.02400420606136322\n",
      "        total_loss: 369.04339599609375\n",
      "        vf_explained_var: 0.6376854181289673\n",
      "        vf_loss: 369.06256103515625\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.55601851851851\n",
      "    ram_util_percent: 34.800000000000004\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10348771222101064\n",
      "    mean_env_wait_ms: 33.88904333816922\n",
      "    mean_inference_ms: 0.9359371545582843\n",
      "    mean_raw_obs_processing_ms: 4.108547321125167\n",
      "  time_since_restore: 22033.132759571075\n",
      "  time_this_iter_s: 151.22176790237427\n",
      "  time_total_s: 22033.132759571075\n",
      "  timers:\n",
      "    learn_throughput: 1697.649\n",
      "    learn_time_ms: 2356.199\n",
      "    load_throughput: 296559.76\n",
      "    load_time_ms: 13.488\n",
      "    sample_throughput: 25.773\n",
      "    sample_time_ms: 155201.714\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1611661215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-42-50\n",
      "  done: false\n",
      "  episode_len_mean: 360.39\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -52.63422381103802\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1948\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1688328981399536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017597418278455734\n",
      "        model: {}\n",
      "        policy_loss: -0.03000926971435547\n",
      "        total_loss: 513.1704711914062\n",
      "        vf_explained_var: 0.5155238509178162\n",
      "        vf_loss: 513.1945190429688\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.70636363636365\n",
      "    ram_util_percent: 34.77045454545455\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349795602797146\n",
      "    mean_env_wait_ms: 33.892744787660114\n",
      "    mean_inference_ms: 0.9360467659484185\n",
      "    mean_raw_obs_processing_ms: 4.1013013291842135\n",
      "  time_since_restore: 22187.360680818558\n",
      "  time_this_iter_s: 154.2279212474823\n",
      "  time_total_s: 22187.360680818558\n",
      "  timers:\n",
      "    learn_throughput: 1729.251\n",
      "    learn_time_ms: 2313.14\n",
      "    load_throughput: 283087.141\n",
      "    load_time_ms: 14.13\n",
      "    sample_throughput: 25.682\n",
      "    sample_time_ms: 155748.713\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1611661370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-45-19\n",
      "  done: false\n",
      "  episode_len_mean: 365.5\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -54.67636528963056\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1957\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1073373556137085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016820475459098816\n",
      "        model: {}\n",
      "        policy_loss: -0.02976510487496853\n",
      "        total_loss: 417.847412109375\n",
      "        vf_explained_var: 0.5910403728485107\n",
      "        vf_loss: 417.8714599609375\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81728971962617\n",
      "    ram_util_percent: 34.80000000000001\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349976593631198\n",
      "    mean_env_wait_ms: 33.89202402954905\n",
      "    mean_inference_ms: 0.9360734404673198\n",
      "    mean_raw_obs_processing_ms: 4.096266493006806\n",
      "  time_since_restore: 22336.777822732925\n",
      "  time_this_iter_s: 149.41714191436768\n",
      "  time_total_s: 22336.777822732925\n",
      "  timers:\n",
      "    learn_throughput: 1763.639\n",
      "    learn_time_ms: 2268.038\n",
      "    load_throughput: 284057.722\n",
      "    load_time_ms: 14.082\n",
      "    sample_throughput: 26.046\n",
      "    sample_time_ms: 153576.804\n",
      "    update_time_ms: 1.494\n",
      "  timestamp: 1611661519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 364.31\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -56.48268305775254\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1969\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5037990808486938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019131533801555634\n",
      "        model: {}\n",
      "        policy_loss: -0.03461463749408722\n",
      "        total_loss: 486.2737121582031\n",
      "        vf_explained_var: 0.484618216753006\n",
      "        vf_loss: 486.3018493652344\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.72602739726027\n",
      "    ram_util_percent: 34.81050228310503\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349811783697505\n",
      "    mean_env_wait_ms: 33.8887223197805\n",
      "    mean_inference_ms: 0.9360642299223844\n",
      "    mean_raw_obs_processing_ms: 4.089840400243502\n",
      "  time_since_restore: 22490.625410318375\n",
      "  time_this_iter_s: 153.84758758544922\n",
      "  time_total_s: 22490.625410318375\n",
      "  timers:\n",
      "    learn_throughput: 1799.543\n",
      "    learn_time_ms: 2222.787\n",
      "    load_throughput: 270064.92\n",
      "    load_time_ms: 14.811\n",
      "    sample_throughput: 26.284\n",
      "    sample_time_ms: 152181.619\n",
      "    update_time_ms: 1.503\n",
      "  timestamp: 1611661673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-50-26\n",
      "  done: false\n",
      "  episode_len_mean: 357.31\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -50.491518438642785\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1980\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2712035179138184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014948103576898575\n",
      "        model: {}\n",
      "        policy_loss: -0.02978729084134102\n",
      "        total_loss: 640.6610717773438\n",
      "        vf_explained_var: 0.5516942143440247\n",
      "        vf_loss: 640.685791015625\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66238532110091\n",
      "    ram_util_percent: 34.800000000000004\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10349396369269087\n",
      "    mean_env_wait_ms: 33.884713548181644\n",
      "    mean_inference_ms: 0.9360296200518406\n",
      "    mean_raw_obs_processing_ms: 4.0841612921742545\n",
      "  time_since_restore: 22643.107018232346\n",
      "  time_this_iter_s: 152.48160791397095\n",
      "  time_total_s: 22643.107018232346\n",
      "  timers:\n",
      "    learn_throughput: 1810.5\n",
      "    learn_time_ms: 2209.334\n",
      "    load_throughput: 262312.454\n",
      "    load_time_ms: 15.249\n",
      "    sample_throughput: 26.488\n",
      "    sample_time_ms: 151009.335\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1611661826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-52-59\n",
      "  done: false\n",
      "  episode_len_mean: 365.91\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -54.436942355996734\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1991\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1336647272109985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01281044539064169\n",
      "        model: {}\n",
      "        policy_loss: -0.02131205052137375\n",
      "        total_loss: 351.1839904785156\n",
      "        vf_explained_var: 0.7030878663063049\n",
      "        vf_loss: 351.2009582519531\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.47385321100917\n",
      "    ram_util_percent: 34.80963302752294\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10348854409365396\n",
      "    mean_env_wait_ms: 33.88016620733771\n",
      "    mean_inference_ms: 0.9359828861471577\n",
      "    mean_raw_obs_processing_ms: 4.078098253002999\n",
      "  time_since_restore: 22795.556771993637\n",
      "  time_this_iter_s: 152.4497537612915\n",
      "  time_total_s: 22795.556771993637\n",
      "  timers:\n",
      "    learn_throughput: 1859.253\n",
      "    learn_time_ms: 2151.402\n",
      "    load_throughput: 243572.014\n",
      "    load_time_ms: 16.422\n",
      "    sample_throughput: 26.562\n",
      "    sample_time_ms: 150593.733\n",
      "    update_time_ms: 1.463\n",
      "  timestamp: 1611661979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 372.02\n",
      "  episode_reward_max: 118.76656578833816\n",
      "  episode_reward_mean: -54.52696905926495\n",
      "  episode_reward_min: -106.05642950367925\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2001\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6131093502044678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018623007461428642\n",
      "        model: {}\n",
      "        policy_loss: -0.03430799022316933\n",
      "        total_loss: 205.7880096435547\n",
      "        vf_explained_var: 0.7680588364601135\n",
      "        vf_loss: 205.8159942626953\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4188940092166\n",
      "    ram_util_percent: 34.80000000000001\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10348311468539528\n",
      "    mean_env_wait_ms: 33.87598046624059\n",
      "    mean_inference_ms: 0.935938681209999\n",
      "    mean_raw_obs_processing_ms: 4.072160037813364\n",
      "  time_since_restore: 22947.24575328827\n",
      "  time_this_iter_s: 151.68898129463196\n",
      "  time_total_s: 22947.24575328827\n",
      "  timers:\n",
      "    learn_throughput: 1855.919\n",
      "    learn_time_ms: 2155.266\n",
      "    load_throughput: 230223.826\n",
      "    load_time_ms: 17.374\n",
      "    sample_throughput: 26.669\n",
      "    sample_time_ms: 149984.89\n",
      "    update_time_ms: 1.461\n",
      "  timestamp: 1611662131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_12-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 370.27\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -50.10789026556993\n",
      "  episode_reward_min: -101.44504726547002\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2013\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3369526863098145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02268386259675026\n",
      "        model: {}\n",
      "        policy_loss: -0.044037528336048126\n",
      "        total_loss: 338.2911682128906\n",
      "        vf_explained_var: 0.7371991872787476\n",
      "        vf_loss: 338.3276062011719\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.55068493150684\n",
      "    ram_util_percent: 34.8123287671233\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10347606080473842\n",
      "    mean_env_wait_ms: 33.87091553732293\n",
      "    mean_inference_ms: 0.9358804490403333\n",
      "    mean_raw_obs_processing_ms: 4.06537861201211\n",
      "  time_since_restore: 23101.109582185745\n",
      "  time_this_iter_s: 153.8638288974762\n",
      "  time_total_s: 23101.109582185745\n",
      "  timers:\n",
      "    learn_throughput: 1853.75\n",
      "    learn_time_ms: 2157.788\n",
      "    load_throughput: 232166.258\n",
      "    load_time_ms: 17.229\n",
      "    sample_throughput: 26.675\n",
      "    sample_time_ms: 149950.888\n",
      "    update_time_ms: 1.481\n",
      "  timestamp: 1611662285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-00-39\n",
      "  done: false\n",
      "  episode_len_mean: 354.93\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -54.84958381319741\n",
      "  episode_reward_min: -101.44504726547002\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2026\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3907127380371094\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01367670577019453\n",
      "        model: {}\n",
      "        policy_loss: -0.030020613223314285\n",
      "        total_loss: 327.8978576660156\n",
      "        vf_explained_var: 0.7663881182670593\n",
      "        vf_loss: 327.92095947265625\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.65520361990951\n",
      "    ram_util_percent: 34.80090497737557\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346770692251607\n",
      "    mean_env_wait_ms: 33.86541818727542\n",
      "    mean_inference_ms: 0.9358086690874501\n",
      "    mean_raw_obs_processing_ms: 4.059207041251063\n",
      "  time_since_restore: 23255.519967079163\n",
      "  time_this_iter_s: 154.41038489341736\n",
      "  time_total_s: 23255.519967079163\n",
      "  timers:\n",
      "    learn_throughput: 1853.542\n",
      "    learn_time_ms: 2158.03\n",
      "    load_throughput: 226016.346\n",
      "    load_time_ms: 17.698\n",
      "    sample_throughput: 26.629\n",
      "    sample_time_ms: 150209.985\n",
      "    update_time_ms: 1.481\n",
      "  timestamp: 1611662439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-03-14\n",
      "  done: false\n",
      "  episode_len_mean: 344.66\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -53.27579922050901\n",
      "  episode_reward_min: -101.44504726547002\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2039\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.1283464431762695\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013469530269503593\n",
      "        model: {}\n",
      "        policy_loss: -0.03370629996061325\n",
      "        total_loss: 215.5462188720703\n",
      "        vf_explained_var: 0.7999945282936096\n",
      "        vf_loss: 215.57308959960938\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.55610859728507\n",
      "    ram_util_percent: 34.81176470588236\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345941642035371\n",
      "    mean_env_wait_ms: 33.86018290758786\n",
      "    mean_inference_ms: 0.9357327014633395\n",
      "    mean_raw_obs_processing_ms: 4.05408632879918\n",
      "  time_since_restore: 23409.91205215454\n",
      "  time_this_iter_s: 154.39208507537842\n",
      "  time_total_s: 23409.91205215454\n",
      "  timers:\n",
      "    learn_throughput: 1852.987\n",
      "    learn_time_ms: 2158.677\n",
      "    load_throughput: 236118.236\n",
      "    load_time_ms: 16.941\n",
      "    sample_throughput: 26.585\n",
      "    sample_time_ms: 150462.266\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1611662594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-05-45\n",
      "  done: false\n",
      "  episode_len_mean: 353.35\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -47.1172716566625\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2049\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4779279232025146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011074097827076912\n",
      "        model: {}\n",
      "        policy_loss: -0.028958940878510475\n",
      "        total_loss: 432.2514953613281\n",
      "        vf_explained_var: 0.563643753528595\n",
      "        vf_loss: 432.27484130859375\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.86837209302325\n",
      "    ram_util_percent: 34.80000000000001\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345295875736388\n",
      "    mean_env_wait_ms: 33.85613354654151\n",
      "    mean_inference_ms: 0.9356723982415499\n",
      "    mean_raw_obs_processing_ms: 4.049640289507907\n",
      "  time_since_restore: 23560.762031316757\n",
      "  time_this_iter_s: 150.8499791622162\n",
      "  time_total_s: 23560.762031316757\n",
      "  timers:\n",
      "    learn_throughput: 1855.868\n",
      "    learn_time_ms: 2155.325\n",
      "    load_throughput: 241504.775\n",
      "    load_time_ms: 16.563\n",
      "    sample_throughput: 26.59\n",
      "    sample_time_ms: 150432.816\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1611662745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-08-18\n",
      "  done: false\n",
      "  episode_len_mean: 350.1\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -43.317653183744\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2060\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.2996811866760254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010315977968275547\n",
      "        model: {}\n",
      "        policy_loss: -0.022261999547481537\n",
      "        total_loss: 593.256591796875\n",
      "        vf_explained_var: 0.5155063271522522\n",
      "        vf_loss: 593.2735595703125\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.51422018348623\n",
      "    ram_util_percent: 34.83027522935781\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10344736091483028\n",
      "    mean_env_wait_ms: 33.85201255465748\n",
      "    mean_inference_ms: 0.9356159885614048\n",
      "    mean_raw_obs_processing_ms: 4.04523773269826\n",
      "  time_since_restore: 23713.44313645363\n",
      "  time_this_iter_s: 152.68110513687134\n",
      "  time_total_s: 23713.44313645363\n",
      "  timers:\n",
      "    learn_throughput: 1855.584\n",
      "    learn_time_ms: 2155.655\n",
      "    load_throughput: 252482.216\n",
      "    load_time_ms: 15.843\n",
      "    sample_throughput: 26.616\n",
      "    sample_time_ms: 150284.397\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1611662898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 353.23\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -43.50900745636821\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2070\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5579707622528076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008397271856665611\n",
      "        model: {}\n",
      "        policy_loss: -0.02495497651398182\n",
      "        total_loss: 169.61660766601562\n",
      "        vf_explained_var: 0.8093098402023315\n",
      "        vf_loss: 169.63731384277344\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.42626728110599\n",
      "    ram_util_percent: 34.813364055299544\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10344238963081744\n",
      "    mean_env_wait_ms: 33.84833198613695\n",
      "    mean_inference_ms: 0.9355700309959567\n",
      "    mean_raw_obs_processing_ms: 4.0408226643226355\n",
      "  time_since_restore: 23865.306466817856\n",
      "  time_this_iter_s: 151.8633303642273\n",
      "  time_total_s: 23865.306466817856\n",
      "  timers:\n",
      "    learn_throughput: 1857.655\n",
      "    learn_time_ms: 2153.252\n",
      "    load_throughput: 250070.294\n",
      "    load_time_ms: 15.996\n",
      "    sample_throughput: 26.573\n",
      "    sample_time_ms: 150531.241\n",
      "    update_time_ms: 1.484\n",
      "  timestamp: 1611663050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 360.96\n",
      "  episode_reward_max: 118.76692717411922\n",
      "  episode_reward_mean: -47.609608185255205\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6828417778015137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01073751226067543\n",
      "        model: {}\n",
      "        policy_loss: -0.02813609130680561\n",
      "        total_loss: 270.8625793457031\n",
      "        vf_explained_var: 0.657319962978363\n",
      "        vf_loss: 270.8852844238281\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66697247706422\n",
      "    ram_util_percent: 34.881192660550454\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343821886889396\n",
      "    mean_env_wait_ms: 33.8448229644712\n",
      "    mean_inference_ms: 0.9355295096897781\n",
      "    mean_raw_obs_processing_ms: 4.036181374236346\n",
      "  time_since_restore: 24017.446093320847\n",
      "  time_this_iter_s: 152.13962650299072\n",
      "  time_total_s: 24017.446093320847\n",
      "  timers:\n",
      "    learn_throughput: 1856.4\n",
      "    learn_time_ms: 2154.707\n",
      "    load_throughput: 244178.562\n",
      "    load_time_ms: 16.381\n",
      "    sample_throughput: 26.604\n",
      "    sample_time_ms: 150354.65\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1611663202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 349.39\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -50.193218413631975\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2094\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.529315710067749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007406224031001329\n",
      "        model: {}\n",
      "        policy_loss: -0.012340251356363297\n",
      "        total_loss: 522.0036010742188\n",
      "        vf_explained_var: 0.6262385249137878\n",
      "        vf_loss: 522.01220703125\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.44304932735425\n",
      "    ram_util_percent: 34.899999999999984\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343286995456454\n",
      "    mean_env_wait_ms: 33.84025878429727\n",
      "    mean_inference_ms: 0.9354838031946372\n",
      "    mean_raw_obs_processing_ms: 4.030756222850143\n",
      "  time_since_restore: 24174.03461432457\n",
      "  time_this_iter_s: 156.58852100372314\n",
      "  time_total_s: 24174.03461432457\n",
      "  timers:\n",
      "    learn_throughput: 1856.14\n",
      "    learn_time_ms: 2155.01\n",
      "    load_throughput: 252655.217\n",
      "    load_time_ms: 15.832\n",
      "    sample_throughput: 26.532\n",
      "    sample_time_ms: 150760.155\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1611663359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-18-31\n",
      "  done: false\n",
      "  episode_len_mean: 348.49\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -42.078834373606526\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2104\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.499214768409729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012153337709605694\n",
      "        model: {}\n",
      "        policy_loss: -0.02446810156106949\n",
      "        total_loss: 346.0559387207031\n",
      "        vf_explained_var: 0.721730649471283\n",
      "        vf_loss: 346.0742492675781\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5184331797235\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342937923877564\n",
      "    mean_env_wait_ms: 33.83710942834106\n",
      "    mean_inference_ms: 0.9354502562625276\n",
      "    mean_raw_obs_processing_ms: 4.026894296317385\n",
      "  time_since_restore: 24325.85023713112\n",
      "  time_this_iter_s: 151.81562280654907\n",
      "  time_total_s: 24325.85023713112\n",
      "  timers:\n",
      "    learn_throughput: 1857.195\n",
      "    learn_time_ms: 2153.786\n",
      "    load_throughput: 254616.115\n",
      "    load_time_ms: 15.71\n",
      "    sample_throughput: 26.543\n",
      "    sample_time_ms: 150700.78\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1611663511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-21-08\n",
      "  done: false\n",
      "  episode_len_mean: 344.85\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -42.12707754842034\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2119\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3939141035079956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009520258754491806\n",
      "        model: {}\n",
      "        policy_loss: -0.024170123040676117\n",
      "        total_loss: 464.3696594238281\n",
      "        vf_explained_var: 0.6985509991645813\n",
      "        vf_loss: 464.3890380859375\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25866666666667\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034243084917037\n",
      "    mean_env_wait_ms: 33.832567703664466\n",
      "    mean_inference_ms: 0.9354009559406458\n",
      "    mean_raw_obs_processing_ms: 4.021877864506336\n",
      "  time_since_restore: 24482.901455163956\n",
      "  time_this_iter_s: 157.0512180328369\n",
      "  time_total_s: 24482.901455163956\n",
      "  timers:\n",
      "    learn_throughput: 1857.41\n",
      "    learn_time_ms: 2153.537\n",
      "    load_throughput: 251455.19\n",
      "    load_time_ms: 15.907\n",
      "    sample_throughput: 26.448\n",
      "    sample_time_ms: 151239.942\n",
      "    update_time_ms: 1.431\n",
      "  timestamp: 1611663668\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 337.65\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -40.43013223700536\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2132\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4012231826782227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010830054059624672\n",
      "        model: {}\n",
      "        policy_loss: -0.02773212641477585\n",
      "        total_loss: 394.112060546875\n",
      "        vf_explained_var: 0.743609607219696\n",
      "        vf_loss: 394.13433837890625\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.59411764705881\n",
      "    ram_util_percent: 34.91131221719456\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342057667789223\n",
      "    mean_env_wait_ms: 33.828840282463545\n",
      "    mean_inference_ms: 0.9353649120656544\n",
      "    mean_raw_obs_processing_ms: 4.01756155274802\n",
      "  time_since_restore: 24637.70982837677\n",
      "  time_this_iter_s: 154.80837321281433\n",
      "  time_total_s: 24637.70982837677\n",
      "  timers:\n",
      "    learn_throughput: 1862.836\n",
      "    learn_time_ms: 2147.264\n",
      "    load_throughput: 249888.529\n",
      "    load_time_ms: 16.007\n",
      "    sample_throughput: 26.43\n",
      "    sample_time_ms: 151342.271\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1611663823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 338.75\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -36.41083878741873\n",
      "  episode_reward_min: -102.1259344950027\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 2145\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5229002237319946\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010454526171088219\n",
      "        model: {}\n",
      "        policy_loss: -0.020826447755098343\n",
      "        total_loss: 434.46551513671875\n",
      "        vf_explained_var: 0.6993650197982788\n",
      "        vf_loss: 434.4810485839844\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.52851063829787\n",
      "    ram_util_percent: 34.899999999999984\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342216532020661\n",
      "    mean_env_wait_ms: 33.827053463538256\n",
      "    mean_inference_ms: 0.9353806635916029\n",
      "    mean_raw_obs_processing_ms: 4.013679086193036\n",
      "  time_since_restore: 24802.06212735176\n",
      "  time_this_iter_s: 164.35229897499084\n",
      "  time_total_s: 24802.06212735176\n",
      "  timers:\n",
      "    learn_throughput: 1835.739\n",
      "    learn_time_ms: 2178.958\n",
      "    load_throughput: 244691.387\n",
      "    load_time_ms: 16.347\n",
      "    sample_throughput: 26.264\n",
      "    sample_time_ms: 152301.514\n",
      "    update_time_ms: 1.414\n",
      "  timestamp: 1611663988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 341.1\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -42.341992732077664\n",
      "  episode_reward_min: -101.79678610430507\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2154\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.682672142982483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011391866020858288\n",
      "        model: {}\n",
      "        policy_loss: -0.026369428262114525\n",
      "        total_loss: 376.4119567871094\n",
      "        vf_explained_var: 0.5586150884628296\n",
      "        vf_loss: 376.4325256347656\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.23452914798206\n",
      "    ram_util_percent: 35.0390134529148\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342553644814391\n",
      "    mean_env_wait_ms: 33.82672015897654\n",
      "    mean_inference_ms: 0.9354064467192367\n",
      "    mean_raw_obs_processing_ms: 4.010850024584196\n",
      "  time_since_restore: 24958.285739660263\n",
      "  time_this_iter_s: 156.2236123085022\n",
      "  time_total_s: 24958.285739660263\n",
      "  timers:\n",
      "    learn_throughput: 1825.624\n",
      "    learn_time_ms: 2191.031\n",
      "    load_throughput: 245659.481\n",
      "    load_time_ms: 16.283\n",
      "    sample_throughput: 26.234\n",
      "    sample_time_ms: 152471.875\n",
      "    update_time_ms: 1.418\n",
      "  timestamp: 1611664144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 345.79\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -39.84486305697899\n",
      "  episode_reward_min: -101.79678610430507\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2164\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.37051522731781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012413566932082176\n",
      "        model: {}\n",
      "        policy_loss: -0.0332590751349926\n",
      "        total_loss: 218.31851196289062\n",
      "        vf_explained_var: 0.81561279296875\n",
      "        vf_loss: 218.34552001953125\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.6765765765766\n",
      "    ram_util_percent: 35.031981981981986\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10342995769053434\n",
      "    mean_env_wait_ms: 33.82681106726555\n",
      "    mean_inference_ms: 0.9354433866302471\n",
      "    mean_raw_obs_processing_ms: 4.007585649376691\n",
      "  time_since_restore: 25113.40749168396\n",
      "  time_this_iter_s: 155.1217520236969\n",
      "  time_total_s: 25113.40749168396\n",
      "  timers:\n",
      "    learn_throughput: 1816.552\n",
      "    learn_time_ms: 2201.974\n",
      "    load_throughput: 245873.332\n",
      "    load_time_ms: 16.269\n",
      "    sample_throughput: 26.163\n",
      "    sample_time_ms: 152887.162\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1611664300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-34-14\n",
      "  done: false\n",
      "  episode_len_mean: 338.35\n",
      "  episode_reward_max: 118.77060717829765\n",
      "  episode_reward_mean: -40.034638605721945\n",
      "  episode_reward_min: -101.31067032877206\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2175\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7095822095870972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014221789315342903\n",
      "        model: {}\n",
      "        policy_loss: -0.030697543174028397\n",
      "        total_loss: 203.43519592285156\n",
      "        vf_explained_var: 0.8034267425537109\n",
      "        vf_loss: 203.45870971679688\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66045454545456\n",
      "    ram_util_percent: 34.95409090909091\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10343572111645512\n",
      "    mean_env_wait_ms: 33.82709709182701\n",
      "    mean_inference_ms: 0.9354895720774088\n",
      "    mean_raw_obs_processing_ms: 4.004307936213268\n",
      "  time_since_restore: 25267.492656946182\n",
      "  time_this_iter_s: 154.0851652622223\n",
      "  time_total_s: 25267.492656946182\n",
      "  timers:\n",
      "    learn_throughput: 1815.841\n",
      "    learn_time_ms: 2202.836\n",
      "    load_throughput: 246962.737\n",
      "    load_time_ms: 16.197\n",
      "    sample_throughput: 26.139\n",
      "    sample_time_ms: 153027.316\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1611664454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 344.67\n",
      "  episode_reward_max: 118.75791342757846\n",
      "  episode_reward_mean: -40.10880947870303\n",
      "  episode_reward_min: -101.31067032877206\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2186\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5358611345291138\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011573647148907185\n",
      "        model: {}\n",
      "        policy_loss: -0.03223288431763649\n",
      "        total_loss: 431.39373779296875\n",
      "        vf_explained_var: 0.588223397731781\n",
      "        vf_loss: 431.420166015625\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48944954128439\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10344134549682905\n",
      "    mean_env_wait_ms: 33.8273577518898\n",
      "    mean_inference_ms: 0.9355324993519281\n",
      "    mean_raw_obs_processing_ms: 4.000898753337591\n",
      "  time_since_restore: 25420.255608558655\n",
      "  time_this_iter_s: 152.76295161247253\n",
      "  time_total_s: 25420.255608558655\n",
      "  timers:\n",
      "    learn_throughput: 1813.246\n",
      "    learn_time_ms: 2205.989\n",
      "    load_throughput: 246131.962\n",
      "    load_time_ms: 16.251\n",
      "    sample_throughput: 26.125\n",
      "    sample_time_ms: 153111.654\n",
      "    update_time_ms: 1.399\n",
      "  timestamp: 1611664607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-39-20\n",
      "  done: false\n",
      "  episode_len_mean: 347.42\n",
      "  episode_reward_max: 118.75791342757846\n",
      "  episode_reward_mean: -43.985070051170815\n",
      "  episode_reward_min: -101.31067032877206\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2197\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4678137302398682\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009478874504566193\n",
      "        model: {}\n",
      "        policy_loss: -0.022793224081397057\n",
      "        total_loss: 576.6333618164062\n",
      "        vf_explained_var: 0.5500765442848206\n",
      "        vf_loss: 576.6514282226562\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.60365296803653\n",
      "    ram_util_percent: 34.91095890410958\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10344682914344606\n",
      "    mean_env_wait_ms: 33.827543662780414\n",
      "    mean_inference_ms: 0.9355716761037641\n",
      "    mean_raw_obs_processing_ms: 3.997183332606322\n",
      "  time_since_restore: 25573.392394065857\n",
      "  time_this_iter_s: 153.13678550720215\n",
      "  time_total_s: 25573.392394065857\n",
      "  timers:\n",
      "    learn_throughput: 1813.219\n",
      "    learn_time_ms: 2206.022\n",
      "    load_throughput: 249865.827\n",
      "    load_time_ms: 16.009\n",
      "    sample_throughput: 26.107\n",
      "    sample_time_ms: 153213.685\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1611664760\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 348.0\n",
      "  episode_reward_max: 118.72715646705194\n",
      "  episode_reward_mean: -43.64576462772731\n",
      "  episode_reward_min: -101.31067032877206\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2209\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4615819454193115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007770773954689503\n",
      "        model: {}\n",
      "        policy_loss: -0.02151787467300892\n",
      "        total_loss: 467.3253173828125\n",
      "        vf_explained_var: 0.6650226712226868\n",
      "        vf_loss: 467.3429260253906\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345295229444097\n",
      "    mean_env_wait_ms: 33.82784440061071\n",
      "    mean_inference_ms: 0.9356160036825776\n",
      "    mean_raw_obs_processing_ms: 3.9933022295233265\n",
      "  time_since_restore: 25727.625649929047\n",
      "  time_this_iter_s: 154.2332558631897\n",
      "  time_total_s: 25727.625649929047\n",
      "  timers:\n",
      "    learn_throughput: 1813.463\n",
      "    learn_time_ms: 2205.725\n",
      "    load_throughput: 248150.631\n",
      "    load_time_ms: 16.119\n",
      "    sample_throughput: 26.147\n",
      "    sample_time_ms: 152982.675\n",
      "    update_time_ms: 1.439\n",
      "  timestamp: 1611664915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-44-27\n",
      "  done: false\n",
      "  episode_len_mean: 357.75\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -39.32061408774169\n",
      "  episode_reward_min: -101.02711188321113\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2219\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.474679708480835\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0118100019171834\n",
      "        model: {}\n",
      "        policy_loss: -0.03148152306675911\n",
      "        total_loss: 260.65338134765625\n",
      "        vf_explained_var: 0.7502896189689636\n",
      "        vf_loss: 260.6788635253906\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.52949308755761\n",
      "    ram_util_percent: 34.91059907834101\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345745122900267\n",
      "    mean_env_wait_ms: 33.82797545267319\n",
      "    mean_inference_ms: 0.9356466436195795\n",
      "    mean_raw_obs_processing_ms: 3.9893423485243904\n",
      "  time_since_restore: 25879.269134283066\n",
      "  time_this_iter_s: 151.64348435401917\n",
      "  time_total_s: 25879.269134283066\n",
      "  timers:\n",
      "    learn_throughput: 1815.108\n",
      "    learn_time_ms: 2203.725\n",
      "    load_throughput: 241960.697\n",
      "    load_time_ms: 16.532\n",
      "    sample_throughput: 26.15\n",
      "    sample_time_ms: 152963.489\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1611665067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 371.32\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -32.5666076782012\n",
      "  episode_reward_min: -101.02711188321113\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2228\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3859680891036987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009132204577326775\n",
      "        model: {}\n",
      "        policy_loss: -0.020375417545437813\n",
      "        total_loss: 568.387451171875\n",
      "        vf_explained_var: 0.5049242377281189\n",
      "        vf_loss: 568.4031982421875\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.81728971962617\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346089990197255\n",
      "    mean_env_wait_ms: 33.828093488167816\n",
      "    mean_inference_ms: 0.9356729413025022\n",
      "    mean_raw_obs_processing_ms: 3.985174985161084\n",
      "  time_since_restore: 26029.335047245026\n",
      "  time_this_iter_s: 150.06591296195984\n",
      "  time_total_s: 26029.335047245026\n",
      "  timers:\n",
      "    learn_throughput: 1817.221\n",
      "    learn_time_ms: 2201.163\n",
      "    load_throughput: 249037.247\n",
      "    load_time_ms: 16.062\n",
      "    sample_throughput: 26.269\n",
      "    sample_time_ms: 152269.141\n",
      "    update_time_ms: 1.429\n",
      "  timestamp: 1611665217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-49-29\n",
      "  done: false\n",
      "  episode_len_mean: 374.8\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -38.8829260719582\n",
      "  episode_reward_min: -100.80549703404904\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2239\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6980115175247192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009911920875310898\n",
      "        model: {}\n",
      "        policy_loss: -0.024206452071666718\n",
      "        total_loss: 398.5809020996094\n",
      "        vf_explained_var: 0.629813551902771\n",
      "        vf_loss: 398.60015869140625\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.58899082568809\n",
      "    ram_util_percent: 34.91972477064219\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346192806881971\n",
      "    mean_env_wait_ms: 33.827056356402835\n",
      "    mean_inference_ms: 0.9356734082895342\n",
      "    mean_raw_obs_processing_ms: 3.9796717356277553\n",
      "  time_since_restore: 26181.338111639023\n",
      "  time_this_iter_s: 152.0030643939972\n",
      "  time_total_s: 26181.338111639023\n",
      "  timers:\n",
      "    learn_throughput: 1814.12\n",
      "    learn_time_ms: 2204.925\n",
      "    load_throughput: 249747.918\n",
      "    load_time_ms: 16.016\n",
      "    sample_throughput: 26.319\n",
      "    sample_time_ms: 151979.302\n",
      "    update_time_ms: 1.437\n",
      "  timestamp: 1611665369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-52-09\n",
      "  done: false\n",
      "  episode_len_mean: 378.94\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -32.61250427711177\n",
      "  episode_reward_min: -100.80549703404904\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2250\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5583139657974243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014247424900531769\n",
      "        model: {}\n",
      "        policy_loss: -0.029309870675206184\n",
      "        total_loss: 452.5817565917969\n",
      "        vf_explained_var: 0.6949717998504639\n",
      "        vf_loss: 452.6038818359375\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.87631578947367\n",
      "    ram_util_percent: 35.025000000000006\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346239372644002\n",
      "    mean_env_wait_ms: 33.826110370730355\n",
      "    mean_inference_ms: 0.9356717914311109\n",
      "    mean_raw_obs_processing_ms: 3.974169297963208\n",
      "  time_since_restore: 26341.63053917885\n",
      "  time_this_iter_s: 160.29242753982544\n",
      "  time_total_s: 26341.63053917885\n",
      "  timers:\n",
      "    learn_throughput: 1794.968\n",
      "    learn_time_ms: 2228.452\n",
      "    load_throughput: 272369.783\n",
      "    load_time_ms: 14.686\n",
      "    sample_throughput: 26.394\n",
      "    sample_time_ms: 151551.777\n",
      "    update_time_ms: 1.466\n",
      "  timestamp: 1611665529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 372.93\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -32.90346482970526\n",
      "  episode_reward_min: -101.70495006591081\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2261\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6947499513626099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006073500961065292\n",
      "        model: {}\n",
      "        policy_loss: -0.013925821520388126\n",
      "        total_loss: 625.36865234375\n",
      "        vf_explained_var: 0.47972771525382996\n",
      "        vf_loss: 625.3794555664062\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.35515695067265\n",
      "    ram_util_percent: 35.11793721973095\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346112033159269\n",
      "    mean_env_wait_ms: 33.82487055556594\n",
      "    mean_inference_ms: 0.9356504775087879\n",
      "    mean_raw_obs_processing_ms: 3.969170665618226\n",
      "  time_since_restore: 26497.61432981491\n",
      "  time_this_iter_s: 155.98379063606262\n",
      "  time_total_s: 26497.61432981491\n",
      "  timers:\n",
      "    learn_throughput: 1793.161\n",
      "    learn_time_ms: 2230.698\n",
      "    load_throughput: 272361.382\n",
      "    load_time_ms: 14.686\n",
      "    sample_throughput: 26.398\n",
      "    sample_time_ms: 151524.544\n",
      "    update_time_ms: 1.423\n",
      "  timestamp: 1611665686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 369.65\n",
      "  episode_reward_max: 118.75720857651108\n",
      "  episode_reward_mean: -32.9040283897235\n",
      "  episode_reward_min: -101.70495006591081\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2273\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6539714336395264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014385830610990524\n",
      "        model: {}\n",
      "        policy_loss: -0.034084707498550415\n",
      "        total_loss: 284.6285400390625\n",
      "        vf_explained_var: 0.7656801342964172\n",
      "        vf_loss: 284.65533447265625\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.96367713004484\n",
      "    ram_util_percent: 35.044843049327355\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345822275681929\n",
      "    mean_env_wait_ms: 33.82356854470715\n",
      "    mean_inference_ms: 0.935618624374523\n",
      "    mean_raw_obs_processing_ms: 3.9641045459935755\n",
      "  time_since_restore: 26653.55234336853\n",
      "  time_this_iter_s: 155.93801355361938\n",
      "  time_total_s: 26653.55234336853\n",
      "  timers:\n",
      "    learn_throughput: 1803.895\n",
      "    learn_time_ms: 2217.425\n",
      "    load_throughput: 267300.392\n",
      "    load_time_ms: 14.964\n",
      "    sample_throughput: 26.382\n",
      "    sample_time_ms: 151620.46\n",
      "    update_time_ms: 1.42\n",
      "  timestamp: 1611665842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_13-59-55\n",
      "  done: false\n",
      "  episode_len_mean: 365.02\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -28.566604551213356\n",
      "  episode_reward_min: -101.70495006591081\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2284\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5390174388885498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008021856658160686\n",
      "        model: {}\n",
      "        policy_loss: -0.026660919189453125\n",
      "        total_loss: 423.54913330078125\n",
      "        vf_explained_var: 0.7121642827987671\n",
      "        vf_loss: 423.57171630859375\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.30776255707761\n",
      "    ram_util_percent: 34.91232876712328\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345613895370485\n",
      "    mean_env_wait_ms: 33.82246043966941\n",
      "    mean_inference_ms: 0.9355918276203836\n",
      "    mean_raw_obs_processing_ms: 3.959554076644368\n",
      "  time_since_restore: 26806.894790410995\n",
      "  time_this_iter_s: 153.3424470424652\n",
      "  time_total_s: 26806.894790410995\n",
      "  timers:\n",
      "    learn_throughput: 1804.438\n",
      "    learn_time_ms: 2216.757\n",
      "    load_throughput: 250140.388\n",
      "    load_time_ms: 15.991\n",
      "    sample_throughput: 26.395\n",
      "    sample_time_ms: 151543.19\n",
      "    update_time_ms: 1.425\n",
      "  timestamp: 1611665995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 371.65\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -24.31874258644934\n",
      "  episode_reward_min: -101.70495006591081\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2294\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.512488842010498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011938629671931267\n",
      "        model: {}\n",
      "        policy_loss: -0.024324724450707436\n",
      "        total_loss: 436.1488037109375\n",
      "        vf_explained_var: 0.6371859908103943\n",
      "        vf_loss: 436.1670837402344\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53899082568807\n",
      "    ram_util_percent: 34.929357798165135\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345426623556879\n",
      "    mean_env_wait_ms: 33.82151231208016\n",
      "    mean_inference_ms: 0.9355694756183608\n",
      "    mean_raw_obs_processing_ms: 3.9552856132095253\n",
      "  time_since_restore: 26959.120131969452\n",
      "  time_this_iter_s: 152.22534155845642\n",
      "  time_total_s: 26959.120131969452\n",
      "  timers:\n",
      "    learn_throughput: 1804.469\n",
      "    learn_time_ms: 2216.718\n",
      "    load_throughput: 267345.968\n",
      "    load_time_ms: 14.962\n",
      "    sample_throughput: 26.405\n",
      "    sample_time_ms: 151489.142\n",
      "    update_time_ms: 1.424\n",
      "  timestamp: 1611666148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 379.76\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -18.061525519316646\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2304\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5417107343673706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015243676491081715\n",
      "        model: {}\n",
      "        policy_loss: -0.03134544938802719\n",
      "        total_loss: 322.19708251953125\n",
      "        vf_explained_var: 0.7502300143241882\n",
      "        vf_loss: 322.22064208984375\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.60599078341014\n",
      "    ram_util_percent: 34.93041474654377\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345242740132274\n",
      "    mean_env_wait_ms: 33.82059248814594\n",
      "    mean_inference_ms: 0.9355475419205895\n",
      "    mean_raw_obs_processing_ms: 3.950726903125402\n",
      "  time_since_restore: 27111.281017780304\n",
      "  time_this_iter_s: 152.16088581085205\n",
      "  time_total_s: 27111.281017780304\n",
      "  timers:\n",
      "    learn_throughput: 1804.448\n",
      "    learn_time_ms: 2216.744\n",
      "    load_throughput: 266436.966\n",
      "    load_time_ms: 15.013\n",
      "    sample_throughput: 26.421\n",
      "    sample_time_ms: 151392.238\n",
      "    update_time_ms: 1.462\n",
      "  timestamp: 1611666300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-07-31\n",
      "  done: false\n",
      "  episode_len_mean: 385.97\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -13.663536883068446\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2313\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5524725914001465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012178760021924973\n",
      "        model: {}\n",
      "        policy_loss: -0.035642534494400024\n",
      "        total_loss: 285.8280029296875\n",
      "        vf_explained_var: 0.7605799436569214\n",
      "        vf_loss: 285.85748291015625\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.54675925925925\n",
      "    ram_util_percent: 34.962037037037035\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345075222683597\n",
      "    mean_env_wait_ms: 33.8197332046156\n",
      "    mean_inference_ms: 0.9355319030731198\n",
      "    mean_raw_obs_processing_ms: 3.9463185575271167\n",
      "  time_since_restore: 27262.29904651642\n",
      "  time_this_iter_s: 151.0180287361145\n",
      "  time_total_s: 27262.29904651642\n",
      "  timers:\n",
      "    learn_throughput: 1805.063\n",
      "    learn_time_ms: 2215.989\n",
      "    load_throughput: 258528.639\n",
      "    load_time_ms: 15.472\n",
      "    sample_throughput: 26.478\n",
      "    sample_time_ms: 151068.535\n",
      "    update_time_ms: 1.445\n",
      "  timestamp: 1611666451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-10-03\n",
      "  done: false\n",
      "  episode_len_mean: 384.23\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -18.014089137219784\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2323\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4356844425201416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011187887750566006\n",
      "        model: {}\n",
      "        policy_loss: -0.02666325867176056\n",
      "        total_loss: 500.64813232421875\n",
      "        vf_explained_var: 0.6253787279129028\n",
      "        vf_loss: 500.6691589355469\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48755760368664\n",
      "    ram_util_percent: 34.994470046082945\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034495750883344\n",
      "    mean_env_wait_ms: 33.81882093663966\n",
      "    mean_inference_ms: 0.9355200035277567\n",
      "    mean_raw_obs_processing_ms: 3.941503579992767\n",
      "  time_since_restore: 27413.852867126465\n",
      "  time_this_iter_s: 151.5538206100464\n",
      "  time_total_s: 27413.852867126465\n",
      "  timers:\n",
      "    learn_throughput: 1806.527\n",
      "    learn_time_ms: 2214.194\n",
      "    load_throughput: 274632.033\n",
      "    load_time_ms: 14.565\n",
      "    sample_throughput: 26.479\n",
      "    sample_time_ms: 151064.864\n",
      "    update_time_ms: 1.433\n",
      "  timestamp: 1611666603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-12-43\n",
      "  done: false\n",
      "  episode_len_mean: 384.39\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -19.758977255288215\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2333\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6293009519577026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01194019429385662\n",
      "        model: {}\n",
      "        policy_loss: -0.032768864184617996\n",
      "        total_loss: 789.809814453125\n",
      "        vf_explained_var: 0.4159635007381439\n",
      "        vf_loss: 789.8364868164062\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.42763157894737\n",
      "    ram_util_percent: 35.097807017543865\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034518332210708\n",
      "    mean_env_wait_ms: 33.8190620108709\n",
      "    mean_inference_ms: 0.9355389722197368\n",
      "    mean_raw_obs_processing_ms: 3.9368382382969607\n",
      "  time_since_restore: 27573.453204870224\n",
      "  time_this_iter_s: 159.60033774375916\n",
      "  time_total_s: 27573.453204870224\n",
      "  timers:\n",
      "    learn_throughput: 1759.585\n",
      "    learn_time_ms: 2273.264\n",
      "    load_throughput: 286600.909\n",
      "    load_time_ms: 13.957\n",
      "    sample_throughput: 26.324\n",
      "    sample_time_ms: 151952.93\n",
      "    update_time_ms: 1.441\n",
      "  timestamp: 1611666763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 381.53\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -19.774705601467954\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2344\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5707980394363403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011929419822990894\n",
      "        model: {}\n",
      "        policy_loss: -0.03163907304406166\n",
      "        total_loss: 364.8385925292969\n",
      "        vf_explained_var: 0.7132954001426697\n",
      "        vf_loss: 364.8642578125\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.13139013452916\n",
      "    ram_util_percent: 35.10582959641257\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345439213028924\n",
      "    mean_env_wait_ms: 33.81935776822546\n",
      "    mean_inference_ms: 0.9355592070575426\n",
      "    mean_raw_obs_processing_ms: 3.9317606735254107\n",
      "  time_since_restore: 27729.556223154068\n",
      "  time_this_iter_s: 156.103018283844\n",
      "  time_total_s: 27729.556223154068\n",
      "  timers:\n",
      "    learn_throughput: 1748.948\n",
      "    learn_time_ms: 2287.089\n",
      "    load_throughput: 316055.694\n",
      "    load_time_ms: 12.656\n",
      "    sample_throughput: 26.255\n",
      "    sample_time_ms: 152354.152\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1611666919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 386.2\n",
      "  episode_reward_max: 118.77471798744803\n",
      "  episode_reward_mean: -17.468388324456896\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2354\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5170271396636963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013422242365777493\n",
      "        model: {}\n",
      "        policy_loss: -0.029811717569828033\n",
      "        total_loss: 685.3043823242188\n",
      "        vf_explained_var: 0.44630685448646545\n",
      "        vf_loss: 685.3273315429688\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8743119266055\n",
      "    ram_util_percent: 35.04908256880734\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345598154049991\n",
      "    mean_env_wait_ms: 33.81908749201462\n",
      "    mean_inference_ms: 0.9355676735834336\n",
      "    mean_raw_obs_processing_ms: 3.9269997894106154\n",
      "  time_since_restore: 27882.536075115204\n",
      "  time_this_iter_s: 152.97985196113586\n",
      "  time_total_s: 27882.536075115204\n",
      "  timers:\n",
      "    learn_throughput: 1793.671\n",
      "    learn_time_ms: 2230.063\n",
      "    load_throughput: 283589.096\n",
      "    load_time_ms: 14.105\n",
      "    sample_throughput: 26.371\n",
      "    sample_time_ms: 151682.584\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1611667073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 393.63\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -19.574552176261708\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2363\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7219531536102295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010117704048752785\n",
      "        model: {}\n",
      "        policy_loss: -0.02550852671265602\n",
      "        total_loss: 348.2267761230469\n",
      "        vf_explained_var: 0.6720924377441406\n",
      "        vf_loss: 348.2471618652344\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53209302325583\n",
      "    ram_util_percent: 35.0\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345751652818493\n",
      "    mean_env_wait_ms: 33.81847372308753\n",
      "    mean_inference_ms: 0.9355779333109243\n",
      "    mean_raw_obs_processing_ms: 3.922380340917381\n",
      "  time_since_restore: 28032.88585138321\n",
      "  time_this_iter_s: 150.34977626800537\n",
      "  time_total_s: 28032.88585138321\n",
      "  timers:\n",
      "    learn_throughput: 1803.648\n",
      "    learn_time_ms: 2217.728\n",
      "    load_throughput: 268736.871\n",
      "    load_time_ms: 14.884\n",
      "    sample_throughput: 26.467\n",
      "    sample_time_ms: 151132.766\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1611667223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 391.97\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -17.32316836275865\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2374\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.4903260469436646\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010420231148600578\n",
      "        model: {}\n",
      "        policy_loss: -0.02943216823041439\n",
      "        total_loss: 347.51116943359375\n",
      "        vf_explained_var: 0.7402763962745667\n",
      "        vf_loss: 347.5353088378906\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.48904109589041\n",
      "    ram_util_percent: 35.01461187214612\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345973905279614\n",
      "    mean_env_wait_ms: 33.81746181586937\n",
      "    mean_inference_ms: 0.935588632859733\n",
      "    mean_raw_obs_processing_ms: 3.9165980694359144\n",
      "  time_since_restore: 28185.89177083969\n",
      "  time_this_iter_s: 153.00591945648193\n",
      "  time_total_s: 28185.89177083969\n",
      "  timers:\n",
      "    learn_throughput: 1802.728\n",
      "    learn_time_ms: 2218.86\n",
      "    load_throughput: 286795.9\n",
      "    load_time_ms: 13.947\n",
      "    sample_throughput: 26.519\n",
      "    sample_time_ms: 150837.44\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1611667376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-25-29\n",
      "  done: false\n",
      "  episode_len_mean: 391.65\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -23.709431216776316\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2385\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8245549201965332\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013306183740496635\n",
      "        model: {}\n",
      "        policy_loss: -0.033170055598020554\n",
      "        total_loss: 545.715576171875\n",
      "        vf_explained_var: 0.5247998833656311\n",
      "        vf_loss: 545.7420043945312\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.34587155963304\n",
      "    ram_util_percent: 35.0\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346149644036651\n",
      "    mean_env_wait_ms: 33.81635723087523\n",
      "    mean_inference_ms: 0.9355954326653042\n",
      "    mean_raw_obs_processing_ms: 3.9109126379927277\n",
      "  time_since_restore: 28338.68222284317\n",
      "  time_this_iter_s: 152.790452003479\n",
      "  time_total_s: 28338.68222284317\n",
      "  timers:\n",
      "    learn_throughput: 1799.031\n",
      "    learn_time_ms: 2223.419\n",
      "    load_throughput: 287527.759\n",
      "    load_time_ms: 13.912\n",
      "    sample_throughput: 26.529\n",
      "    sample_time_ms: 150776.837\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1611667529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-28-04\n",
      "  done: false\n",
      "  episode_len_mean: 385.77\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -30.136045858682984\n",
      "  episode_reward_min: -105.25143738951681\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2397\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6183775663375854\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012876336462795734\n",
      "        model: {}\n",
      "        policy_loss: -0.033639293164014816\n",
      "        total_loss: 366.1680908203125\n",
      "        vf_explained_var: 0.6938746571540833\n",
      "        vf_loss: 366.1951904296875\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.60227272727273\n",
      "    ram_util_percent: 35.02045454545455\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346334007246824\n",
      "    mean_env_wait_ms: 33.81516169580882\n",
      "    mean_inference_ms: 0.9356021016820066\n",
      "    mean_raw_obs_processing_ms: 3.9052720979856654\n",
      "  time_since_restore: 28492.84882235527\n",
      "  time_this_iter_s: 154.16659951210022\n",
      "  time_total_s: 28492.84882235527\n",
      "  timers:\n",
      "    learn_throughput: 1799.71\n",
      "    learn_time_ms: 2222.58\n",
      "    load_throughput: 269069.168\n",
      "    load_time_ms: 14.866\n",
      "    sample_throughput: 26.495\n",
      "    sample_time_ms: 150974.366\n",
      "    update_time_ms: 1.506\n",
      "  timestamp: 1611667684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-30-35\n",
      "  done: false\n",
      "  episode_len_mean: 384.82\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -28.242912980731326\n",
      "  episode_reward_min: -101.9805007655263\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2406\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6905261278152466\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008483183570206165\n",
      "        model: {}\n",
      "        policy_loss: -0.02219773270189762\n",
      "        total_loss: 391.7032775878906\n",
      "        vf_explained_var: 0.6678943037986755\n",
      "        vf_loss: 391.7211608886719\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57083333333334\n",
      "    ram_util_percent: 35.00092592592593\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346483016160082\n",
      "    mean_env_wait_ms: 33.81423721049197\n",
      "    mean_inference_ms: 0.9356116138612629\n",
      "    mean_raw_obs_processing_ms: 3.9009449348174314\n",
      "  time_since_restore: 28643.400097608566\n",
      "  time_this_iter_s: 150.5512752532959\n",
      "  time_total_s: 28643.400097608566\n",
      "  timers:\n",
      "    learn_throughput: 1798.447\n",
      "    learn_time_ms: 2224.141\n",
      "    load_throughput: 269068.736\n",
      "    load_time_ms: 14.866\n",
      "    sample_throughput: 26.523\n",
      "    sample_time_ms: 150812.199\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1611667835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-33-09\n",
      "  done: false\n",
      "  episode_len_mean: 379.46\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -34.40662376434072\n",
      "  episode_reward_min: -102.33342059427771\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2418\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6519771814346313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01088185515254736\n",
      "        model: {}\n",
      "        policy_loss: -0.029156487435102463\n",
      "        total_loss: 351.8611145019531\n",
      "        vf_explained_var: 0.7344253063201904\n",
      "        vf_loss: 351.884765625\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.58325791855202\n",
      "    ram_util_percent: 35.047963800904974\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346705630427185\n",
      "    mean_env_wait_ms: 33.81319598329597\n",
      "    mean_inference_ms: 0.9356254949200898\n",
      "    mean_raw_obs_processing_ms: 3.8958389504948774\n",
      "  time_since_restore: 28797.994354248047\n",
      "  time_this_iter_s: 154.5942566394806\n",
      "  time_total_s: 28797.994354248047\n",
      "  timers:\n",
      "    learn_throughput: 1796.333\n",
      "    learn_time_ms: 2226.758\n",
      "    load_throughput: 265957.582\n",
      "    load_time_ms: 15.04\n",
      "    sample_throughput: 26.46\n",
      "    sample_time_ms: 151168.974\n",
      "    update_time_ms: 1.449\n",
      "  timestamp: 1611667989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-35-42\n",
      "  done: false\n",
      "  episode_len_mean: 374.79\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -36.66553956501587\n",
      "  episode_reward_min: -102.33342059427771\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2429\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6521610021591187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01014857366681099\n",
      "        model: {}\n",
      "        policy_loss: -0.02455693855881691\n",
      "        total_loss: 452.6013488769531\n",
      "        vf_explained_var: 0.6333678960800171\n",
      "        vf_loss: 452.6207580566406\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.64285714285714\n",
      "    ram_util_percent: 34.93502304147464\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034676839004041\n",
      "    mean_env_wait_ms: 33.811638628022415\n",
      "    mean_inference_ms: 0.9356233886730982\n",
      "    mean_raw_obs_processing_ms: 3.8914464413186662\n",
      "  time_since_restore: 28950.307906866074\n",
      "  time_this_iter_s: 152.31355261802673\n",
      "  time_total_s: 28950.307906866074\n",
      "  timers:\n",
      "    learn_throughput: 1793.803\n",
      "    learn_time_ms: 2229.9\n",
      "    load_throughput: 252972.559\n",
      "    load_time_ms: 15.812\n",
      "    sample_throughput: 26.448\n",
      "    sample_time_ms: 151238.633\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1611668142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 373.56\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -42.847331295948116\n",
      "  episode_reward_min: -102.33342059427771\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2439\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7132138013839722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013542022556066513\n",
      "        model: {}\n",
      "        policy_loss: -0.028213832527399063\n",
      "        total_loss: 695.177734375\n",
      "        vf_explained_var: 0.3740282952785492\n",
      "        vf_loss: 695.19921875\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.63577981651376\n",
      "    ram_util_percent: 34.918807339449536\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346695594078355\n",
      "    mean_env_wait_ms: 33.80958792494909\n",
      "    mean_inference_ms: 0.9356075394758193\n",
      "    mean_raw_obs_processing_ms: 3.887431134699515\n",
      "  time_since_restore: 29102.364214658737\n",
      "  time_this_iter_s: 152.05630779266357\n",
      "  time_total_s: 29102.364214658737\n",
      "  timers:\n",
      "    learn_throughput: 1841.562\n",
      "    learn_time_ms: 2172.069\n",
      "    load_throughput: 239921.634\n",
      "    load_time_ms: 16.672\n",
      "    sample_throughput: 26.57\n",
      "    sample_time_ms: 150546.78\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1611668294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 375.52\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -40.66808460411086\n",
      "  episode_reward_min: -102.33342059427771\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2450\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5979443788528442\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010239888913929462\n",
      "        model: {}\n",
      "        policy_loss: -0.02076919935643673\n",
      "        total_loss: 504.8765563964844\n",
      "        vf_explained_var: 0.6071620583534241\n",
      "        vf_loss: 504.89215087890625\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.50504587155963\n",
      "    ram_util_percent: 34.900458715596315\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346603942470171\n",
      "    mean_env_wait_ms: 33.80701916165322\n",
      "    mean_inference_ms: 0.9355897922801714\n",
      "    mean_raw_obs_processing_ms: 3.8831478182921884\n",
      "  time_since_restore: 29255.208285331726\n",
      "  time_this_iter_s: 152.8440706729889\n",
      "  time_total_s: 29255.208285331726\n",
      "  timers:\n",
      "    learn_throughput: 1852.247\n",
      "    learn_time_ms: 2159.54\n",
      "    load_throughput: 224242.468\n",
      "    load_time_ms: 17.838\n",
      "    sample_throughput: 26.626\n",
      "    sample_time_ms: 150229.193\n",
      "    update_time_ms: 1.454\n",
      "  timestamp: 1611668447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 379.75\n",
      "  episode_reward_max: 118.77846724609945\n",
      "  episode_reward_mean: -38.73476281605566\n",
      "  episode_reward_min: -102.33342059427771\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2459\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7451729774475098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009505685418844223\n",
      "        model: {}\n",
      "        policy_loss: -0.028353547677397728\n",
      "        total_loss: 294.0326232910156\n",
      "        vf_explained_var: 0.7424744367599487\n",
      "        vf_loss: 294.0561828613281\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5939534883721\n",
      "    ram_util_percent: 34.921860465116275\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346496402934567\n",
      "    mean_env_wait_ms: 33.804861854746676\n",
      "    mean_inference_ms: 0.9355732387496367\n",
      "    mean_raw_obs_processing_ms: 3.87961847577689\n",
      "  time_since_restore: 29405.412881851196\n",
      "  time_this_iter_s: 150.20459651947021\n",
      "  time_total_s: 29405.412881851196\n",
      "  timers:\n",
      "    learn_throughput: 1853.592\n",
      "    learn_time_ms: 2157.973\n",
      "    load_throughput: 237955.456\n",
      "    load_time_ms: 16.81\n",
      "    sample_throughput: 26.675\n",
      "    sample_time_ms: 149953.289\n",
      "    update_time_ms: 1.404\n",
      "  timestamp: 1611668598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-45-49\n",
      "  done: false\n",
      "  episode_len_mean: 377.7\n",
      "  episode_reward_max: 118.77222062046573\n",
      "  episode_reward_mean: -40.91932974314862\n",
      "  episode_reward_min: -102.9730617872119\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2469\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6330214738845825\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014716536737978458\n",
      "        model: {}\n",
      "        policy_loss: -0.035358525812625885\n",
      "        total_loss: 373.5780944824219\n",
      "        vf_explained_var: 0.7109832763671875\n",
      "        vf_loss: 373.60601806640625\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57870370370371\n",
      "    ram_util_percent: 34.900462962962955\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346368541233998\n",
      "    mean_env_wait_ms: 33.80247153523964\n",
      "    mean_inference_ms: 0.9355546321792855\n",
      "    mean_raw_obs_processing_ms: 3.875724139510181\n",
      "  time_since_restore: 29556.737880706787\n",
      "  time_this_iter_s: 151.32499885559082\n",
      "  time_total_s: 29556.737880706787\n",
      "  timers:\n",
      "    learn_throughput: 1853.902\n",
      "    learn_time_ms: 2157.611\n",
      "    load_throughput: 248084.215\n",
      "    load_time_ms: 16.124\n",
      "    sample_throughput: 26.658\n",
      "    sample_time_ms: 150050.741\n",
      "    update_time_ms: 1.392\n",
      "  timestamp: 1611668749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 377.54\n",
      "  episode_reward_max: 118.77222062046573\n",
      "  episode_reward_mean: -36.68837379469508\n",
      "  episode_reward_min: -102.9730617872119\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2480\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7898037433624268\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01248551532626152\n",
      "        model: {}\n",
      "        policy_loss: -0.02795090153813362\n",
      "        total_loss: 600.681884765625\n",
      "        vf_explained_var: 0.5740993022918701\n",
      "        vf_loss: 600.7034912109375\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69216589861752\n",
      "    ram_util_percent: 34.920276497695845\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10346168907311636\n",
      "    mean_env_wait_ms: 33.79978631645759\n",
      "    mean_inference_ms: 0.9355341787587804\n",
      "    mean_raw_obs_processing_ms: 3.8714156011312686\n",
      "  time_since_restore: 29708.79607129097\n",
      "  time_this_iter_s: 152.05819058418274\n",
      "  time_total_s: 29708.79607129097\n",
      "  timers:\n",
      "    learn_throughput: 1853.36\n",
      "    learn_time_ms: 2158.242\n",
      "    load_throughput: 248393.48\n",
      "    load_time_ms: 16.103\n",
      "    sample_throughput: 26.675\n",
      "    sample_time_ms: 149955.562\n",
      "    update_time_ms: 1.374\n",
      "  timestamp: 1611668902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-50-54\n",
      "  done: false\n",
      "  episode_len_mean: 379.06\n",
      "  episode_reward_max: 118.77222062046573\n",
      "  episode_reward_mean: -38.75750461713104\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2490\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.845771074295044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015498933382332325\n",
      "        model: {}\n",
      "        policy_loss: -0.03169221058487892\n",
      "        total_loss: 564.5392456054688\n",
      "        vf_explained_var: 0.5054690837860107\n",
      "        vf_loss: 564.5631103515625\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.61336405529953\n",
      "    ram_util_percent: 34.89999999999999\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345958246882059\n",
      "    mean_env_wait_ms: 33.79735415740793\n",
      "    mean_inference_ms: 0.9355168205071933\n",
      "    mean_raw_obs_processing_ms: 3.8673099202067216\n",
      "  time_since_restore: 29860.706499814987\n",
      "  time_this_iter_s: 151.91042852401733\n",
      "  time_total_s: 29860.706499814987\n",
      "  timers:\n",
      "    learn_throughput: 1853.618\n",
      "    learn_time_ms: 2157.942\n",
      "    load_throughput: 264893.512\n",
      "    load_time_ms: 15.1\n",
      "    sample_throughput: 26.69\n",
      "    sample_time_ms: 149866.567\n",
      "    update_time_ms: 1.396\n",
      "  timestamp: 1611669054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-53-26\n",
      "  done: false\n",
      "  episode_len_mean: 380.02\n",
      "  episode_reward_max: 118.77222062046573\n",
      "  episode_reward_mean: -38.65214578572803\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6078321933746338\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013141529634594917\n",
      "        model: {}\n",
      "        policy_loss: -0.02827215939760208\n",
      "        total_loss: 371.12957763671875\n",
      "        vf_explained_var: 0.5666503310203552\n",
      "        vf_loss: 371.1511535644531\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.47385321100919\n",
      "    ram_util_percent: 34.91146788990825\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345737797968443\n",
      "    mean_env_wait_ms: 33.794900919878216\n",
      "    mean_inference_ms: 0.9354988308559748\n",
      "    mean_raw_obs_processing_ms: 3.863041842523115\n",
      "  time_since_restore: 30012.954414844513\n",
      "  time_this_iter_s: 152.24791502952576\n",
      "  time_total_s: 30012.954414844513\n",
      "  timers:\n",
      "    learn_throughput: 1823.819\n",
      "    learn_time_ms: 2193.201\n",
      "    load_throughput: 259925.294\n",
      "    load_time_ms: 15.389\n",
      "    sample_throughput: 26.732\n",
      "    sample_time_ms: 149635.442\n",
      "    update_time_ms: 1.43\n",
      "  timestamp: 1611669206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 381.18\n",
      "  episode_reward_max: 118.77222062046573\n",
      "  episode_reward_mean: -42.429502824242576\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2510\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.767256498336792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016531892120838165\n",
      "        model: {}\n",
      "        policy_loss: -0.030737929046154022\n",
      "        total_loss: 465.02301025390625\n",
      "        vf_explained_var: 0.5705326199531555\n",
      "        vf_loss: 465.04534912109375\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.5184331797235\n",
      "    ram_util_percent: 34.90506912442395\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345509154730599\n",
      "    mean_env_wait_ms: 33.792446820136206\n",
      "    mean_inference_ms: 0.9354737442706802\n",
      "    mean_raw_obs_processing_ms: 3.8588649365584033\n",
      "  time_since_restore: 30164.507142066956\n",
      "  time_this_iter_s: 151.55272722244263\n",
      "  time_total_s: 30164.507142066956\n",
      "  timers:\n",
      "    learn_throughput: 1823.313\n",
      "    learn_time_ms: 2193.808\n",
      "    load_throughput: 274906.986\n",
      "    load_time_ms: 14.55\n",
      "    sample_throughput: 26.714\n",
      "    sample_time_ms: 149734.586\n",
      "    update_time_ms: 1.412\n",
      "  timestamp: 1611669358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_14-58-31\n",
      "  done: false\n",
      "  episode_len_mean: 391.25\n",
      "  episode_reward_max: 118.76751556601259\n",
      "  episode_reward_mean: -48.636785985107196\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2520\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.824163794517517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011819428764283657\n",
      "        model: {}\n",
      "        policy_loss: -0.029813095927238464\n",
      "        total_loss: 223.80429077148438\n",
      "        vf_explained_var: 0.7585655450820923\n",
      "        vf_loss: 223.82809448242188\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.38486238532111\n",
      "    ram_util_percent: 34.91513761467889\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345312188058858\n",
      "    mean_env_wait_ms: 33.790005348309485\n",
      "    mean_inference_ms: 0.9354507355813001\n",
      "    mean_raw_obs_processing_ms: 3.854443764064227\n",
      "  time_since_restore: 30317.00974559784\n",
      "  time_this_iter_s: 152.5026035308838\n",
      "  time_total_s: 30317.00974559784\n",
      "  timers:\n",
      "    learn_throughput: 1825.423\n",
      "    learn_time_ms: 2191.273\n",
      "    load_throughput: 266184.177\n",
      "    load_time_ms: 15.027\n",
      "    sample_throughput: 26.751\n",
      "    sample_time_ms: 149526.561\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1611669511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 391.5\n",
      "  episode_reward_max: 118.76751556601259\n",
      "  episode_reward_mean: -40.387994475130945\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2531\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.792824387550354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012838895432651043\n",
      "        model: {}\n",
      "        policy_loss: -0.024537432938814163\n",
      "        total_loss: 572.2603149414062\n",
      "        vf_explained_var: 0.622509777545929\n",
      "        vf_loss: 572.2784423828125\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.60575221238939\n",
      "    ram_util_percent: 34.91991150442477\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034524271287178\n",
      "    mean_env_wait_ms: 33.7881894658005\n",
      "    mean_inference_ms: 0.9354442899809047\n",
      "    mean_raw_obs_processing_ms: 3.849648037457448\n",
      "  time_since_restore: 30475.742475748062\n",
      "  time_this_iter_s: 158.73273015022278\n",
      "  time_total_s: 30475.742475748062\n",
      "  timers:\n",
      "    learn_throughput: 1810.576\n",
      "    learn_time_ms: 2209.241\n",
      "    load_throughput: 289384.086\n",
      "    load_time_ms: 13.822\n",
      "    sample_throughput: 26.639\n",
      "    sample_time_ms: 150153.026\n",
      "    update_time_ms: 1.372\n",
      "  timestamp: 1611669670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-03-48\n",
      "  done: false\n",
      "  episode_len_mean: 388.16\n",
      "  episode_reward_max: 118.76751556601259\n",
      "  episode_reward_mean: -46.48544774058214\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 2543\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7435879707336426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00789868924766779\n",
      "        model: {}\n",
      "        policy_loss: -0.020463529974222183\n",
      "        total_loss: 481.4888610839844\n",
      "        vf_explained_var: 0.543531060218811\n",
      "        vf_loss: 481.50531005859375\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.084140969163\n",
      "    ram_util_percent: 34.9237885462555\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345189656944359\n",
      "    mean_env_wait_ms: 33.786841165211996\n",
      "    mean_inference_ms: 0.9354461981317225\n",
      "    mean_raw_obs_processing_ms: 3.8448043235309357\n",
      "  time_since_restore: 30634.093824625015\n",
      "  time_this_iter_s: 158.35134887695312\n",
      "  time_total_s: 30634.093824625015\n",
      "  timers:\n",
      "    learn_throughput: 1760.088\n",
      "    learn_time_ms: 2272.613\n",
      "    load_throughput: 319154.462\n",
      "    load_time_ms: 12.533\n",
      "    sample_throughput: 26.54\n",
      "    sample_time_ms: 150715.03\n",
      "    update_time_ms: 1.378\n",
      "  timestamp: 1611669828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 385.55\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -48.69330344184762\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2554\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8902513980865479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011506481096148491\n",
      "        model: {}\n",
      "        policy_loss: -0.02764335460960865\n",
      "        total_loss: 271.8599853515625\n",
      "        vf_explained_var: 0.7817303538322449\n",
      "        vf_loss: 271.88177490234375\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.45022831050228\n",
      "    ram_util_percent: 34.97853881278539\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345180712607276\n",
      "    mean_env_wait_ms: 33.78577180423595\n",
      "    mean_inference_ms: 0.9354531072737902\n",
      "    mean_raw_obs_processing_ms: 3.8405213596735446\n",
      "  time_since_restore: 30787.734164714813\n",
      "  time_this_iter_s: 153.64034008979797\n",
      "  time_total_s: 30787.734164714813\n",
      "  timers:\n",
      "    learn_throughput: 1758.202\n",
      "    learn_time_ms: 2275.051\n",
      "    load_throughput: 334722.91\n",
      "    load_time_ms: 11.95\n",
      "    sample_throughput: 26.526\n",
      "    sample_time_ms: 150793.834\n",
      "    update_time_ms: 1.401\n",
      "  timestamp: 1611669982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-08-56\n",
      "  done: false\n",
      "  episode_len_mean: 374.19\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -51.005655973837484\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 2565\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.016026258468628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012068577110767365\n",
      "        model: {}\n",
      "        policy_loss: -0.027809903025627136\n",
      "        total_loss: 453.0721130371094\n",
      "        vf_explained_var: 0.609210193157196\n",
      "        vf_loss: 453.0937805175781\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.43105022831051\n",
      "    ram_util_percent: 35.01004566210045\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345203213852998\n",
      "    mean_env_wait_ms: 33.78484513260021\n",
      "    mean_inference_ms: 0.9354640452067207\n",
      "    mean_raw_obs_processing_ms: 3.836678260787069\n",
      "  time_since_restore: 30940.940858602524\n",
      "  time_this_iter_s: 153.20669388771057\n",
      "  time_total_s: 30940.940858602524\n",
      "  timers:\n",
      "    learn_throughput: 1755.828\n",
      "    learn_time_ms: 2278.128\n",
      "    load_throughput: 336533.729\n",
      "    load_time_ms: 11.886\n",
      "    sample_throughput: 26.474\n",
      "    sample_time_ms: 151090.059\n",
      "    update_time_ms: 1.409\n",
      "  timestamp: 1611670136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-11-28\n",
      "  done: false\n",
      "  episode_len_mean: 379.79\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -50.67395048163702\n",
      "  episode_reward_min: -102.97456559545995\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2575\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8864610195159912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012699386104941368\n",
      "        model: {}\n",
      "        policy_loss: -0.0344255231320858\n",
      "        total_loss: 161.35731506347656\n",
      "        vf_explained_var: 0.8456730246543884\n",
      "        vf_loss: 161.3853302001953\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.44423963133642\n",
      "    ram_util_percent: 35.0\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345252051309017\n",
      "    mean_env_wait_ms: 33.784120500178446\n",
      "    mean_inference_ms: 0.9354798452144822\n",
      "    mean_raw_obs_processing_ms: 3.8331909006235336\n",
      "  time_since_restore: 31092.961750030518\n",
      "  time_this_iter_s: 152.02089142799377\n",
      "  time_total_s: 31092.961750030518\n",
      "  timers:\n",
      "    learn_throughput: 1755.642\n",
      "    learn_time_ms: 2278.369\n",
      "    load_throughput: 314854.004\n",
      "    load_time_ms: 12.704\n",
      "    sample_throughput: 26.462\n",
      "    sample_time_ms: 151160.016\n",
      "    update_time_ms: 1.41\n",
      "  timestamp: 1611670288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 390.97\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -52.73399773803939\n",
      "  episode_reward_min: -103.75011805795525\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 2584\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.093599319458008\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011702686548233032\n",
      "        model: {}\n",
      "        policy_loss: -0.03032788448035717\n",
      "        total_loss: 486.96038818359375\n",
      "        vf_explained_var: 0.5310864448547363\n",
      "        vf_loss: 486.9847412109375\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.35648148148148\n",
      "    ram_util_percent: 34.98611111111111\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345347079961285\n",
      "    mean_env_wait_ms: 33.783533793639315\n",
      "    mean_inference_ms: 0.9354986814424788\n",
      "    mean_raw_obs_processing_ms: 3.8298397393165455\n",
      "  time_since_restore: 31244.0710375309\n",
      "  time_this_iter_s: 151.10928750038147\n",
      "  time_total_s: 31244.0710375309\n",
      "  timers:\n",
      "    learn_throughput: 1754.336\n",
      "    learn_time_ms: 2280.066\n",
      "    load_throughput: 317774.377\n",
      "    load_time_ms: 12.588\n",
      "    sample_throughput: 26.479\n",
      "    sample_time_ms: 151064.31\n",
      "    update_time_ms: 1.405\n",
      "  timestamp: 1611670439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-16-31\n",
      "  done: false\n",
      "  episode_len_mean: 386.35\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -52.847346929239755\n",
      "  episode_reward_min: -103.75011805795525\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2594\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9817619323730469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01474989764392376\n",
      "        model: {}\n",
      "        policy_loss: -0.031137237325310707\n",
      "        total_loss: 458.06988525390625\n",
      "        vf_explained_var: 0.578584611415863\n",
      "        vf_loss: 458.0935974121094\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.57592592592592\n",
      "    ram_util_percent: 34.986111111111114\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345445141914647\n",
      "    mean_env_wait_ms: 33.78279366465508\n",
      "    mean_inference_ms: 0.935515185009406\n",
      "    mean_raw_obs_processing_ms: 3.826133036048574\n",
      "  time_since_restore: 31395.14123749733\n",
      "  time_this_iter_s: 151.07019996643066\n",
      "  time_total_s: 31395.14123749733\n",
      "  timers:\n",
      "    learn_throughput: 1756.135\n",
      "    learn_time_ms: 2277.73\n",
      "    load_throughput: 276291.416\n",
      "    load_time_ms: 14.477\n",
      "    sample_throughput: 26.493\n",
      "    sample_time_ms: 150984.244\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1611670591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 394.9\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -50.69618915132535\n",
      "  episode_reward_min: -103.75011805795525\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 2601\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0910069942474365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00941460207104683\n",
      "        model: {}\n",
      "        policy_loss: -0.019016293808817863\n",
      "        total_loss: 345.13885498046875\n",
      "        vf_explained_var: 0.5712634325027466\n",
      "        vf_loss: 345.153076171875\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.97630331753554\n",
      "    ram_util_percent: 35.00947867298578\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10345473947298178\n",
      "    mean_env_wait_ms: 33.782192018356845\n",
      "    mean_inference_ms: 0.9355223391519476\n",
      "    mean_raw_obs_processing_ms: 3.823179982162311\n",
      "  time_since_restore: 31542.82270717621\n",
      "  time_this_iter_s: 147.68146967887878\n",
      "  time_total_s: 31542.82270717621\n",
      "  timers:\n",
      "    learn_throughput: 1781.548\n",
      "    learn_time_ms: 2245.238\n",
      "    load_throughput: 285516.662\n",
      "    load_time_ms: 14.01\n",
      "    sample_throughput: 26.567\n",
      "    sample_time_ms: 150562.26\n",
      "    update_time_ms: 1.357\n",
      "  timestamp: 1611670739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 201\n",
      "  trial_id: e4643_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_e4643_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-21-31\n",
      "  done: false\n",
      "  episode_len_mean: 398.92\n",
      "  episode_reward_max: 118.77238846635454\n",
      "  episode_reward_mean: -53.11156626889481\n",
      "  episode_reward_min: -103.75011805795525\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2611\n",
      "  experiment_id: 4fd798fb8b014314b30afa9ee1f6a89e\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9561455249786377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01902627758681774\n",
      "        model: {}\n",
      "        policy_loss: -0.04284447804093361\n",
      "        total_loss: 446.2754821777344\n",
      "        vf_explained_var: 0.5338572263717651\n",
      "        vf_loss: 446.30865478515625\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.43824884792626\n",
      "    ram_util_percent: 35.0\n",
      "  pid: 3113369\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1034549918379659\n",
      "    mean_env_wait_ms: 33.78132142750553\n",
      "    mean_inference_ms: 0.9355358594585897\n",
      "    mean_raw_obs_processing_ms: 3.8190055464871726\n",
      "  time_since_restore: 31694.512301445007\n",
      "  time_this_iter_s: 151.68959426879883\n",
      "  time_total_s: 31694.512301445007\n",
      "  timers:\n",
      "    learn_throughput: 1782.039\n",
      "    learn_time_ms: 2244.619\n",
      "    load_throughput: 274630.235\n",
      "    load_time_ms: 14.565\n",
      "    sample_throughput: 26.564\n",
      "    sample_time_ms: 150577.204\n",
      "    update_time_ms: 1.411\n",
      "  timestamp: 1611670891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: e4643_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         230.598</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-98.3014</td><td style=\"text-align: right;\">            -92.7907</td><td style=\"text-align: right;\">            -101.983</td><td style=\"text-align: right;\">           53.8082</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         462.332</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-98.2902</td><td style=\"text-align: right;\">            -92.7286</td><td style=\"text-align: right;\">            -101.251</td><td style=\"text-align: right;\">             51.92</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         687.551</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-98.5091</td><td style=\"text-align: right;\">            -92.7444</td><td style=\"text-align: right;\">            -102.512</td><td style=\"text-align: right;\">             53.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         900.199</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-98.2404</td><td style=\"text-align: right;\">            -91.0479</td><td style=\"text-align: right;\">            -102.104</td><td style=\"text-align: right;\">             58.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1106.97</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-98.5189</td><td style=\"text-align: right;\">            -84.4644</td><td style=\"text-align: right;\">            -104.744</td><td style=\"text-align: right;\">              64.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         1298.86</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-98.5013</td><td style=\"text-align: right;\">            -84.4644</td><td style=\"text-align: right;\">            -105.041</td><td style=\"text-align: right;\">             76.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         1478.59</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-97.9081</td><td style=\"text-align: right;\">            -82.4376</td><td style=\"text-align: right;\">            -105.041</td><td style=\"text-align: right;\">             95.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1654.18</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-96.9987</td><td style=\"text-align: right;\">            -82.4376</td><td style=\"text-align: right;\">            -105.041</td><td style=\"text-align: right;\">            109.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         1827.85</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-96.0541</td><td style=\"text-align: right;\">            -82.4376</td><td style=\"text-align: right;\">             -105.04</td><td style=\"text-align: right;\">            123.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         1990.84</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-93.9328</td><td style=\"text-align: right;\">             114.331</td><td style=\"text-align: right;\">            -106.943</td><td style=\"text-align: right;\">            140.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         2156.12</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-93.5003</td><td style=\"text-align: right;\">             114.331</td><td style=\"text-align: right;\">            -106.943</td><td style=\"text-align: right;\">            148.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         2316.77</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-93.3671</td><td style=\"text-align: right;\">             114.331</td><td style=\"text-align: right;\">            -106.943</td><td style=\"text-align: right;\">            168.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         2474.88</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-91.0207</td><td style=\"text-align: right;\">             115.107</td><td style=\"text-align: right;\">            -106.943</td><td style=\"text-align: right;\">            186.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         2632.22</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-88.5531</td><td style=\"text-align: right;\">             115.575</td><td style=\"text-align: right;\">            -106.943</td><td style=\"text-align: right;\">            206.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         2794.87</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-85.7616</td><td style=\"text-align: right;\">             117.456</td><td style=\"text-align: right;\">            -106.817</td><td style=\"text-align: right;\">            209.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         2957.93</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-83.3965</td><td style=\"text-align: right;\">             117.456</td><td style=\"text-align: right;\">            -104.491</td><td style=\"text-align: right;\">            215.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         3116.41</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-80.6415</td><td style=\"text-align: right;\">             117.456</td><td style=\"text-align: right;\">            -104.491</td><td style=\"text-align: right;\">            227.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         3275.23</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-78.4942</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -104.491</td><td style=\"text-align: right;\">            224.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         3433.56</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-73.7879</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -104.491</td><td style=\"text-align: right;\">            227.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         3588.42</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-69.8819</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            233.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         3746.23</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-72.4675</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            237.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         3896.31</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-69.8619</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            256.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         4047.45</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-69.4806</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            276.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         4199.95</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-65.2419</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            281.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         4355.33</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-64.8136</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            286.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         4515.52</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">-63.1856</td><td style=\"text-align: right;\">             118.641</td><td style=\"text-align: right;\">            -107.184</td><td style=\"text-align: right;\">            287.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         4670.35</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-58.7781</td><td style=\"text-align: right;\">             118.641</td><td style=\"text-align: right;\">            -102.926</td><td style=\"text-align: right;\">            292.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         4822.05</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-58.5476</td><td style=\"text-align: right;\">             118.723</td><td style=\"text-align: right;\">            -102.926</td><td style=\"text-align: right;\">            299.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          4972.4</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">-57.8293</td><td style=\"text-align: right;\">             118.723</td><td style=\"text-align: right;\">            -102.926</td><td style=\"text-align: right;\">            317.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         5125.52</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-55.9527</td><td style=\"text-align: right;\">             118.723</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            310.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         5278.59</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-47.9396</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            303.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         5429.11</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">-49.9944</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            311.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         5580.23</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-47.6572</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            324.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">          5733.5</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -51.378</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            333.89</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         5885.29</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">-50.9427</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            347.57</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         6035.03</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-56.8328</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            354.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         6189.22</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-52.6281</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            352.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         6340.88</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">-46.8033</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -105.183</td><td style=\"text-align: right;\">            345.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         6494.36</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-42.5355</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            353.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         6644.08</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-50.3079</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">             358.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         6796.32</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-46.2927</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            350.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         6956.03</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -46.347</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            351.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         7121.19</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-44.0156</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            363.67</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         7276.55</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -37.738</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            361.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         7430.56</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-31.8462</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            347.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         7580.67</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-27.8184</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -102.185</td><td style=\"text-align: right;\">            357.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         7733.54</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-33.8391</td><td style=\"text-align: right;\">             118.736</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            355.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         7885.75</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-27.3578</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            359.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         8035.64</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-15.2319</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            354.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         8186.61</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-8.97491</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            359.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         8337.58</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -2.8408</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            356.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         8491.36</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> 3.43089</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            350.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         8641.89</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> 5.73355</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            353.69</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         8790.43</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> 12.0478</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            366.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         8943.41</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> 14.0954</td><td style=\"text-align: right;\">             118.759</td><td style=\"text-align: right;\">             -101.61</td><td style=\"text-align: right;\">            356.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         9097.36</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> 22.0783</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            356.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">          9248.1</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> 13.5969</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">             356.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         9398.02</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> 13.7725</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            356.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         9550.75</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> 17.9111</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">               357</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         9703.16</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> 15.6651</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            349.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         9855.07</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> 21.6597</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            358.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         10015.3</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 25.7002</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            357.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         10171.9</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> 21.4583</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            346.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         10327.1</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> 23.5653</td><td style=\"text-align: right;\">             118.774</td><td style=\"text-align: right;\">            -102.384</td><td style=\"text-align: right;\">            348.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         10479.6</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> 25.6925</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -100.784</td><td style=\"text-align: right;\">            345.67</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         10628.7</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> 36.2841</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -100.784</td><td style=\"text-align: right;\">            352.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         10781.6</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> 34.1922</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -101.709</td><td style=\"text-align: right;\">            353.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         10932.6</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\"> 36.2946</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -101.709</td><td style=\"text-align: right;\">            356.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         11084.1</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> 40.4249</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -101.709</td><td style=\"text-align: right;\">            360.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         11233.7</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> 40.5679</td><td style=\"text-align: right;\">              118.77</td><td style=\"text-align: right;\">            -101.709</td><td style=\"text-align: right;\">            364.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         11385.9</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> 34.4972</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            367.26</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         11538.1</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> 40.3517</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            370.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         11691.9</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> 42.4641</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            368.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         11859.3</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> 44.4966</td><td style=\"text-align: right;\">             118.769</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">             370.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         12014.2</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> 42.4283</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            371.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">           12172</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> 39.9928</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            368.26</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         12323.2</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> 42.0225</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            370.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         12475.9</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> 44.0518</td><td style=\"text-align: right;\">             118.765</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            372.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         12625.9</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> 41.8014</td><td style=\"text-align: right;\">             118.765</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            376.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         12778.7</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 47.5327</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.792</td><td style=\"text-align: right;\">            371.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         12932.8</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> 41.4506</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.061</td><td style=\"text-align: right;\">            362.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         13083.1</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> 41.4656</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.061</td><td style=\"text-align: right;\">            373.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         13235.7</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> 41.1839</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.162</td><td style=\"text-align: right;\">            380.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         13395.3</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 38.7821</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.401</td><td style=\"text-align: right;\">            370.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         13548.1</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> 36.3332</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.401</td><td style=\"text-align: right;\">            367.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         13699.7</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 36.4498</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.401</td><td style=\"text-align: right;\">            371.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         13851.3</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">  34.213</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            371.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">           14011</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> 34.1037</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            363.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         14181.9</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 38.4704</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            372.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         14356.5</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 34.1511</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            369.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         14526.3</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> 29.8321</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            356.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         14698.8</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> 34.4691</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            354.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">           14868</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 34.6555</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            355.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">           15035</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> 41.2964</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            367.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         15202.3</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 41.2298</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.835</td><td style=\"text-align: right;\">            361.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         15374.1</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 35.1362</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.402</td><td style=\"text-align: right;\">            358.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         15548.6</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 26.8248</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -101.214</td><td style=\"text-align: right;\">            347.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         15718.9</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> 22.8696</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -102.045</td><td style=\"text-align: right;\">            343.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         15892.4</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 22.9529</td><td style=\"text-align: right;\">             118.781</td><td style=\"text-align: right;\">            -102.045</td><td style=\"text-align: right;\">            341.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         16059.8</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 25.0705</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.045</td><td style=\"text-align: right;\">            345.38</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         16234.4</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> 26.7334</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">             -104.34</td><td style=\"text-align: right;\">            346.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">           16410</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> 9.71525</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">             -104.34</td><td style=\"text-align: right;\">            319.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         16575.1</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">-1.30267</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -109.226</td><td style=\"text-align: right;\">            321.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         16749.1</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">-3.96455</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -109.226</td><td style=\"text-align: right;\">            313.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         16900.1</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">-6.08838</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -109.226</td><td style=\"text-align: right;\">            319.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         17047.5</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">  -12.68</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -115.317</td><td style=\"text-align: right;\">             336.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         17208.3</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">   -19.7</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -115.558</td><td style=\"text-align: right;\">            355.92</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         17354.5</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">-20.0498</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            369.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         17499.7</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">-26.7414</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            407.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         17652.1</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\"> -35.452</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            401.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         17793.2</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">-37.7238</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">             419.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         17943.2</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">-50.2228</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            452.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         18098.4</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">-56.3905</td><td style=\"text-align: right;\">             118.766</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            454.26</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         18256.1</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">-51.7412</td><td style=\"text-align: right;\">             118.761</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            440.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         18418.5</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">-60.0445</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">             421.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         18569.2</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">-59.6262</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -116.202</td><td style=\"text-align: right;\">            385.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         18725.8</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">-63.1154</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -108.471</td><td style=\"text-align: right;\">            352.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         18872.3</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">-62.9368</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -108.471</td><td style=\"text-align: right;\">            326.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         19012.5</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-62.9368</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -108.471</td><td style=\"text-align: right;\">            326.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         19154.6</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">-63.3055</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -137.279</td><td style=\"text-align: right;\">            404.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         19295.2</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">-63.3055</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -137.279</td><td style=\"text-align: right;\">            404.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">           19435</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">-63.3055</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -137.279</td><td style=\"text-align: right;\">            404.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         19579.5</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\">-63.8853</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            532.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         19733.9</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">-59.4994</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            480.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         19885.4</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">-61.4417</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            493.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         20030.6</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">-58.9953</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            501.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         20170.8</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">-58.9953</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            501.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         20314.5</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">-61.3798</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            586.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         20455.8</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">-61.3762</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            588.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">           20605</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">-63.3841</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            649.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         20776.6</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">-60.8884</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            669.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         20944.9</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\">-58.1424</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            684.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         21109.2</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">-55.6343</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">             685.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         21266.4</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">-57.0177</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">             705.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         21424.1</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">-48.7464</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -151.122</td><td style=\"text-align: right;\">            707.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         21578.2</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">-49.4984</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -116.293</td><td style=\"text-align: right;\">             499.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">           21730</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">-51.4027</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -116.293</td><td style=\"text-align: right;\">            500.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         21881.9</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">-47.2626</td><td style=\"text-align: right;\">             118.776</td><td style=\"text-align: right;\">            -116.293</td><td style=\"text-align: right;\">            507.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         22033.1</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">-48.5879</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            364.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         22187.4</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">-52.6342</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            360.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         22336.8</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">-54.6764</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">             365.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         22490.6</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">-56.4827</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            364.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         22643.1</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">-50.4915</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            357.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         22795.6</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">-54.4369</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            365.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         22947.2</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> -54.527</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -106.056</td><td style=\"text-align: right;\">            372.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         23101.1</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">-50.1079</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -101.445</td><td style=\"text-align: right;\">            370.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         23255.5</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">-54.8496</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -101.445</td><td style=\"text-align: right;\">            354.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         23409.9</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">-53.2758</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -101.445</td><td style=\"text-align: right;\">            344.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         23560.8</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">-47.1173</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            353.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         23713.4</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">-43.3177</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">             350.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         23865.3</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> -43.509</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            353.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         24017.4</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">-47.6096</td><td style=\"text-align: right;\">             118.767</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            360.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">           24174</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">-50.1932</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            349.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         24325.9</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">-42.0788</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            348.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         24482.9</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">-42.1271</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            344.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         24637.7</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">-40.4301</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            337.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         24802.1</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">-36.4108</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -102.126</td><td style=\"text-align: right;\">            338.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         24958.3</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> -42.342</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -101.797</td><td style=\"text-align: right;\">             341.1</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         25113.4</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">-39.8449</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -101.797</td><td style=\"text-align: right;\">            345.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         25267.5</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">-40.0346</td><td style=\"text-align: right;\">             118.771</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">            338.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         25420.3</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">-40.1088</td><td style=\"text-align: right;\">             118.758</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">            344.67</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         25573.4</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">-43.9851</td><td style=\"text-align: right;\">             118.758</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">            347.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         25727.6</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">-43.6458</td><td style=\"text-align: right;\">             118.727</td><td style=\"text-align: right;\">            -101.311</td><td style=\"text-align: right;\">               348</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         25879.3</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">-39.3206</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -101.027</td><td style=\"text-align: right;\">            357.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         26029.3</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">-32.5666</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -101.027</td><td style=\"text-align: right;\">            371.32</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         26181.3</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">-38.8829</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -100.805</td><td style=\"text-align: right;\">             374.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         26341.6</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">-32.6125</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -100.805</td><td style=\"text-align: right;\">            378.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         26497.6</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">-32.9035</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -101.705</td><td style=\"text-align: right;\">            372.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         26653.6</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> -32.904</td><td style=\"text-align: right;\">             118.757</td><td style=\"text-align: right;\">            -101.705</td><td style=\"text-align: right;\">            369.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         26806.9</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">-28.5666</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.705</td><td style=\"text-align: right;\">            365.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         26959.1</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\">-24.3187</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -101.705</td><td style=\"text-align: right;\">            371.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         27111.3</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\">-18.0615</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            379.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         27262.3</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">-13.6635</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            385.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         27413.9</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">-18.0141</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            384.23</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         27573.5</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\"> -19.759</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            384.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         27729.6</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">-19.7747</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            381.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         27882.5</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\">-17.4684</td><td style=\"text-align: right;\">             118.775</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">             386.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         28032.9</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">-19.5746</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            393.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         28185.9</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">-17.3232</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            391.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         28338.7</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">-23.7094</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            391.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         28492.8</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\"> -30.136</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -105.251</td><td style=\"text-align: right;\">            385.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         28643.4</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">-28.2429</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -101.981</td><td style=\"text-align: right;\">            384.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">           28798</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">-34.4066</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.333</td><td style=\"text-align: right;\">            379.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         28950.3</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\">-36.6655</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.333</td><td style=\"text-align: right;\">            374.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         29102.4</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">-42.8473</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.333</td><td style=\"text-align: right;\">            373.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         29255.2</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\">-40.6681</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.333</td><td style=\"text-align: right;\">            375.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         29405.4</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">-38.7348</td><td style=\"text-align: right;\">             118.778</td><td style=\"text-align: right;\">            -102.333</td><td style=\"text-align: right;\">            379.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         29556.7</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\">-40.9193</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.973</td><td style=\"text-align: right;\">             377.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         29708.8</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">-36.6884</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.973</td><td style=\"text-align: right;\">            377.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         29860.7</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\">-38.7575</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            379.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">           30013</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\">-38.6521</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            380.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         30164.5</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\">-42.4295</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            381.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">           30317</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\">-48.6368</td><td style=\"text-align: right;\">             118.768</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            391.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         30475.7</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\"> -40.388</td><td style=\"text-align: right;\">             118.768</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">             391.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         30634.1</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\">-46.4854</td><td style=\"text-align: right;\">             118.768</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            388.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         30787.7</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">-48.6933</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            385.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         30940.9</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\">-51.0057</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            374.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">           31093</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\"> -50.674</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">            -102.975</td><td style=\"text-align: right;\">            379.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         31244.1</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> -52.734</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">             -103.75</td><td style=\"text-align: right;\">            390.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         31395.1</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">-52.8473</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">             -103.75</td><td style=\"text-align: right;\">            386.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         31542.8</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\">-50.6962</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">             -103.75</td><td style=\"text-align: right;\">             394.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.89 GiB heap, 0.0/5.13 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_06-32-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_e4643_00000</td><td>RUNNING </td><td>192.168.178.60:3113369</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         31694.5</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\">-53.1116</td><td style=\"text-align: right;\">             118.772</td><td style=\"text-align: right;\">             -103.75</td><td style=\"text-align: right;\">            398.92</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-15-bfa73d41f32f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtune\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPPOTrainer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_freq\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheckpoint_at_end\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/tune.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001B[0m\n\u001B[1;32m    417\u001B[0m     \u001B[0mtune_start\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    418\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mrunner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_finished\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 419\u001B[0;31m         \u001B[0mrunner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    420\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhas_verbosity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mVerbosity\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mV1_EXPERIMENT\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    421\u001B[0m             \u001B[0m_report_progress\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrunner\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprogress_reporter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    358\u001B[0m                     trial=next_trial)\n\u001B[1;32m    359\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrial_executor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_running_trials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_process_events\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# blocking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    361\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrial_executor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_no_available_trials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\u001B[0m in \u001B[0;36m_process_events\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    467\u001B[0m             \u001B[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    468\u001B[0m             \u001B[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 469\u001B[0;31m             \u001B[0mtrial\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrial_executor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_next_available_trial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# blocking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    470\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtrial\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_restoring\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mwarn_if_slow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"process_trial_restore\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\u001B[0m in \u001B[0;36mget_next_available_trial\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    470\u001B[0m         \u001B[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    471\u001B[0m         \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 472\u001B[0;31m         \u001B[0;34m[\u001B[0m\u001B[0mresult_id\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshuffled_results\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    473\u001B[0m         \u001B[0mwait_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    474\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mwait_time\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(object_refs, num_returns, timeout)\u001B[0m\n\u001B[1;32m   1507\u001B[0m         \u001B[0mtimeout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1508\u001B[0m         \u001B[0mtimeout_milliseconds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1509\u001B[0;31m         ready_ids, remaining_ids = worker.core_worker.wait(\n\u001B[0m\u001B[1;32m   1510\u001B[0m             \u001B[0mobject_refs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1511\u001B[0m             \u001B[0mnum_returns\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpython/ray/_raylet.pyx\u001B[0m in \u001B[0;36mray._raylet.CoreWorker.wait\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpython/ray/_raylet.pyx\u001B[0m in \u001B[0;36mray._raylet.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#ray.tune.run(PPOTrainer, config=config, stop=stop, checkpoint_freq=1, checkpoint_at_end=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m 2021-01-25 21:38:52,380\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3113370)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_1/checkpoint-1\n",
      "custom_metrics: {}\n",
      "date: 2021-01-25_21-42-24\n",
      "done: false\n",
      "episode_len_mean: 58.46268656716418\n",
      "episode_reward_max: -93.31922394531603\n",
      "episode_reward_mean: -98.34086626783368\n",
      "episode_reward_min: -101.87034592068791\n",
      "episodes_this_iter: 67\n",
      "episodes_total: 67\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4172453880310059\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009320477955043316\n",
      "      model: {}\n",
      "      policy_loss: -0.01237860880792141\n",
      "      total_loss: 4126.09814453125\n",
      "      vf_explained_var: -7.066034413583111e-06\n",
      "      vf_loss: 4126.10888671875\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 74.27737704918032\n",
      "  ram_util_percent: 28.134754098360652\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1013659739667134\n",
      "  mean_env_wait_ms: 32.12437007582744\n",
      "  mean_inference_ms: 0.935365962433952\n",
      "  mean_raw_obs_processing_ms: 19.462066362929676\n",
      "time_since_restore: 213.10421657562256\n",
      "time_this_iter_s: 213.10421657562256\n",
      "time_total_s: 213.10421657562256\n",
      "timers:\n",
      "  learn_throughput: 1748.558\n",
      "  learn_time_ms: 2287.599\n",
      "  load_throughput: 87497.085\n",
      "  load_time_ms: 45.716\n",
      "  sample_throughput: 18.993\n",
      "  sample_time_ms: 210604.195\n",
      "  update_time_ms: 1.913\n",
      "timestamp: 1611607344\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_6/checkpoint-6\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_11/checkpoint-11\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_16/checkpoint-16\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_21/checkpoint-21\n",
      "custom_metrics: {}\n",
      "date: 2021-01-25_22-36-54\n",
      "done: false\n",
      "episode_len_mean: 270.99\n",
      "episode_reward_max: 118.74325698574572\n",
      "episode_reward_mean: -47.736346869768205\n",
      "episode_reward_min: -104.8072543595016\n",
      "episodes_this_iter: 12\n",
      "episodes_total: 575\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.6840067505836487\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.010016297921538353\n",
      "      model: {}\n",
      "      policy_loss: -0.018736304715275764\n",
      "      total_loss: 347.1327819824219\n",
      "      vf_explained_var: 0.7663781642913818\n",
      "      vf_loss: 347.1495056152344\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.57772511848341\n",
      "  ram_util_percent: 28.203317535545022\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10109034222957365\n",
      "  mean_env_wait_ms: 31.891186696870026\n",
      "  mean_inference_ms: 0.9270505487317167\n",
      "  mean_raw_obs_processing_ms: 8.655301157704182\n",
      "time_since_restore: 3482.6791536808014\n",
      "time_this_iter_s: 148.10243201255798\n",
      "time_total_s: 3482.6791536808014\n",
      "timers:\n",
      "  learn_throughput: 1857.741\n",
      "  learn_time_ms: 2153.153\n",
      "  load_throughput: 295085.19\n",
      "  load_time_ms: 13.555\n",
      "  sample_throughput: 26.708\n",
      "  sample_time_ms: 149767.143\n",
      "  update_time_ms: 1.541\n",
      "timestamp: 1611610614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_26/checkpoint-26\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_31/checkpoint-31\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_36/checkpoint-36\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_41/checkpoint-41\n",
      "custom_metrics: {}\n",
      "date: 2021-01-25_23-26-10\n",
      "done: false\n",
      "episode_len_mean: 357.79\n",
      "episode_reward_max: 118.7648540306461\n",
      "episode_reward_mean: 14.083039833143475\n",
      "episode_reward_min: -99.65931811356016\n",
      "episodes_this_iter: 11\n",
      "episodes_total: 810\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.9825814366340637\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.018439771607518196\n",
      "      model: {}\n",
      "      policy_loss: -0.03294432908296585\n",
      "      total_loss: 508.6861877441406\n",
      "      vf_explained_var: 0.6448116898536682\n",
      "      vf_loss: 508.7154541015625\n",
      "  num_steps_sampled: 164000\n",
      "  num_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.62095238095237\n",
      "  ram_util_percent: 28.220952380952376\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10107567644802094\n",
      "  mean_env_wait_ms: 31.87989410361377\n",
      "  mean_inference_ms: 0.927975927952916\n",
      "  mean_raw_obs_processing_ms: 6.048313130581636\n",
      "time_since_restore: 6438.720455408096\n",
      "time_this_iter_s: 146.8975875377655\n",
      "time_total_s: 6438.720455408096\n",
      "timers:\n",
      "  learn_throughput: 1844.092\n",
      "  learn_time_ms: 2169.09\n",
      "  load_throughput: 324372.29\n",
      "  load_time_ms: 12.332\n",
      "  sample_throughput: 27.564\n",
      "  sample_time_ms: 145114.524\n",
      "  update_time_ms: 1.544\n",
      "timestamp: 1611613570\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_46/checkpoint-46\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_51/checkpoint-51\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_56/checkpoint-56\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_61/checkpoint-61\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_00-12-53\n",
      "done: false\n",
      "episode_len_mean: 803.92\n",
      "episode_reward_max: 118.74851956295045\n",
      "episode_reward_mean: -15.402254062623303\n",
      "episode_reward_min: -179.94012319807263\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 908\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.16875000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0640777349472046\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01774364337325096\n",
      "      model: {}\n",
      "      policy_loss: -0.02421625331044197\n",
      "      total_loss: 216.44921875\n",
      "      vf_explained_var: 0.7909051179885864\n",
      "      vf_loss: 216.47042846679688\n",
      "  num_steps_sampled: 244000\n",
      "  num_steps_trained: 244000\n",
      "iterations_since_restore: 61\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.85463414634147\n",
      "  ram_util_percent: 28.234146341463404\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10110492696375846\n",
      "  mean_env_wait_ms: 31.88132100360094\n",
      "  mean_inference_ms: 0.9280585723601029\n",
      "  mean_raw_obs_processing_ms: 5.126461350693721\n",
      "time_since_restore: 9241.682275772095\n",
      "time_this_iter_s: 143.7334427833557\n",
      "time_total_s: 9241.682275772095\n",
      "timers:\n",
      "  learn_throughput: 1791.978\n",
      "  learn_time_ms: 2232.17\n",
      "  load_throughput: 276988.872\n",
      "  load_time_ms: 14.441\n",
      "  sample_throughput: 29.478\n",
      "  sample_time_ms: 135692.838\n",
      "  update_time_ms: 1.615\n",
      "timestamp: 1611616373\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 244000\n",
      "training_iteration: 61\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_66/checkpoint-66\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_71/checkpoint-71\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_76/checkpoint-76\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_81/checkpoint-81\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_00-58-29\n",
      "done: false\n",
      "episode_len_mean: 1183.92\n",
      "episode_reward_max: 118.74851956295045\n",
      "episode_reward_mean: -26.781828840450213\n",
      "episode_reward_min: -179.94012319807263\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 947\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.25312501192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.2768357992172241\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.013970650732517242\n",
      "      model: {}\n",
      "      policy_loss: -0.021195651963353157\n",
      "      total_loss: 0.13764527440071106\n",
      "      vf_explained_var: -0.715692400932312\n",
      "      vf_loss: 0.15530461072921753\n",
      "  num_steps_sampled: 324000\n",
      "  num_steps_trained: 324000\n",
      "iterations_since_restore: 81\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 76.31041666666665\n",
      "  ram_util_percent: 28.15729166666667\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1011124105045533\n",
      "  mean_env_wait_ms: 31.885432515943286\n",
      "  mean_inference_ms: 0.9280686723209536\n",
      "  mean_raw_obs_processing_ms: 4.501587531424979\n",
      "time_since_restore: 11977.093272686005\n",
      "time_this_iter_s: 134.97593784332275\n",
      "time_total_s: 11977.093272686005\n",
      "timers:\n",
      "  learn_throughput: 1820.432\n",
      "  learn_time_ms: 2197.281\n",
      "  load_throughput: 236812.466\n",
      "  load_time_ms: 16.891\n",
      "  sample_throughput: 29.871\n",
      "  sample_time_ms: 133907.657\n",
      "  update_time_ms: 1.612\n",
      "timestamp: 1611619109\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 324000\n",
      "training_iteration: 81\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_86/checkpoint-86\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_91/checkpoint-91\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_96/checkpoint-96\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_101/checkpoint-101\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_01-45-54\n",
      "done: false\n",
      "episode_len_mean: 389.66\n",
      "episode_reward_max: 118.7314033922452\n",
      "episode_reward_mean: -27.571894394196182\n",
      "episode_reward_min: -101.93538853459667\n",
      "episodes_this_iter: 12\n",
      "episodes_total: 1083\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.37968748807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.250816822052002\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.018389098346233368\n",
      "      model: {}\n",
      "      policy_loss: -0.03216300532221794\n",
      "      total_loss: 600.997802734375\n",
      "      vf_explained_var: 0.5352338552474976\n",
      "      vf_loss: 601.02294921875\n",
      "  num_steps_sampled: 404000\n",
      "  num_steps_trained: 404000\n",
      "iterations_since_restore: 101\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.74123222748814\n",
      "  ram_util_percent: 28.181990521327013\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10111604133660218\n",
      "  mean_env_wait_ms: 31.89155679464068\n",
      "  mean_inference_ms: 0.9282905058977113\n",
      "  mean_raw_obs_processing_ms: 3.157247463875458\n",
      "time_since_restore: 14821.541270971298\n",
      "time_this_iter_s: 148.09461379051208\n",
      "time_total_s: 14821.541270971298\n",
      "timers:\n",
      "  learn_throughput: 1815.518\n",
      "  learn_time_ms: 2203.227\n",
      "  load_throughput: 256206.808\n",
      "  load_time_ms: 15.612\n",
      "  sample_throughput: 27.809\n",
      "  sample_time_ms: 143837.744\n",
      "  update_time_ms: 1.471\n",
      "timestamp: 1611621954\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 404000\n",
      "training_iteration: 101\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_106/checkpoint-106\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_111/checkpoint-111\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_116/checkpoint-116\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_121/checkpoint-121\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_02-32-57\n",
      "done: false\n",
      "episode_len_mean: 744.94\n",
      "episode_reward_max: 118.75928222500193\n",
      "episode_reward_mean: -46.68644552301048\n",
      "episode_reward_min: -119.78731626842395\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 1199\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.569531261920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.3110929727554321\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009327355772256851\n",
      "      model: {}\n",
      "      policy_loss: -0.029186686500906944\n",
      "      total_loss: 178.05006408691406\n",
      "      vf_explained_var: 0.6455972790718079\n",
      "      vf_loss: 178.07391357421875\n",
      "  num_steps_sampled: 484000\n",
      "  num_steps_trained: 484000\n",
      "iterations_since_restore: 121\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.67164179104478\n",
      "  ram_util_percent: 28.237313432835816\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10114334216016108\n",
      "  mean_env_wait_ms: 31.891516935330955\n",
      "  mean_inference_ms: 0.9283643021850303\n",
      "  mean_raw_obs_processing_ms: 3.1078460846921026\n",
      "time_since_restore: 17644.29953598976\n",
      "time_this_iter_s: 140.65821480751038\n",
      "time_total_s: 17644.29953598976\n",
      "timers:\n",
      "  learn_throughput: 1876.22\n",
      "  learn_time_ms: 2131.947\n",
      "  load_throughput: 282628.851\n",
      "  load_time_ms: 14.153\n",
      "  sample_throughput: 29.437\n",
      "  sample_time_ms: 135884.95\n",
      "  update_time_ms: 1.591\n",
      "timestamp: 1611624777\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 484000\n",
      "training_iteration: 121\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_126/checkpoint-126\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_131/checkpoint-131\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_136/checkpoint-136\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_141/checkpoint-141\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_03-18-20\n",
      "done: false\n",
      "episode_len_mean: 1209.8\n",
      "episode_reward_max: 118.75928222500193\n",
      "episode_reward_mean: -61.51939774610044\n",
      "episode_reward_min: -157.6537872901881\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 1226\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.32036131620407104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.0972133874893188\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01459091529250145\n",
      "      model: {}\n",
      "      policy_loss: -0.02553454600274563\n",
      "      total_loss: 0.09792113304138184\n",
      "      vf_explained_var: -0.46674296259880066\n",
      "      vf_loss: 0.11878131330013275\n",
      "  num_steps_sampled: 564000\n",
      "  num_steps_trained: 564000\n",
      "iterations_since_restore: 141\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.953125\n",
      "  ram_util_percent: 28.338020833333335\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10115094701409161\n",
      "  mean_env_wait_ms: 31.892216345013303\n",
      "  mean_inference_ms: 0.9284118302237383\n",
      "  mean_raw_obs_processing_ms: 3.006066271129056\n",
      "time_since_restore: 20367.592545986176\n",
      "time_this_iter_s: 134.40976428985596\n",
      "time_total_s: 20367.592545986176\n",
      "timers:\n",
      "  learn_throughput: 1820.599\n",
      "  learn_time_ms: 2197.08\n",
      "  load_throughput: 252568.117\n",
      "  load_time_ms: 15.837\n",
      "  sample_throughput: 29.886\n",
      "  sample_time_ms: 133841.724\n",
      "  update_time_ms: 1.574\n",
      "timestamp: 1611627500\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 564000\n",
      "training_iteration: 141\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_146/checkpoint-146\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_151/checkpoint-151\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_156/checkpoint-156\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_161/checkpoint-161\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_04-04-55\n",
      "done: false\n",
      "episode_len_mean: 1130.59\n",
      "episode_reward_max: 116.44352974985873\n",
      "episode_reward_mean: -90.11263834090259\n",
      "episode_reward_min: -157.3551838314612\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 1315\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.32036131620407104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.507287859916687\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01862277276813984\n",
      "      model: {}\n",
      "      policy_loss: -0.035342659801244736\n",
      "      total_loss: 138.2281036376953\n",
      "      vf_explained_var: 0.7899430990219116\n",
      "      vf_loss: 138.25747680664062\n",
      "  num_steps_sampled: 644000\n",
      "  num_steps_trained: 644000\n",
      "iterations_since_restore: 161\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.92439024390244\n",
      "  ram_util_percent: 28.3058536585366\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10117513920578451\n",
      "  mean_env_wait_ms: 31.89657240816765\n",
      "  mean_inference_ms: 0.9287515803467702\n",
      "  mean_raw_obs_processing_ms: 2.503083788820063\n",
      "time_since_restore: 23162.364614009857\n",
      "time_this_iter_s: 143.7098319530487\n",
      "time_total_s: 23162.364614009857\n",
      "timers:\n",
      "  learn_throughput: 1815.615\n",
      "  learn_time_ms: 2203.11\n",
      "  load_throughput: 308049.09\n",
      "  load_time_ms: 12.985\n",
      "  sample_throughput: 28.558\n",
      "  sample_time_ms: 140067.602\n",
      "  update_time_ms: 1.53\n",
      "timestamp: 1611630295\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 644000\n",
      "training_iteration: 161\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_166/checkpoint-166\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_171/checkpoint-171\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_176/checkpoint-176\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_181/checkpoint-181\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_04-51-29\n",
      "done: false\n",
      "episode_len_mean: 790.21\n",
      "episode_reward_max: -82.84774038877126\n",
      "episode_reward_mean: -91.3281655704292\n",
      "episode_reward_min: -116.01672175913104\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1412\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.48054200410842896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 1.4728329181671143\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00990846287459135\n",
      "      model: {}\n",
      "      policy_loss: -0.019732311367988586\n",
      "      total_loss: 33.857078552246094\n",
      "      vf_explained_var: 0.4740474224090576\n",
      "      vf_loss: 33.87205505371094\n",
      "  num_steps_sampled: 724000\n",
      "  num_steps_trained: 724000\n",
      "iterations_since_restore: 181\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 76.24766839378238\n",
      "  ram_util_percent: 28.300000000000008\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10113481769880611\n",
      "  mean_env_wait_ms: 31.890944493912073\n",
      "  mean_inference_ms: 0.9285242941559898\n",
      "  mean_raw_obs_processing_ms: 2.3839842417464476\n",
      "time_since_restore: 25955.767273426056\n",
      "time_this_iter_s: 135.38732862472534\n",
      "time_total_s: 25955.767273426056\n",
      "timers:\n",
      "  learn_throughput: 1813.291\n",
      "  learn_time_ms: 2205.934\n",
      "  load_throughput: 280606.13\n",
      "  load_time_ms: 14.255\n",
      "  sample_throughput: 29.054\n",
      "  sample_time_ms: 137676.933\n",
      "  update_time_ms: 1.652\n",
      "timestamp: 1611633089\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 724000\n",
      "training_iteration: 181\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_186/checkpoint-186\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_191/checkpoint-191\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_196/checkpoint-196\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_201/checkpoint-201\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_05-38-02\n",
      "done: false\n",
      "episode_len_mean: 844.13\n",
      "episode_reward_max: 117.06619506554905\n",
      "episode_reward_mean: -89.17074696831526\n",
      "episode_reward_min: -99.97004298863801\n",
      "episodes_this_iter: 8\n",
      "episodes_total: 1511\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.48054200410842896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.3564658164978027\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01591477356851101\n",
      "      model: {}\n",
      "      policy_loss: -0.03351357951760292\n",
      "      total_loss: 142.53472900390625\n",
      "      vf_explained_var: 0.7882806658744812\n",
      "      vf_loss: 142.56060791015625\n",
      "  num_steps_sampled: 804000\n",
      "  num_steps_trained: 804000\n",
      "iterations_since_restore: 201\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.95467980295567\n",
      "  ram_util_percent: 28.41428571428571\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10105973834874331\n",
      "  mean_env_wait_ms: 31.879322325410158\n",
      "  mean_inference_ms: 0.9279119774926617\n",
      "  mean_raw_obs_processing_ms: 2.289351972802628\n",
      "time_since_restore: 28748.200335025787\n",
      "time_this_iter_s: 142.40360236167908\n",
      "time_total_s: 28748.200335025787\n",
      "timers:\n",
      "  learn_throughput: 1844.217\n",
      "  learn_time_ms: 2168.942\n",
      "  load_throughput: 298858.272\n",
      "  load_time_ms: 13.384\n",
      "  sample_throughput: 29.113\n",
      "  sample_time_ms: 137396.352\n",
      "  update_time_ms: 1.553\n",
      "timestamp: 1611635882\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 804000\n",
      "training_iteration: 201\n",
      "\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_206/checkpoint-206\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_211/checkpoint-211\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_216/checkpoint-216\n",
      "checkpoint saved at /home/dschori/ray_results/PPO_ScoutingDiscreteTask_2021-01-25_21-38-42t2fe461o/checkpoint_221/checkpoint-221\n",
      "custom_metrics: {}\n",
      "date: 2021-01-26_06-26-13\n",
      "done: false\n",
      "episode_len_mean: 435.54\n",
      "episode_reward_max: 116.88198403651013\n",
      "episode_reward_mean: -89.313771836349\n",
      "episode_reward_min: -102.04348357159495\n",
      "episodes_this_iter: 9\n",
      "episodes_total: 1698\n",
      "experiment_id: 7fae3e80ca2a43bbb11958a087576346\n",
      "hostname: workstation\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.48054200410842896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.2570860385894775\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008593595586717129\n",
      "      model: {}\n",
      "      policy_loss: -0.02037789300084114\n",
      "      total_loss: 67.3127670288086\n",
      "      vf_explained_var: 0.8939911723136902\n",
      "      vf_loss: 67.32901763916016\n",
      "  num_steps_sampled: 884000\n",
      "  num_steps_trained: 884000\n",
      "iterations_since_restore: 221\n",
      "node_ip: 192.168.178.60\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.98585365853658\n",
      "  ram_util_percent: 28.412682926829266\n",
      "pid: 3113126\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.10090757191605441\n",
      "  mean_env_wait_ms: 31.864007423145935\n",
      "  mean_inference_ms: 0.9266233722065519\n",
      "  mean_raw_obs_processing_ms: 2.297770484705145\n",
      "time_since_restore: 31639.136588335037\n",
      "time_this_iter_s: 143.7863712310791\n",
      "time_total_s: 31639.136588335037\n",
      "timers:\n",
      "  learn_throughput: 1850.946\n",
      "  learn_time_ms: 2161.058\n",
      "  load_throughput: 246327.467\n",
      "  load_time_ms: 16.239\n",
      "  sample_throughput: 28.32\n",
      "  sample_time_ms: 141245.125\n",
      "  update_time_ms: 1.558\n",
      "timestamp: 1611638773\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 884000\n",
      "training_iteration: 221\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-49cdfa9648c0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m300\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mresults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    492\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mMAX_WORKER_FAILURE_RETRIES\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    493\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 494\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrainable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    495\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mRayError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    496\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"ignore_worker_failures\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    181\u001B[0m         \"\"\"\n\u001B[1;32m    182\u001B[0m         \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 183\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    184\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"step() needs to return a dict.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0;34m@\u001B[0m\u001B[0moverride\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTrainer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_exec_impl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    148\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    754\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    755\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_build_once\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 756\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilt_iterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    757\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    758\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__str__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_filter\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    841\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mCallable\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"LocalIterator[T]\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    842\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mapply_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 843\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    844\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_metrics_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_filter\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    841\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mCallable\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"LocalIterator[T]\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    842\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mapply_filter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 843\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    844\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_metrics_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_flatten\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    874\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"LocalIterator[T[0]]\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    875\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mapply_flatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 876\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    877\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    878\u001B[0m                     \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36madd_wait_hooks\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    826\u001B[0m                             \u001B[0mfn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_on_fetch_start\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    827\u001B[0m                         \u001B[0mnew_item\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 828\u001B[0;31m                     \u001B[0mitem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    829\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    830\u001B[0m                         \u001B[0mnew_item\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mapply_foreach\u001B[0;34m(it)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m             \u001B[0;32mdef\u001B[0m \u001B[0mapply_foreach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m                 \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_NextValueNotReady\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0mitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/util/iter.py\u001B[0m in \u001B[0;36mbase_iterator\u001B[0;34m(timeout)\u001B[0m\n\u001B[1;32m    469\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0mactive\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    470\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 471\u001B[0;31m                     \u001B[0;32myield\u001B[0m \u001B[0mray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfutures\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    472\u001B[0m                     \u001B[0mfutures\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpar_iter_next\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mremote\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mactive\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    473\u001B[0m                     \u001B[0;31m# Always yield after each round of gets with timeout.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(object_refs, timeout)\u001B[0m\n\u001B[1;32m   1369\u001B[0m         \u001B[0;32mglobal\u001B[0m \u001B[0mlast_task_error_raise_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1370\u001B[0m         \u001B[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1371\u001B[0;31m         values, debugger_breakpoint = worker.get_objects(\n\u001B[0m\u001B[1;32m   1372\u001B[0m             object_refs, timeout=timeout)\n\u001B[1;32m   1373\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001B[0m in \u001B[0;36mget_objects\u001B[0;34m(self, object_refs, timeout)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m         \u001B[0mtimeout_ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m         data_metadata_pairs = self.core_worker.get_objects(\n\u001B[0m\u001B[1;32m    304\u001B[0m             object_refs, self.current_task_id, timeout_ms)\n\u001B[1;32m    305\u001B[0m         \u001B[0mdebugger_breakpoint\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mb\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpython/ray/_raylet.pyx\u001B[0m in \u001B[0;36mray._raylet.CoreWorker.get_objects\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpython/ray/_raylet.pyx\u001B[0m in \u001B[0;36mray._raylet.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(300):\n",
    "    result = trainer.train()\n",
    "    results.append(result)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(pretty_print(result))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 8.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.36 GiB heap, 0.0/4.93 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_15-52-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_17c34_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m 2021-01-26 15:52:24,185\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m 2021-01-26 15:52:24,185\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m [ERROR] [1611672747.629200, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m [WARN] [1611672747.634996, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m [WARN] [1611672747.636399, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m 2021-01-26 15:52:33,573\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m 2021-01-26 15:52:34,692\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2021-01-26 15:56:32,466\tINFO tune.py:448 -- Total run time: 251.44 seconds (250.19 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1707211)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=1707213)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_17c34_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-01-26_15-56-31\n",
      "  done: true\n",
      "  episode_len_mean: 51.467532467532465\n",
      "  episode_reward_max: -93.34017255756284\n",
      "  episode_reward_mean: -98.97171318466\n",
      "  episode_reward_min: -101.41004032678605\n",
      "  episodes_this_iter: 77\n",
      "  episodes_total: 77\n",
      "  experiment_id: 84ed0f4990c941418572603a47229e4b\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3943806886672974\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00637952471151948\n",
      "        model: {}\n",
      "        policy_loss: -0.008917012251913548\n",
      "        total_loss: 4439.38427734375\n",
      "        vf_explained_var: 8.779187737673055e-06\n",
      "        vf_loss: 4439.39111328125\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.49441176470587\n",
      "    ram_util_percent: 32.09970588235294\n",
      "  pid: 1707211\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10501864670932483\n",
      "    mean_env_wait_ms: 35.38147195283546\n",
      "    mean_inference_ms: 0.9762364844923106\n",
      "    mean_raw_obs_processing_ms: 22.13980173713056\n",
      "  time_since_restore: 237.67810082435608\n",
      "  time_this_iter_s: 237.67810082435608\n",
      "  time_total_s: 237.67810082435608\n",
      "  timers:\n",
      "    learn_throughput: 1396.372\n",
      "    learn_time_ms: 2864.567\n",
      "    load_throughput: 85165.262\n",
      "    load_time_ms: 46.968\n",
      "    sample_throughput: 17.055\n",
      "    sample_time_ms: 234533.438\n",
      "    update_time_ms: 2.034\n",
      "  timestamp: 1611672991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 17c34_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/14.36 GiB heap, 0.0/4.93 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_15-52-21<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_17c34_00000</td><td>RUNNING </td><td>192.168.178.60:1707211</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         237.678</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-98.9717</td><td style=\"text-align: right;\">            -93.3402</td><td style=\"text-align: right;\">             -101.41</td><td style=\"text-align: right;\">           51.4675</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 10.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/14.36 GiB heap, 0.0/4.93 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-01-26_15-52-21<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_17c34_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         237.678</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-98.9717</td><td style=\"text-align: right;\">            -93.3402</td><td style=\"text-align: right;\">             -101.41</td><td style=\"text-align: right;\">           51.4675</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-26 15:58:28,089\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-01-26 15:58:28,090\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m [ERROR] [1611673111.620246, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m [WARN] [1611673111.628420, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m [WARN] [1611673111.629552, 0.000000]: END Init ControllersConnection\n",
      "2021-01-26 15:58:37,797\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-01-26 15:58:37,880\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-01-26_15-52-21/PPO_ScoutingDiscreteTask_17c34_00000_0_2021-01-26_15-52-21/checkpoint_1/checkpoint-1\n",
      "2021-01-26 15:58:37,881\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 237.67810082435608, '_episodes_total': 77}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=1707218)\u001B[0m None\n"
     ]
    },
    {
     "data": {
      "text/plain": "-100.93780072577"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = load(checkpoint_path=checkpoint_path, config=config)\n",
    "\n",
    "episode_reward = test(agent=agent, env=env)\n",
    "episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f1f49343cd0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}