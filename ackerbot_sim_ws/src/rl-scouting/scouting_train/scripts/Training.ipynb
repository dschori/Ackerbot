{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1613973981.772365, 177.975000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1613973981.801449, 0.025000]: Start Init ControllersConnection\n",
      "[WARN] [1613973981.803610, 0.025000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0.0, 1.0, (84, 84, 4), float32)\n",
      "Action Space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(\"Observation Space: {}\".format(env.observation_space))\n",
    "print(\"Action Space: {}\".format(env.action_space))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Environment State"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1):\n",
    "    obs, _, _, _ = env.step(action=2)\n",
    "plt.imshow(obs[:, :, 0])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ray Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1,\n",
    "    \"model\": {\"dim\": 84,\n",
    "              \"conv_filters\":\n",
    "                  [[16, [3, 3], 2], [32, [3, 3], 2], [64, [3, 3], 2], [128, [11, 11], 1]]}\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 6000,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 07:06:34,878\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-02-22_07-06-34_285720_5535/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-02-22_07-06-34_285720_5535/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-02-22_07-06-34_285720_5535',\n 'metrics_export_port': 62341,\n 'node_id': 'fdcae062471f2f41b9da0a875b6587b229cdaa52'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward\n",
    "\n",
    "def test_traj(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    positions = []\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        positions.append(info['position'])\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward, positions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m 2021-02-20 14:08:50,275\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m 2021-02-20 14:08:50,275\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m [ERROR] [1613826533.817069, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m [WARN] [1613826533.820390, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m [WARN] [1613826533.821599, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m 2021-02-20 14:09:02,828\tINFO trainable.py:99 -- Trainable.setup took 12.554 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m 2021-02-20 14:09:02,828\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m 2021-02-20 14:09:04,010\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "2021-02-21 19:20:56,027\tINFO tune.py:448 -- Total run time: 105129.44 seconds (105127.92 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=84297)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=84296)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_14-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 42.774193548387096\n",
      "  episode_reward_max: 118.3770226650548\n",
      "  episode_reward_mean: -93.49711607437895\n",
      "  episode_reward_min: -108.06729370123062\n",
      "  episodes_this_iter: 93\n",
      "  episodes_total: 93\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0761983394622803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02286132611334324\n",
      "        model: {}\n",
      "        policy_loss: -0.07078494131565094\n",
      "        total_loss: 4563.99365234375\n",
      "        vf_explained_var: 0.00946330837905407\n",
      "        vf_loss: 4564.0595703125\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.48023952095808\n",
      "    ram_util_percent: 37.813532934131736\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07197494001514879\n",
      "    mean_env_wait_ms: 112.34553633139029\n",
      "    mean_inference_ms: 1.7809661082463453\n",
      "    mean_raw_obs_processing_ms: 27.807813261604643\n",
      "  time_since_restore: 584.9675097465515\n",
      "  time_this_iter_s: 584.9675097465515\n",
      "  time_total_s: 584.9675097465515\n",
      "  timers:\n",
      "    learn_throughput: 253.426\n",
      "    learn_time_ms: 15783.684\n",
      "    load_throughput: 7741.232\n",
      "    load_time_ms: 516.714\n",
      "    sample_throughput: 7.037\n",
      "    sample_time_ms: 568442.875\n",
      "    update_time_ms: 3.667\n",
      "  timestamp: 1613827127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_14-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 45.28\n",
      "  episode_reward_max: 114.28859343326474\n",
      "  episode_reward_mean: -92.27542371741615\n",
      "  episode_reward_min: -108.3686866748935\n",
      "  episodes_this_iter: 87\n",
      "  episodes_total: 180\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0438920259475708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023191144689917564\n",
      "        model: {}\n",
      "        policy_loss: -0.07785279303789139\n",
      "        total_loss: 3480.796875\n",
      "        vf_explained_var: 0.046449340879917145\n",
      "        vf_loss: 3480.867431640625\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.84639303482587\n",
      "    ram_util_percent: 40.14216417910448\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07122021483620367\n",
      "    mean_env_wait_ms: 110.89191863652708\n",
      "    mean_inference_ms: 1.7778514251688329\n",
      "    mean_raw_obs_processing_ms: 26.994923029883047\n",
      "  time_since_restore: 1148.2525837421417\n",
      "  time_this_iter_s: 563.2850739955902\n",
      "  time_total_s: 1148.2525837421417\n",
      "  timers:\n",
      "    learn_throughput: 257.011\n",
      "    learn_time_ms: 15563.538\n",
      "    load_throughput: 8988.19\n",
      "    load_time_ms: 445.028\n",
      "    sample_throughput: 7.17\n",
      "    sample_time_ms: 557910.93\n",
      "    update_time_ms: 3.753\n",
      "  timestamp: 1613827691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_14-38-14\n",
      "  done: false\n",
      "  episode_len_mean: 54.83\n",
      "  episode_reward_max: 118.27492096385373\n",
      "  episode_reward_mean: -83.1376979845979\n",
      "  episode_reward_min: -108.48955659836412\n",
      "  episodes_this_iter: 66\n",
      "  episodes_total: 246\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.0064120292663574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02330060675740242\n",
      "        model: {}\n",
      "        policy_loss: -0.10005206614732742\n",
      "        total_loss: 2177.936767578125\n",
      "        vf_explained_var: 0.1864335983991623\n",
      "        vf_loss: 2178.0263671875\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.02032520325203\n",
      "    ram_util_percent: 41.75830429732869\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07286805882756153\n",
      "    mean_env_wait_ms: 113.7639370369599\n",
      "    mean_inference_ms: 1.8338129633350764\n",
      "    mean_raw_obs_processing_ms: 25.34804746344035\n",
      "  time_since_restore: 1751.5004198551178\n",
      "  time_this_iter_s: 603.2478361129761\n",
      "  time_total_s: 1751.5004198551178\n",
      "  timers:\n",
      "    learn_throughput: 258.053\n",
      "    learn_time_ms: 15500.72\n",
      "    load_throughput: 9547.432\n",
      "    load_time_ms: 418.961\n",
      "    sample_throughput: 7.046\n",
      "    sample_time_ms: 567723.594\n",
      "    update_time_ms: 3.715\n",
      "  timestamp: 1613828294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_14-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 68.51\n",
      "  episode_reward_max: 118.31157546697884\n",
      "  episode_reward_mean: -76.31402308357566\n",
      "  episode_reward_min: -108.48955659836412\n",
      "  episodes_this_iter: 52\n",
      "  episodes_total: 298\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9740287661552429\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019892195239663124\n",
      "        model: {}\n",
      "        policy_loss: -0.10140877962112427\n",
      "        total_loss: 1925.67626953125\n",
      "        vf_explained_var: 0.3578816056251526\n",
      "        vf_loss: 1925.76416015625\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.986577181208055\n",
      "    ram_util_percent: 42.26724832214765\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07324381118410192\n",
      "    mean_env_wait_ms: 114.54295829627914\n",
      "    mean_inference_ms: 1.8481601318124876\n",
      "    mean_raw_obs_processing_ms: 23.396152967037246\n",
      "  time_since_restore: 2273.680137872696\n",
      "  time_this_iter_s: 522.1797180175781\n",
      "  time_total_s: 2273.680137872696\n",
      "  timers:\n",
      "    learn_throughput: 258.695\n",
      "    learn_time_ms: 15462.215\n",
      "    load_throughput: 9614.248\n",
      "    load_time_ms: 416.049\n",
      "    sample_throughput: 7.242\n",
      "    sample_time_ms: 552365.006\n",
      "    update_time_ms: 3.703\n",
      "  timestamp: 1613828816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_14-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 80.5\n",
      "  episode_reward_max: 118.31157546697884\n",
      "  episode_reward_mean: -74.5112076002168\n",
      "  episode_reward_min: -107.92826443871176\n",
      "  episodes_this_iter: 47\n",
      "  episodes_total: 345\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9424479007720947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02245231159031391\n",
      "        model: {}\n",
      "        policy_loss: -0.11653962731361389\n",
      "        total_loss: 1288.709228515625\n",
      "        vf_explained_var: 0.3869043290615082\n",
      "        vf_loss: 1288.8104248046875\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.72577181208053\n",
      "    ram_util_percent: 42.22281879194631\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07249784922092387\n",
      "    mean_env_wait_ms: 113.50241287673661\n",
      "    mean_inference_ms: 1.828461425537306\n",
      "    mean_raw_obs_processing_ms: 21.573286487015874\n",
      "  time_since_restore: 2795.7982223033905\n",
      "  time_this_iter_s: 522.1180844306946\n",
      "  time_total_s: 2795.7982223033905\n",
      "  timers:\n",
      "    learn_throughput: 259.098\n",
      "    learn_time_ms: 15438.197\n",
      "    load_throughput: 9892.16\n",
      "    load_time_ms: 404.361\n",
      "    sample_throughput: 7.365\n",
      "    sample_time_ms: 543144.949\n",
      "    update_time_ms: 3.796\n",
      "  timestamp: 1613829338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 82.55\n",
      "  episode_reward_max: 118.2661259262627\n",
      "  episode_reward_mean: -74.3606947192576\n",
      "  episode_reward_min: -106.65765904648111\n",
      "  episodes_this_iter: 49\n",
      "  episodes_total: 394\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.9189553260803223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017654838040471077\n",
      "        model: {}\n",
      "        policy_loss: -0.11349598318338394\n",
      "        total_loss: 1229.2088623046875\n",
      "        vf_explained_var: 0.5053565502166748\n",
      "        vf_loss: 1229.3043212890625\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.80053547523427\n",
      "    ram_util_percent: 42.14631860776439\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07204796356766235\n",
      "    mean_env_wait_ms: 112.94424999099478\n",
      "    mean_inference_ms: 1.816813155558226\n",
      "    mean_raw_obs_processing_ms: 20.25822482921732\n",
      "  time_since_restore: 3319.5827116966248\n",
      "  time_this_iter_s: 523.7844893932343\n",
      "  time_total_s: 3319.5827116966248\n",
      "  timers:\n",
      "    learn_throughput: 259.328\n",
      "    learn_time_ms: 15424.463\n",
      "    load_throughput: 9852.188\n",
      "    load_time_ms: 406.001\n",
      "    sample_throughput: 7.445\n",
      "    sample_time_ms: 537267.036\n",
      "    update_time_ms: 3.745\n",
      "  timestamp: 1613829862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 87.35\n",
      "  episode_reward_max: 118.30713776466433\n",
      "  episode_reward_mean: -63.523381138135264\n",
      "  episode_reward_min: -106.50578201134127\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 435\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8848854899406433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02142045460641384\n",
      "        model: {}\n",
      "        policy_loss: -0.13100093603134155\n",
      "        total_loss: 1744.0987548828125\n",
      "        vf_explained_var: 0.4920734167098999\n",
      "        vf_loss: 1744.2078857421875\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.57983651226158\n",
      "    ram_util_percent: 42.138555858310625\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07175735216903688\n",
      "    mean_env_wait_ms: 112.58858042984367\n",
      "    mean_inference_ms: 1.8089444376977242\n",
      "    mean_raw_obs_processing_ms: 19.35233802635811\n",
      "  time_since_restore: 3833.387715578079\n",
      "  time_this_iter_s: 513.8050038814545\n",
      "  time_total_s: 3833.387715578079\n",
      "  timers:\n",
      "    learn_throughput: 259.526\n",
      "    learn_time_ms: 15412.732\n",
      "    load_throughput: 9930.622\n",
      "    load_time_ms: 402.794\n",
      "    sample_throughput: 7.524\n",
      "    sample_time_ms: 531642.14\n",
      "    update_time_ms: 3.693\n",
      "  timestamp: 1613830376\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 94.72\n",
      "  episode_reward_max: 118.39888024524822\n",
      "  episode_reward_mean: -59.63567158117983\n",
      "  episode_reward_min: -107.17238065761742\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 475\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.893488347530365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014194161631166935\n",
      "        model: {}\n",
      "        policy_loss: -0.11310682445764542\n",
      "        total_loss: 1079.6142578125\n",
      "        vf_explained_var: 0.5432910919189453\n",
      "        vf_loss: 1079.7060546875\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.78260273972602\n",
      "    ram_util_percent: 42.12671232876713\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07151093329723217\n",
      "    mean_env_wait_ms: 112.28665160518358\n",
      "    mean_inference_ms: 1.8033442706119143\n",
      "    mean_raw_obs_processing_ms: 18.502838213535505\n",
      "  time_since_restore: 4344.969002723694\n",
      "  time_this_iter_s: 511.5812871456146\n",
      "  time_total_s: 4344.969002723694\n",
      "  timers:\n",
      "    learn_throughput: 259.672\n",
      "    learn_time_ms: 15404.019\n",
      "    load_throughput: 10033.785\n",
      "    load_time_ms: 398.653\n",
      "    sample_throughput: 7.588\n",
      "    sample_time_ms: 527152.073\n",
      "    update_time_ms: 3.679\n",
      "  timestamp: 1613830888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-30-01\n",
      "  done: false\n",
      "  episode_len_mean: 99.61\n",
      "  episode_reward_max: 118.39888024524822\n",
      "  episode_reward_mean: -61.824132908364724\n",
      "  episode_reward_min: -107.17238065761742\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 514\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8881871104240417\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014235087670385838\n",
      "        model: {}\n",
      "        policy_loss: -0.11436835676431656\n",
      "        total_loss: 903.1644897460938\n",
      "        vf_explained_var: 0.5682339668273926\n",
      "        vf_loss: 903.2572631835938\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.68016415868673\n",
      "    ram_util_percent: 42.191928864569086\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07130826269094545\n",
      "    mean_env_wait_ms: 112.04481898841567\n",
      "    mean_inference_ms: 1.7991444980096603\n",
      "    mean_raw_obs_processing_ms: 17.715990003541425\n",
      "  time_since_restore: 4857.632814407349\n",
      "  time_this_iter_s: 512.6638116836548\n",
      "  time_total_s: 4857.632814407349\n",
      "  timers:\n",
      "    learn_throughput: 259.793\n",
      "    learn_time_ms: 15396.888\n",
      "    load_throughput: 10105.322\n",
      "    load_time_ms: 395.831\n",
      "    sample_throughput: 7.637\n",
      "    sample_time_ms: 523776.26\n",
      "    update_time_ms: 3.693\n",
      "  timestamp: 1613831401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 105.53\n",
      "  episode_reward_max: 118.39888024524822\n",
      "  episode_reward_mean: -57.32057116780643\n",
      "  episode_reward_min: -107.17238065761742\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 551\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8375014662742615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016036443412303925\n",
      "        model: {}\n",
      "        policy_loss: -0.12444860488176346\n",
      "        total_loss: 1020.59912109375\n",
      "        vf_explained_var: 0.6986218690872192\n",
      "        vf_loss: 1020.6992797851562\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.7113854595336\n",
      "    ram_util_percent: 42.184636488340196\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0711633401790614\n",
      "    mean_env_wait_ms: 111.87942778208779\n",
      "    mean_inference_ms: 1.796204792466029\n",
      "    mean_raw_obs_processing_ms: 17.068492334631227\n",
      "  time_since_restore: 5368.266561508179\n",
      "  time_this_iter_s: 510.6337471008301\n",
      "  time_total_s: 5368.266561508179\n",
      "  timers:\n",
      "    learn_throughput: 259.868\n",
      "    learn_time_ms: 15392.406\n",
      "    load_throughput: 10161.363\n",
      "    load_time_ms: 393.648\n",
      "    sample_throughput: 7.679\n",
      "    sample_time_ms: 520870.662\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1613831911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-46-55\n",
      "  done: false\n",
      "  episode_len_mean: 107.74\n",
      "  episode_reward_max: 118.37779062602058\n",
      "  episode_reward_mean: -44.2649893294039\n",
      "  episode_reward_min: -106.6639466464975\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 584\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.8471274971961975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017741737887263298\n",
      "        model: {}\n",
      "        policy_loss: -0.13405659794807434\n",
      "        total_loss: 949.0671997070312\n",
      "        vf_explained_var: 0.7107579708099365\n",
      "        vf_loss: 949.1742553710938\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.71237830319888\n",
      "    ram_util_percent: 42.22141863699583\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07104209578305797\n",
      "    mean_env_wait_ms: 111.75151112031331\n",
      "    mean_inference_ms: 1.7937164648929005\n",
      "    mean_raw_obs_processing_ms: 16.50943981550422\n",
      "  time_since_restore: 5871.953165054321\n",
      "  time_this_iter_s: 503.6866035461426\n",
      "  time_total_s: 5871.953165054321\n",
      "  timers:\n",
      "    learn_throughput: 260.637\n",
      "    learn_time_ms: 15347.012\n",
      "    load_throughput: 10523.864\n",
      "    load_time_ms: 380.089\n",
      "    sample_throughput: 7.8\n",
      "    sample_time_ms: 512807.791\n",
      "    update_time_ms: 3.673\n",
      "  timestamp: 1613832415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_15-55-26\n",
      "  done: false\n",
      "  episode_len_mean: 112.6\n",
      "  episode_reward_max: 118.37779062602058\n",
      "  episode_reward_mean: -37.80436355311525\n",
      "  episode_reward_min: -107.50796406036544\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 620\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.833489179611206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01759730838239193\n",
      "        model: {}\n",
      "        policy_loss: -0.13340024650096893\n",
      "        total_loss: 1048.4493408203125\n",
      "        vf_explained_var: 0.6537086367607117\n",
      "        vf_loss: 1048.555908203125\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.61522633744856\n",
      "    ram_util_percent: 42.17366255144033\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07093759001518152\n",
      "    mean_env_wait_ms: 111.64461784042756\n",
      "    mean_inference_ms: 1.7913958009287085\n",
      "    mean_raw_obs_processing_ms: 15.962087600789314\n",
      "  time_since_restore: 6383.151445388794\n",
      "  time_this_iter_s: 511.19828033447266\n",
      "  time_total_s: 6383.151445388794\n",
      "  timers:\n",
      "    learn_throughput: 260.664\n",
      "    learn_time_ms: 15345.431\n",
      "    load_throughput: 10541.219\n",
      "    load_time_ms: 379.463\n",
      "    sample_throughput: 7.88\n",
      "    sample_time_ms: 507605.385\n",
      "    update_time_ms: 3.876\n",
      "  timestamp: 1613832926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-03-50\n",
      "  done: false\n",
      "  episode_len_mean: 117.99\n",
      "  episode_reward_max: 118.32474575086673\n",
      "  episode_reward_mean: -27.58161138389983\n",
      "  episode_reward_min: -107.50796406036544\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 652\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7921907305717468\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01740110106766224\n",
      "        model: {}\n",
      "        policy_loss: -0.13286353647708893\n",
      "        total_loss: 986.4422607421875\n",
      "        vf_explained_var: 0.7495697736740112\n",
      "        vf_loss: 986.5487670898438\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.74923504867871\n",
      "    ram_util_percent: 42.19179415855354\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0708550657451176\n",
      "    mean_env_wait_ms: 111.5553781191144\n",
      "    mean_inference_ms: 1.7895062860252369\n",
      "    mean_raw_obs_processing_ms: 15.49720560298924\n",
      "  time_since_restore: 6886.85665345192\n",
      "  time_this_iter_s: 503.7052080631256\n",
      "  time_total_s: 6886.85665345192\n",
      "  timers:\n",
      "    learn_throughput: 260.747\n",
      "    learn_time_ms: 15340.552\n",
      "    load_throughput: 10429.202\n",
      "    load_time_ms: 383.538\n",
      "    sample_throughput: 8.038\n",
      "    sample_time_ms: 497648.979\n",
      "    update_time_ms: 3.965\n",
      "  timestamp: 1613833430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 114.17\n",
      "  episode_reward_max: 118.34763032056395\n",
      "  episode_reward_mean: -27.616167549093642\n",
      "  episode_reward_min: -107.50796406036544\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 691\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7926270961761475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016526449471712112\n",
      "        model: {}\n",
      "        policy_loss: -0.12517505884170532\n",
      "        total_loss: 1409.384521484375\n",
      "        vf_explained_var: 0.6078544855117798\n",
      "        vf_loss: 1409.4847412109375\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.75342465753425\n",
      "    ram_util_percent: 42.14630136986302\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07078901034130622\n",
      "    mean_env_wait_ms: 111.47776106342957\n",
      "    mean_inference_ms: 1.7877140804958083\n",
      "    mean_raw_obs_processing_ms: 15.09152646575455\n",
      "  time_since_restore: 7398.7250781059265\n",
      "  time_this_iter_s: 511.86842465400696\n",
      "  time_total_s: 7398.7250781059265\n",
      "  timers:\n",
      "    learn_throughput: 260.776\n",
      "    learn_time_ms: 15338.848\n",
      "    load_throughput: 10544.393\n",
      "    load_time_ms: 379.349\n",
      "    sample_throughput: 8.054\n",
      "    sample_time_ms: 496622.122\n",
      "    update_time_ms: 3.964\n",
      "  timestamp: 1613833942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-20-50\n",
      "  done: false\n",
      "  episode_len_mean: 112.11\n",
      "  episode_reward_max: 118.35346512980267\n",
      "  episode_reward_mean: -20.962442561296843\n",
      "  episode_reward_min: -107.14378950829905\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 725\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7813454866409302\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017482509836554527\n",
      "        model: {}\n",
      "        policy_loss: -0.1358514428138733\n",
      "        total_loss: 1851.878173828125\n",
      "        vf_explained_var: 0.5138440132141113\n",
      "        vf_loss: 1851.9873046875\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.659862068965516\n",
      "    ram_util_percent: 42.23862068965517\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07073730141570324\n",
      "    mean_env_wait_ms: 111.41417937432274\n",
      "    mean_inference_ms: 1.7862244263998557\n",
      "    mean_raw_obs_processing_ms: 14.776521986543223\n",
      "  time_since_restore: 7906.858117580414\n",
      "  time_this_iter_s: 508.1330394744873\n",
      "  time_total_s: 7906.858117580414\n",
      "  timers:\n",
      "    learn_throughput: 260.787\n",
      "    learn_time_ms: 15338.184\n",
      "    load_throughput: 10444.494\n",
      "    load_time_ms: 382.977\n",
      "    sample_throughput: 8.077\n",
      "    sample_time_ms: 495219.611\n",
      "    update_time_ms: 3.916\n",
      "  timestamp: 1613834450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 109.96\n",
      "  episode_reward_max: 118.35346512980267\n",
      "  episode_reward_mean: -18.75715464557466\n",
      "  episode_reward_min: -108.64998050586729\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 761\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7650612592697144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01819213666021824\n",
      "        model: {}\n",
      "        policy_loss: -0.13488051295280457\n",
      "        total_loss: 1805.126708984375\n",
      "        vf_explained_var: 0.504750669002533\n",
      "        vf_loss: 1805.23388671875\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.68347107438016\n",
      "    ram_util_percent: 42.208953168044076\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07069342223139513\n",
      "    mean_env_wait_ms: 111.3599021075677\n",
      "    mean_inference_ms: 1.7847514919566214\n",
      "    mean_raw_obs_processing_ms: 14.519880312636424\n",
      "  time_since_restore: 8415.256796121597\n",
      "  time_this_iter_s: 508.3986785411835\n",
      "  time_total_s: 8415.256796121597\n",
      "  timers:\n",
      "    learn_throughput: 260.81\n",
      "    learn_time_ms: 15336.824\n",
      "    load_throughput: 10583.22\n",
      "    load_time_ms: 377.957\n",
      "    sample_throughput: 8.102\n",
      "    sample_time_ms: 493684.154\n",
      "    update_time_ms: 4.014\n",
      "  timestamp: 1613834959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 120.49\n",
      "  episode_reward_max: 118.3939902776051\n",
      "  episode_reward_mean: -14.285936706308428\n",
      "  episode_reward_min: -108.64998050586729\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 791\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7496641874313354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019364701583981514\n",
      "        model: {}\n",
      "        policy_loss: -0.13387431204319\n",
      "        total_loss: 1098.99560546875\n",
      "        vf_explained_var: 0.661304235458374\n",
      "        vf_loss: 1099.0999755859375\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.60835654596101\n",
      "    ram_util_percent: 42.167270194986074\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07065488171285099\n",
      "    mean_env_wait_ms: 111.3241307552929\n",
      "    mean_inference_ms: 1.7835782570333196\n",
      "    mean_raw_obs_processing_ms: 14.269352176899874\n",
      "  time_since_restore: 8918.76670885086\n",
      "  time_this_iter_s: 503.5099127292633\n",
      "  time_total_s: 8918.76670885086\n",
      "  timers:\n",
      "    learn_throughput: 260.854\n",
      "    learn_time_ms: 15334.221\n",
      "    load_throughput: 10626.154\n",
      "    load_time_ms: 376.43\n",
      "    sample_throughput: 8.119\n",
      "    sample_time_ms: 492661.291\n",
      "    update_time_ms: 4.04\n",
      "  timestamp: 1613835462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 123.46\n",
      "  episode_reward_max: 118.3939902776051\n",
      "  episode_reward_mean: -16.785158096241034\n",
      "  episode_reward_min: -108.64998050586729\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 822\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7540880441665649\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019368959590792656\n",
      "        model: {}\n",
      "        policy_loss: -0.14248614013195038\n",
      "        total_loss: 1216.87744140625\n",
      "        vf_explained_var: 0.6238676905632019\n",
      "        vf_loss: 1216.9906005859375\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.7082402234637\n",
      "    ram_util_percent: 42.18617318435754\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07060540628940473\n",
      "    mean_env_wait_ms: 111.28075957730394\n",
      "    mean_inference_ms: 1.7825459579854874\n",
      "    mean_raw_obs_processing_ms: 14.023184899113176\n",
      "  time_since_restore: 9420.529680490494\n",
      "  time_this_iter_s: 501.7629716396332\n",
      "  time_total_s: 9420.529680490494\n",
      "  timers:\n",
      "    learn_throughput: 260.88\n",
      "    learn_time_ms: 15332.692\n",
      "    load_throughput: 10536.835\n",
      "    load_time_ms: 379.621\n",
      "    sample_throughput: 8.135\n",
      "    sample_time_ms: 491674.687\n",
      "    update_time_ms: 4.023\n",
      "  timestamp: 1613835964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_16-54-28\n",
      "  done: false\n",
      "  episode_len_mean: 125.66\n",
      "  episode_reward_max: 118.3939902776051\n",
      "  episode_reward_mean: -16.635151349911744\n",
      "  episode_reward_min: -107.60329027263762\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 855\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7352050542831421\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01947021298110485\n",
      "        model: {}\n",
      "        policy_loss: -0.1371580809354782\n",
      "        total_loss: 1364.107177734375\n",
      "        vf_explained_var: 0.6220147013664246\n",
      "        vf_loss: 1364.2147216796875\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.838942976356044\n",
      "    ram_util_percent: 42.21223922114048\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07055628986635981\n",
      "    mean_env_wait_ms: 111.23097080172302\n",
      "    mean_inference_ms: 1.7815564425989334\n",
      "    mean_raw_obs_processing_ms: 13.76615522826784\n",
      "  time_since_restore: 9924.060046195984\n",
      "  time_this_iter_s: 503.5303657054901\n",
      "  time_total_s: 9924.060046195984\n",
      "  timers:\n",
      "    learn_throughput: 260.866\n",
      "    learn_time_ms: 15333.563\n",
      "    load_throughput: 10489.54\n",
      "    load_time_ms: 381.332\n",
      "    sample_throughput: 8.151\n",
      "    sample_time_ms: 490761.917\n",
      "    update_time_ms: 3.993\n",
      "  timestamp: 1613836468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-02-55\n",
      "  done: false\n",
      "  episode_len_mean: 122.28\n",
      "  episode_reward_max: 118.3939902776051\n",
      "  episode_reward_mean: -19.05427828901017\n",
      "  episode_reward_min: -107.60329027263762\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 888\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.7261008024215698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02093971148133278\n",
      "        model: {}\n",
      "        policy_loss: -0.13702255487442017\n",
      "        total_loss: 1568.1337890625\n",
      "        vf_explained_var: 0.5397771596908569\n",
      "        vf_loss: 1568.239013671875\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.70828729281767\n",
      "    ram_util_percent: 42.19530386740332\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07051613655744564\n",
      "    mean_env_wait_ms: 111.18505007432478\n",
      "    mean_inference_ms: 1.780629552901428\n",
      "    mean_raw_obs_processing_ms: 13.540986767124712\n",
      "  time_since_restore: 10431.394457101822\n",
      "  time_this_iter_s: 507.334410905838\n",
      "  time_total_s: 10431.394457101822\n",
      "  timers:\n",
      "    learn_throughput: 260.875\n",
      "    learn_time_ms: 15333.029\n",
      "    load_throughput: 10507.372\n",
      "    load_time_ms: 380.685\n",
      "    sample_throughput: 8.156\n",
      "    sample_time_ms: 490433.243\n",
      "    update_time_ms: 3.988\n",
      "  timestamp: 1613836975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 120.9\n",
      "  episode_reward_max: 118.38878743679915\n",
      "  episode_reward_mean: -12.310829991559114\n",
      "  episode_reward_min: -107.60329027263762\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 920\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.70824134349823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01423901692032814\n",
      "        model: {}\n",
      "        policy_loss: -0.12159629911184311\n",
      "        total_loss: 1377.1143798828125\n",
      "        vf_explained_var: 0.5997397303581238\n",
      "        vf_loss: 1377.2037353515625\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.57621696801112\n",
      "    ram_util_percent: 42.23449235048678\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07049318642704192\n",
      "    mean_env_wait_ms: 111.14931717979958\n",
      "    mean_inference_ms: 1.7797591977790392\n",
      "    mean_raw_obs_processing_ms: 13.358750470278427\n",
      "  time_since_restore: 10935.541148424149\n",
      "  time_this_iter_s: 504.14669132232666\n",
      "  time_total_s: 10935.541148424149\n",
      "  timers:\n",
      "    learn_throughput: 260.831\n",
      "    learn_time_ms: 15335.624\n",
      "    load_throughput: 10536.602\n",
      "    load_time_ms: 379.629\n",
      "    sample_throughput: 8.155\n",
      "    sample_time_ms: 490478.487\n",
      "    update_time_ms: 3.98\n",
      "  timestamp: 1613837479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-19-46\n",
      "  done: false\n",
      "  episode_len_mean: 119.66\n",
      "  episode_reward_max: 118.38878743679915\n",
      "  episode_reward_mean: -5.866658288115945\n",
      "  episode_reward_min: -106.86675580611211\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 955\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6930056810379028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01464045513421297\n",
      "        model: {}\n",
      "        policy_loss: -0.12477900087833405\n",
      "        total_loss: 1798.8236083984375\n",
      "        vf_explained_var: 0.5352810025215149\n",
      "        vf_loss: 1798.9150390625\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.71424619640386\n",
      "    ram_util_percent: 42.19128630705394\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0704745226511796\n",
      "    mean_env_wait_ms: 111.11939499798135\n",
      "    mean_inference_ms: 1.7788941975198371\n",
      "    mean_raw_obs_processing_ms: 13.19077736985796\n",
      "  time_since_restore: 11442.23236489296\n",
      "  time_this_iter_s: 506.69121646881104\n",
      "  time_total_s: 11442.23236489296\n",
      "  timers:\n",
      "    learn_throughput: 260.819\n",
      "    learn_time_ms: 15336.297\n",
      "    load_throughput: 10483.409\n",
      "    load_time_ms: 381.555\n",
      "    sample_throughput: 8.163\n",
      "    sample_time_ms: 490023.271\n",
      "    update_time_ms: 3.734\n",
      "  timestamp: 1613837986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-28-08\n",
      "  done: false\n",
      "  episode_len_mean: 124.01\n",
      "  episode_reward_max: 118.38878743679915\n",
      "  episode_reward_mean: 6.998789734124133\n",
      "  episode_reward_min: -106.7017113793944\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 985\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6945086121559143\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014527100138366222\n",
      "        model: {}\n",
      "        policy_loss: -0.12096773833036423\n",
      "        total_loss: 1275.7939453125\n",
      "        vf_explained_var: 0.6098142862319946\n",
      "        vf_loss: 1275.8819580078125\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.636541143654114\n",
      "    ram_util_percent: 42.231659693165966\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07045896523118514\n",
      "    mean_env_wait_ms: 111.09149651191204\n",
      "    mean_inference_ms: 1.778265931415495\n",
      "    mean_raw_obs_processing_ms: 13.050278562975482\n",
      "  time_since_restore: 11944.09179854393\n",
      "  time_this_iter_s: 501.85943365097046\n",
      "  time_total_s: 11944.09179854393\n",
      "  timers:\n",
      "    learn_throughput: 260.827\n",
      "    learn_time_ms: 15335.839\n",
      "    load_throughput: 10589.766\n",
      "    load_time_ms: 377.723\n",
      "    sample_throughput: 8.166\n",
      "    sample_time_ms: 489846.579\n",
      "    update_time_ms: 3.607\n",
      "  timestamp: 1613838488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 123.87\n",
      "  episode_reward_max: 118.3395831387293\n",
      "  episode_reward_mean: 15.293911476606176\n",
      "  episode_reward_min: -106.7017113793944\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1018\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6948832273483276\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01476839929819107\n",
      "        model: {}\n",
      "        policy_loss: -0.12513484060764313\n",
      "        total_loss: 1115.804931640625\n",
      "        vf_explained_var: 0.6886588931083679\n",
      "        vf_loss: 1115.8966064453125\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.62991689750693\n",
      "    ram_util_percent: 42.211634349030476\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07044008192298663\n",
      "    mean_env_wait_ms: 111.06321753615448\n",
      "    mean_inference_ms: 1.7776164476564702\n",
      "    mean_raw_obs_processing_ms: 12.909237082108405\n",
      "  time_since_restore: 12450.006348371506\n",
      "  time_this_iter_s: 505.9145498275757\n",
      "  time_total_s: 12450.006348371506\n",
      "  timers:\n",
      "    learn_throughput: 260.756\n",
      "    learn_time_ms: 15340.008\n",
      "    load_throughput: 10459.126\n",
      "    load_time_ms: 382.441\n",
      "    sample_throughput: 8.176\n",
      "    sample_time_ms: 489242.181\n",
      "    update_time_ms: 3.58\n",
      "  timestamp: 1613838994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-44-58\n",
      "  done: false\n",
      "  episode_len_mean: 126.73\n",
      "  episode_reward_max: 118.37376180931558\n",
      "  episode_reward_mean: 30.233502197167073\n",
      "  episode_reward_min: -105.22484963962395\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1049\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6816383600234985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015095611102879047\n",
      "        model: {}\n",
      "        policy_loss: -0.1279529482126236\n",
      "        total_loss: 1255.4622802734375\n",
      "        vf_explained_var: 0.6161641478538513\n",
      "        vf_loss: 1255.5557861328125\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.64283727399165\n",
      "    ram_util_percent: 42.207788595271204\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0704241765388529\n",
      "    mean_env_wait_ms: 111.04518944599339\n",
      "    mean_inference_ms: 1.7770956349540592\n",
      "    mean_raw_obs_processing_ms: 12.775610119884405\n",
      "  time_since_restore: 12954.356755018234\n",
      "  time_this_iter_s: 504.3504066467285\n",
      "  time_total_s: 12954.356755018234\n",
      "  timers:\n",
      "    learn_throughput: 260.752\n",
      "    learn_time_ms: 15340.229\n",
      "    load_throughput: 10501.392\n",
      "    load_time_ms: 380.902\n",
      "    sample_throughput: 8.182\n",
      "    sample_time_ms: 488865.351\n",
      "    update_time_ms: 3.546\n",
      "  timestamp: 1613839498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_17-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 125.61\n",
      "  episode_reward_max: 118.37376180931558\n",
      "  episode_reward_mean: 23.559431281202528\n",
      "  episode_reward_min: -105.22484963962395\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1082\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6820441484451294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01384813617914915\n",
      "        model: {}\n",
      "        policy_loss: -0.11520286649465561\n",
      "        total_loss: 1187.2332763671875\n",
      "        vf_explained_var: 0.6505010724067688\n",
      "        vf_loss: 1187.31689453125\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.7009735744089\n",
      "    ram_util_percent: 42.20639777468706\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07040890131315722\n",
      "    mean_env_wait_ms: 111.020601745741\n",
      "    mean_inference_ms: 1.77661679011248\n",
      "    mean_raw_obs_processing_ms: 12.648610157034309\n",
      "  time_since_restore: 13457.7450299263\n",
      "  time_this_iter_s: 503.3882749080658\n",
      "  time_total_s: 13457.7450299263\n",
      "  timers:\n",
      "    learn_throughput: 260.752\n",
      "    learn_time_ms: 15340.253\n",
      "    load_throughput: 10496.954\n",
      "    load_time_ms: 381.063\n",
      "    sample_throughput: 8.191\n",
      "    sample_time_ms: 488365.09\n",
      "    update_time_ms: 3.474\n",
      "  timestamp: 1613840002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 127.16\n",
      "  episode_reward_max: 118.37376180931558\n",
      "  episode_reward_mean: 36.48697008197477\n",
      "  episode_reward_min: -105.02049054797493\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1112\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6504806280136108\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012804980389773846\n",
      "        model: {}\n",
      "        policy_loss: -0.11266468465328217\n",
      "        total_loss: 1185.9178466796875\n",
      "        vf_explained_var: 0.542258083820343\n",
      "        vf_loss: 1186.001220703125\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.65815899581591\n",
      "    ram_util_percent: 42.21924686192469\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07039732274589429\n",
      "    mean_env_wait_ms: 111.00094092547454\n",
      "    mean_inference_ms: 1.7761994000854702\n",
      "    mean_raw_obs_processing_ms: 12.53645227464082\n",
      "  time_since_restore: 13960.061575174332\n",
      "  time_this_iter_s: 502.3165452480316\n",
      "  time_total_s: 13960.061575174332\n",
      "  timers:\n",
      "    learn_throughput: 260.719\n",
      "    learn_time_ms: 15342.191\n",
      "    load_throughput: 10401.78\n",
      "    load_time_ms: 384.55\n",
      "    sample_throughput: 8.193\n",
      "    sample_time_ms: 488238.812\n",
      "    update_time_ms: 3.457\n",
      "  timestamp: 1613840504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 131.44\n",
      "  episode_reward_max: 118.36507583714197\n",
      "  episode_reward_mean: 36.23231503613656\n",
      "  episode_reward_min: -105.78551474080567\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1140\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6783497929573059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013919226825237274\n",
      "        model: {}\n",
      "        policy_loss: -0.1166716143488884\n",
      "        total_loss: 1227.0704345703125\n",
      "        vf_explained_var: 0.5477025508880615\n",
      "        vf_loss: 1227.1553955078125\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.758426966292134\n",
      "    ram_util_percent: 42.23160112359551\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07038291347658035\n",
      "    mean_env_wait_ms: 110.98060862891019\n",
      "    mean_inference_ms: 1.775780173131198\n",
      "    mean_raw_obs_processing_ms: 12.425000683116933\n",
      "  time_since_restore: 14459.337463378906\n",
      "  time_this_iter_s: 499.2758882045746\n",
      "  time_total_s: 14459.337463378906\n",
      "  timers:\n",
      "    learn_throughput: 260.671\n",
      "    learn_time_ms: 15344.991\n",
      "    load_throughput: 10511.413\n",
      "    load_time_ms: 380.539\n",
      "    sample_throughput: 8.197\n",
      "    sample_time_ms: 487992.957\n",
      "    update_time_ms: 3.48\n",
      "  timestamp: 1613841004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 135.09\n",
      "  episode_reward_max: 118.39714415632916\n",
      "  episode_reward_mean: 32.10021681225889\n",
      "  episode_reward_min: -106.25160441791377\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1170\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6964059472084045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013909266330301762\n",
      "        model: {}\n",
      "        policy_loss: -0.11934468150138855\n",
      "        total_loss: 1181.1661376953125\n",
      "        vf_explained_var: 0.5709272623062134\n",
      "        vf_loss: 1181.2537841796875\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.857697642163664\n",
      "    ram_util_percent: 39.77101248266297\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0703686888714543\n",
      "    mean_env_wait_ms: 110.97097829864457\n",
      "    mean_inference_ms: 1.7753516723352671\n",
      "    mean_raw_obs_processing_ms: 12.302189914191656\n",
      "  time_since_restore: 14964.054445505142\n",
      "  time_this_iter_s: 504.71698212623596\n",
      "  time_total_s: 14964.054445505142\n",
      "  timers:\n",
      "    learn_throughput: 260.711\n",
      "    learn_time_ms: 15342.681\n",
      "    load_throughput: 10415.753\n",
      "    load_time_ms: 384.034\n",
      "    sample_throughput: 8.195\n",
      "    sample_time_ms: 488109.406\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1613841508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 140.88\n",
      "  episode_reward_max: 118.39714415632916\n",
      "  episode_reward_mean: 40.35233156049454\n",
      "  episode_reward_min: -106.25160441791377\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1196\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.674450695514679\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013536256738007069\n",
      "        model: {}\n",
      "        policy_loss: -0.1175597608089447\n",
      "        total_loss: 1170.9039306640625\n",
      "        vf_explained_var: 0.5870176553726196\n",
      "        vf_loss: 1170.9906005859375\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50622347949081\n",
      "    ram_util_percent: 36.924328147100425\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07035949048792381\n",
      "    mean_env_wait_ms: 110.96037487540852\n",
      "    mean_inference_ms: 1.7749863169787523\n",
      "    mean_raw_obs_processing_ms: 12.184954414521753\n",
      "  time_since_restore: 15459.731275558472\n",
      "  time_this_iter_s: 495.67683005332947\n",
      "  time_total_s: 15459.731275558472\n",
      "  timers:\n",
      "    learn_throughput: 260.758\n",
      "    learn_time_ms: 15339.91\n",
      "    load_throughput: 10387.519\n",
      "    load_time_ms: 385.078\n",
      "    sample_throughput: 8.214\n",
      "    sample_time_ms: 486947.215\n",
      "    update_time_ms: 3.492\n",
      "  timestamp: 1613842004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-35-03\n",
      "  done: false\n",
      "  episode_len_mean: 144.89\n",
      "  episode_reward_max: 118.39714415632916\n",
      "  episode_reward_mean: 47.023250764741846\n",
      "  episode_reward_min: -106.25160441791377\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1223\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6427279710769653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011490685865283012\n",
      "        model: {}\n",
      "        policy_loss: -0.10050224512815475\n",
      "        total_loss: 756.8341674804688\n",
      "        vf_explained_var: 0.6174790859222412\n",
      "        vf_loss: 756.9085083007812\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.377387640449435\n",
      "    ram_util_percent: 36.87710674157304\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07035092872158756\n",
      "    mean_env_wait_ms: 110.95021412938355\n",
      "    mean_inference_ms: 1.774614179317349\n",
      "    mean_raw_obs_processing_ms: 12.061422517334409\n",
      "  time_since_restore: 15958.507922649384\n",
      "  time_this_iter_s: 498.77664709091187\n",
      "  time_total_s: 15958.507922649384\n",
      "  timers:\n",
      "    learn_throughput: 260.83\n",
      "    learn_time_ms: 15335.636\n",
      "    load_throughput: 10336.547\n",
      "    load_time_ms: 386.976\n",
      "    sample_throughput: 8.224\n",
      "    sample_time_ms: 486409.376\n",
      "    update_time_ms: 3.548\n",
      "  timestamp: 1613842503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-43-20\n",
      "  done: false\n",
      "  episode_len_mean: 143.39\n",
      "  episode_reward_max: 118.39714415632916\n",
      "  episode_reward_mean: 45.02980851393272\n",
      "  episode_reward_min: -106.13181011328527\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1252\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6459044218063354\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013180318288505077\n",
      "        model: {}\n",
      "        policy_loss: -0.10961834341287613\n",
      "        total_loss: 1246.991455078125\n",
      "        vf_explained_var: 0.6224071383476257\n",
      "        vf_loss: 1247.071044921875\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.74138418079096\n",
      "    ram_util_percent: 36.838418079096044\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07034702179490523\n",
      "    mean_env_wait_ms: 110.92817621890161\n",
      "    mean_inference_ms: 1.774262699782484\n",
      "    mean_raw_obs_processing_ms: 11.937789569935498\n",
      "  time_since_restore: 16455.04742050171\n",
      "  time_this_iter_s: 496.53949785232544\n",
      "  time_total_s: 16455.04742050171\n",
      "  timers:\n",
      "    learn_throughput: 260.844\n",
      "    learn_time_ms: 15334.842\n",
      "    load_throughput: 10334.558\n",
      "    load_time_ms: 387.051\n",
      "    sample_throughput: 8.241\n",
      "    sample_time_ms: 485394.114\n",
      "    update_time_ms: 3.588\n",
      "  timestamp: 1613843000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_18-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 143.61\n",
      "  episode_reward_max: 118.37212468138358\n",
      "  episode_reward_mean: 42.767731893081546\n",
      "  episode_reward_min: -105.07795578663634\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1282\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6835929155349731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015338728204369545\n",
      "        model: {}\n",
      "        policy_loss: -0.13167516887187958\n",
      "        total_loss: 1369.089599609375\n",
      "        vf_explained_var: 0.6222342252731323\n",
      "        vf_loss: 1369.1864013671875\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.51510489510489\n",
      "    ram_util_percent: 36.884195804195805\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07034467118532628\n",
      "    mean_env_wait_ms: 110.89991957438093\n",
      "    mean_inference_ms: 1.7738559267543803\n",
      "    mean_raw_obs_processing_ms: 11.822127288039512\n",
      "  time_since_restore: 16955.506905317307\n",
      "  time_this_iter_s: 500.45948481559753\n",
      "  time_total_s: 16955.506905317307\n",
      "  timers:\n",
      "    learn_throughput: 260.835\n",
      "    learn_time_ms: 15335.365\n",
      "    load_throughput: 10245.805\n",
      "    load_time_ms: 390.404\n",
      "    sample_throughput: 8.243\n",
      "    sample_time_ms: 485247.888\n",
      "    update_time_ms: 3.616\n",
      "  timestamp: 1613843500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 133.63\n",
      "  episode_reward_max: 118.39840912865664\n",
      "  episode_reward_mean: 32.1404880050737\n",
      "  episode_reward_min: -105.07795578663634\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1313\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6472254991531372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014062902890145779\n",
      "        model: {}\n",
      "        policy_loss: -0.1172453835606575\n",
      "        total_loss: 1262.2076416015625\n",
      "        vf_explained_var: 0.57198566198349\n",
      "        vf_loss: 1262.29296875\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.61216783216783\n",
      "    ram_util_percent: 36.84965034965035\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07033984018987388\n",
      "    mean_env_wait_ms: 110.87097809766998\n",
      "    mean_inference_ms: 1.7733750635766539\n",
      "    mean_raw_obs_processing_ms: 11.728371523588248\n",
      "  time_since_restore: 17456.943140506744\n",
      "  time_this_iter_s: 501.43623518943787\n",
      "  time_total_s: 17456.943140506744\n",
      "  timers:\n",
      "    learn_throughput: 260.908\n",
      "    learn_time_ms: 15331.056\n",
      "    load_throughput: 10305.266\n",
      "    load_time_ms: 388.151\n",
      "    sample_throughput: 8.251\n",
      "    sample_time_ms: 484805.591\n",
      "    update_time_ms: 3.648\n",
      "  timestamp: 1613844002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 133.22\n",
      "  episode_reward_max: 118.39840912865664\n",
      "  episode_reward_mean: 15.54194706711927\n",
      "  episode_reward_min: -107.43915598280276\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1343\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6586072444915771\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015374414622783661\n",
      "        model: {}\n",
      "        policy_loss: -0.13172367215156555\n",
      "        total_loss: 1605.39208984375\n",
      "        vf_explained_var: 0.5467179417610168\n",
      "        vf_loss: 1605.4888916015625\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.994027777777774\n",
      "    ram_util_percent: 36.948611111111106\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.070340489915848\n",
      "    mean_env_wait_ms: 110.85563415838286\n",
      "    mean_inference_ms: 1.773001997473154\n",
      "    mean_raw_obs_processing_ms: 11.65145366998234\n",
      "  time_since_restore: 17961.057772874832\n",
      "  time_this_iter_s: 504.11463236808777\n",
      "  time_total_s: 17961.057772874832\n",
      "  timers:\n",
      "    learn_throughput: 260.897\n",
      "    learn_time_ms: 15331.723\n",
      "    load_throughput: 10218.106\n",
      "    load_time_ms: 391.462\n",
      "    sample_throughput: 8.251\n",
      "    sample_time_ms: 484779.773\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1613844506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 130.9\n",
      "  episode_reward_max: 118.39840912865664\n",
      "  episode_reward_mean: 25.976085320763044\n",
      "  episode_reward_min: -107.43915598280276\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1371\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6460509896278381\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013093412853777409\n",
      "        model: {}\n",
      "        policy_loss: -0.11080330610275269\n",
      "        total_loss: 818.1026000976562\n",
      "        vf_explained_var: 0.7011681199073792\n",
      "        vf_loss: 818.1834106445312\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.60070621468927\n",
      "    ram_util_percent: 36.98601694915254\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07033730600624276\n",
      "    mean_env_wait_ms: 110.84213018906237\n",
      "    mean_inference_ms: 1.772571484547563\n",
      "    mean_raw_obs_processing_ms: 11.580043678765687\n",
      "  time_since_restore: 18457.762900829315\n",
      "  time_this_iter_s: 496.70512795448303\n",
      "  time_total_s: 18457.762900829315\n",
      "  timers:\n",
      "    learn_throughput: 260.95\n",
      "    learn_time_ms: 15328.593\n",
      "    load_throughput: 10136.049\n",
      "    load_time_ms: 394.631\n",
      "    sample_throughput: 8.263\n",
      "    sample_time_ms: 484109.867\n",
      "    update_time_ms: 3.626\n",
      "  timestamp: 1613845003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-25-04\n",
      "  done: false\n",
      "  episode_len_mean: 133.62\n",
      "  episode_reward_max: 118.37555030139066\n",
      "  episode_reward_mean: 29.844075293892665\n",
      "  episode_reward_min: -107.43915598280276\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1402\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.666848361492157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014040189795196056\n",
      "        model: {}\n",
      "        policy_loss: -0.1256057620048523\n",
      "        total_loss: 841.550537109375\n",
      "        vf_explained_var: 0.7548153400421143\n",
      "        vf_loss: 841.644287109375\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.59581005586592\n",
      "    ram_util_percent: 36.905726256983236\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07033396749830349\n",
      "    mean_env_wait_ms: 110.82677575562188\n",
      "    mean_inference_ms: 1.7721333454132469\n",
      "    mean_raw_obs_processing_ms: 11.505523211637954\n",
      "  time_since_restore: 18958.79090666771\n",
      "  time_this_iter_s: 501.02800583839417\n",
      "  time_total_s: 18958.79090666771\n",
      "  timers:\n",
      "    learn_throughput: 261.019\n",
      "    learn_time_ms: 15324.581\n",
      "    load_throughput: 10157.973\n",
      "    load_time_ms: 393.779\n",
      "    sample_throughput: 8.265\n",
      "    sample_time_ms: 483988.533\n",
      "    update_time_ms: 3.616\n",
      "  timestamp: 1613845504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 133.4\n",
      "  episode_reward_max: 118.36373005371863\n",
      "  episode_reward_mean: 45.9344929154487\n",
      "  episode_reward_min: -107.43915598280276\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1433\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6199125051498413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01168330293148756\n",
      "        model: {}\n",
      "        policy_loss: -0.09821189194917679\n",
      "        total_loss: 510.17413330078125\n",
      "        vf_explained_var: 0.738914966583252\n",
      "        vf_loss: 510.2457580566406\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.09030470914127\n",
      "    ram_util_percent: 36.96204986149584\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07032787706505493\n",
      "    mean_env_wait_ms: 110.81615481171275\n",
      "    mean_inference_ms: 1.7717424464307339\n",
      "    mean_raw_obs_processing_ms: 11.43692149765393\n",
      "  time_since_restore: 19465.000452041626\n",
      "  time_this_iter_s: 506.2095453739166\n",
      "  time_total_s: 19465.000452041626\n",
      "  timers:\n",
      "    learn_throughput: 261.036\n",
      "    learn_time_ms: 15323.56\n",
      "    load_throughput: 10089.997\n",
      "    load_time_ms: 396.432\n",
      "    sample_throughput: 8.253\n",
      "    sample_time_ms: 484680.927\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1613846010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-41-51\n",
      "  done: false\n",
      "  episode_len_mean: 129.59\n",
      "  episode_reward_max: 118.3958968507586\n",
      "  episode_reward_mean: 46.317101289409294\n",
      "  episode_reward_min: -107.07387182033435\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1464\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6443673968315125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011803336441516876\n",
      "        model: {}\n",
      "        policy_loss: -0.10504046827554703\n",
      "        total_loss: 382.9107666015625\n",
      "        vf_explained_var: 0.8292127251625061\n",
      "        vf_loss: 382.9889221191406\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.601820728291315\n",
      "    ram_util_percent: 36.94327731092437\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07032114966698425\n",
      "    mean_env_wait_ms: 110.8041680554541\n",
      "    mean_inference_ms: 1.7714408506252282\n",
      "    mean_raw_obs_processing_ms: 11.37802606642925\n",
      "  time_since_restore: 19965.705763101578\n",
      "  time_this_iter_s: 500.7053110599518\n",
      "  time_total_s: 19965.705763101578\n",
      "  timers:\n",
      "    learn_throughput: 261.01\n",
      "    learn_time_ms: 15325.089\n",
      "    load_throughput: 10215.797\n",
      "    load_time_ms: 391.55\n",
      "    sample_throughput: 8.26\n",
      "    sample_time_ms: 484282.824\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1613846511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-50-10\n",
      "  done: false\n",
      "  episode_len_mean: 131.63\n",
      "  episode_reward_max: 118.3958968507586\n",
      "  episode_reward_mean: 54.302023289065644\n",
      "  episode_reward_min: -107.07387182033435\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1492\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6468275189399719\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011806544847786427\n",
      "        model: {}\n",
      "        policy_loss: -0.11051733046770096\n",
      "        total_loss: 671.3834838867188\n",
      "        vf_explained_var: 0.7356114983558655\n",
      "        vf_loss: 671.4671630859375\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.430294530154285\n",
      "    ram_util_percent: 36.95918653576438\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0703123966149883\n",
      "    mean_env_wait_ms: 110.79749318914422\n",
      "    mean_inference_ms: 1.7712056836951258\n",
      "    mean_raw_obs_processing_ms: 11.324981364048007\n",
      "  time_since_restore: 20464.761628866196\n",
      "  time_this_iter_s: 499.0558657646179\n",
      "  time_total_s: 20464.761628866196\n",
      "  timers:\n",
      "    learn_throughput: 260.995\n",
      "    learn_time_ms: 15325.971\n",
      "    load_throughput: 10218.388\n",
      "    load_time_ms: 391.451\n",
      "    sample_throughput: 8.254\n",
      "    sample_time_ms: 484621.248\n",
      "    update_time_ms: 3.575\n",
      "  timestamp: 1613847010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_19-58-31\n",
      "  done: false\n",
      "  episode_len_mean: 133.34\n",
      "  episode_reward_max: 118.3958968507586\n",
      "  episode_reward_mean: 52.70478508017718\n",
      "  episode_reward_min: -107.07387182033435\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1522\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.641582727432251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011860525235533714\n",
      "        model: {}\n",
      "        policy_loss: -0.10475405305624008\n",
      "        total_loss: 674.5768432617188\n",
      "        vf_explained_var: 0.7677860260009766\n",
      "        vf_loss: 674.654541015625\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.861398601398605\n",
      "    ram_util_percent: 36.989090909090905\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0703088672631849\n",
      "    mean_env_wait_ms: 110.7861624123223\n",
      "    mean_inference_ms: 1.7709939268138148\n",
      "    mean_raw_obs_processing_ms: 11.26685085159765\n",
      "  time_since_restore: 20965.715186834335\n",
      "  time_this_iter_s: 500.95355796813965\n",
      "  time_total_s: 20965.715186834335\n",
      "  timers:\n",
      "    learn_throughput: 260.957\n",
      "    learn_time_ms: 15328.172\n",
      "    load_throughput: 10266.039\n",
      "    load_time_ms: 389.634\n",
      "    sample_throughput: 8.25\n",
      "    sample_time_ms: 484840.9\n",
      "    update_time_ms: 3.537\n",
      "  timestamp: 1613847511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 135.94\n",
      "  episode_reward_max: 118.3958968507586\n",
      "  episode_reward_mean: 48.804333945869594\n",
      "  episode_reward_min: -106.32114279569402\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1552\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6589558720588684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014141380786895752\n",
      "        model: {}\n",
      "        policy_loss: -0.11567607522010803\n",
      "        total_loss: 1036.539306640625\n",
      "        vf_explained_var: 0.7052029967308044\n",
      "        vf_loss: 1036.62255859375\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.51076923076923\n",
      "    ram_util_percent: 37.02181818181818\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07030557654297072\n",
      "    mean_env_wait_ms: 110.77457488319823\n",
      "    mean_inference_ms: 1.7706725015214595\n",
      "    mean_raw_obs_processing_ms: 11.208994106094684\n",
      "  time_since_restore: 21466.86609196663\n",
      "  time_this_iter_s: 501.1509051322937\n",
      "  time_total_s: 21466.86609196663\n",
      "  timers:\n",
      "    learn_throughput: 260.938\n",
      "    learn_time_ms: 15329.292\n",
      "    load_throughput: 10276.51\n",
      "    load_time_ms: 389.237\n",
      "    sample_throughput: 8.242\n",
      "    sample_time_ms: 485304.007\n",
      "    update_time_ms: 3.501\n",
      "  timestamp: 1613848012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 129.75\n",
      "  episode_reward_max: 118.37743755854237\n",
      "  episode_reward_mean: 40.85230381990975\n",
      "  episode_reward_min: -105.6278206838217\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1586\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6482423543930054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013159703463315964\n",
      "        model: {}\n",
      "        policy_loss: -0.11065053939819336\n",
      "        total_loss: 1050.6982421875\n",
      "        vf_explained_var: 0.6948574781417847\n",
      "        vf_loss: 1050.77880859375\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.62097222222222\n",
      "    ram_util_percent: 36.95305555555555\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07030399131913781\n",
      "    mean_env_wait_ms: 110.7607417073646\n",
      "    mean_inference_ms: 1.7702370076517295\n",
      "    mean_raw_obs_processing_ms: 11.158182205607885\n",
      "  time_since_restore: 21971.43261051178\n",
      "  time_this_iter_s: 504.56651854515076\n",
      "  time_total_s: 21971.43261051178\n",
      "  timers:\n",
      "    learn_throughput: 260.953\n",
      "    learn_time_ms: 15328.46\n",
      "    load_throughput: 10343.974\n",
      "    load_time_ms: 386.699\n",
      "    sample_throughput: 8.235\n",
      "    sample_time_ms: 485719.358\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1613848517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-23-37\n",
      "  done: false\n",
      "  episode_len_mean: 129.15\n",
      "  episode_reward_max: 118.37543209998196\n",
      "  episode_reward_mean: 42.897584452962874\n",
      "  episode_reward_min: -105.6278206838217\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1615\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6413830518722534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010721690952777863\n",
      "        model: {}\n",
      "        policy_loss: -0.09945659339427948\n",
      "        total_loss: 531.45947265625\n",
      "        vf_explained_var: 0.7469833493232727\n",
      "        vf_loss: 531.5345458984375\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.49859943977591\n",
      "    ram_util_percent: 36.96624649859944\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029959625995104\n",
      "    mean_env_wait_ms: 110.7510495015338\n",
      "    mean_inference_ms: 1.7698440937330673\n",
      "    mean_raw_obs_processing_ms: 11.118542366978442\n",
      "  time_since_restore: 22471.852631807327\n",
      "  time_this_iter_s: 500.4200212955475\n",
      "  time_total_s: 22471.852631807327\n",
      "  timers:\n",
      "    learn_throughput: 260.93\n",
      "    learn_time_ms: 15329.771\n",
      "    load_throughput: 10357.307\n",
      "    load_time_ms: 386.201\n",
      "    sample_throughput: 8.237\n",
      "    sample_time_ms: 485618.441\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1613849017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 130.06\n",
      "  episode_reward_max: 118.38189377218501\n",
      "  episode_reward_mean: 62.065262610967785\n",
      "  episode_reward_min: -105.6278206838217\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1645\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6233407258987427\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00898065697401762\n",
      "        model: {}\n",
      "        policy_loss: -0.08020798116922379\n",
      "        total_loss: 356.6605529785156\n",
      "        vf_explained_var: 0.7845202088356018\n",
      "        vf_loss: 356.7203369140625\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.57478991596639\n",
      "    ram_util_percent: 36.95280112044817\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0702960367516641\n",
      "    mean_env_wait_ms: 110.74041825539024\n",
      "    mean_inference_ms: 1.7694404392189858\n",
      "    mean_raw_obs_processing_ms: 11.07920669127588\n",
      "  time_since_restore: 22972.452386140823\n",
      "  time_this_iter_s: 500.5997543334961\n",
      "  time_total_s: 22972.452386140823\n",
      "  timers:\n",
      "    learn_throughput: 260.929\n",
      "    learn_time_ms: 15329.818\n",
      "    load_throughput: 10425.457\n",
      "    load_time_ms: 383.676\n",
      "    sample_throughput: 8.243\n",
      "    sample_time_ms: 485266.468\n",
      "    update_time_ms: 3.686\n",
      "  timestamp: 1613849518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 134.84\n",
      "  episode_reward_max: 118.38189377218501\n",
      "  episode_reward_mean: 59.985331006414555\n",
      "  episode_reward_min: -101.4588881123555\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1673\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6626541614532471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012667709030210972\n",
      "        model: {}\n",
      "        policy_loss: -0.11400982737541199\n",
      "        total_loss: 800.5023803710938\n",
      "        vf_explained_var: 0.7248692512512207\n",
      "        vf_loss: 800.5874633789062\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.70496551724138\n",
      "    ram_util_percent: 37.08303448275863\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029657900746815\n",
      "    mean_env_wait_ms: 110.74568080511251\n",
      "    mean_inference_ms: 1.769301121008906\n",
      "    mean_raw_obs_processing_ms: 11.034907993071792\n",
      "  time_since_restore: 23480.277863502502\n",
      "  time_this_iter_s: 507.8254773616791\n",
      "  time_total_s: 23480.277863502502\n",
      "  timers:\n",
      "    learn_throughput: 260.852\n",
      "    learn_time_ms: 15334.355\n",
      "    load_throughput: 10488.971\n",
      "    load_time_ms: 381.353\n",
      "    sample_throughput: 8.224\n",
      "    sample_time_ms: 486377.911\n",
      "    update_time_ms: 3.724\n",
      "  timestamp: 1613850026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 144.04\n",
      "  episode_reward_max: 118.38189377218501\n",
      "  episode_reward_mean: 74.52875837504715\n",
      "  episode_reward_min: -106.34779912746163\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 1698\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6407293677330017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009831038303673267\n",
      "        model: {}\n",
      "        policy_loss: -0.08865351229906082\n",
      "        total_loss: 250.5148468017578\n",
      "        vf_explained_var: 0.8557254076004028\n",
      "        vf_loss: 250.5811004638672\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53465346534654\n",
      "    ram_util_percent: 37.012588401697315\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029727781938253\n",
      "    mean_env_wait_ms: 110.74992239858908\n",
      "    mean_inference_ms: 1.7692241671302469\n",
      "    mean_raw_obs_processing_ms: 10.985394316609364\n",
      "  time_since_restore: 23975.74490594864\n",
      "  time_this_iter_s: 495.4670424461365\n",
      "  time_total_s: 23975.74490594864\n",
      "  timers:\n",
      "    learn_throughput: 260.761\n",
      "    learn_time_ms: 15339.736\n",
      "    load_throughput: 10497.116\n",
      "    load_time_ms: 381.057\n",
      "    sample_throughput: 8.234\n",
      "    sample_time_ms: 485815.988\n",
      "    update_time_ms: 3.755\n",
      "  timestamp: 1613850522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_20-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 142.98\n",
      "  episode_reward_max: 118.38189377218501\n",
      "  episode_reward_mean: 67.98287444602884\n",
      "  episode_reward_min: -106.34779912746163\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1727\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6419114470481873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008923264220356941\n",
      "        model: {}\n",
      "        policy_loss: -0.0866682380437851\n",
      "        total_loss: 338.417236328125\n",
      "        vf_explained_var: 0.8316975831985474\n",
      "        vf_loss: 338.4835510253906\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.60056179775281\n",
      "    ram_util_percent: 37.017977528089894\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0702983928846514\n",
      "    mean_env_wait_ms: 110.75403191743374\n",
      "    mean_inference_ms: 1.7691664603468693\n",
      "    mean_raw_obs_processing_ms: 10.928172516827154\n",
      "  time_since_restore: 24475.107447862625\n",
      "  time_this_iter_s: 499.3625419139862\n",
      "  time_total_s: 24475.107447862625\n",
      "  timers:\n",
      "    learn_throughput: 260.765\n",
      "    learn_time_ms: 15339.472\n",
      "    load_throughput: 10453.94\n",
      "    load_time_ms: 382.631\n",
      "    sample_throughput: 8.245\n",
      "    sample_time_ms: 485125.038\n",
      "    update_time_ms: 3.776\n",
      "  timestamp: 1613851021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-05-20\n",
      "  done: false\n",
      "  episode_len_mean: 148.73\n",
      "  episode_reward_max: 118.37884731579629\n",
      "  episode_reward_mean: 67.8799255374538\n",
      "  episode_reward_min: -106.34779912746163\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1754\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.636057436466217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009555695578455925\n",
      "        model: {}\n",
      "        policy_loss: -0.08797115087509155\n",
      "        total_loss: 359.5047607421875\n",
      "        vf_explained_var: 0.8124749064445496\n",
      "        vf_loss: 359.57098388671875\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.45519662921349\n",
      "    ram_util_percent: 37.0685393258427\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029628939055829\n",
      "    mean_env_wait_ms: 110.75578300235905\n",
      "    mean_inference_ms: 1.7690876277659227\n",
      "    mean_raw_obs_processing_ms: 10.871889056857041\n",
      "  time_since_restore: 24973.8474817276\n",
      "  time_this_iter_s: 498.740033864975\n",
      "  time_total_s: 24973.8474817276\n",
      "  timers:\n",
      "    learn_throughput: 260.758\n",
      "    learn_time_ms: 15339.912\n",
      "    load_throughput: 10499.603\n",
      "    load_time_ms: 380.967\n",
      "    sample_throughput: 8.249\n",
      "    sample_time_ms: 484929.998\n",
      "    update_time_ms: 3.814\n",
      "  timestamp: 1613851520\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-13-39\n",
      "  done: false\n",
      "  episode_len_mean: 144.17\n",
      "  episode_reward_max: 118.39421396908376\n",
      "  episode_reward_mean: 63.92768333430419\n",
      "  episode_reward_min: -105.11605887423045\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1783\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6578298211097717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013607810251414776\n",
      "        model: {}\n",
      "        policy_loss: -0.12195522338151932\n",
      "        total_loss: 667.0701904296875\n",
      "        vf_explained_var: 0.7740439176559448\n",
      "        vf_loss: 667.1612548828125\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.572230014025244\n",
      "    ram_util_percent: 37.04431977559607\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029345552180817\n",
      "    mean_env_wait_ms: 110.74755407693898\n",
      "    mean_inference_ms: 1.7689476389640095\n",
      "    mean_raw_obs_processing_ms: 10.817247726676902\n",
      "  time_since_restore: 25473.063342809677\n",
      "  time_this_iter_s: 499.215861082077\n",
      "  time_total_s: 25473.063342809677\n",
      "  timers:\n",
      "    learn_throughput: 260.76\n",
      "    learn_time_ms: 15339.749\n",
      "    load_throughput: 10514.559\n",
      "    load_time_ms: 380.425\n",
      "    sample_throughput: 8.248\n",
      "    sample_time_ms: 484944.65\n",
      "    update_time_ms: 3.846\n",
      "  timestamp: 1613852019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-21-58\n",
      "  done: false\n",
      "  episode_len_mean: 142.79\n",
      "  episode_reward_max: 118.39693910591514\n",
      "  episode_reward_mean: 59.382111297120936\n",
      "  episode_reward_min: -105.11605887423045\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1811\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6633873581886292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011062879115343094\n",
      "        model: {}\n",
      "        policy_loss: -0.1058695986866951\n",
      "        total_loss: 324.61956787109375\n",
      "        vf_explained_var: 0.883109450340271\n",
      "        vf_loss: 324.7002868652344\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.56047819971871\n",
      "    ram_util_percent: 37.102390998593535\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0702925175851268\n",
      "    mean_env_wait_ms: 110.73998913003665\n",
      "    mean_inference_ms: 1.768822287760936\n",
      "    mean_raw_obs_processing_ms: 10.771063188632224\n",
      "  time_since_restore: 25971.54581975937\n",
      "  time_this_iter_s: 498.4824769496918\n",
      "  time_total_s: 25971.54581975937\n",
      "  timers:\n",
      "    learn_throughput: 260.797\n",
      "    learn_time_ms: 15337.601\n",
      "    load_throughput: 10527.517\n",
      "    load_time_ms: 379.957\n",
      "    sample_throughput: 8.253\n",
      "    sample_time_ms: 484701.47\n",
      "    update_time_ms: 3.849\n",
      "  timestamp: 1613852518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 133.19\n",
      "  episode_reward_max: 118.39693910591514\n",
      "  episode_reward_mean: 59.00244247417059\n",
      "  episode_reward_min: -107.14513596437897\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1844\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.650313138961792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009966370649635792\n",
      "        model: {}\n",
      "        policy_loss: -0.09712912142276764\n",
      "        total_loss: 640.95947265625\n",
      "        vf_explained_var: 0.7460017800331116\n",
      "        vf_loss: 641.033935546875\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.71061452513966\n",
      "    ram_util_percent: 37.14916201117319\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07028977365919233\n",
      "    mean_env_wait_ms: 110.72607899184827\n",
      "    mean_inference_ms: 1.7686210479903621\n",
      "    mean_raw_obs_processing_ms: 10.728863856971861\n",
      "  time_since_restore: 26472.941111803055\n",
      "  time_this_iter_s: 501.3952920436859\n",
      "  time_total_s: 26472.941111803055\n",
      "  timers:\n",
      "    learn_throughput: 260.83\n",
      "    learn_time_ms: 15335.64\n",
      "    load_throughput: 10540.876\n",
      "    load_time_ms: 379.475\n",
      "    sample_throughput: 8.252\n",
      "    sample_time_ms: 484724.861\n",
      "    update_time_ms: 3.85\n",
      "  timestamp: 1613853019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-38-49\n",
      "  done: false\n",
      "  episode_len_mean: 132.54\n",
      "  episode_reward_max: 118.39693910591514\n",
      "  episode_reward_mean: 59.091052037477894\n",
      "  episode_reward_min: -107.14513596437897\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1874\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6425826549530029\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011709527112543583\n",
      "        model: {}\n",
      "        policy_loss: -0.10362938046455383\n",
      "        total_loss: 691.9244995117188\n",
      "        vf_explained_var: 0.7485781908035278\n",
      "        vf_loss: 692.0013427734375\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.53287482806053\n",
      "    ram_util_percent: 37.163273727647876\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07029267823481428\n",
      "    mean_env_wait_ms: 110.72434566450103\n",
      "    mean_inference_ms: 1.7685486715322718\n",
      "    mean_raw_obs_processing_ms: 10.697146182569476\n",
      "  time_since_restore: 26982.726992607117\n",
      "  time_this_iter_s: 509.7858808040619\n",
      "  time_total_s: 26982.726992607117\n",
      "  timers:\n",
      "    learn_throughput: 258.57\n",
      "    learn_time_ms: 15469.72\n",
      "    load_throughput: 10456.634\n",
      "    load_time_ms: 382.532\n",
      "    sample_throughput: 8.246\n",
      "    sample_time_ms: 485101.827\n",
      "    update_time_ms: 3.818\n",
      "  timestamp: 1613853529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-47-38\n",
      "  done: false\n",
      "  episode_len_mean: 128.61\n",
      "  episode_reward_max: 118.3323445817829\n",
      "  episode_reward_mean: 65.39139703471686\n",
      "  episode_reward_min: -107.14513596437897\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 1905\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6302710175514221\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008997817523777485\n",
      "        model: {}\n",
      "        policy_loss: -0.08477241545915604\n",
      "        total_loss: 500.035400390625\n",
      "        vf_explained_var: 0.7220929861068726\n",
      "        vf_loss: 500.0996398925781\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.83245033112583\n",
      "    ram_util_percent: 37.139602649006626\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07032062926629683\n",
      "    mean_env_wait_ms: 110.75891469656686\n",
      "    mean_inference_ms: 1.7687999750663093\n",
      "    mean_raw_obs_processing_ms: 10.670978443075951\n",
      "  time_since_restore: 27511.451284885406\n",
      "  time_this_iter_s: 528.7242922782898\n",
      "  time_total_s: 27511.451284885406\n",
      "  timers:\n",
      "    learn_throughput: 256.256\n",
      "    learn_time_ms: 15609.362\n",
      "    load_throughput: 10477.919\n",
      "    load_time_ms: 381.755\n",
      "    sample_throughput: 8.2\n",
      "    sample_time_ms: 487789.587\n",
      "    update_time_ms: 3.665\n",
      "  timestamp: 1613854058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_21-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 133.35\n",
      "  episode_reward_max: 118.38506214263124\n",
      "  episode_reward_mean: 65.80513212460696\n",
      "  episode_reward_min: -106.74443176576358\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1935\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6315209865570068\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010787761770188808\n",
      "        model: {}\n",
      "        policy_loss: -0.09967464208602905\n",
      "        total_loss: 428.6216735839844\n",
      "        vf_explained_var: 0.8370494842529297\n",
      "        vf_loss: 428.6967468261719\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.20964332892999\n",
      "    ram_util_percent: 37.19075297225891\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07037049189297422\n",
      "    mean_env_wait_ms: 110.83342209570523\n",
      "    mean_inference_ms: 1.769447076085045\n",
      "    mean_raw_obs_processing_ms: 10.645196327199976\n",
      "  time_since_restore: 28042.300265789032\n",
      "  time_this_iter_s: 530.8489809036255\n",
      "  time_total_s: 28042.300265789032\n",
      "  timers:\n",
      "    learn_throughput: 254.037\n",
      "    learn_time_ms: 15745.751\n",
      "    load_throughput: 10468.624\n",
      "    load_time_ms: 382.094\n",
      "    sample_throughput: 8.152\n",
      "    sample_time_ms: 490678.066\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1613854589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 126.14\n",
      "  episode_reward_max: 118.38506214263124\n",
      "  episode_reward_mean: 55.31096174174058\n",
      "  episode_reward_min: -106.74443176576358\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 1968\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6478879451751709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013791813515126705\n",
      "        model: {}\n",
      "        policy_loss: -0.12108524143695831\n",
      "        total_loss: 1167.9112548828125\n",
      "        vf_explained_var: 0.663340151309967\n",
      "        vf_loss: 1168.000732421875\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.46315086782377\n",
      "    ram_util_percent: 37.16234979973297\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07043649977748466\n",
      "    mean_env_wait_ms: 110.93319091066641\n",
      "    mean_inference_ms: 1.770354065129902\n",
      "    mean_raw_obs_processing_ms: 10.622040529207887\n",
      "  time_since_restore: 28566.90573167801\n",
      "  time_this_iter_s: 524.605465888977\n",
      "  time_total_s: 28566.90573167801\n",
      "  timers:\n",
      "    learn_throughput: 252.023\n",
      "    learn_time_ms: 15871.551\n",
      "    load_throughput: 10398.317\n",
      "    load_time_ms: 384.678\n",
      "    sample_throughput: 8.126\n",
      "    sample_time_ms: 492226.153\n",
      "    update_time_ms: 3.722\n",
      "  timestamp: 1613855114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-13-54\n",
      "  done: false\n",
      "  episode_len_mean: 136.33\n",
      "  episode_reward_max: 118.39686184711292\n",
      "  episode_reward_mean: 68.16007260489052\n",
      "  episode_reward_min: -101.23240739758147\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 1995\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6253086924552917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00979804340749979\n",
      "        model: {}\n",
      "        policy_loss: -0.09172672033309937\n",
      "        total_loss: 141.4500732421875\n",
      "        vf_explained_var: 0.8915672898292542\n",
      "        vf_loss: 141.51947021484375\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.003099730458224\n",
      "    ram_util_percent: 37.12250673854448\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07047223222355245\n",
      "    mean_env_wait_ms: 111.01302383516594\n",
      "    mean_inference_ms: 1.7708231004163704\n",
      "    mean_raw_obs_processing_ms: 10.59936403226933\n",
      "  time_since_restore: 29086.573880910873\n",
      "  time_this_iter_s: 519.6681492328644\n",
      "  time_total_s: 29086.573880910873\n",
      "  timers:\n",
      "    learn_throughput: 252.296\n",
      "    learn_time_ms: 15854.379\n",
      "    load_throughput: 10465.866\n",
      "    load_time_ms: 382.195\n",
      "    sample_throughput: 8.086\n",
      "    sample_time_ms: 494666.712\n",
      "    update_time_ms: 3.682\n",
      "  timestamp: 1613855634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-23-26\n",
      "  done: false\n",
      "  episode_len_mean: 136.2\n",
      "  episode_reward_max: 118.39686184711292\n",
      "  episode_reward_mean: 66.33935965107635\n",
      "  episode_reward_min: -101.23240739758147\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2023\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6489342451095581\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012554348446428776\n",
      "        model: {}\n",
      "        policy_loss: -0.11330427974462509\n",
      "        total_loss: 1351.4207763671875\n",
      "        vf_explained_var: 0.45022523403167725\n",
      "        vf_loss: 1351.5054931640625\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.32729498164015\n",
      "    ram_util_percent: 37.1123623011016\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07046659993996357\n",
      "    mean_env_wait_ms: 111.15418143970867\n",
      "    mean_inference_ms: 1.7705213443087429\n",
      "    mean_raw_obs_processing_ms: 10.57231127310461\n",
      "  time_since_restore: 29659.22930073738\n",
      "  time_this_iter_s: 572.6554198265076\n",
      "  time_total_s: 29659.22930073738\n",
      "  timers:\n",
      "    learn_throughput: 252.608\n",
      "    learn_time_ms: 15834.826\n",
      "    load_throughput: 10554.629\n",
      "    load_time_ms: 378.981\n",
      "    sample_throughput: 7.968\n",
      "    sample_time_ms: 502023.27\n",
      "    update_time_ms: 3.668\n",
      "  timestamp: 1613856206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-32-58\n",
      "  done: false\n",
      "  episode_len_mean: 141.39\n",
      "  episode_reward_max: 118.39686184711292\n",
      "  episode_reward_mean: 70.29793638788594\n",
      "  episode_reward_min: -100.23973123547925\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2050\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6639514565467834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010656061582267284\n",
      "        model: {}\n",
      "        policy_loss: -0.10116995871067047\n",
      "        total_loss: 572.2363891601562\n",
      "        vf_explained_var: 0.7428793907165527\n",
      "        vf_loss: 572.3132934570312\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.33382352941177\n",
      "    ram_util_percent: 37.13541666666668\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07042244921524958\n",
      "    mean_env_wait_ms: 111.34900230644934\n",
      "    mean_inference_ms: 1.7695008968193702\n",
      "    mean_raw_obs_processing_ms: 10.539710406986257\n",
      "  time_since_restore: 30230.81455373764\n",
      "  time_this_iter_s: 571.5852530002594\n",
      "  time_total_s: 30230.81455373764\n",
      "  timers:\n",
      "    learn_throughput: 252.938\n",
      "    learn_time_ms: 15814.134\n",
      "    load_throughput: 10533.311\n",
      "    load_time_ms: 379.748\n",
      "    sample_throughput: 7.853\n",
      "    sample_time_ms: 509327.434\n",
      "    update_time_ms: 3.716\n",
      "  timestamp: 1613856778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 141.34\n",
      "  episode_reward_max: 118.39686184711292\n",
      "  episode_reward_mean: 74.43891614908803\n",
      "  episode_reward_min: -99.9359350861686\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 2081\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.660055935382843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010436595417559147\n",
      "        model: {}\n",
      "        policy_loss: -0.10442720353603363\n",
      "        total_loss: 374.71368408203125\n",
      "        vf_explained_var: 0.8304740190505981\n",
      "        vf_loss: 374.7943115234375\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.46243902439024\n",
      "    ram_util_percent: 37.222682926829265\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07033987487465286\n",
      "    mean_env_wait_ms: 111.63979372562945\n",
      "    mean_inference_ms: 1.7676407091750688\n",
      "    mean_raw_obs_processing_ms: 10.503065870075869\n",
      "  time_since_restore: 30805.672479391098\n",
      "  time_this_iter_s: 574.8579256534576\n",
      "  time_total_s: 30805.672479391098\n",
      "  timers:\n",
      "    learn_throughput: 253.234\n",
      "    learn_time_ms: 15795.638\n",
      "    load_throughput: 10549.057\n",
      "    load_time_ms: 379.181\n",
      "    sample_throughput: 7.738\n",
      "    sample_time_ms: 516912.277\n",
      "    update_time_ms: 3.702\n",
      "  timestamp: 1613857353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_22-52-05\n",
      "  done: false\n",
      "  episode_len_mean: 141.05\n",
      "  episode_reward_max: 118.37612607047315\n",
      "  episode_reward_mean: 70.01738176891863\n",
      "  episode_reward_min: -105.68695967977602\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2109\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6409180164337158\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011094327084720135\n",
      "        model: {}\n",
      "        policy_loss: -0.09974440932273865\n",
      "        total_loss: 499.4038391113281\n",
      "        vf_explained_var: 0.801602303981781\n",
      "        vf_loss: 499.4782409667969\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38492647058823\n",
      "    ram_util_percent: 37.204779411764704\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07025371107998246\n",
      "    mean_env_wait_ms: 111.92239434537707\n",
      "    mean_inference_ms: 1.7657413100059511\n",
      "    mean_raw_obs_processing_ms: 10.472746551317464\n",
      "  time_since_restore: 31377.506068468094\n",
      "  time_this_iter_s: 571.8335890769958\n",
      "  time_total_s: 31377.506068468094\n",
      "  timers:\n",
      "    learn_throughput: 253.487\n",
      "    learn_time_ms: 15779.897\n",
      "    load_throughput: 10510.437\n",
      "    load_time_ms: 380.574\n",
      "    sample_throughput: 7.63\n",
      "    sample_time_ms: 524260.903\n",
      "    update_time_ms: 3.69\n",
      "  timestamp: 1613857925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 141.05\n",
      "  episode_reward_max: 118.37612607047315\n",
      "  episode_reward_mean: 71.77740101151602\n",
      "  episode_reward_min: -106.74472136491764\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2137\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.647716224193573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011794942431151867\n",
      "        model: {}\n",
      "        policy_loss: -0.10908807814121246\n",
      "        total_loss: 351.6716003417969\n",
      "        vf_explained_var: 0.8702437281608582\n",
      "        vf_loss: 351.75384521484375\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38235294117647\n",
      "    ram_util_percent: 37.1375\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07017104526738993\n",
      "    mean_env_wait_ms: 112.1944696480094\n",
      "    mean_inference_ms: 1.7639460613750333\n",
      "    mean_raw_obs_processing_ms: 10.444230884135225\n",
      "  time_since_restore: 31949.255333185196\n",
      "  time_this_iter_s: 571.749264717102\n",
      "  time_total_s: 31949.255333185196\n",
      "  timers:\n",
      "    learn_throughput: 253.781\n",
      "    learn_time_ms: 15761.617\n",
      "    load_throughput: 10554.807\n",
      "    load_time_ms: 378.974\n",
      "    sample_throughput: 7.528\n",
      "    sample_time_ms: 531319.258\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1613858497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 137.19\n",
      "  episode_reward_max: 118.37612607047315\n",
      "  episode_reward_mean: 59.12739302951584\n",
      "  episode_reward_min: -106.74472136491764\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 2168\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6523399353027344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015450678765773773\n",
      "        model: {}\n",
      "        policy_loss: -0.13272224366664886\n",
      "        total_loss: 956.6912841796875\n",
      "        vf_explained_var: 0.767768144607544\n",
      "        vf_loss: 956.7888793945312\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.49\n",
      "    ram_util_percent: 37.12756097560976\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07008540600133706\n",
      "    mean_env_wait_ms: 112.47689126731144\n",
      "    mean_inference_ms: 1.7620962489760057\n",
      "    mean_raw_obs_processing_ms: 10.417165729232902\n",
      "  time_since_restore: 32523.346776008606\n",
      "  time_this_iter_s: 574.09144282341\n",
      "  time_total_s: 32523.346776008606\n",
      "  timers:\n",
      "    learn_throughput: 256.26\n",
      "    learn_time_ms: 15609.142\n",
      "    load_throughput: 10676.985\n",
      "    load_time_ms: 374.638\n",
      "    sample_throughput: 7.436\n",
      "    sample_time_ms: 537915.417\n",
      "    update_time_ms: 3.694\n",
      "  timestamp: 1613859071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 136.33\n",
      "  episode_reward_max: 118.37612607047315\n",
      "  episode_reward_mean: 55.33395688133094\n",
      "  episode_reward_min: -106.74472136491764\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2198\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6516956686973572\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0131781455129385\n",
      "        model: {}\n",
      "        policy_loss: -0.11240272223949432\n",
      "        total_loss: 1057.59326171875\n",
      "        vf_explained_var: 0.663433849811554\n",
      "        vf_loss: 1057.675537109375\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.41907090464548\n",
      "    ram_util_percent: 37.134229828850856\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07000277572387784\n",
      "    mean_env_wait_ms: 112.7419967737489\n",
      "    mean_inference_ms: 1.760387598784\n",
      "    mean_raw_obs_processing_ms: 10.393010508156653\n",
      "  time_since_restore: 33097.06607270241\n",
      "  time_this_iter_s: 573.7192966938019\n",
      "  time_total_s: 33097.06607270241\n",
      "  timers:\n",
      "    learn_throughput: 258.926\n",
      "    learn_time_ms: 15448.431\n",
      "    load_throughput: 10711.259\n",
      "    load_time_ms: 373.439\n",
      "    sample_throughput: 7.372\n",
      "    sample_time_ms: 542579.658\n",
      "    update_time_ms: 3.673\n",
      "  timestamp: 1613859645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 137.77\n",
      "  episode_reward_max: 118.39154367729488\n",
      "  episode_reward_mean: 53.52548591209393\n",
      "  episode_reward_min: -106.74472136491764\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2224\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6383890509605408\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011560847982764244\n",
      "        model: {}\n",
      "        policy_loss: -0.11173100769519806\n",
      "        total_loss: 480.3727722167969\n",
      "        vf_explained_var: 0.7947722673416138\n",
      "        vf_loss: 480.45819091796875\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.11481028151774\n",
      "    ram_util_percent: 37.14039167686659\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06993340126743414\n",
      "    mean_env_wait_ms: 112.96682997525554\n",
      "    mean_inference_ms: 1.7589298887241482\n",
      "    mean_raw_obs_processing_ms: 10.37049473048591\n",
      "  time_since_restore: 33669.42586064339\n",
      "  time_this_iter_s: 572.359787940979\n",
      "  time_total_s: 33669.42586064339\n",
      "  timers:\n",
      "    learn_throughput: 261.575\n",
      "    learn_time_ms: 15291.97\n",
      "    load_throughput: 10783.415\n",
      "    load_time_ms: 370.94\n",
      "    sample_throughput: 7.314\n",
      "    sample_time_ms: 546890.711\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1613860217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 146.28\n",
      "  episode_reward_max: 118.39154367729488\n",
      "  episode_reward_mean: 53.7095330284575\n",
      "  episode_reward_min: -107.53358028733636\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2249\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6173250079154968\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011564599350094795\n",
      "        model: {}\n",
      "        policy_loss: -0.10615907609462738\n",
      "        total_loss: 386.08203125\n",
      "        vf_explained_var: 0.8317555785179138\n",
      "        vf_loss: 386.1618957519531\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.330996309963105\n",
      "    ram_util_percent: 37.14329643296433\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06986738502176004\n",
      "    mean_env_wait_ms: 113.1841719552211\n",
      "    mean_inference_ms: 1.7575292602693813\n",
      "    mean_raw_obs_processing_ms: 10.344066500444315\n",
      "  time_since_restore: 34238.99902796745\n",
      "  time_this_iter_s: 569.5731673240662\n",
      "  time_total_s: 34238.99902796745\n",
      "  timers:\n",
      "    learn_throughput: 264.128\n",
      "    learn_time_ms: 15144.152\n",
      "    load_throughput: 10874.314\n",
      "    load_time_ms: 367.839\n",
      "    sample_throughput: 7.252\n",
      "    sample_time_ms: 551540.589\n",
      "    update_time_ms: 3.637\n",
      "  timestamp: 1613860787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 153.63\n",
      "  episode_reward_max: 118.39154367729488\n",
      "  episode_reward_mean: 72.57427585311832\n",
      "  episode_reward_min: -107.53358028733636\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2274\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6286678314208984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009843600913882256\n",
      "        model: {}\n",
      "        policy_loss: -0.09355968236923218\n",
      "        total_loss: 192.63748168945312\n",
      "        vf_explained_var: 0.8682405948638916\n",
      "        vf_loss: 192.7086181640625\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.22223587223587\n",
      "    ram_util_percent: 37.13353808353809\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06979846207047757\n",
      "    mean_env_wait_ms: 113.41209413425135\n",
      "    mean_inference_ms: 1.7561167436421823\n",
      "    mean_raw_obs_processing_ms: 10.311088908911454\n",
      "  time_since_restore: 34809.06482720375\n",
      "  time_this_iter_s: 570.0657992362976\n",
      "  time_total_s: 34809.06482720375\n",
      "  timers:\n",
      "    learn_throughput: 264.182\n",
      "    learn_time_ms: 15141.078\n",
      "    load_throughput: 10717.712\n",
      "    load_time_ms: 373.214\n",
      "    sample_throughput: 7.187\n",
      "    sample_time_ms: 556576.911\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1613861357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-20_23-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 156.83\n",
      "  episode_reward_max: 118.39154367729488\n",
      "  episode_reward_mean: 72.23397259447046\n",
      "  episode_reward_min: -107.53358028733636\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2300\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.631923258304596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011478865519165993\n",
      "        model: {}\n",
      "        policy_loss: -0.09653747826814651\n",
      "        total_loss: 591.7462158203125\n",
      "        vf_explained_var: 0.7493155002593994\n",
      "        vf_loss: 591.816650390625\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.23815950920246\n",
      "    ram_util_percent: 37.20539877300614\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06972687405379784\n",
      "    mean_env_wait_ms: 113.6544200247218\n",
      "    mean_inference_ms: 1.7546103799009267\n",
      "    mean_raw_obs_processing_ms: 10.272346092030155\n",
      "  time_since_restore: 35380.29538941383\n",
      "  time_this_iter_s: 571.230562210083\n",
      "  time_total_s: 35380.29538941383\n",
      "  timers:\n",
      "    learn_throughput: 264.19\n",
      "    learn_time_ms: 15140.642\n",
      "    load_throughput: 10734.354\n",
      "    load_time_ms: 372.635\n",
      "    sample_throughput: 7.189\n",
      "    sample_time_ms: 556435.211\n",
      "    update_time_ms: 3.696\n",
      "  timestamp: 1613861928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 153.58\n",
      "  episode_reward_max: 118.39008429300002\n",
      "  episode_reward_mean: 76.41749330845737\n",
      "  episode_reward_min: -100.56077546933885\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2329\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.642822802066803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01272502075880766\n",
      "        model: {}\n",
      "        policy_loss: -0.10968878120183945\n",
      "        total_loss: 596.0990600585938\n",
      "        vf_explained_var: 0.8381652235984802\n",
      "        vf_loss: 596.1798095703125\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.50269277845777\n",
      "    ram_util_percent: 37.21064871481028\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06965382674759313\n",
      "    mean_env_wait_ms: 113.90630316099855\n",
      "    mean_inference_ms: 1.7530769794469412\n",
      "    mean_raw_obs_processing_ms: 10.235387185274998\n",
      "  time_since_restore: 35952.65245747566\n",
      "  time_this_iter_s: 572.3570680618286\n",
      "  time_total_s: 35952.65245747566\n",
      "  timers:\n",
      "    learn_throughput: 264.188\n",
      "    learn_time_ms: 15140.748\n",
      "    load_throughput: 10743.574\n",
      "    load_time_ms: 372.316\n",
      "    sample_throughput: 7.188\n",
      "    sample_time_ms: 556512.121\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1613862501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 146.37\n",
      "  episode_reward_max: 118.39008429300002\n",
      "  episode_reward_mean: 72.25315348460688\n",
      "  episode_reward_min: -106.2353593540642\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2357\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6185662150382996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00849862489849329\n",
      "        model: {}\n",
      "        policy_loss: -0.08290470391511917\n",
      "        total_loss: 276.4397888183594\n",
      "        vf_explained_var: 0.8126450777053833\n",
      "        vf_loss: 276.5033264160156\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.28716381418093\n",
      "    ram_util_percent: 37.147432762836196\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06958714884610312\n",
      "    mean_env_wait_ms: 114.13442536546694\n",
      "    mean_inference_ms: 1.7516627542151186\n",
      "    mean_raw_obs_processing_ms: 10.205676917041576\n",
      "  time_since_restore: 36525.324548244476\n",
      "  time_this_iter_s: 572.6720907688141\n",
      "  time_total_s: 36525.324548244476\n",
      "  timers:\n",
      "    learn_throughput: 264.169\n",
      "    learn_time_ms: 15141.83\n",
      "    load_throughput: 10747.316\n",
      "    load_time_ms: 372.186\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556292.137\n",
      "    update_time_ms: 3.573\n",
      "  timestamp: 1613863074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 138.47\n",
      "  episode_reward_max: 118.39008429300002\n",
      "  episode_reward_mean: 70.33908638281463\n",
      "  episode_reward_min: -106.2353593540642\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2387\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.620513916015625\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0102561479434371\n",
      "        model: {}\n",
      "        policy_loss: -0.09688905626535416\n",
      "        total_loss: 486.55255126953125\n",
      "        vf_explained_var: 0.7785946130752563\n",
      "        vf_loss: 486.62603759765625\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.37048780487805\n",
      "    ram_util_percent: 37.14780487804879\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06952000873593146\n",
      "    mean_env_wait_ms: 114.36072750793537\n",
      "    mean_inference_ms: 1.7502122374436757\n",
      "    mean_raw_obs_processing_ms: 10.18208803001135\n",
      "  time_since_restore: 37100.08389925957\n",
      "  time_this_iter_s: 574.7593510150909\n",
      "  time_total_s: 37100.08389925957\n",
      "  timers:\n",
      "    learn_throughput: 264.18\n",
      "    learn_time_ms: 15141.198\n",
      "    load_throughput: 10789.537\n",
      "    load_time_ms: 370.73\n",
      "    sample_throughput: 7.187\n",
      "    sample_time_ms: 556585.99\n",
      "    update_time_ms: 3.57\n",
      "  timestamp: 1613863649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 139.74\n",
      "  episode_reward_max: 118.39008429300002\n",
      "  episode_reward_mean: 76.67791844425611\n",
      "  episode_reward_min: -106.2353593540642\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2413\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6387255787849426\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009160628542304039\n",
      "        model: {}\n",
      "        policy_loss: -0.08888468146324158\n",
      "        total_loss: 340.0338439941406\n",
      "        vf_explained_var: 0.8237649202346802\n",
      "        vf_loss: 340.10186767578125\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.28171779141104\n",
      "    ram_util_percent: 37.229325153374226\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06946422410688925\n",
      "    mean_env_wait_ms: 114.54833315283351\n",
      "    mean_inference_ms: 1.7490402840484245\n",
      "    mean_raw_obs_processing_ms: 10.161260996546233\n",
      "  time_since_restore: 37671.0840651989\n",
      "  time_this_iter_s: 571.000165939331\n",
      "  time_total_s: 37671.0840651989\n",
      "  timers:\n",
      "    learn_throughput: 264.143\n",
      "    learn_time_ms: 15143.286\n",
      "    load_throughput: 10806.218\n",
      "    load_time_ms: 370.157\n",
      "    sample_throughput: 7.188\n",
      "    sample_time_ms: 556508.3\n",
      "    update_time_ms: 3.563\n",
      "  timestamp: 1613864220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-46-33\n",
      "  done: false\n",
      "  episode_len_mean: 143.7\n",
      "  episode_reward_max: 118.38248894924733\n",
      "  episode_reward_mean: 68.12918005097758\n",
      "  episode_reward_min: -106.27659037451741\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2441\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6250879764556885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012200719676911831\n",
      "        model: {}\n",
      "        policy_loss: -0.09789183735847473\n",
      "        total_loss: 1297.644287109375\n",
      "        vf_explained_var: 0.42024028301239014\n",
      "        vf_loss: 1297.71435546875\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.3661369193154\n",
      "    ram_util_percent: 37.14352078239609\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06940225042377678\n",
      "    mean_env_wait_ms: 114.75023934436805\n",
      "    mean_inference_ms: 1.747791142768952\n",
      "    mean_raw_obs_processing_ms: 10.138264605055625\n",
      "  time_since_restore: 38244.19073843956\n",
      "  time_this_iter_s: 573.1066732406616\n",
      "  time_total_s: 38244.19073843956\n",
      "  timers:\n",
      "    learn_throughput: 264.134\n",
      "    learn_time_ms: 15143.817\n",
      "    load_throughput: 10801.332\n",
      "    load_time_ms: 370.325\n",
      "    sample_throughput: 7.189\n",
      "    sample_time_ms: 556407.515\n",
      "    update_time_ms: 3.553\n",
      "  timestamp: 1613864793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_00-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 139.98\n",
      "  episode_reward_max: 118.39758531399424\n",
      "  episode_reward_mean: 63.93510598160217\n",
      "  episode_reward_min: -106.27659037451741\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2470\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.634858250617981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013342119753360748\n",
      "        model: {}\n",
      "        policy_loss: -0.11227316409349442\n",
      "        total_loss: 958.5897827148438\n",
      "        vf_explained_var: 0.658909261226654\n",
      "        vf_loss: 958.671630859375\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.47836185819071\n",
      "    ram_util_percent: 37.15929095354524\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06934058052233977\n",
      "    mean_env_wait_ms: 114.95184865274535\n",
      "    mean_inference_ms: 1.746543330488703\n",
      "    mean_raw_obs_processing_ms: 10.115347232298536\n",
      "  time_since_restore: 38817.36667227745\n",
      "  time_this_iter_s: 573.1759338378906\n",
      "  time_total_s: 38817.36667227745\n",
      "  timers:\n",
      "    learn_throughput: 264.048\n",
      "    learn_time_ms: 15148.775\n",
      "    load_throughput: 10813.873\n",
      "    load_time_ms: 369.895\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556349.164\n",
      "    update_time_ms: 3.516\n",
      "  timestamp: 1613865366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-05-38\n",
      "  done: false\n",
      "  episode_len_mean: 140.98\n",
      "  episode_reward_max: 118.39758531399424\n",
      "  episode_reward_mean: 68.35963371424914\n",
      "  episode_reward_min: -106.27659037451741\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2499\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6206836700439453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008158518932759762\n",
      "        model: {}\n",
      "        policy_loss: -0.08408588171005249\n",
      "        total_loss: 402.3951721191406\n",
      "        vf_explained_var: 0.7547827959060669\n",
      "        vf_loss: 402.46063232421875\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.487132352941174\n",
      "    ram_util_percent: 37.5827205882353\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928106152625599\n",
      "    mean_env_wait_ms: 115.1479551787551\n",
      "    mean_inference_ms: 1.7452991217292035\n",
      "    mean_raw_obs_processing_ms: 10.093613805166735\n",
      "  time_since_restore: 39389.221237659454\n",
      "  time_this_iter_s: 571.8545653820038\n",
      "  time_total_s: 39389.221237659454\n",
      "  timers:\n",
      "    learn_throughput: 264.064\n",
      "    learn_time_ms: 15147.838\n",
      "    load_throughput: 10820.519\n",
      "    load_time_ms: 369.668\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556299.514\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1613865938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-15-06\n",
      "  done: false\n",
      "  episode_len_mean: 144.33\n",
      "  episode_reward_max: 118.39841749360913\n",
      "  episode_reward_mean: 70.53637365108484\n",
      "  episode_reward_min: -106.27659037451741\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2524\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6282891631126404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008960314095020294\n",
      "        model: {}\n",
      "        policy_loss: -0.09238934516906738\n",
      "        total_loss: 583.287841796875\n",
      "        vf_explained_var: 0.6873043775558472\n",
      "        vf_loss: 583.3598022460938\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.34315659679409\n",
      "    ram_util_percent: 37.643403205918624\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0692304309933987\n",
      "    mean_env_wait_ms: 115.30995820023833\n",
      "    mean_inference_ms: 1.7442220421493562\n",
      "    mean_raw_obs_processing_ms: 10.07362794284602\n",
      "  time_since_restore: 39957.19925880432\n",
      "  time_this_iter_s: 567.978021144867\n",
      "  time_total_s: 39957.19925880432\n",
      "  timers:\n",
      "    learn_throughput: 264.03\n",
      "    learn_time_ms: 15149.767\n",
      "    load_throughput: 10824.969\n",
      "    load_time_ms: 369.516\n",
      "    sample_throughput: 7.192\n",
      "    sample_time_ms: 556136.573\n",
      "    update_time_ms: 3.526\n",
      "  timestamp: 1613866506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-24-41\n",
      "  done: false\n",
      "  episode_len_mean: 138.2\n",
      "  episode_reward_max: 118.39841749360913\n",
      "  episode_reward_mean: 74.77718520882522\n",
      "  episode_reward_min: -98.78478614366963\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 2556\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6217712163925171\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010317807085812092\n",
      "        model: {}\n",
      "        policy_loss: -0.09684766829013824\n",
      "        total_loss: 471.45892333984375\n",
      "        vf_explained_var: 0.8024389147758484\n",
      "        vf_loss: 471.5322570800781\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.57780487804878\n",
      "    ram_util_percent: 37.633170731707324\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06916814033471935\n",
      "    mean_env_wait_ms: 115.5077081205219\n",
      "    mean_inference_ms: 1.7428648770769166\n",
      "    mean_raw_obs_processing_ms: 10.053242277202832\n",
      "  time_since_restore: 40532.23022842407\n",
      "  time_this_iter_s: 575.030969619751\n",
      "  time_total_s: 40532.23022842407\n",
      "  timers:\n",
      "    learn_throughput: 264.061\n",
      "    learn_time_ms: 15148.015\n",
      "    load_throughput: 10993.946\n",
      "    load_time_ms: 363.837\n",
      "    sample_throughput: 7.186\n",
      "    sample_time_ms: 556641.082\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1613867081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-34-14\n",
      "  done: false\n",
      "  episode_len_mean: 142.53\n",
      "  episode_reward_max: 118.39841749360913\n",
      "  episode_reward_mean: 78.98087082098333\n",
      "  episode_reward_min: -98.7417065471983\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2584\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6119922995567322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011394803412258625\n",
      "        model: {}\n",
      "        policy_loss: -0.09888093918561935\n",
      "        total_loss: 356.1640625\n",
      "        vf_explained_var: 0.8513246178627014\n",
      "        vf_loss: 356.23699951171875\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.410281517747855\n",
      "    ram_util_percent: 37.63818849449205\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06911540389132194\n",
      "    mean_env_wait_ms: 115.67576647059974\n",
      "    mean_inference_ms: 1.741715638472674\n",
      "    mean_raw_obs_processing_ms: 10.034937057725308\n",
      "  time_since_restore: 41104.826088905334\n",
      "  time_this_iter_s: 572.5958604812622\n",
      "  time_total_s: 41104.826088905334\n",
      "  timers:\n",
      "    learn_throughput: 264.058\n",
      "    learn_time_ms: 15148.206\n",
      "    load_throughput: 10988.041\n",
      "    load_time_ms: 364.032\n",
      "    sample_throughput: 7.184\n",
      "    sample_time_ms: 556776.6\n",
      "    update_time_ms: 3.482\n",
      "  timestamp: 1613867654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-43-49\n",
      "  done: false\n",
      "  episode_len_mean: 134.93\n",
      "  episode_reward_max: 118.38809567022436\n",
      "  episode_reward_mean: 66.23044507460551\n",
      "  episode_reward_min: -100.33457397895387\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2614\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6089362502098083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011843282729387283\n",
      "        model: {}\n",
      "        policy_loss: -0.10525768995285034\n",
      "        total_loss: 799.7393188476562\n",
      "        vf_explained_var: 0.6889356970787048\n",
      "        vf_loss: 799.8175659179688\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.479390243902444\n",
      "    ram_util_percent: 37.5989024390244\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06906043211441769\n",
      "    mean_env_wait_ms: 115.85209515094088\n",
      "    mean_inference_ms: 1.7405040158566727\n",
      "    mean_raw_obs_processing_ms: 10.01951022321941\n",
      "  time_since_restore: 41679.47191905975\n",
      "  time_this_iter_s: 574.645830154419\n",
      "  time_total_s: 41679.47191905975\n",
      "  timers:\n",
      "    learn_throughput: 264.04\n",
      "    learn_time_ms: 15149.244\n",
      "    load_throughput: 10991.512\n",
      "    load_time_ms: 363.917\n",
      "    sample_throughput: 7.181\n",
      "    sample_time_ms: 557005.718\n",
      "    update_time_ms: 3.47\n",
      "  timestamp: 1613868229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_01-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 129.62\n",
      "  episode_reward_max: 118.38994456942554\n",
      "  episode_reward_mean: 66.30857952219603\n",
      "  episode_reward_min: -100.72462310708792\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 2648\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.612834095954895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011830639094114304\n",
      "        model: {}\n",
      "        policy_loss: -0.10811416059732437\n",
      "        total_loss: 721.0752563476562\n",
      "        vf_explained_var: 0.7420914173126221\n",
      "        vf_loss: 721.1563720703125\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.457506053268766\n",
      "    ram_util_percent: 37.61973365617434\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0690039489207212\n",
      "    mean_env_wait_ms: 116.03571795318099\n",
      "    mean_inference_ms: 1.73923570012803\n",
      "    mean_raw_obs_processing_ms: 10.009942111648098\n",
      "  time_since_restore: 42258.06580400467\n",
      "  time_this_iter_s: 578.5938849449158\n",
      "  time_total_s: 42258.06580400467\n",
      "  timers:\n",
      "    learn_throughput: 264.09\n",
      "    learn_time_ms: 15146.35\n",
      "    load_throughput: 10990.486\n",
      "    load_time_ms: 363.951\n",
      "    sample_throughput: 7.174\n",
      "    sample_time_ms: 557601.894\n",
      "    update_time_ms: 3.449\n",
      "  timestamp: 1613868808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 128.4\n",
      "  episode_reward_max: 118.38994456942554\n",
      "  episode_reward_mean: 60.033566095574386\n",
      "  episode_reward_min: -100.72462310708792\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2677\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6140277981758118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008003169670701027\n",
      "        model: {}\n",
      "        policy_loss: -0.08613423258066177\n",
      "        total_loss: 503.7268981933594\n",
      "        vf_explained_var: 0.7947396636009216\n",
      "        vf_loss: 503.7947998046875\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.43577533577534\n",
      "    ram_util_percent: 37.60830280830282\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06895626932661361\n",
      "    mean_env_wait_ms: 116.18808191371505\n",
      "    mean_inference_ms: 1.7382185606484102\n",
      "    mean_raw_obs_processing_ms: 10.002050044919022\n",
      "  time_since_restore: 42831.9421107769\n",
      "  time_this_iter_s: 573.876306772232\n",
      "  time_total_s: 42831.9421107769\n",
      "  timers:\n",
      "    learn_throughput: 264.097\n",
      "    learn_time_ms: 15145.95\n",
      "    load_throughput: 11019.773\n",
      "    load_time_ms: 362.984\n",
      "    sample_throughput: 7.175\n",
      "    sample_time_ms: 557513.472\n",
      "    update_time_ms: 3.463\n",
      "  timestamp: 1613869382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 126.19\n",
      "  episode_reward_max: 118.38994456942554\n",
      "  episode_reward_mean: 55.84332330127691\n",
      "  episode_reward_min: -100.72462310708792\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 2709\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6395218968391418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010636094957590103\n",
      "        model: {}\n",
      "        policy_loss: -0.1009235680103302\n",
      "        total_loss: 1037.17919921875\n",
      "        vf_explained_var: 0.5761825442314148\n",
      "        vf_loss: 1037.2559814453125\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.428883495145634\n",
      "    ram_util_percent: 37.59550970873787\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06890648467670632\n",
      "    mean_env_wait_ms: 116.34881545523703\n",
      "    mean_inference_ms: 1.7371903569719798\n",
      "    mean_raw_obs_processing_ms: 9.996829275702517\n",
      "  time_since_restore: 43408.906421899796\n",
      "  time_this_iter_s: 576.9643111228943\n",
      "  time_total_s: 43408.906421899796\n",
      "  timers:\n",
      "    learn_throughput: 264.108\n",
      "    learn_time_ms: 15145.311\n",
      "    load_throughput: 11021.604\n",
      "    load_time_ms: 362.924\n",
      "    sample_throughput: 7.167\n",
      "    sample_time_ms: 558111.411\n",
      "    update_time_ms: 3.453\n",
      "  timestamp: 1613869959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 134.56\n",
      "  episode_reward_max: 118.37185952922911\n",
      "  episode_reward_mean: 64.17353366881248\n",
      "  episode_reward_min: -100.72462310708792\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2737\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6272621750831604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00920060183852911\n",
      "        model: {}\n",
      "        policy_loss: -0.09657780081033707\n",
      "        total_loss: 591.6361694335938\n",
      "        vf_explained_var: 0.675640881061554\n",
      "        vf_loss: 591.7117309570312\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38996328029376\n",
      "    ram_util_percent: 37.62325581395349\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688652520254107\n",
      "    mean_env_wait_ms: 116.4851184964402\n",
      "    mean_inference_ms: 1.7363364645416846\n",
      "    mean_raw_obs_processing_ms: 9.986971637905215\n",
      "  time_since_restore: 43981.41079258919\n",
      "  time_this_iter_s: 572.5043706893921\n",
      "  time_total_s: 43981.41079258919\n",
      "  timers:\n",
      "    learn_throughput: 264.109\n",
      "    learn_time_ms: 15145.289\n",
      "    load_throughput: 11025.657\n",
      "    load_time_ms: 362.79\n",
      "    sample_throughput: 7.168\n",
      "    sample_time_ms: 558050.636\n",
      "    update_time_ms: 3.474\n",
      "  timestamp: 1613870531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-31-44\n",
      "  done: false\n",
      "  episode_len_mean: 139.17\n",
      "  episode_reward_max: 118.36569738858549\n",
      "  episode_reward_mean: 74.64579489491926\n",
      "  episode_reward_min: -100.4791623546053\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2765\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5827945470809937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006520960479974747\n",
      "        model: {}\n",
      "        policy_loss: -0.06697624921798706\n",
      "        total_loss: 203.43817138671875\n",
      "        vf_explained_var: 0.8140708208084106\n",
      "        vf_loss: 203.49026489257812\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.3812729498164\n",
      "    ram_util_percent: 37.64088127294982\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688231221143372\n",
      "    mean_env_wait_ms: 116.62777605461848\n",
      "    mean_inference_ms: 1.735407625887317\n",
      "    mean_raw_obs_processing_ms: 9.974503712169184\n",
      "  time_since_restore: 44553.97675514221\n",
      "  time_this_iter_s: 572.5659625530243\n",
      "  time_total_s: 44553.97675514221\n",
      "  timers:\n",
      "    learn_throughput: 264.173\n",
      "    learn_time_ms: 15141.617\n",
      "    load_throughput: 11003.219\n",
      "    load_time_ms: 363.53\n",
      "    sample_throughput: 7.169\n",
      "    sample_time_ms: 557992.14\n",
      "    update_time_ms: 3.51\n",
      "  timestamp: 1613871104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-41-18\n",
      "  done: false\n",
      "  episode_len_mean: 135.91\n",
      "  episode_reward_max: 118.36569738858549\n",
      "  episode_reward_mean: 72.48419406469598\n",
      "  episode_reward_min: -100.4791623546053\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2795\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6341033577919006\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01128568034619093\n",
      "        model: {}\n",
      "        policy_loss: -0.1100163459777832\n",
      "        total_loss: 805.0309448242188\n",
      "        vf_explained_var: 0.7002952694892883\n",
      "        vf_loss: 805.1151733398438\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.42576312576312\n",
      "    ram_util_percent: 37.62319902319903\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0687784308105397\n",
      "    mean_env_wait_ms: 116.77627190738745\n",
      "    mean_inference_ms: 1.7344180800662337\n",
      "    mean_raw_obs_processing_ms: 9.960494716418623\n",
      "  time_since_restore: 45127.50771832466\n",
      "  time_this_iter_s: 573.5309631824493\n",
      "  time_total_s: 45127.50771832466\n",
      "  timers:\n",
      "    learn_throughput: 264.164\n",
      "    learn_time_ms: 15142.128\n",
      "    load_throughput: 10999.194\n",
      "    load_time_ms: 363.663\n",
      "    sample_throughput: 7.166\n",
      "    sample_time_ms: 558160.031\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1613871678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_02-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 141.88\n",
      "  episode_reward_max: 118.36569738858549\n",
      "  episode_reward_mean: 74.47919637461047\n",
      "  episode_reward_min: -100.33541309345999\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2822\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6619164347648621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012647053226828575\n",
      "        model: {}\n",
      "        policy_loss: -0.11788468062877655\n",
      "        total_loss: 890.1051635742188\n",
      "        vf_explained_var: 0.667721152305603\n",
      "        vf_loss: 890.1942749023438\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.31176470588235\n",
      "    ram_util_percent: 37.63468137254902\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06873833848833105\n",
      "    mean_env_wait_ms: 116.91226338279147\n",
      "    mean_inference_ms: 1.7335231801235562\n",
      "    mean_raw_obs_processing_ms: 9.945158745382983\n",
      "  time_since_restore: 45699.621527433395\n",
      "  time_this_iter_s: 572.1138091087341\n",
      "  time_total_s: 45699.621527433395\n",
      "  timers:\n",
      "    learn_throughput: 264.207\n",
      "    learn_time_ms: 15139.67\n",
      "    load_throughput: 11016.095\n",
      "    load_time_ms: 363.105\n",
      "    sample_throughput: 7.161\n",
      "    sample_time_ms: 558577.777\n",
      "    update_time_ms: 3.457\n",
      "  timestamp: 1613872250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 141.63\n",
      "  episode_reward_max: 118.39972560355228\n",
      "  episode_reward_mean: 70.23463403469647\n",
      "  episode_reward_min: -100.48570400254098\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2849\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6279558539390564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008916793391108513\n",
      "        model: {}\n",
      "        policy_loss: -0.08972213417291641\n",
      "        total_loss: 401.4216003417969\n",
      "        vf_explained_var: 0.7704207301139832\n",
      "        vf_loss: 401.4909973144531\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.31370869033047\n",
      "    ram_util_percent: 37.6670746634027\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06869867081991787\n",
      "    mean_env_wait_ms: 117.04716744173258\n",
      "    mean_inference_ms: 1.7326487293140218\n",
      "    mean_raw_obs_processing_ms: 9.929296489515842\n",
      "  time_since_restore: 46272.038051605225\n",
      "  time_this_iter_s: 572.4165241718292\n",
      "  time_total_s: 46272.038051605225\n",
      "  timers:\n",
      "    learn_throughput: 264.183\n",
      "    learn_time_ms: 15141.005\n",
      "    load_throughput: 11017.058\n",
      "    load_time_ms: 363.073\n",
      "    sample_throughput: 7.164\n",
      "    sample_time_ms: 558314.856\n",
      "    update_time_ms: 3.489\n",
      "  timestamp: 1613872823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-09-53\n",
      "  done: false\n",
      "  episode_len_mean: 145.91\n",
      "  episode_reward_max: 118.39972560355228\n",
      "  episode_reward_mean: 78.59757373313865\n",
      "  episode_reward_min: -101.48718799048909\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2874\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.609645426273346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010317403823137283\n",
      "        model: {}\n",
      "        policy_loss: -0.09546666592359543\n",
      "        total_loss: 51.94847106933594\n",
      "        vf_explained_var: 0.9522272348403931\n",
      "        vf_loss: 52.02043151855469\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.37235872235873\n",
      "    ram_util_percent: 37.65970515970516\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06866188360220725\n",
      "    mean_env_wait_ms: 117.1731756625959\n",
      "    mean_inference_ms: 1.7318602560513392\n",
      "    mean_raw_obs_processing_ms: 9.911490483068429\n",
      "  time_since_restore: 46842.017905950546\n",
      "  time_this_iter_s: 569.9798543453217\n",
      "  time_total_s: 46842.017905950546\n",
      "  timers:\n",
      "    learn_throughput: 264.185\n",
      "    learn_time_ms: 15140.929\n",
      "    load_throughput: 11015.432\n",
      "    load_time_ms: 363.127\n",
      "    sample_throughput: 7.168\n",
      "    sample_time_ms: 558051.999\n",
      "    update_time_ms: 3.523\n",
      "  timestamp: 1613873393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 151.03\n",
      "  episode_reward_max: 118.39972560355228\n",
      "  episode_reward_mean: 86.74178242094577\n",
      "  episode_reward_min: -106.32426879110315\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2901\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6083473563194275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006297805346548557\n",
      "        model: {}\n",
      "        policy_loss: -0.06047149375081062\n",
      "        total_loss: 84.03146362304688\n",
      "        vf_explained_var: 0.9165517687797546\n",
      "        vf_loss: 84.07758331298828\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.36279069767442\n",
      "    ram_util_percent: 37.65250917992656\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06862152295259255\n",
      "    mean_env_wait_ms: 117.31282317533726\n",
      "    mean_inference_ms: 1.731016450706324\n",
      "    mean_raw_obs_processing_ms: 9.890075394770157\n",
      "  time_since_restore: 47414.542248249054\n",
      "  time_this_iter_s: 572.5243422985077\n",
      "  time_total_s: 47414.542248249054\n",
      "  timers:\n",
      "    learn_throughput: 264.178\n",
      "    learn_time_ms: 15141.325\n",
      "    load_throughput: 10993.389\n",
      "    load_time_ms: 363.855\n",
      "    sample_throughput: 7.171\n",
      "    sample_time_ms: 557836.4\n",
      "    update_time_ms: 3.545\n",
      "  timestamp: 1613873965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 152.05\n",
      "  episode_reward_max: 118.39972560355228\n",
      "  episode_reward_mean: 95.47943811053409\n",
      "  episode_reward_min: -106.32426879110315\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 2927\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6245173215866089\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008704432286322117\n",
      "        model: {}\n",
      "        policy_loss: -0.07819437235593796\n",
      "        total_loss: 741.48486328125\n",
      "        vf_explained_var: 0.49536240100860596\n",
      "        vf_loss: 741.543212890625\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.23799019607843\n",
      "    ram_util_percent: 37.60428921568628\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06858305551282297\n",
      "    mean_env_wait_ms: 117.44568936548764\n",
      "    mean_inference_ms: 1.730216980502728\n",
      "    mean_raw_obs_processing_ms: 9.868999614297818\n",
      "  time_since_restore: 47986.192225933075\n",
      "  time_this_iter_s: 571.649977684021\n",
      "  time_total_s: 47986.192225933075\n",
      "  timers:\n",
      "    learn_throughput: 264.133\n",
      "    learn_time_ms: 15143.879\n",
      "    load_throughput: 10988.587\n",
      "    load_time_ms: 364.014\n",
      "    sample_throughput: 7.18\n",
      "    sample_time_ms: 557138.008\n",
      "    update_time_ms: 3.569\n",
      "  timestamp: 1613874537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 148.45\n",
      "  episode_reward_max: 118.36638515678655\n",
      "  episode_reward_mean: 86.87700961007907\n",
      "  episode_reward_min: -106.32426879110315\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2956\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6059015989303589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011917388066649437\n",
      "        model: {}\n",
      "        policy_loss: -0.10298958420753479\n",
      "        total_loss: 748.3084106445312\n",
      "        vf_explained_var: 0.6828364729881287\n",
      "        vf_loss: 748.38427734375\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.4583129584352\n",
      "    ram_util_percent: 37.63496332518338\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06854048496142572\n",
      "    mean_env_wait_ms: 117.58732035088931\n",
      "    mean_inference_ms: 1.7293595306129033\n",
      "    mean_raw_obs_processing_ms: 9.84859556894841\n",
      "  time_since_restore: 48559.266986608505\n",
      "  time_this_iter_s: 573.0747606754303\n",
      "  time_total_s: 48559.266986608505\n",
      "  timers:\n",
      "    learn_throughput: 264.152\n",
      "    learn_time_ms: 15142.82\n",
      "    load_throughput: 10967.227\n",
      "    load_time_ms: 364.723\n",
      "    sample_throughput: 7.181\n",
      "    sample_time_ms: 557059.872\n",
      "    update_time_ms: 3.568\n",
      "  timestamp: 1613875110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 147.16\n",
      "  episode_reward_max: 118.36638515678655\n",
      "  episode_reward_mean: 78.53596061698514\n",
      "  episode_reward_min: -104.59048121757738\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2984\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6384892463684082\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014252190478146076\n",
      "        model: {}\n",
      "        policy_loss: -0.12938052415847778\n",
      "        total_loss: 1040.5548095703125\n",
      "        vf_explained_var: 0.6462556719779968\n",
      "        vf_loss: 1040.651611328125\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.31528117359414\n",
      "    ram_util_percent: 37.71638141809291\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0685013111223692\n",
      "    mean_env_wait_ms: 117.71636245049473\n",
      "    mean_inference_ms: 1.7285761946448637\n",
      "    mean_raw_obs_processing_ms: 9.832269351667906\n",
      "  time_since_restore: 49132.35148835182\n",
      "  time_this_iter_s: 573.0845017433167\n",
      "  time_total_s: 49132.35148835182\n",
      "  timers:\n",
      "    learn_throughput: 264.17\n",
      "    learn_time_ms: 15141.759\n",
      "    load_throughput: 10932.693\n",
      "    load_time_ms: 365.875\n",
      "    sample_throughput: 7.186\n",
      "    sample_time_ms: 556670.702\n",
      "    update_time_ms: 3.56\n",
      "  timestamp: 1613875683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_03-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 148.59\n",
      "  episode_reward_max: 118.3861980285146\n",
      "  episode_reward_mean: 76.27011604884156\n",
      "  episode_reward_min: -104.59048121757738\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3009\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6160101294517517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008099525235593319\n",
      "        model: {}\n",
      "        policy_loss: -0.08657196909189224\n",
      "        total_loss: 311.40777587890625\n",
      "        vf_explained_var: 0.7959339618682861\n",
      "        vf_loss: 311.4758605957031\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.257493857493856\n",
      "    ram_util_percent: 37.72186732186732\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684672460741856\n",
      "    mean_env_wait_ms: 117.83028991731415\n",
      "    mean_inference_ms: 1.727895503967211\n",
      "    mean_raw_obs_processing_ms: 9.816608724019238\n",
      "  time_since_restore: 49702.79315876961\n",
      "  time_this_iter_s: 570.4416704177856\n",
      "  time_total_s: 49702.79315876961\n",
      "  timers:\n",
      "    learn_throughput: 264.149\n",
      "    learn_time_ms: 15142.975\n",
      "    load_throughput: 10922.587\n",
      "    load_time_ms: 366.214\n",
      "    sample_throughput: 7.188\n",
      "    sample_time_ms: 556462.525\n",
      "    update_time_ms: 3.59\n",
      "  timestamp: 1613876254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-07-06\n",
      "  done: false\n",
      "  episode_len_mean: 144.0\n",
      "  episode_reward_max: 118.3861980285146\n",
      "  episode_reward_mean: 76.09072176362433\n",
      "  episode_reward_min: -104.59048121757738\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3036\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.620825469493866\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007575416937470436\n",
      "        model: {}\n",
      "        policy_loss: -0.07958129048347473\n",
      "        total_loss: 195.9188232421875\n",
      "        vf_explained_var: 0.8314293622970581\n",
      "        vf_loss: 195.98106384277344\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38223039215686\n",
      "    ram_util_percent: 37.66531862745098\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06843085041858095\n",
      "    mean_env_wait_ms: 117.94984110136356\n",
      "    mean_inference_ms: 1.7271565825316366\n",
      "    mean_raw_obs_processing_ms: 9.80010601973607\n",
      "  time_since_restore: 50274.3007338047\n",
      "  time_this_iter_s: 571.5075750350952\n",
      "  time_total_s: 50274.3007338047\n",
      "  timers:\n",
      "    learn_throughput: 264.177\n",
      "    learn_time_ms: 15141.335\n",
      "    load_throughput: 10932.309\n",
      "    load_time_ms: 365.888\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556358.174\n",
      "    update_time_ms: 3.595\n",
      "  timestamp: 1613876826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 154.13\n",
      "  episode_reward_max: 118.3861980285146\n",
      "  episode_reward_mean: 90.7099485422507\n",
      "  episode_reward_min: -101.66335937256399\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3061\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6019314527511597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00965670682489872\n",
      "        model: {}\n",
      "        policy_loss: -0.09145587682723999\n",
      "        total_loss: 34.35165023803711\n",
      "        vf_explained_var: 0.9595359563827515\n",
      "        vf_loss: 34.421104431152344\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.28488943488943\n",
      "    ram_util_percent: 37.66216216216216\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06839668156279691\n",
      "    mean_env_wait_ms: 118.06309445303968\n",
      "    mean_inference_ms: 1.7264800011997932\n",
      "    mean_raw_obs_processing_ms: 9.781802359337073\n",
      "  time_since_restore: 50844.359786748886\n",
      "  time_this_iter_s: 570.0590529441833\n",
      "  time_total_s: 50844.359786748886\n",
      "  timers:\n",
      "    learn_throughput: 264.148\n",
      "    learn_time_ms: 15143.038\n",
      "    load_throughput: 10942.055\n",
      "    load_time_ms: 365.562\n",
      "    sample_throughput: 7.194\n",
      "    sample_time_ms: 556008.715\n",
      "    update_time_ms: 3.629\n",
      "  timestamp: 1613877396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-26-11\n",
      "  done: false\n",
      "  episode_len_mean: 151.55\n",
      "  episode_reward_max: 118.3754436889049\n",
      "  episode_reward_mean: 90.73738237431702\n",
      "  episode_reward_min: -105.80907272476225\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3090\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6180492639541626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01009252667427063\n",
      "        model: {}\n",
      "        policy_loss: -0.0961713045835495\n",
      "        total_loss: 532.2601928710938\n",
      "        vf_explained_var: 0.7508637309074402\n",
      "        vf_loss: 532.3333740234375\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.34731707317073\n",
      "    ram_util_percent: 37.65280487804879\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06835815682144136\n",
      "    mean_env_wait_ms: 118.19191298830596\n",
      "    mean_inference_ms: 1.725703411393963\n",
      "    mean_raw_obs_processing_ms: 9.76259727065554\n",
      "  time_since_restore: 51419.60346984863\n",
      "  time_this_iter_s: 575.2436830997467\n",
      "  time_total_s: 51419.60346984863\n",
      "  timers:\n",
      "    learn_throughput: 264.126\n",
      "    learn_time_ms: 15144.265\n",
      "    load_throughput: 10931.565\n",
      "    load_time_ms: 365.913\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556320.133\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1613877971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-35-44\n",
      "  done: false\n",
      "  episode_len_mean: 148.99\n",
      "  episode_reward_max: 118.3754436889049\n",
      "  episode_reward_mean: 95.00853702159911\n",
      "  episode_reward_min: -105.80907272476225\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3117\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6246620416641235\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006574051920324564\n",
      "        model: {}\n",
      "        policy_loss: -0.06613514572381973\n",
      "        total_loss: 164.94639587402344\n",
      "        vf_explained_var: 0.8330784440040588\n",
      "        vf_loss: 164.99755859375\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.297188264058676\n",
      "    ram_util_percent: 37.675794621026895\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0683223278202251\n",
      "    mean_env_wait_ms: 118.30677647535566\n",
      "    mean_inference_ms: 1.7250091530794134\n",
      "    mean_raw_obs_processing_ms: 9.746823513430975\n",
      "  time_since_restore: 51992.47012424469\n",
      "  time_this_iter_s: 572.8666543960571\n",
      "  time_total_s: 51992.47012424469\n",
      "  timers:\n",
      "    learn_throughput: 264.124\n",
      "    learn_time_ms: 15144.379\n",
      "    load_throughput: 10917.265\n",
      "    load_time_ms: 366.392\n",
      "    sample_throughput: 7.19\n",
      "    sample_time_ms: 556364.217\n",
      "    update_time_ms: 3.558\n",
      "  timestamp: 1613878544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 148.49\n",
      "  episode_reward_max: 118.3754436889049\n",
      "  episode_reward_mean: 92.9885752701855\n",
      "  episode_reward_min: -105.80907272476225\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3143\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6129283308982849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007502316031605005\n",
      "        model: {}\n",
      "        policy_loss: -0.07701314240694046\n",
      "        total_loss: 418.00335693359375\n",
      "        vf_explained_var: 0.7102574110031128\n",
      "        vf_loss: 418.063232421875\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.31176470588235\n",
      "    ram_util_percent: 37.693137254901956\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06828794418556879\n",
      "    mean_env_wait_ms: 118.41655315671734\n",
      "    mean_inference_ms: 1.7243705784511854\n",
      "    mean_raw_obs_processing_ms: 9.731387966899245\n",
      "  time_since_restore: 52563.917846918106\n",
      "  time_this_iter_s: 571.4477226734161\n",
      "  time_total_s: 52563.917846918106\n",
      "  timers:\n",
      "    learn_throughput: 264.134\n",
      "    learn_time_ms: 15143.824\n",
      "    load_throughput: 10913.187\n",
      "    load_time_ms: 366.529\n",
      "    sample_throughput: 7.188\n",
      "    sample_time_ms: 556511.074\n",
      "    update_time_ms: 3.511\n",
      "  timestamp: 1613879116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_04-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 145.7\n",
      "  episode_reward_max: 118.33897899119724\n",
      "  episode_reward_mean: 88.84848190246228\n",
      "  episode_reward_min: -105.80907272476225\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3170\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6175667643547058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006731769535690546\n",
      "        model: {}\n",
      "        policy_loss: -0.06991294771432877\n",
      "        total_loss: 247.48231506347656\n",
      "        vf_explained_var: 0.7953845262527466\n",
      "        vf_loss: 247.5369110107422\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.353545232273845\n",
      "    ram_util_percent: 37.635330073349635\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06825275092281008\n",
      "    mean_env_wait_ms: 118.52690351343179\n",
      "    mean_inference_ms: 1.7237184069547202\n",
      "    mean_raw_obs_processing_ms: 9.71665519707479\n",
      "  time_since_restore: 53137.459812402725\n",
      "  time_this_iter_s: 573.5419654846191\n",
      "  time_total_s: 53137.459812402725\n",
      "  timers:\n",
      "    learn_throughput: 264.145\n",
      "    learn_time_ms: 15143.219\n",
      "    load_throughput: 10953.005\n",
      "    load_time_ms: 365.197\n",
      "    sample_throughput: 7.186\n",
      "    sample_time_ms: 556616.653\n",
      "    update_time_ms: 3.54\n",
      "  timestamp: 1613879690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 147.11\n",
      "  episode_reward_max: 118.33897899119724\n",
      "  episode_reward_mean: 93.14223227238632\n",
      "  episode_reward_min: -102.63207047748058\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3199\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6179011464118958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011885471642017365\n",
      "        model: {}\n",
      "        policy_loss: -0.11724616587162018\n",
      "        total_loss: 561.7438354492188\n",
      "        vf_explained_var: 0.7938085198402405\n",
      "        vf_loss: 561.8339233398438\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.43780487804878\n",
      "    ram_util_percent: 37.717317073170726\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06821509692921657\n",
      "    mean_env_wait_ms: 118.64317735068947\n",
      "    mean_inference_ms: 1.7230314647557228\n",
      "    mean_raw_obs_processing_ms: 9.701350603621318\n",
      "  time_since_restore: 53711.56699895859\n",
      "  time_this_iter_s: 574.1071865558624\n",
      "  time_total_s: 53711.56699895859\n",
      "  timers:\n",
      "    learn_throughput: 264.16\n",
      "    learn_time_ms: 15142.316\n",
      "    load_throughput: 10965.797\n",
      "    load_time_ms: 364.771\n",
      "    sample_throughput: 7.183\n",
      "    sample_time_ms: 556863.546\n",
      "    update_time_ms: 3.559\n",
      "  timestamp: 1613880264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 145.16\n",
      "  episode_reward_max: 118.3942711538609\n",
      "  episode_reward_mean: 87.0959860036248\n",
      "  episode_reward_min: -102.63207047748058\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 3227\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6058357954025269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01053436566144228\n",
      "        model: {}\n",
      "        policy_loss: -0.09249967336654663\n",
      "        total_loss: 812.6344604492188\n",
      "        vf_explained_var: 0.5682348012924194\n",
      "        vf_loss: 812.703125\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.33614163614164\n",
      "    ram_util_percent: 37.71672771672772\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06817984997533298\n",
      "    mean_env_wait_ms: 118.75171630927481\n",
      "    mean_inference_ms: 1.722390448427243\n",
      "    mean_raw_obs_processing_ms: 9.688252713676322\n",
      "  time_since_restore: 54285.57768249512\n",
      "  time_this_iter_s: 574.0106835365295\n",
      "  time_total_s: 54285.57768249512\n",
      "  timers:\n",
      "    learn_throughput: 264.144\n",
      "    learn_time_ms: 15143.28\n",
      "    load_throughput: 10978.317\n",
      "    load_time_ms: 364.355\n",
      "    sample_throughput: 7.182\n",
      "    sample_time_ms: 556955.563\n",
      "    update_time_ms: 3.554\n",
      "  timestamp: 1613880838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-23-29\n",
      "  done: false\n",
      "  episode_len_mean: 147.48\n",
      "  episode_reward_max: 118.3942711538609\n",
      "  episode_reward_mean: 89.24138729586062\n",
      "  episode_reward_min: -102.63207047748058\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3252\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6104090213775635\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005886872764676809\n",
      "        model: {}\n",
      "        policy_loss: -0.06748391687870026\n",
      "        total_loss: 433.7013244628906\n",
      "        vf_explained_var: 0.6146568655967712\n",
      "        vf_loss: 433.75531005859375\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.2123774509804\n",
      "    ram_util_percent: 37.689338235294116\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06814926838970166\n",
      "    mean_env_wait_ms: 118.84731336256696\n",
      "    mean_inference_ms: 1.7218322173745153\n",
      "    mean_raw_obs_processing_ms: 9.675675333114134\n",
      "  time_since_restore: 54857.00827693939\n",
      "  time_this_iter_s: 571.4305944442749\n",
      "  time_total_s: 54857.00827693939\n",
      "  timers:\n",
      "    learn_throughput: 264.127\n",
      "    learn_time_ms: 15144.202\n",
      "    load_throughput: 10984.24\n",
      "    load_time_ms: 364.158\n",
      "    sample_throughput: 7.184\n",
      "    sample_time_ms: 556790.183\n",
      "    update_time_ms: 3.583\n",
      "  timestamp: 1613881409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 139.98\n",
      "  episode_reward_max: 118.39790261515664\n",
      "  episode_reward_mean: 80.92598589407584\n",
      "  episode_reward_min: -102.63207047748058\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 3283\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5881627798080444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009681874886155128\n",
      "        model: {}\n",
      "        policy_loss: -0.09490907937288284\n",
      "        total_loss: 613.488525390625\n",
      "        vf_explained_var: 0.7268031239509583\n",
      "        vf_loss: 613.5614624023438\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.415188335358444\n",
      "    ram_util_percent: 37.6681652490887\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06811326274325878\n",
      "    mean_env_wait_ms: 118.9614087773805\n",
      "    mean_inference_ms: 1.7211691998654428\n",
      "    mean_raw_obs_processing_ms: 9.663272480891708\n",
      "  time_since_restore: 55434.1220228672\n",
      "  time_this_iter_s: 577.1137459278107\n",
      "  time_total_s: 55434.1220228672\n",
      "  timers:\n",
      "    learn_throughput: 264.149\n",
      "    learn_time_ms: 15142.988\n",
      "    load_throughput: 11012.724\n",
      "    load_time_ms: 363.216\n",
      "    sample_throughput: 7.175\n",
      "    sample_time_ms: 557460.417\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1613881987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 143.48\n",
      "  episode_reward_max: 118.39790261515664\n",
      "  episode_reward_mean: 82.98365740609349\n",
      "  episode_reward_min: -97.20121284343826\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3312\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6338304281234741\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00997522659599781\n",
      "        model: {}\n",
      "        policy_loss: -0.10173895955085754\n",
      "        total_loss: 616.504150390625\n",
      "        vf_explained_var: 0.6925467252731323\n",
      "        vf_loss: 616.583251953125\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.386026731470224\n",
      "    ram_util_percent: 37.688092345078985\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06808057343717405\n",
      "    mean_env_wait_ms: 119.06631584219063\n",
      "    mean_inference_ms: 1.7205515196110677\n",
      "    mean_raw_obs_processing_ms: 9.652490343340176\n",
      "  time_since_restore: 56010.4632897377\n",
      "  time_this_iter_s: 576.3412668704987\n",
      "  time_total_s: 56010.4632897377\n",
      "  timers:\n",
      "    learn_throughput: 264.144\n",
      "    learn_time_ms: 15143.269\n",
      "    load_throughput: 11032.888\n",
      "    load_time_ms: 362.552\n",
      "    sample_throughput: 7.169\n",
      "    sample_time_ms: 557944.608\n",
      "    update_time_ms: 3.504\n",
      "  timestamp: 1613882563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_05-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 136.03\n",
      "  episode_reward_max: 118.39790261515664\n",
      "  episode_reward_mean: 78.5377385374056\n",
      "  episode_reward_min: -105.91604945757742\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 3342\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6088712811470032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006714586168527603\n",
      "        model: {}\n",
      "        policy_loss: -0.0737038105726242\n",
      "        total_loss: 247.77517700195312\n",
      "        vf_explained_var: 0.8366903066635132\n",
      "        vf_loss: 247.83358764648438\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.42153284671533\n",
      "    ram_util_percent: 37.645012165450126\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06804771686450926\n",
      "    mean_env_wait_ms: 119.17053416729992\n",
      "    mean_inference_ms: 1.7199192467552313\n",
      "    mean_raw_obs_processing_ms: 9.644637584509693\n",
      "  time_since_restore: 56586.31327319145\n",
      "  time_this_iter_s: 575.8499834537506\n",
      "  time_total_s: 56586.31327319145\n",
      "  timers:\n",
      "    learn_throughput: 264.127\n",
      "    learn_time_ms: 15144.238\n",
      "    load_throughput: 11031.408\n",
      "    load_time_ms: 362.601\n",
      "    sample_throughput: 7.162\n",
      "    sample_time_ms: 558521.569\n",
      "    update_time_ms: 3.565\n",
      "  timestamp: 1613883139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 137.12\n",
      "  episode_reward_max: 118.37974768340268\n",
      "  episode_reward_mean: 78.33170541121916\n",
      "  episode_reward_min: -105.91604945757742\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3368\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6366078853607178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007359534502029419\n",
      "        model: {}\n",
      "        policy_loss: -0.07867537438869476\n",
      "        total_loss: 246.06504821777344\n",
      "        vf_explained_var: 0.7994463443756104\n",
      "        vf_loss: 246.12693786621094\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.38002450980393\n",
      "    ram_util_percent: 37.65894607843138\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06802034589653698\n",
      "    mean_env_wait_ms: 119.25600639206192\n",
      "    mean_inference_ms: 1.7194048049155586\n",
      "    mean_raw_obs_processing_ms: 9.636769088035123\n",
      "  time_since_restore: 57158.2123401165\n",
      "  time_this_iter_s: 571.8990669250488\n",
      "  time_total_s: 57158.2123401165\n",
      "  timers:\n",
      "    learn_throughput: 264.131\n",
      "    learn_time_ms: 15144.006\n",
      "    load_throughput: 11025.053\n",
      "    load_time_ms: 362.81\n",
      "    sample_throughput: 7.166\n",
      "    sample_time_ms: 558185.885\n",
      "    update_time_ms: 3.584\n",
      "  timestamp: 1613883711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 143.85\n",
      "  episode_reward_max: 118.37974768340268\n",
      "  episode_reward_mean: 78.0710623959597\n",
      "  episode_reward_min: -105.91604945757742\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 3396\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5976383686065674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011524615809321404\n",
      "        model: {}\n",
      "        policy_loss: -0.11197109520435333\n",
      "        total_loss: 734.0401611328125\n",
      "        vf_explained_var: 0.7225897312164307\n",
      "        vf_loss: 734.125732421875\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.314268292682925\n",
      "    ram_util_percent: 37.653536585365856\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06799000126610652\n",
      "    mean_env_wait_ms: 119.35082608143959\n",
      "    mean_inference_ms: 1.7188636168057259\n",
      "    mean_raw_obs_processing_ms: 9.626542268431034\n",
      "  time_since_restore: 57732.9291408062\n",
      "  time_this_iter_s: 574.7168006896973\n",
      "  time_total_s: 57732.9291408062\n",
      "  timers:\n",
      "    learn_throughput: 264.133\n",
      "    learn_time_ms: 15143.884\n",
      "    load_throughput: 11038.27\n",
      "    load_time_ms: 362.376\n",
      "    sample_throughput: 7.164\n",
      "    sample_time_ms: 558371.727\n",
      "    update_time_ms: 3.582\n",
      "  timestamp: 1613884286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 140.5\n",
      "  episode_reward_max: 118.38921432646491\n",
      "  episode_reward_mean: 78.42548913021338\n",
      "  episode_reward_min: -103.57103560791688\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3425\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6155497431755066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010168452747166157\n",
      "        model: {}\n",
      "        policy_loss: -0.09731052070856094\n",
      "        total_loss: 613.9942016601562\n",
      "        vf_explained_var: 0.6528792977333069\n",
      "        vf_loss: 614.0682373046875\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.426431181486\n",
      "    ram_util_percent: 37.73580998781973\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06795917030188195\n",
      "    mean_env_wait_ms: 119.44732461749955\n",
      "    mean_inference_ms: 1.7183229932026485\n",
      "    mean_raw_obs_processing_ms: 9.615730870533078\n",
      "  time_since_restore: 58308.05718636513\n",
      "  time_this_iter_s: 575.1280455589294\n",
      "  time_total_s: 58308.05718636513\n",
      "  timers:\n",
      "    learn_throughput: 264.162\n",
      "    learn_time_ms: 15142.241\n",
      "    load_throughput: 11043.36\n",
      "    load_time_ms: 362.209\n",
      "    sample_throughput: 7.159\n",
      "    sample_time_ms: 558743.547\n",
      "    update_time_ms: 3.583\n",
      "  timestamp: 1613884861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 144.48\n",
      "  episode_reward_max: 118.38921432646491\n",
      "  episode_reward_mean: 76.40687688578578\n",
      "  episode_reward_min: -103.57103560791688\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 3453\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6024710536003113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008523493073880672\n",
      "        model: {}\n",
      "        policy_loss: -0.08596337586641312\n",
      "        total_loss: 467.1566467285156\n",
      "        vf_explained_var: 0.7673541903495789\n",
      "        vf_loss: 467.22320556640625\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.04768292682927\n",
      "    ram_util_percent: 37.73085365853658\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06793033424235004\n",
      "    mean_env_wait_ms: 119.54059989938149\n",
      "    mean_inference_ms: 1.717827526785704\n",
      "    mean_raw_obs_processing_ms: 9.605143538899718\n",
      "  time_since_restore: 58882.57932472229\n",
      "  time_this_iter_s: 574.5221383571625\n",
      "  time_total_s: 58882.57932472229\n",
      "  timers:\n",
      "    learn_throughput: 264.188\n",
      "    learn_time_ms: 15140.746\n",
      "    load_throughput: 11022.084\n",
      "    load_time_ms: 362.908\n",
      "    sample_throughput: 7.158\n",
      "    sample_time_ms: 558841.478\n",
      "    update_time_ms: 3.553\n",
      "  timestamp: 1613885436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 141.72\n",
      "  episode_reward_max: 118.38921432646491\n",
      "  episode_reward_mean: 74.62970423392105\n",
      "  episode_reward_min: -105.48941655446629\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3482\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6014758944511414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012446741573512554\n",
      "        model: {}\n",
      "        policy_loss: -0.1125083863735199\n",
      "        total_loss: 926.1072387695312\n",
      "        vf_explained_var: 0.653423011302948\n",
      "        vf_loss: 926.19140625\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.57551766138855\n",
      "    ram_util_percent: 37.717783191230204\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06790196336835012\n",
      "    mean_env_wait_ms: 119.63217729878222\n",
      "    mean_inference_ms: 1.7173653624665526\n",
      "    mean_raw_obs_processing_ms: 9.596482230104904\n",
      "  time_since_restore: 59457.173558950424\n",
      "  time_this_iter_s: 574.5942342281342\n",
      "  time_total_s: 59457.173558950424\n",
      "  timers:\n",
      "    learn_throughput: 264.167\n",
      "    learn_time_ms: 15141.953\n",
      "    load_throughput: 11015.528\n",
      "    load_time_ms: 363.124\n",
      "    sample_throughput: 7.157\n",
      "    sample_time_ms: 558888.116\n",
      "    update_time_ms: 3.526\n",
      "  timestamp: 1613886011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-49-46\n",
      "  done: false\n",
      "  episode_len_mean: 139.05\n",
      "  episode_reward_max: 118.38598378043406\n",
      "  episode_reward_mean: 76.65343491029904\n",
      "  episode_reward_min: -105.48941655446629\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 3512\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.590711236000061\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009029347449541092\n",
      "        model: {}\n",
      "        policy_loss: -0.09848328679800034\n",
      "        total_loss: 387.76031494140625\n",
      "        vf_explained_var: 0.8040342926979065\n",
      "        vf_loss: 387.8382568359375\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.46406820950061\n",
      "    ram_util_percent: 37.733617539585865\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787322891278114\n",
      "    mean_env_wait_ms: 119.7227312530466\n",
      "    mean_inference_ms: 1.716910321714039\n",
      "    mean_raw_obs_processing_ms: 9.589092465889095\n",
      "  time_since_restore: 60032.382362127304\n",
      "  time_this_iter_s: 575.2088031768799\n",
      "  time_total_s: 60032.382362127304\n",
      "  timers:\n",
      "    learn_throughput: 264.251\n",
      "    learn_time_ms: 15137.109\n",
      "    load_throughput: 11010.962\n",
      "    load_time_ms: 363.274\n",
      "    sample_throughput: 7.155\n",
      "    sample_time_ms: 559013.784\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1613886586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_06-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 137.41\n",
      "  episode_reward_max: 118.39091820189648\n",
      "  episode_reward_mean: 72.27378223507384\n",
      "  episode_reward_min: -105.48941655446629\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3539\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5984408259391785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0075990972109138966\n",
      "        model: {}\n",
      "        policy_loss: -0.08635248988866806\n",
      "        total_loss: 253.9138641357422\n",
      "        vf_explained_var: 0.8253668546676636\n",
      "        vf_loss: 253.98289489746094\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.209157509157514\n",
      "    ram_util_percent: 37.84822954822955\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06784796767137134\n",
      "    mean_env_wait_ms: 119.80372562977568\n",
      "    mean_inference_ms: 1.7165187067082488\n",
      "    mean_raw_obs_processing_ms: 9.58153597114176\n",
      "  time_since_restore: 60606.63854265213\n",
      "  time_this_iter_s: 574.256180524826\n",
      "  time_total_s: 60606.63854265213\n",
      "  timers:\n",
      "    learn_throughput: 264.217\n",
      "    learn_time_ms: 15139.074\n",
      "    load_throughput: 10981.592\n",
      "    load_time_ms: 364.246\n",
      "    sample_throughput: 7.152\n",
      "    sample_time_ms: 559292.284\n",
      "    update_time_ms: 3.547\n",
      "  timestamp: 1613887161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-09-05\n",
      "  done: false\n",
      "  episode_len_mean: 141.31\n",
      "  episode_reward_max: 118.39091820189648\n",
      "  episode_reward_mean: 78.43174718781738\n",
      "  episode_reward_min: -105.48941655446629\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3568\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6062167882919312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00849900022149086\n",
      "        model: {}\n",
      "        policy_loss: -0.08769343048334122\n",
      "        total_loss: 283.53985595703125\n",
      "        vf_explained_var: 0.8445320129394531\n",
      "        vf_loss: 283.6081848144531\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.00335329341318\n",
      "    ram_util_percent: 38.13353293413174\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06782688799794695\n",
      "    mean_env_wait_ms: 119.89516835768752\n",
      "    mean_inference_ms: 1.716184160343747\n",
      "    mean_raw_obs_processing_ms: 9.573978922801206\n",
      "  time_since_restore: 61191.2335793972\n",
      "  time_this_iter_s: 584.5950367450714\n",
      "  time_total_s: 61191.2335793972\n",
      "  timers:\n",
      "    learn_throughput: 264.295\n",
      "    learn_time_ms: 15134.624\n",
      "    load_throughput: 10961.008\n",
      "    load_time_ms: 364.93\n",
      "    sample_throughput: 7.142\n",
      "    sample_time_ms: 560044.648\n",
      "    update_time_ms: 3.61\n",
      "  timestamp: 1613887745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-18-37\n",
      "  done: false\n",
      "  episode_len_mean: 144.6\n",
      "  episode_reward_max: 118.39091820189648\n",
      "  episode_reward_mean: 86.82461647095136\n",
      "  episode_reward_min: -105.0321410534626\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3594\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5918995141983032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006438602227717638\n",
      "        model: {}\n",
      "        policy_loss: -0.06593605875968933\n",
      "        total_loss: 332.7830505371094\n",
      "        vf_explained_var: 0.6599776148796082\n",
      "        vf_loss: 332.8342590332031\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.34240196078431\n",
      "    ram_util_percent: 38.063357843137254\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06780827979209664\n",
      "    mean_env_wait_ms: 119.97759657799472\n",
      "    mean_inference_ms: 1.7158729855139294\n",
      "    mean_raw_obs_processing_ms: 9.564777671008347\n",
      "  time_since_restore: 61762.852212667465\n",
      "  time_this_iter_s: 571.6186332702637\n",
      "  time_total_s: 61762.852212667465\n",
      "  timers:\n",
      "    learn_throughput: 264.397\n",
      "    learn_time_ms: 15128.754\n",
      "    load_throughput: 10947.792\n",
      "    load_time_ms: 365.37\n",
      "    sample_throughput: 7.148\n",
      "    sample_time_ms: 559576.306\n",
      "    update_time_ms: 3.667\n",
      "  timestamp: 1613888317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-28-17\n",
      "  done: false\n",
      "  episode_len_mean: 144.62\n",
      "  episode_reward_max: 118.39091820189648\n",
      "  episode_reward_mean: 86.80167187729789\n",
      "  episode_reward_min: -105.09060085253891\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 3622\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6433344483375549\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010839796625077724\n",
      "        model: {}\n",
      "        policy_loss: -0.10590440779924393\n",
      "        total_loss: 570.0552368164062\n",
      "        vf_explained_var: 0.7485004663467407\n",
      "        vf_loss: 570.1365966796875\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.352841596130595\n",
      "    ram_util_percent: 38.07920193470375\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778949191005351\n",
      "    mean_env_wait_ms: 120.07001042963421\n",
      "    mean_inference_ms: 1.7155426799237201\n",
      "    mean_raw_obs_processing_ms: 9.554230492816803\n",
      "  time_since_restore: 62342.62043309212\n",
      "  time_this_iter_s: 579.7682204246521\n",
      "  time_total_s: 62342.62043309212\n",
      "  timers:\n",
      "    learn_throughput: 261.992\n",
      "    learn_time_ms: 15267.62\n",
      "    load_throughput: 10942.687\n",
      "    load_time_ms: 365.541\n",
      "    sample_throughput: 7.145\n",
      "    sample_time_ms: 559828.975\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1613888897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 147.86\n",
      "  episode_reward_max: 118.39873892711245\n",
      "  episode_reward_mean: 93.33501276419842\n",
      "  episode_reward_min: -105.09060085253891\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3649\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5805870294570923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009480483829975128\n",
      "        model: {}\n",
      "        policy_loss: -0.09276328235864639\n",
      "        total_loss: 34.91984939575195\n",
      "        vf_explained_var: 0.9700055718421936\n",
      "        vf_loss: 34.99101638793945\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.62633495145631\n",
      "    ram_util_percent: 38.1372572815534\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677720796855766\n",
      "    mean_env_wait_ms: 120.15735014680446\n",
      "    mean_inference_ms: 1.7152309768018645\n",
      "    mean_raw_obs_processing_ms: 9.543755308827134\n",
      "  time_since_restore: 62919.88499903679\n",
      "  time_this_iter_s: 577.2645659446716\n",
      "  time_total_s: 62919.88499903679\n",
      "  timers:\n",
      "    learn_throughput: 262.086\n",
      "    learn_time_ms: 15262.151\n",
      "    load_throughput: 10951.716\n",
      "    load_time_ms: 365.24\n",
      "    sample_throughput: 7.138\n",
      "    sample_time_ms: 560371.214\n",
      "    update_time_ms: 3.602\n",
      "  timestamp: 1613889475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 151.34\n",
      "  episode_reward_max: 118.39873892711245\n",
      "  episode_reward_mean: 97.33516881296664\n",
      "  episode_reward_min: -106.43562513752948\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 3674\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6057289838790894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00902612879872322\n",
      "        model: {}\n",
      "        policy_loss: -0.08407626301050186\n",
      "        total_loss: 47.36072540283203\n",
      "        vf_explained_var: 0.9562988877296448\n",
      "        vf_loss: 47.42424011230469\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.257720588235294\n",
      "    ram_util_percent: 38.134068627450986\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06775213187945846\n",
      "    mean_env_wait_ms: 120.23711540446456\n",
      "    mean_inference_ms: 1.7148622716953952\n",
      "    mean_raw_obs_processing_ms: 9.531843481668078\n",
      "  time_since_restore: 63491.65019154549\n",
      "  time_this_iter_s: 571.7651925086975\n",
      "  time_total_s: 63491.65019154549\n",
      "  timers:\n",
      "    learn_throughput: 262.208\n",
      "    learn_time_ms: 15255.09\n",
      "    load_throughput: 10960.437\n",
      "    load_time_ms: 364.949\n",
      "    sample_throughput: 7.142\n",
      "    sample_time_ms: 560080.637\n",
      "    update_time_ms: 3.619\n",
      "  timestamp: 1613890046\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_07-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 146.86\n",
      "  episode_reward_max: 118.39873892711245\n",
      "  episode_reward_mean: 91.05291780335374\n",
      "  episode_reward_min: -106.43562513752948\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 3704\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6012541651725769\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00911764707416296\n",
      "        model: {}\n",
      "        policy_loss: -0.09254691749811172\n",
      "        total_loss: 685.6951293945312\n",
      "        vf_explained_var: 0.6572573184967041\n",
      "        vf_loss: 685.7669067382812\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.51634146341465\n",
      "    ram_util_percent: 38.13914634146342\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06772919668110616\n",
      "    mean_env_wait_ms: 120.32691297202932\n",
      "    mean_inference_ms: 1.7144107681494976\n",
      "    mean_raw_obs_processing_ms: 9.520671146772086\n",
      "  time_since_restore: 64066.185210466385\n",
      "  time_this_iter_s: 574.5350189208984\n",
      "  time_total_s: 64066.185210466385\n",
      "  timers:\n",
      "    learn_throughput: 262.137\n",
      "    learn_time_ms: 15259.186\n",
      "    load_throughput: 10969.156\n",
      "    load_time_ms: 364.659\n",
      "    sample_throughput: 7.143\n",
      "    sample_time_ms: 560016.439\n",
      "    update_time_ms: 3.648\n",
      "  timestamp: 1613890621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 148.66\n",
      "  episode_reward_max: 118.39873892711245\n",
      "  episode_reward_mean: 89.15732775802172\n",
      "  episode_reward_min: -106.43562513752948\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3730\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6236892342567444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013307305052876472\n",
      "        model: {}\n",
      "        policy_loss: -0.1155630424618721\n",
      "        total_loss: 1010.5505981445312\n",
      "        vf_explained_var: 0.5766982436180115\n",
      "        vf_loss: 1010.6359252929688\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.30882352941177\n",
      "    ram_util_percent: 38.13970588235295\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06770796026275733\n",
      "    mean_env_wait_ms: 120.40130526884197\n",
      "    mean_inference_ms: 1.7140039793359307\n",
      "    mean_raw_obs_processing_ms: 9.510031471826066\n",
      "  time_since_restore: 64637.37950015068\n",
      "  time_this_iter_s: 571.1942896842957\n",
      "  time_total_s: 64637.37950015068\n",
      "  timers:\n",
      "    learn_throughput: 262.135\n",
      "    learn_time_ms: 15259.32\n",
      "    load_throughput: 10973.496\n",
      "    load_time_ms: 364.515\n",
      "    sample_throughput: 7.147\n",
      "    sample_time_ms: 559683.545\n",
      "    update_time_ms: 3.636\n",
      "  timestamp: 1613891193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-16-04\n",
      "  done: false\n",
      "  episode_len_mean: 148.73\n",
      "  episode_reward_max: 118.37573316320498\n",
      "  episode_reward_mean: 87.00290944381177\n",
      "  episode_reward_min: -106.43562513752948\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3756\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5877834558486938\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009241716004908085\n",
      "        model: {}\n",
      "        policy_loss: -0.08691494166851044\n",
      "        total_loss: 67.36650848388672\n",
      "        vf_explained_var: 0.944295346736908\n",
      "        vf_loss: 67.432373046875\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.35693251533743\n",
      "    ram_util_percent: 38.141840490797556\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0676864742568336\n",
      "    mean_env_wait_ms: 120.47292656211799\n",
      "    mean_inference_ms: 1.7135771052734765\n",
      "    mean_raw_obs_processing_ms: 9.499117822285328\n",
      "  time_since_restore: 65208.24702858925\n",
      "  time_this_iter_s: 570.8675284385681\n",
      "  time_total_s: 65208.24702858925\n",
      "  timers:\n",
      "    learn_throughput: 262.126\n",
      "    learn_time_ms: 15259.811\n",
      "    load_throughput: 10962.411\n",
      "    load_time_ms: 364.883\n",
      "    sample_throughput: 7.152\n",
      "    sample_time_ms: 559310.986\n",
      "    update_time_ms: 3.658\n",
      "  timestamp: 1613891764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-24-37\n",
      "  done: false\n",
      "  episode_len_mean: 146.66\n",
      "  episode_reward_max: 118.37573316320498\n",
      "  episode_reward_mean: 85.14329107139147\n",
      "  episode_reward_min: -100.99048036755502\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3785\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5510581731796265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007981202565133572\n",
      "        model: {}\n",
      "        policy_loss: -0.08055435866117477\n",
      "        total_loss: 546.143310546875\n",
      "        vf_explained_var: 0.6825169920921326\n",
      "        vf_loss: 546.2056884765625\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.81270491803278\n",
      "    ram_util_percent: 38.12691256830601\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06767392832483515\n",
      "    mean_env_wait_ms: 120.5110071355082\n",
      "    mean_inference_ms: 1.7133055716367522\n",
      "    mean_raw_obs_processing_ms: 9.489400407951997\n",
      "  time_since_restore: 65721.15237927437\n",
      "  time_this_iter_s: 512.9053506851196\n",
      "  time_total_s: 65721.15237927437\n",
      "  timers:\n",
      "    learn_throughput: 261.767\n",
      "    learn_time_ms: 15280.742\n",
      "    load_throughput: 10970.308\n",
      "    load_time_ms: 364.621\n",
      "    sample_throughput: 7.233\n",
      "    sample_time_ms: 553057.905\n",
      "    update_time_ms: 3.717\n",
      "  timestamp: 1613892277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-33-19\n",
      "  done: false\n",
      "  episode_len_mean: 146.02\n",
      "  episode_reward_max: 118.37573316320498\n",
      "  episode_reward_mean: 85.38652081386765\n",
      "  episode_reward_min: -100.4306152638022\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 3813\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6183977723121643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010066812857985497\n",
      "        model: {}\n",
      "        policy_loss: -0.09824593365192413\n",
      "        total_loss: 581.5679321289062\n",
      "        vf_explained_var: 0.7129077315330505\n",
      "        vf_loss: 581.6431274414062\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.68375838926174\n",
      "    ram_util_percent: 38.153825503355705\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06767878007932275\n",
      "    mean_env_wait_ms: 120.51764218598193\n",
      "    mean_inference_ms: 1.7133476223797286\n",
      "    mean_raw_obs_processing_ms: 9.479803397885286\n",
      "  time_since_restore: 66243.28735876083\n",
      "  time_this_iter_s: 522.1349794864655\n",
      "  time_total_s: 66243.28735876083\n",
      "  timers:\n",
      "    learn_throughput: 259.26\n",
      "    learn_time_ms: 15428.527\n",
      "    load_throughput: 10958.238\n",
      "    load_time_ms: 365.022\n",
      "    sample_throughput: 7.303\n",
      "    sample_time_ms: 547694.19\n",
      "    update_time_ms: 3.689\n",
      "  timestamp: 1613892799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 139.65\n",
      "  episode_reward_max: 118.39513312085698\n",
      "  episode_reward_mean: 89.45473524232285\n",
      "  episode_reward_min: -100.4306152638022\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 3844\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.577362060546875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007396557833999395\n",
      "        model: {}\n",
      "        policy_loss: -0.07415349781513214\n",
      "        total_loss: 183.14419555664062\n",
      "        vf_explained_var: 0.8363568782806396\n",
      "        vf_loss: 183.2015380859375\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.56627140974966\n",
      "    ram_util_percent: 38.189855072463764\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06770722321730716\n",
      "    mean_env_wait_ms: 120.4917917336477\n",
      "    mean_inference_ms: 1.7137769699131404\n",
      "    mean_raw_obs_processing_ms: 9.473527999217005\n",
      "  time_since_restore: 66775.34722495079\n",
      "  time_this_iter_s: 532.0598661899567\n",
      "  time_total_s: 66775.34722495079\n",
      "  timers:\n",
      "    learn_throughput: 256.674\n",
      "    learn_time_ms: 15583.947\n",
      "    load_throughput: 10864.259\n",
      "    load_time_ms: 368.18\n",
      "    sample_throughput: 7.376\n",
      "    sample_time_ms: 542280.96\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1613893331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-50-51\n",
      "  done: false\n",
      "  episode_len_mean: 139.74\n",
      "  episode_reward_max: 118.39513312085698\n",
      "  episode_reward_mean: 84.90426776366216\n",
      "  episode_reward_min: -106.0701544586116\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3871\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6015790104866028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01063192542642355\n",
      "        model: {}\n",
      "        policy_loss: -0.10586980730295181\n",
      "        total_loss: 477.35626220703125\n",
      "        vf_explained_var: 0.7866230010986328\n",
      "        vf_loss: 477.4378356933594\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.47857142857144\n",
      "    ram_util_percent: 38.21563342318059\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0677439447389491\n",
      "    mean_env_wait_ms: 120.45729120222786\n",
      "    mean_inference_ms: 1.7143439789547819\n",
      "    mean_raw_obs_processing_ms: 9.467971795851675\n",
      "  time_since_restore: 67294.9882683754\n",
      "  time_this_iter_s: 519.6410434246063\n",
      "  time_total_s: 67294.9882683754\n",
      "  timers:\n",
      "    learn_throughput: 256.069\n",
      "    learn_time_ms: 15620.818\n",
      "    load_throughput: 10857.938\n",
      "    load_time_ms: 368.394\n",
      "    sample_throughput: 7.448\n",
      "    sample_time_ms: 537043.94\n",
      "    update_time_ms: 3.594\n",
      "  timestamp: 1613893851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_08-59-17\n",
      "  done: false\n",
      "  episode_len_mean: 134.12\n",
      "  episode_reward_max: 118.39513312085698\n",
      "  episode_reward_mean: 84.80443379407956\n",
      "  episode_reward_min: -106.0701544586116\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 3903\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5771121382713318\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008310528472065926\n",
      "        model: {}\n",
      "        policy_loss: -0.08232569694519043\n",
      "        total_loss: 534.6954345703125\n",
      "        vf_explained_var: 0.6438747644424438\n",
      "        vf_loss: 534.7588500976562\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.63836565096952\n",
      "    ram_util_percent: 38.17783933518006\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778464054628712\n",
      "    mean_env_wait_ms: 120.4076427896925\n",
      "    mean_inference_ms: 1.7149836034740622\n",
      "    mean_raw_obs_processing_ms: 9.463932562409434\n",
      "  time_since_restore: 67801.0431239605\n",
      "  time_this_iter_s: 506.05485558509827\n",
      "  time_total_s: 67801.0431239605\n",
      "  timers:\n",
      "    learn_throughput: 258.103\n",
      "    learn_time_ms: 15497.712\n",
      "    load_throughput: 10813.376\n",
      "    load_time_ms: 369.912\n",
      "    sample_throughput: 7.55\n",
      "    sample_time_ms: 529791.92\n",
      "    update_time_ms: 3.559\n",
      "  timestamp: 1613894357\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-07-42\n",
      "  done: false\n",
      "  episode_len_mean: 138.5\n",
      "  episode_reward_max: 118.34709881053\n",
      "  episode_reward_mean: 78.08393089485901\n",
      "  episode_reward_min: -106.32775745859084\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3930\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6117790937423706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010948827490210533\n",
      "        model: {}\n",
      "        policy_loss: -0.10564907640218735\n",
      "        total_loss: 647.387451171875\n",
      "        vf_explained_var: 0.6757882237434387\n",
      "        vf_loss: 647.4682006835938\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.61178918169209\n",
      "    ram_util_percent: 38.201386962552014\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0678116961064663\n",
      "    mean_env_wait_ms: 120.35627306923674\n",
      "    mean_inference_ms: 1.715379314195032\n",
      "    mean_raw_obs_processing_ms: 9.458864461888382\n",
      "  time_since_restore: 68305.61447262764\n",
      "  time_this_iter_s: 504.5713486671448\n",
      "  time_total_s: 68305.61447262764\n",
      "  timers:\n",
      "    learn_throughput: 255.586\n",
      "    learn_time_ms: 15650.32\n",
      "    load_throughput: 10791.392\n",
      "    load_time_ms: 370.666\n",
      "    sample_throughput: 7.657\n",
      "    sample_time_ms: 522369.037\n",
      "    update_time_ms: 3.582\n",
      "  timestamp: 1613894862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 142.47\n",
      "  episode_reward_max: 118.32835011673706\n",
      "  episode_reward_mean: 82.36495306691775\n",
      "  episode_reward_min: -106.32775745859084\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 3957\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5907235741615295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008743084967136383\n",
      "        model: {}\n",
      "        policy_loss: -0.08979450166225433\n",
      "        total_loss: 294.9871826171875\n",
      "        vf_explained_var: 0.8123583197593689\n",
      "        vf_loss: 295.0570373535156\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.94388297872341\n",
      "    ram_util_percent: 38.37739361702128\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06784055093998786\n",
      "    mean_env_wait_ms: 120.30579860423741\n",
      "    mean_inference_ms: 1.7158013710289555\n",
      "    mean_raw_obs_processing_ms: 9.452628401740492\n",
      "  time_since_restore: 68832.49143481255\n",
      "  time_this_iter_s: 526.876962184906\n",
      "  time_total_s: 68832.49143481255\n",
      "  timers:\n",
      "    learn_throughput: 252.888\n",
      "    learn_time_ms: 15817.3\n",
      "    load_throughput: 10764.004\n",
      "    load_time_ms: 371.609\n",
      "    sample_throughput: 7.726\n",
      "    sample_time_ms: 517709.113\n",
      "    update_time_ms: 3.604\n",
      "  timestamp: 1613895389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-25-15\n",
      "  done: false\n",
      "  episode_len_mean: 142.03\n",
      "  episode_reward_max: 118.31354670270113\n",
      "  episode_reward_mean: 82.43069567717554\n",
      "  episode_reward_min: -106.80176523156126\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 3986\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5881098508834839\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010245510376989841\n",
      "        model: {}\n",
      "        policy_loss: -0.09339163452386856\n",
      "        total_loss: 627.2103271484375\n",
      "        vf_explained_var: 0.6385515928268433\n",
      "        vf_loss: 627.2803955078125\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.81226666666666\n",
      "    ram_util_percent: 38.380399999999995\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787463923392938\n",
      "    mean_env_wait_ms: 120.26053147584571\n",
      "    mean_inference_ms: 1.7163259572157061\n",
      "    mean_raw_obs_processing_ms: 9.44585488949249\n",
      "  time_since_restore: 69358.38308548927\n",
      "  time_this_iter_s: 525.8916506767273\n",
      "  time_total_s: 69358.38308548927\n",
      "  timers:\n",
      "    learn_throughput: 252.533\n",
      "    learn_time_ms: 15839.527\n",
      "    load_throughput: 10719.51\n",
      "    load_time_ms: 373.151\n",
      "    sample_throughput: 7.8\n",
      "    sample_time_ms: 512821.553\n",
      "    update_time_ms: 3.569\n",
      "  timestamp: 1613895915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-33-45\n",
      "  done: false\n",
      "  episode_len_mean: 132.8\n",
      "  episode_reward_max: 118.34928746608047\n",
      "  episode_reward_mean: 82.31662489947415\n",
      "  episode_reward_min: -106.89583542846157\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 4021\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5682926177978516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007367397658526897\n",
      "        model: {}\n",
      "        policy_loss: -0.07760433107614517\n",
      "        total_loss: 376.2999572753906\n",
      "        vf_explained_var: 0.7548487186431885\n",
      "        vf_loss: 376.3607482910156\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.78049450549451\n",
      "    ram_util_percent: 38.393956043956045\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06791655003039965\n",
      "    mean_env_wait_ms: 120.20590265398637\n",
      "    mean_inference_ms: 1.7169823124501375\n",
      "    mean_raw_obs_processing_ms: 9.442055682272372\n",
      "  time_since_restore: 69867.91096925735\n",
      "  time_this_iter_s: 509.52788376808167\n",
      "  time_total_s: 69867.91096925735\n",
      "  timers:\n",
      "    learn_throughput: 252.141\n",
      "    learn_time_ms: 15864.164\n",
      "    load_throughput: 10711.439\n",
      "    load_time_ms: 373.433\n",
      "    sample_throughput: 7.895\n",
      "    sample_time_ms: 506628.024\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1613896425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 129.46\n",
      "  episode_reward_max: 118.34928746608047\n",
      "  episode_reward_mean: 80.40744834850254\n",
      "  episode_reward_min: -106.89583542846157\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 4051\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6039897799491882\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010948044247925282\n",
      "        model: {}\n",
      "        policy_loss: -0.10490277409553528\n",
      "        total_loss: 421.2901611328125\n",
      "        vf_explained_var: 0.8155714869499207\n",
      "        vf_loss: 421.3701171875\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53504867872045\n",
      "    ram_util_percent: 38.365507649513205\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06794615383610282\n",
      "    mean_env_wait_ms: 120.15307269858904\n",
      "    mean_inference_ms: 1.7174442956596794\n",
      "    mean_raw_obs_processing_ms: 9.441340066067333\n",
      "  time_since_restore: 70371.77071595192\n",
      "  time_this_iter_s: 503.8597466945648\n",
      "  time_total_s: 70371.77071595192\n",
      "  timers:\n",
      "    learn_throughput: 251.831\n",
      "    learn_time_ms: 15883.643\n",
      "    load_throughput: 10696.796\n",
      "    load_time_ms: 373.944\n",
      "    sample_throughput: 8.001\n",
      "    sample_time_ms: 499906.446\n",
      "    update_time_ms: 3.558\n",
      "  timestamp: 1613896929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 131.5\n",
      "  episode_reward_max: 118.37421759265597\n",
      "  episode_reward_mean: 82.69939237282279\n",
      "  episode_reward_min: -106.89583542846157\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4079\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5987846255302429\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005686358083039522\n",
      "        model: {}\n",
      "        policy_loss: -0.07071297615766525\n",
      "        total_loss: 285.1310119628906\n",
      "        vf_explained_var: 0.7715242505073547\n",
      "        vf_loss: 285.1888122558594\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.519860139860135\n",
      "    ram_util_percent: 38.37874125874126\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06796573233450409\n",
      "    mean_env_wait_ms: 120.09234682412965\n",
      "    mean_inference_ms: 1.7177408642111685\n",
      "    mean_raw_obs_processing_ms: 9.440449571341798\n",
      "  time_since_restore: 70872.90150022507\n",
      "  time_this_iter_s: 501.1307842731476\n",
      "  time_total_s: 70872.90150022507\n",
      "  timers:\n",
      "    learn_throughput: 251.725\n",
      "    learn_time_ms: 15890.363\n",
      "    load_throughput: 10678.404\n",
      "    load_time_ms: 374.588\n",
      "    sample_throughput: 8.02\n",
      "    sample_time_ms: 498723.017\n",
      "    update_time_ms: 3.481\n",
      "  timestamp: 1613897430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_09-58-52\n",
      "  done: false\n",
      "  episode_len_mean: 135.54\n",
      "  episode_reward_max: 118.3795693239413\n",
      "  episode_reward_mean: 82.96836530942372\n",
      "  episode_reward_min: -103.76890862329319\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4108\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5838570594787598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01188985537737608\n",
      "        model: {}\n",
      "        policy_loss: -0.10170167684555054\n",
      "        total_loss: 772.657958984375\n",
      "        vf_explained_var: 0.653910756111145\n",
      "        vf_loss: 772.7326049804688\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53216783216783\n",
      "    ram_util_percent: 38.39090909090909\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06798371611601566\n",
      "    mean_env_wait_ms: 120.02619330508769\n",
      "    mean_inference_ms: 1.7180095488962615\n",
      "    mean_raw_obs_processing_ms: 9.436648742925328\n",
      "  time_since_restore: 71374.011282444\n",
      "  time_this_iter_s: 501.1097822189331\n",
      "  time_total_s: 71374.011282444\n",
      "  timers:\n",
      "    learn_throughput: 253.79\n",
      "    learn_time_ms: 15761.042\n",
      "    load_throughput: 10597.706\n",
      "    load_time_ms: 377.44\n",
      "    sample_throughput: 8.052\n",
      "    sample_time_ms: 496749.288\n",
      "    update_time_ms: 3.498\n",
      "  timestamp: 1613897932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-07-16\n",
      "  done: false\n",
      "  episode_len_mean: 133.58\n",
      "  episode_reward_max: 118.39521154807612\n",
      "  episode_reward_mean: 83.17846175221419\n",
      "  episode_reward_min: -105.48697373799484\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 4139\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5650530457496643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007939182221889496\n",
      "        model: {}\n",
      "        policy_loss: -0.08119844645261765\n",
      "        total_loss: 407.46173095703125\n",
      "        vf_explained_var: 0.7646421194076538\n",
      "        vf_loss: 407.5248718261719\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.55833333333333\n",
      "    ram_util_percent: 38.338055555555556\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06800290073032944\n",
      "    mean_env_wait_ms: 119.95183123516772\n",
      "    mean_inference_ms: 1.7182751771675613\n",
      "    mean_raw_obs_processing_ms: 9.432557149867971\n",
      "  time_since_restore: 71877.89301609993\n",
      "  time_this_iter_s: 503.88173365592957\n",
      "  time_total_s: 71877.89301609993\n",
      "  timers:\n",
      "    learn_throughput: 255.939\n",
      "    learn_time_ms: 15628.753\n",
      "    load_throughput: 10630.193\n",
      "    load_time_ms: 376.287\n",
      "    sample_throughput: 8.096\n",
      "    sample_time_ms: 494064.123\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1613898436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-15-34\n",
      "  done: false\n",
      "  episode_len_mean: 139.25\n",
      "  episode_reward_max: 118.39998757567115\n",
      "  episode_reward_mean: 85.27356494809966\n",
      "  episode_reward_min: -105.48697373799484\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4164\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.600960373878479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006746595725417137\n",
      "        model: {}\n",
      "        policy_loss: -0.07783032953739166\n",
      "        total_loss: 314.6885070800781\n",
      "        vf_explained_var: 0.7607309818267822\n",
      "        vf_loss: 314.7509765625\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.494092827004216\n",
      "    ram_util_percent: 38.360759493670884\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06801850421276065\n",
      "    mean_env_wait_ms: 119.8919756049781\n",
      "    mean_inference_ms: 1.718506779837454\n",
      "    mean_raw_obs_processing_ms: 9.426981561005293\n",
      "  time_since_restore: 72376.05636382103\n",
      "  time_this_iter_s: 498.16334772109985\n",
      "  time_total_s: 72376.05636382103\n",
      "  timers:\n",
      "    learn_throughput: 256.104\n",
      "    learn_time_ms: 15618.666\n",
      "    load_throughput: 10530.235\n",
      "    load_time_ms: 379.859\n",
      "    sample_throughput: 8.131\n",
      "    sample_time_ms: 491923.513\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1613898934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 139.5\n",
      "  episode_reward_max: 118.39998757567115\n",
      "  episode_reward_mean: 95.64621330132915\n",
      "  episode_reward_min: -105.48697373799484\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 4194\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5370206236839294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004629514180123806\n",
      "        model: {}\n",
      "        policy_loss: -0.04861670359969139\n",
      "        total_loss: 137.58197021484375\n",
      "        vf_explained_var: 0.8455789685249329\n",
      "        vf_loss: 137.6200408935547\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.59679665738162\n",
      "    ram_util_percent: 38.40431754874651\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06803823480184848\n",
      "    mean_env_wait_ms: 119.82114675593346\n",
      "    mean_inference_ms: 1.718770904202886\n",
      "    mean_raw_obs_processing_ms: 9.42133230428674\n",
      "  time_since_restore: 72879.4960064888\n",
      "  time_this_iter_s: 503.4396426677704\n",
      "  time_total_s: 72879.4960064888\n",
      "  timers:\n",
      "    learn_throughput: 256.095\n",
      "    learn_time_ms: 15619.196\n",
      "    load_throughput: 10534.313\n",
      "    load_time_ms: 379.712\n",
      "    sample_throughput: 8.136\n",
      "    sample_time_ms: 491663.38\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1613899438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-32-20\n",
      "  done: false\n",
      "  episode_len_mean: 138.83\n",
      "  episode_reward_max: 118.39998757567115\n",
      "  episode_reward_mean: 95.68756577876341\n",
      "  episode_reward_min: -105.48697373799484\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 4224\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5466543436050415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011059517040848732\n",
      "        model: {}\n",
      "        policy_loss: -0.08189653605222702\n",
      "        total_loss: 242.59808349609375\n",
      "        vf_explained_var: 0.8244056701660156\n",
      "        vf_loss: 242.66738891601562\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.639191073919115\n",
      "    ram_util_percent: 38.38967921896793\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06805755736091408\n",
      "    mean_env_wait_ms: 119.75206377602635\n",
      "    mean_inference_ms: 1.7190400202641178\n",
      "    mean_raw_obs_processing_ms: 9.41560263759338\n",
      "  time_since_restore: 73381.71351766586\n",
      "  time_this_iter_s: 502.217511177063\n",
      "  time_total_s: 73381.71351766586\n",
      "  timers:\n",
      "    learn_throughput: 258.177\n",
      "    learn_time_ms: 15493.223\n",
      "    load_throughput: 10535.06\n",
      "    load_time_ms: 379.685\n",
      "    sample_throughput: 8.137\n",
      "    sample_time_ms: 491552.855\n",
      "    update_time_ms: 3.606\n",
      "  timestamp: 1613899940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-40-41\n",
      "  done: false\n",
      "  episode_len_mean: 139.2\n",
      "  episode_reward_max: 118.3980188304479\n",
      "  episode_reward_mean: 95.88512579679904\n",
      "  episode_reward_min: -100.1750695215878\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4251\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5456653237342834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014115593396127224\n",
      "        model: {}\n",
      "        policy_loss: -0.0856579840183258\n",
      "        total_loss: 516.1102294921875\n",
      "        vf_explained_var: 0.619394838809967\n",
      "        vf_loss: 516.1798095703125\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.451468531468535\n",
      "    ram_util_percent: 38.343916083916085\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06807438256326685\n",
      "    mean_env_wait_ms: 119.68962361357994\n",
      "    mean_inference_ms: 1.7193031557236806\n",
      "    mean_raw_obs_processing_ms: 9.409635454162597\n",
      "  time_since_restore: 73882.23320555687\n",
      "  time_this_iter_s: 500.51968789100647\n",
      "  time_total_s: 73882.23320555687\n",
      "  timers:\n",
      "    learn_throughput: 260.521\n",
      "    learn_time_ms: 15353.831\n",
      "    load_throughput: 10524.565\n",
      "    load_time_ms: 380.063\n",
      "    sample_throughput: 8.179\n",
      "    sample_time_ms: 489059.197\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1613900441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-49-01\n",
      "  done: false\n",
      "  episode_len_mean: 142.07\n",
      "  episode_reward_max: 118.3980188304479\n",
      "  episode_reward_mean: 97.81481651326742\n",
      "  episode_reward_min: -105.63298102378208\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 4277\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5156704783439636\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01677967607975006\n",
      "        model: {}\n",
      "        policy_loss: -0.09261477738618851\n",
      "        total_loss: 24.15653419494629\n",
      "        vf_explained_var: 0.974636435508728\n",
      "        vf_loss: 24.230031967163086\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.479663394109394\n",
      "    ram_util_percent: 38.35035063113605\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06808918160052571\n",
      "    mean_env_wait_ms: 119.63162029126406\n",
      "    mean_inference_ms: 1.7195446712067508\n",
      "    mean_raw_obs_processing_ms: 9.403294456473589\n",
      "  time_since_restore: 74381.7987921238\n",
      "  time_this_iter_s: 499.56558656692505\n",
      "  time_total_s: 74381.7987921238\n",
      "  timers:\n",
      "    learn_throughput: 260.567\n",
      "    learn_time_ms: 15351.132\n",
      "    load_throughput: 10558.554\n",
      "    load_time_ms: 378.84\n",
      "    sample_throughput: 8.223\n",
      "    sample_time_ms: 486429.308\n",
      "    update_time_ms: 3.693\n",
      "  timestamp: 1613900941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_10-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 139.24\n",
      "  episode_reward_max: 118.3980188304479\n",
      "  episode_reward_mean: 93.40331968238395\n",
      "  episode_reward_min: -106.52348667552874\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 4309\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5389827489852905\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014672734774649143\n",
      "        model: {}\n",
      "        policy_loss: -0.0996832475066185\n",
      "        total_loss: 407.98126220703125\n",
      "        vf_explained_var: 0.790763258934021\n",
      "        vf_loss: 408.0642395019531\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.656726768377254\n",
      "    ram_util_percent: 38.41900138696255\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06810755744305874\n",
      "    mean_env_wait_ms: 119.56045884890777\n",
      "    mean_inference_ms: 1.7198390813878144\n",
      "    mean_raw_obs_processing_ms: 9.39705959078869\n",
      "  time_since_restore: 74887.08270263672\n",
      "  time_this_iter_s: 505.2839105129242\n",
      "  time_total_s: 74887.08270263672\n",
      "  timers:\n",
      "    learn_throughput: 260.56\n",
      "    learn_time_ms: 15351.536\n",
      "    load_throughput: 10547.286\n",
      "    load_time_ms: 379.244\n",
      "    sample_throughput: 8.23\n",
      "    sample_time_ms: 486000.871\n",
      "    update_time_ms: 3.718\n",
      "  timestamp: 1613901446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 144.46\n",
      "  episode_reward_max: 118.3980188304479\n",
      "  episode_reward_mean: 93.42574276068312\n",
      "  episode_reward_min: -106.52348667552874\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4336\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5067443251609802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010971568524837494\n",
      "        model: {}\n",
      "        policy_loss: -0.06811364740133286\n",
      "        total_loss: 452.9324951171875\n",
      "        vf_explained_var: 0.6769991517066956\n",
      "        vf_loss: 452.988037109375\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.46736694677871\n",
      "    ram_util_percent: 38.378151260504204\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06812365814838989\n",
      "    mean_env_wait_ms: 119.50133882905752\n",
      "    mean_inference_ms: 1.720096938446058\n",
      "    mean_raw_obs_processing_ms: 9.390908318085735\n",
      "  time_since_restore: 75387.62991380692\n",
      "  time_this_iter_s: 500.54721117019653\n",
      "  time_total_s: 75387.62991380692\n",
      "  timers:\n",
      "    learn_throughput: 260.555\n",
      "    learn_time_ms: 15351.824\n",
      "    load_throughput: 10501.801\n",
      "    load_time_ms: 380.887\n",
      "    sample_throughput: 8.236\n",
      "    sample_time_ms: 485666.887\n",
      "    update_time_ms: 3.724\n",
      "  timestamp: 1613901947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 141.74\n",
      "  episode_reward_max: 118.38747272626844\n",
      "  episode_reward_mean: 89.11573236867281\n",
      "  episode_reward_min: -106.52348667552874\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 4362\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5270000100135803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013160942122340202\n",
      "        model: {}\n",
      "        policy_loss: -0.08172133564949036\n",
      "        total_loss: 480.5737609863281\n",
      "        vf_explained_var: 0.6476766467094421\n",
      "        vf_loss: 480.6405029296875\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.1931081081081\n",
      "    ram_util_percent: 38.35702702702703\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06814497785539061\n",
      "    mean_env_wait_ms: 119.45281915560403\n",
      "    mean_inference_ms: 1.720415928621482\n",
      "    mean_raw_obs_processing_ms: 9.384962652788973\n",
      "  time_since_restore: 75906.1450073719\n",
      "  time_this_iter_s: 518.5150935649872\n",
      "  time_total_s: 75906.1450073719\n",
      "  timers:\n",
      "    learn_throughput: 258.701\n",
      "    learn_time_ms: 15461.872\n",
      "    load_throughput: 10395.127\n",
      "    load_time_ms: 384.796\n",
      "    sample_throughput: 8.209\n",
      "    sample_time_ms: 487288.568\n",
      "    update_time_ms: 3.87\n",
      "  timestamp: 1613902466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-22-50\n",
      "  done: false\n",
      "  episode_len_mean: 139.19\n",
      "  episode_reward_max: 118.37581776245045\n",
      "  episode_reward_mean: 85.05349157386325\n",
      "  episode_reward_min: -106.52348667552874\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 4393\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4903784990310669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012807418592274189\n",
      "        model: {}\n",
      "        policy_loss: -0.08418212085962296\n",
      "        total_loss: 243.77842712402344\n",
      "        vf_explained_var: 0.827601969242096\n",
      "        vf_loss: 243.84803771972656\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.514027777777784\n",
      "    ram_util_percent: 38.35930555555556\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06816904112888227\n",
      "    mean_env_wait_ms: 119.39716645914392\n",
      "    mean_inference_ms: 1.7207919127298095\n",
      "    mean_raw_obs_processing_ms: 9.379445230691656\n",
      "  time_since_restore: 76410.0078485012\n",
      "  time_this_iter_s: 503.862841129303\n",
      "  time_total_s: 76410.0078485012\n",
      "  timers:\n",
      "    learn_throughput: 258.661\n",
      "    learn_time_ms: 15464.238\n",
      "    load_throughput: 10496.982\n",
      "    load_time_ms: 381.062\n",
      "    sample_throughput: 8.204\n",
      "    sample_time_ms: 487564.424\n",
      "    update_time_ms: 3.868\n",
      "  timestamp: 1613902970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-31-13\n",
      "  done: false\n",
      "  episode_len_mean: 143.68\n",
      "  episode_reward_max: 118.37902254097419\n",
      "  episode_reward_mean: 91.58540820396809\n",
      "  episode_reward_min: -102.28401878985044\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4421\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5134223699569702\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010403241962194443\n",
      "        model: {}\n",
      "        policy_loss: -0.064632348716259\n",
      "        total_loss: 142.81903076171875\n",
      "        vf_explained_var: 0.8575036525726318\n",
      "        vf_loss: 142.87185668945312\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.490934449093444\n",
      "    ram_util_percent: 38.3768479776848\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0681908187331111\n",
      "    mean_env_wait_ms: 119.34707099125372\n",
      "    mean_inference_ms: 1.7211543747880358\n",
      "    mean_raw_obs_processing_ms: 9.373413416440878\n",
      "  time_since_restore: 76912.78030776978\n",
      "  time_this_iter_s: 502.77245926856995\n",
      "  time_total_s: 76912.78030776978\n",
      "  timers:\n",
      "    learn_throughput: 258.631\n",
      "    learn_time_ms: 15466.04\n",
      "    load_throughput: 10541.963\n",
      "    load_time_ms: 379.436\n",
      "    sample_throughput: 8.206\n",
      "    sample_time_ms: 487454.452\n",
      "    update_time_ms: 3.895\n",
      "  timestamp: 1613903473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 142.24\n",
      "  episode_reward_max: 118.37902254097419\n",
      "  episode_reward_mean: 89.35394718694442\n",
      "  episode_reward_min: -105.46655939619049\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4448\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.530292809009552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013976650312542915\n",
      "        model: {}\n",
      "        policy_loss: -0.0955086275935173\n",
      "        total_loss: 412.23358154296875\n",
      "        vf_explained_var: 0.7463707327842712\n",
      "        vf_loss: 412.3132019042969\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.51969273743018\n",
      "    ram_util_percent: 38.37164804469273\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06820948582796282\n",
      "    mean_env_wait_ms: 119.29646149770547\n",
      "    mean_inference_ms: 1.7214731315585197\n",
      "    mean_raw_obs_processing_ms: 9.367935867080444\n",
      "  time_since_restore: 77414.37563896179\n",
      "  time_this_iter_s: 501.5953311920166\n",
      "  time_total_s: 77414.37563896179\n",
      "  timers:\n",
      "    learn_throughput: 258.62\n",
      "    learn_time_ms: 15466.721\n",
      "    load_throughput: 10471.65\n",
      "    load_time_ms: 381.984\n",
      "    sample_throughput: 8.2\n",
      "    sample_time_ms: 487792.196\n",
      "    update_time_ms: 3.898\n",
      "  timestamp: 1613903974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 146.42\n",
      "  episode_reward_max: 118.39815019229135\n",
      "  episode_reward_mean: 99.8468263089411\n",
      "  episode_reward_min: -105.46655939619049\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4475\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.49783721566200256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011227715760469437\n",
      "        model: {}\n",
      "        policy_loss: -0.07338139414787292\n",
      "        total_loss: 137.79476928710938\n",
      "        vf_explained_var: 0.8573033213615417\n",
      "        vf_loss: 137.8553466796875\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.584129213483145\n",
      "    ram_util_percent: 38.39999999999999\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0682249442096093\n",
      "    mean_env_wait_ms: 119.24202864447405\n",
      "    mean_inference_ms: 1.72173241233231\n",
      "    mean_raw_obs_processing_ms: 9.36173211631575\n",
      "  time_since_restore: 77912.99190068245\n",
      "  time_this_iter_s: 498.61626172065735\n",
      "  time_total_s: 77912.99190068245\n",
      "  timers:\n",
      "    learn_throughput: 258.612\n",
      "    learn_time_ms: 15467.174\n",
      "    load_throughput: 10505.049\n",
      "    load_time_ms: 380.769\n",
      "    sample_throughput: 8.208\n",
      "    sample_time_ms: 487310.161\n",
      "    update_time_ms: 3.911\n",
      "  timestamp: 1613904473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_11-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 144.87\n",
      "  episode_reward_max: 118.39815019229135\n",
      "  episode_reward_mean: 93.49306636159764\n",
      "  episode_reward_min: -105.46655939619049\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4504\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5081709027290344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01671558804810047\n",
      "        model: {}\n",
      "        policy_loss: -0.09919644147157669\n",
      "        total_loss: 722.3006591796875\n",
      "        vf_explained_var: 0.5905051827430725\n",
      "        vf_loss: 722.3807983398438\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.49304589707928\n",
      "    ram_util_percent: 38.400973574408894\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0682419435160842\n",
      "    mean_env_wait_ms: 119.18308159400847\n",
      "    mean_inference_ms: 1.7220153899542447\n",
      "    mean_raw_obs_processing_ms: 9.354646149701574\n",
      "  time_since_restore: 78416.43280887604\n",
      "  time_this_iter_s: 503.44090819358826\n",
      "  time_total_s: 78416.43280887604\n",
      "  timers:\n",
      "    learn_throughput: 258.609\n",
      "    learn_time_ms: 15467.359\n",
      "    load_throughput: 10529.166\n",
      "    load_time_ms: 379.897\n",
      "    sample_throughput: 8.206\n",
      "    sample_time_ms: 487433.854\n",
      "    update_time_ms: 3.808\n",
      "  timestamp: 1613904977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-04-38\n",
      "  done: false\n",
      "  episode_len_mean: 145.58\n",
      "  episode_reward_max: 118.39815019229135\n",
      "  episode_reward_mean: 95.37161295857453\n",
      "  episode_reward_min: -105.46655939619049\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4529\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.498735249042511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013529784977436066\n",
      "        model: {}\n",
      "        policy_loss: -0.07950646430253983\n",
      "        total_loss: 100.89649963378906\n",
      "        vf_explained_var: 0.9133281111717224\n",
      "        vf_loss: 100.9605941772461\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.422797202797206\n",
      "    ram_util_percent: 38.3627972027972\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06825711379685977\n",
      "    mean_env_wait_ms: 119.13289466671448\n",
      "    mean_inference_ms: 1.7222499108562166\n",
      "    mean_raw_obs_processing_ms: 9.347165462326934\n",
      "  time_since_restore: 78917.3786187172\n",
      "  time_this_iter_s: 500.945809841156\n",
      "  time_total_s: 78917.3786187172\n",
      "  timers:\n",
      "    learn_throughput: 258.629\n",
      "    learn_time_ms: 15466.156\n",
      "    load_throughput: 10468.486\n",
      "    load_time_ms: 382.099\n",
      "    sample_throughput: 8.206\n",
      "    sample_time_ms: 487475.379\n",
      "    update_time_ms: 3.788\n",
      "  timestamp: 1613905478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 147.43\n",
      "  episode_reward_max: 118.39815019229135\n",
      "  episode_reward_mean: 101.79849403752138\n",
      "  episode_reward_min: -105.13455363035769\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4556\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5137531757354736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009980542585253716\n",
      "        model: {}\n",
      "        policy_loss: -0.059497371315956116\n",
      "        total_loss: 96.02701568603516\n",
      "        vf_explained_var: 0.8777271509170532\n",
      "        vf_loss: 96.07512664794922\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.493994413407826\n",
      "    ram_util_percent: 38.389525139664805\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06827321060552413\n",
      "    mean_env_wait_ms: 119.07944373877535\n",
      "    mean_inference_ms: 1.7224923543115978\n",
      "    mean_raw_obs_processing_ms: 9.339152265753075\n",
      "  time_since_restore: 79418.83561849594\n",
      "  time_this_iter_s: 501.45699977874756\n",
      "  time_total_s: 79418.83561849594\n",
      "  timers:\n",
      "    learn_throughput: 258.656\n",
      "    learn_time_ms: 15464.54\n",
      "    load_throughput: 10408.79\n",
      "    load_time_ms: 384.291\n",
      "    sample_throughput: 8.202\n",
      "    sample_time_ms: 487663.005\n",
      "    update_time_ms: 3.748\n",
      "  timestamp: 1613905980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-21-27\n",
      "  done: false\n",
      "  episode_len_mean: 145.03\n",
      "  episode_reward_max: 118.39018873968277\n",
      "  episode_reward_mean: 95.70636682832276\n",
      "  episode_reward_min: -104.57549434804909\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 4586\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4809300899505615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010893009603023529\n",
      "        model: {}\n",
      "        policy_loss: -0.0727214515209198\n",
      "        total_loss: 324.16070556640625\n",
      "        vf_explained_var: 0.7522114515304565\n",
      "        vf_loss: 324.2209777832031\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.886445366528356\n",
      "    ram_util_percent: 38.41355463347165\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06829118105209747\n",
      "    mean_env_wait_ms: 119.02403248065319\n",
      "    mean_inference_ms: 1.7227890537873793\n",
      "    mean_raw_obs_processing_ms: 9.331793122015482\n",
      "  time_since_restore: 79925.64782118797\n",
      "  time_this_iter_s: 506.81220269203186\n",
      "  time_total_s: 79925.64782118797\n",
      "  timers:\n",
      "    learn_throughput: 258.702\n",
      "    learn_time_ms: 15461.835\n",
      "    load_throughput: 10365.983\n",
      "    load_time_ms: 385.878\n",
      "    sample_throughput: 8.2\n",
      "    sample_time_ms: 487820.742\n",
      "    update_time_ms: 3.736\n",
      "  timestamp: 1613906487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-29-51\n",
      "  done: false\n",
      "  episode_len_mean: 139.82\n",
      "  episode_reward_max: 118.38711441765899\n",
      "  episode_reward_mean: 93.5739814396936\n",
      "  episode_reward_min: -105.59301154200861\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 4616\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4924875795841217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013687827624380589\n",
      "        model: {}\n",
      "        policy_loss: -0.0899907648563385\n",
      "        total_loss: 656.1348266601562\n",
      "        vf_explained_var: 0.5884491801261902\n",
      "        vf_loss: 656.209228515625\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50681502086231\n",
      "    ram_util_percent: 38.403337969401946\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0683084623806782\n",
      "    mean_env_wait_ms: 118.96917013579419\n",
      "    mean_inference_ms: 1.7230674166506368\n",
      "    mean_raw_obs_processing_ms: 9.325989084085018\n",
      "  time_since_restore: 80429.35766005516\n",
      "  time_this_iter_s: 503.7098388671875\n",
      "  time_total_s: 80429.35766005516\n",
      "  timers:\n",
      "    learn_throughput: 258.785\n",
      "    learn_time_ms: 15456.868\n",
      "    load_throughput: 10437.554\n",
      "    load_time_ms: 383.232\n",
      "    sample_throughput: 8.194\n",
      "    sample_time_ms: 488145.28\n",
      "    update_time_ms: 3.731\n",
      "  timestamp: 1613906991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 136.29\n",
      "  episode_reward_max: 118.38711441765899\n",
      "  episode_reward_mean: 89.21912044415974\n",
      "  episode_reward_min: -105.59301154200861\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4644\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47926148772239685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010709519498050213\n",
      "        model: {}\n",
      "        policy_loss: -0.07456857711076736\n",
      "        total_loss: 257.04217529296875\n",
      "        vf_explained_var: 0.8113617897033691\n",
      "        vf_loss: 257.1045227050781\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.72726008344924\n",
      "    ram_util_percent: 38.4279554937413\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06832272952601366\n",
      "    mean_env_wait_ms: 118.92079189347946\n",
      "    mean_inference_ms: 1.7233032813045441\n",
      "    mean_raw_obs_processing_ms: 9.322109060917926\n",
      "  time_since_restore: 80933.29212784767\n",
      "  time_this_iter_s: 503.934467792511\n",
      "  time_total_s: 80933.29212784767\n",
      "  timers:\n",
      "    learn_throughput: 260.669\n",
      "    learn_time_ms: 15345.127\n",
      "    load_throughput: 10533.839\n",
      "    load_time_ms: 379.729\n",
      "    sample_throughput: 8.217\n",
      "    sample_time_ms: 486801.282\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1613907495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 137.96\n",
      "  episode_reward_max: 118.37702578963545\n",
      "  episode_reward_mean: 91.00470087653838\n",
      "  episode_reward_min: -105.59301154200861\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4672\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.483931303024292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00852456409484148\n",
      "        model: {}\n",
      "        policy_loss: -0.05814878270030022\n",
      "        total_loss: 143.52391052246094\n",
      "        vf_explained_var: 0.851458728313446\n",
      "        vf_loss: 143.57232666015625\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.409052924791084\n",
      "    ram_util_percent: 38.43802228412256\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06833577200353232\n",
      "    mean_env_wait_ms: 118.87303903000677\n",
      "    mean_inference_ms: 1.7235220631363324\n",
      "    mean_raw_obs_processing_ms: 9.317992589804787\n",
      "  time_since_restore: 81435.83357095718\n",
      "  time_this_iter_s: 502.54144310951233\n",
      "  time_total_s: 81435.83357095718\n",
      "  timers:\n",
      "    learn_throughput: 260.723\n",
      "    learn_time_ms: 15341.943\n",
      "    load_throughput: 10450.76\n",
      "    load_time_ms: 382.747\n",
      "    sample_throughput: 8.219\n",
      "    sample_time_ms: 486669.763\n",
      "    update_time_ms: 3.57\n",
      "  timestamp: 1613907997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_12-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 141.33\n",
      "  episode_reward_max: 118.35859723926629\n",
      "  episode_reward_mean: 91.164402135341\n",
      "  episode_reward_min: -107.67796134002458\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4700\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45184624195098877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007745564449578524\n",
      "        model: {}\n",
      "        policy_loss: -0.06402236968278885\n",
      "        total_loss: 214.5061492919922\n",
      "        vf_explained_var: 0.8167248964309692\n",
      "        vf_loss: 214.5613555908203\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.58587412587412\n",
      "    ram_util_percent: 38.42741258741258\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06834859427636221\n",
      "    mean_env_wait_ms: 118.82406410294276\n",
      "    mean_inference_ms: 1.7237331875441542\n",
      "    mean_raw_obs_processing_ms: 9.31296374518676\n",
      "  time_since_restore: 81937.16941165924\n",
      "  time_this_iter_s: 501.3358407020569\n",
      "  time_total_s: 81937.16941165924\n",
      "  timers:\n",
      "    learn_throughput: 260.717\n",
      "    learn_time_ms: 15342.318\n",
      "    load_throughput: 10364.624\n",
      "    load_time_ms: 385.928\n",
      "    sample_throughput: 8.222\n",
      "    sample_time_ms: 486521.161\n",
      "    update_time_ms: 3.544\n",
      "  timestamp: 1613908499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-03-34\n",
      "  done: false\n",
      "  episode_len_mean: 138.78\n",
      "  episode_reward_max: 118.35859723926629\n",
      "  episode_reward_mean: 95.12206202352436\n",
      "  episode_reward_min: -107.67796134002458\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4729\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4588473439216614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009239591658115387\n",
      "        model: {}\n",
      "        policy_loss: -0.06830652803182602\n",
      "        total_loss: 257.38818359375\n",
      "        vf_explained_var: 0.7817555069923401\n",
      "        vf_loss: 257.4459533691406\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.95006802721089\n",
      "    ram_util_percent: 38.445306122448976\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06836559118749184\n",
      "    mean_env_wait_ms: 118.7779867877806\n",
      "    mean_inference_ms: 1.724009615335417\n",
      "    mean_raw_obs_processing_ms: 9.308062446937843\n",
      "  time_since_restore: 82451.84921574593\n",
      "  time_this_iter_s: 514.6798040866852\n",
      "  time_total_s: 82451.84921574593\n",
      "  timers:\n",
      "    learn_throughput: 260.67\n",
      "    learn_time_ms: 15345.067\n",
      "    load_throughput: 10505.511\n",
      "    load_time_ms: 380.753\n",
      "    sample_throughput: 8.199\n",
      "    sample_time_ms: 487836.555\n",
      "    update_time_ms: 3.516\n",
      "  timestamp: 1613909014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-11-53\n",
      "  done: false\n",
      "  episode_len_mean: 142.57\n",
      "  episode_reward_max: 118.33498898653042\n",
      "  episode_reward_mean: 93.09943605118976\n",
      "  episode_reward_min: -107.67796134002458\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 4754\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.48156386613845825\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008385772816836834\n",
      "        model: {}\n",
      "        policy_loss: -0.061471596360206604\n",
      "        total_loss: 148.40931701660156\n",
      "        vf_explained_var: 0.8710538744926453\n",
      "        vf_loss: 148.46124267578125\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.47201125175808\n",
      "    ram_util_percent: 38.43445850914205\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06838140315100122\n",
      "    mean_env_wait_ms: 118.73755850186022\n",
      "    mean_inference_ms: 1.7242594547379972\n",
      "    mean_raw_obs_processing_ms: 9.302440959107404\n",
      "  time_since_restore: 82950.16076350212\n",
      "  time_this_iter_s: 498.31154775619507\n",
      "  time_total_s: 82950.16076350212\n",
      "  timers:\n",
      "    learn_throughput: 260.64\n",
      "    learn_time_ms: 15346.855\n",
      "    load_throughput: 10452.15\n",
      "    load_time_ms: 382.696\n",
      "    sample_throughput: 8.2\n",
      "    sample_time_ms: 487801.336\n",
      "    update_time_ms: 3.506\n",
      "  timestamp: 1613909513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-20-14\n",
      "  done: false\n",
      "  episode_len_mean: 144.83\n",
      "  episode_reward_max: 118.39037298642431\n",
      "  episode_reward_mean: 97.17337704752077\n",
      "  episode_reward_min: -107.67796134002458\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4783\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4631302058696747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022198306396603584\n",
      "        model: {}\n",
      "        policy_loss: -0.1054145023226738\n",
      "        total_loss: 4.604348659515381\n",
      "        vf_explained_var: 0.9934733510017395\n",
      "        vf_loss: 4.684477806091309\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.636033519553074\n",
      "    ram_util_percent: 38.420251396648034\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06840029110874933\n",
      "    mean_env_wait_ms: 118.69003585615252\n",
      "    mean_inference_ms: 1.7245456448276337\n",
      "    mean_raw_obs_processing_ms: 9.29664632881619\n",
      "  time_since_restore: 83451.58774232864\n",
      "  time_this_iter_s: 501.4269788265228\n",
      "  time_total_s: 83451.58774232864\n",
      "  timers:\n",
      "    learn_throughput: 260.667\n",
      "    learn_time_ms: 15345.235\n",
      "    load_throughput: 10398.272\n",
      "    load_time_ms: 384.679\n",
      "    sample_throughput: 8.203\n",
      "    sample_time_ms: 487600.048\n",
      "    update_time_ms: 3.536\n",
      "  timestamp: 1613910014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-28-46\n",
      "  done: false\n",
      "  episode_len_mean: 140.43\n",
      "  episode_reward_max: 118.39037298642431\n",
      "  episode_reward_mean: 86.73863290371777\n",
      "  episode_reward_min: -106.73063507915431\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 4814\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4861518442630768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014259721152484417\n",
      "        model: {}\n",
      "        policy_loss: -0.1085641160607338\n",
      "        total_loss: 890.0173950195312\n",
      "        vf_explained_var: 0.6484097242355347\n",
      "        vf_loss: 890.1015014648438\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.30109439124487\n",
      "    ram_util_percent: 38.44856361149111\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684201847453603\n",
      "    mean_env_wait_ms: 118.64182070849293\n",
      "    mean_inference_ms: 1.7248468653481246\n",
      "    mean_raw_obs_processing_ms: 9.292025198467657\n",
      "  time_since_restore: 83963.45505332947\n",
      "  time_this_iter_s: 511.867311000824\n",
      "  time_total_s: 83963.45505332947\n",
      "  timers:\n",
      "    learn_throughput: 260.609\n",
      "    learn_time_ms: 15348.685\n",
      "    load_throughput: 10471.676\n",
      "    load_time_ms: 381.983\n",
      "    sample_throughput: 8.185\n",
      "    sample_time_ms: 488687.637\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1613910526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 134.17\n",
      "  episode_reward_max: 118.39037298642431\n",
      "  episode_reward_mean: 82.49990093625473\n",
      "  episode_reward_min: -108.49692041121378\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 4846\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4706670641899109\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010649780742824078\n",
      "        model: {}\n",
      "        policy_loss: -0.09022080153226852\n",
      "        total_loss: 630.833251953125\n",
      "        vf_explained_var: 0.6915320158004761\n",
      "        vf_loss: 630.9052124023438\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.651523545706375\n",
      "    ram_util_percent: 38.46246537396121\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06843881547686444\n",
      "    mean_env_wait_ms: 118.59046576565484\n",
      "    mean_inference_ms: 1.7251222264353583\n",
      "    mean_raw_obs_processing_ms: 9.290385136188767\n",
      "  time_since_restore: 84469.41310858727\n",
      "  time_this_iter_s: 505.95805525779724\n",
      "  time_total_s: 84469.41310858727\n",
      "  timers:\n",
      "    learn_throughput: 260.553\n",
      "    learn_time_ms: 15351.982\n",
      "    load_throughput: 10518.798\n",
      "    load_time_ms: 380.272\n",
      "    sample_throughput: 8.178\n",
      "    sample_time_ms: 489137.309\n",
      "    update_time_ms: 3.528\n",
      "  timestamp: 1613911032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-45-35\n",
      "  done: false\n",
      "  episode_len_mean: 132.61\n",
      "  episode_reward_max: 118.36625902754591\n",
      "  episode_reward_mean: 73.9377834859567\n",
      "  episode_reward_min: -108.49692041121378\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4874\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4453345835208893\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008407643996179104\n",
      "        model: {}\n",
      "        policy_loss: -0.06736180186271667\n",
      "        total_loss: 399.472412109375\n",
      "        vf_explained_var: 0.7319225072860718\n",
      "        vf_loss: 399.5254211425781\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.49818941504178\n",
      "    ram_util_percent: 38.534958217270194\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06845410040293094\n",
      "    mean_env_wait_ms: 118.54950370540271\n",
      "    mean_inference_ms: 1.7253463498119288\n",
      "    mean_raw_obs_processing_ms: 9.289647632741172\n",
      "  time_since_restore: 84972.18417716026\n",
      "  time_this_iter_s: 502.77106857299805\n",
      "  time_total_s: 84972.18417716026\n",
      "  timers:\n",
      "    learn_throughput: 260.555\n",
      "    learn_time_ms: 15351.834\n",
      "    load_throughput: 10563.555\n",
      "    load_time_ms: 378.66\n",
      "    sample_throughput: 8.184\n",
      "    sample_time_ms: 488735.908\n",
      "    update_time_ms: 3.542\n",
      "  timestamp: 1613911535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_13-53-58\n",
      "  done: false\n",
      "  episode_len_mean: 137.36\n",
      "  episode_reward_max: 118.34410772090696\n",
      "  episode_reward_mean: 82.48837827915752\n",
      "  episode_reward_min: -108.49692041121378\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 4902\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.48138877749443054\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006494223140180111\n",
      "        model: {}\n",
      "        policy_loss: -0.05838268622756004\n",
      "        total_loss: 196.99696350097656\n",
      "        vf_explained_var: 0.8003584742546082\n",
      "        vf_loss: 197.0442352294922\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.44755927475594\n",
      "    ram_util_percent: 38.53626220362622\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06846836310301345\n",
      "    mean_env_wait_ms: 118.5073979587736\n",
      "    mean_inference_ms: 1.725555821609911\n",
      "    mean_raw_obs_processing_ms: 9.287606818681013\n",
      "  time_since_restore: 85475.04824829102\n",
      "  time_this_iter_s: 502.86407113075256\n",
      "  time_total_s: 85475.04824829102\n",
      "  timers:\n",
      "    learn_throughput: 260.487\n",
      "    learn_time_ms: 15355.868\n",
      "    load_throughput: 10579.935\n",
      "    load_time_ms: 378.074\n",
      "    sample_throughput: 8.186\n",
      "    sample_time_ms: 488648.57\n",
      "    update_time_ms: 3.521\n",
      "  timestamp: 1613912038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 138.9\n",
      "  episode_reward_max: 118.34560936490521\n",
      "  episode_reward_mean: 82.44115056898363\n",
      "  episode_reward_min: -108.49692041121378\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4931\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.482185035943985\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009056269191205502\n",
      "        model: {}\n",
      "        policy_loss: -0.0668887123465538\n",
      "        total_loss: 415.8406677246094\n",
      "        vf_explained_var: 0.6967179775238037\n",
      "        vf_loss: 415.8920593261719\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.52962447844229\n",
      "    ram_util_percent: 38.519888734353266\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684830063540474\n",
      "    mean_env_wait_ms: 118.46261854802353\n",
      "    mean_inference_ms: 1.7257911941250763\n",
      "    mean_raw_obs_processing_ms: 9.284068747045817\n",
      "  time_since_restore: 85978.68153715134\n",
      "  time_this_iter_s: 503.63328886032104\n",
      "  time_total_s: 85978.68153715134\n",
      "  timers:\n",
      "    learn_throughput: 260.529\n",
      "    learn_time_ms: 15353.371\n",
      "    load_throughput: 10575.344\n",
      "    load_time_ms: 378.238\n",
      "    sample_throughput: 8.186\n",
      "    sample_time_ms: 488620.749\n",
      "    update_time_ms: 3.547\n",
      "  timestamp: 1613912542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 147.27\n",
      "  episode_reward_max: 118.34560936490521\n",
      "  episode_reward_mean: 93.17703128492795\n",
      "  episode_reward_min: -105.14930449860003\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 4957\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5064404010772705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008462098427116871\n",
      "        model: {}\n",
      "        policy_loss: -0.07593437284231186\n",
      "        total_loss: 485.3306884765625\n",
      "        vf_explained_var: 0.6405306458473206\n",
      "        vf_loss: 485.3921203613281\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.34888268156424\n",
      "    ram_util_percent: 38.46480446927374\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0684963894114604\n",
      "    mean_env_wait_ms: 118.42176954953226\n",
      "    mean_inference_ms: 1.7260068639032915\n",
      "    mean_raw_obs_processing_ms: 9.278926797780018\n",
      "  time_since_restore: 86480.10954904556\n",
      "  time_this_iter_s: 501.4280118942261\n",
      "  time_total_s: 86480.10954904556\n",
      "  timers:\n",
      "    learn_throughput: 260.49\n",
      "    learn_time_ms: 15355.671\n",
      "    load_throughput: 10668.732\n",
      "    load_time_ms: 374.927\n",
      "    sample_throughput: 8.188\n",
      "    sample_time_ms: 488510.573\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1613913044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-19-07\n",
      "  done: false\n",
      "  episode_len_mean: 146.68\n",
      "  episode_reward_max: 118.34560936490521\n",
      "  episode_reward_mean: 90.83358563840925\n",
      "  episode_reward_min: -106.96458987683931\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 4984\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4691942632198334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007456604856997728\n",
      "        model: {}\n",
      "        policy_loss: -0.06715694814920425\n",
      "        total_loss: 167.9293975830078\n",
      "        vf_explained_var: 0.876112699508667\n",
      "        vf_loss: 167.9838409423828\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.44568245125348\n",
      "    ram_util_percent: 38.464623955431755\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06851008589635542\n",
      "    mean_env_wait_ms: 118.37968971116618\n",
      "    mean_inference_ms: 1.7262374059642367\n",
      "    mean_raw_obs_processing_ms: 9.273064479720702\n",
      "  time_since_restore: 86982.80327177048\n",
      "  time_this_iter_s: 502.69372272491455\n",
      "  time_total_s: 86982.80327177048\n",
      "  timers:\n",
      "    learn_throughput: 260.482\n",
      "    learn_time_ms: 15356.137\n",
      "    load_throughput: 10752.641\n",
      "    load_time_ms: 372.002\n",
      "    sample_throughput: 8.186\n",
      "    sample_time_ms: 488648.189\n",
      "    update_time_ms: 3.569\n",
      "  timestamp: 1613913547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-27-31\n",
      "  done: false\n",
      "  episode_len_mean: 145.8\n",
      "  episode_reward_max: 118.3816349940228\n",
      "  episode_reward_mean: 92.87013160735619\n",
      "  episode_reward_min: -106.96458987683931\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5012\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4480264186859131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006355721037834883\n",
      "        model: {}\n",
      "        policy_loss: -0.05137742683291435\n",
      "        total_loss: 176.2694549560547\n",
      "        vf_explained_var: 0.8188726305961609\n",
      "        vf_loss: 176.30995178222656\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.84214186369958\n",
      "    ram_util_percent: 38.53407510431154\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06852427330376414\n",
      "    mean_env_wait_ms: 118.3369487778647\n",
      "    mean_inference_ms: 1.7264821033324285\n",
      "    mean_raw_obs_processing_ms: 9.266807857567928\n",
      "  time_since_restore: 87486.67959189415\n",
      "  time_this_iter_s: 503.8763201236725\n",
      "  time_total_s: 87486.67959189415\n",
      "  timers:\n",
      "    learn_throughput: 260.502\n",
      "    learn_time_ms: 15354.973\n",
      "    load_throughput: 10772.593\n",
      "    load_time_ms: 371.313\n",
      "    sample_throughput: 8.204\n",
      "    sample_time_ms: 487568.193\n",
      "    update_time_ms: 3.575\n",
      "  timestamp: 1613914051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 145.21\n",
      "  episode_reward_max: 118.3816349940228\n",
      "  episode_reward_mean: 92.70610928211364\n",
      "  episode_reward_min: -107.0333886008569\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5040\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46228596568107605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006902496796101332\n",
      "        model: {}\n",
      "        policy_loss: -0.059275072067976\n",
      "        total_loss: 207.67410278320312\n",
      "        vf_explained_var: 0.8182961940765381\n",
      "        vf_loss: 207.72158813476562\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.528351955307265\n",
      "    ram_util_percent: 38.51368715083799\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06853728558560826\n",
      "    mean_env_wait_ms: 118.29366629630957\n",
      "    mean_inference_ms: 1.7266945183978606\n",
      "    mean_raw_obs_processing_ms: 9.260610431761378\n",
      "  time_since_restore: 87988.05377340317\n",
      "  time_this_iter_s: 501.37418150901794\n",
      "  time_total_s: 87988.05377340317\n",
      "  timers:\n",
      "    learn_throughput: 260.559\n",
      "    learn_time_ms: 15351.598\n",
      "    load_throughput: 10820.894\n",
      "    load_time_ms: 369.655\n",
      "    sample_throughput: 8.199\n",
      "    sample_time_ms: 487880.544\n",
      "    update_time_ms: 3.574\n",
      "  timestamp: 1613914553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 146.88\n",
      "  episode_reward_max: 118.3816349940228\n",
      "  episode_reward_mean: 94.76799787769761\n",
      "  episode_reward_min: -107.0333886008569\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5067\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4856013059616089\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00634542154148221\n",
      "        model: {}\n",
      "        policy_loss: -0.0633016899228096\n",
      "        total_loss: 289.1907653808594\n",
      "        vf_explained_var: 0.7325077652931213\n",
      "        vf_loss: 289.24322509765625\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.39762237762238\n",
      "    ram_util_percent: 38.476083916083915\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06854874999601568\n",
      "    mean_env_wait_ms: 118.25243670852525\n",
      "    mean_inference_ms: 1.7268768816733582\n",
      "    mean_raw_obs_processing_ms: 9.255162367471213\n",
      "  time_since_restore: 88489.42773461342\n",
      "  time_this_iter_s: 501.37396121025085\n",
      "  time_total_s: 88489.42773461342\n",
      "  timers:\n",
      "    learn_throughput: 260.505\n",
      "    learn_time_ms: 15354.819\n",
      "    load_throughput: 10840.526\n",
      "    load_time_ms: 368.986\n",
      "    sample_throughput: 8.199\n",
      "    sample_time_ms: 487869.291\n",
      "    update_time_ms: 3.56\n",
      "  timestamp: 1613915054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_14-52-34\n",
      "  done: false\n",
      "  episode_len_mean: 147.78\n",
      "  episode_reward_max: 118.37371998946179\n",
      "  episode_reward_mean: 101.20822458460525\n",
      "  episode_reward_min: -107.0333886008569\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5094\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4862886667251587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010524340905249119\n",
      "        model: {}\n",
      "        policy_loss: -0.07467919588088989\n",
      "        total_loss: 25.640653610229492\n",
      "        vf_explained_var: 0.9721167683601379\n",
      "        vf_loss: 25.697351455688477\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50560224089636\n",
      "    ram_util_percent: 38.51330532212885\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06855955403778359\n",
      "    mean_env_wait_ms: 118.21057267026022\n",
      "    mean_inference_ms: 1.7270392530979362\n",
      "    mean_raw_obs_processing_ms: 9.249604904265391\n",
      "  time_since_restore: 88989.29530453682\n",
      "  time_this_iter_s: 499.8675699234009\n",
      "  time_total_s: 88989.29530453682\n",
      "  timers:\n",
      "    learn_throughput: 260.548\n",
      "    learn_time_ms: 15352.241\n",
      "    load_throughput: 10808.73\n",
      "    load_time_ms: 370.071\n",
      "    sample_throughput: 8.219\n",
      "    sample_time_ms: 486675.69\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1613915554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 141.56\n",
      "  episode_reward_max: 118.38313091783473\n",
      "  episode_reward_mean: 90.63242252174445\n",
      "  episode_reward_min: -107.0333886008569\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5125\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47574055194854736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008839340880513191\n",
      "        model: {}\n",
      "        policy_loss: -0.07316874712705612\n",
      "        total_loss: 419.13763427734375\n",
      "        vf_explained_var: 0.7200050950050354\n",
      "        vf_loss: 419.1956481933594\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.53033240997229\n",
      "    ram_util_percent: 38.537257617728535\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06857235438949527\n",
      "    mean_env_wait_ms: 118.1635145875799\n",
      "    mean_inference_ms: 1.7272286108516621\n",
      "    mean_raw_obs_processing_ms: 9.245073630018629\n",
      "  time_since_restore: 89495.47280859947\n",
      "  time_this_iter_s: 506.1775040626526\n",
      "  time_total_s: 89495.47280859947\n",
      "  timers:\n",
      "    learn_throughput: 260.591\n",
      "    learn_time_ms: 15349.74\n",
      "    load_throughput: 10819.157\n",
      "    load_time_ms: 369.715\n",
      "    sample_throughput: 8.219\n",
      "    sample_time_ms: 486700.839\n",
      "    update_time_ms: 3.745\n",
      "  timestamp: 1613916061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 140.49\n",
      "  episode_reward_max: 118.38313091783473\n",
      "  episode_reward_mean: 94.99584880547278\n",
      "  episode_reward_min: -106.97773984480573\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5154\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45154133439064026\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006227417383342981\n",
      "        model: {}\n",
      "        policy_loss: -0.05392485484480858\n",
      "        total_loss: 98.27760314941406\n",
      "        vf_explained_var: 0.9074139595031738\n",
      "        vf_loss: 98.32087707519531\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48326359832636\n",
      "    ram_util_percent: 38.480334728033476\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06858522124725461\n",
      "    mean_env_wait_ms: 118.1203477170672\n",
      "    mean_inference_ms: 1.7274251660541209\n",
      "    mean_raw_obs_processing_ms: 9.241736193590187\n",
      "  time_since_restore: 89997.56052470207\n",
      "  time_this_iter_s: 502.0877161026001\n",
      "  time_total_s: 89997.56052470207\n",
      "  timers:\n",
      "    learn_throughput: 260.591\n",
      "    learn_time_ms: 15349.742\n",
      "    load_throughput: 10824.405\n",
      "    load_time_ms: 369.535\n",
      "    sample_throughput: 8.22\n",
      "    sample_time_ms: 486632.964\n",
      "    update_time_ms: 3.733\n",
      "  timestamp: 1613916563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-17-45\n",
      "  done: false\n",
      "  episode_len_mean: 141.28\n",
      "  episode_reward_max: 118.38313091783473\n",
      "  episode_reward_mean: 95.40361710728526\n",
      "  episode_reward_min: -106.97773984480573\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5181\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.456531286239624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00835502427071333\n",
      "        model: {}\n",
      "        policy_loss: -0.07291597872972488\n",
      "        total_loss: 438.1560363769531\n",
      "        vf_explained_var: 0.6468444466590881\n",
      "        vf_loss: 438.214599609375\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.589385474860336\n",
      "    ram_util_percent: 38.45991620111731\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06859723994206116\n",
      "    mean_env_wait_ms: 118.08135252751872\n",
      "    mean_inference_ms: 1.72762725266968\n",
      "    mean_raw_obs_processing_ms: 9.238742112961548\n",
      "  time_since_restore: 90498.98913621902\n",
      "  time_this_iter_s: 501.4286115169525\n",
      "  time_total_s: 90498.98913621902\n",
      "  timers:\n",
      "    learn_throughput: 260.605\n",
      "    learn_time_ms: 15348.918\n",
      "    load_throughput: 10734.402\n",
      "    load_time_ms: 372.634\n",
      "    sample_throughput: 8.222\n",
      "    sample_time_ms: 486485.14\n",
      "    update_time_ms: 3.781\n",
      "  timestamp: 1613917065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-26-09\n",
      "  done: false\n",
      "  episode_len_mean: 141.21\n",
      "  episode_reward_max: 118.38904413532654\n",
      "  episode_reward_mean: 99.79128053003798\n",
      "  episode_reward_min: -106.6297525379055\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 5207\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4708069860935211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0068193464539945126\n",
      "        model: {}\n",
      "        policy_loss: -0.0499524287879467\n",
      "        total_loss: 160.94512939453125\n",
      "        vf_explained_var: 0.8235843777656555\n",
      "        vf_loss: 160.98341369628906\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.758750000000006\n",
      "    ram_util_percent: 38.46527777777777\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06860977762982276\n",
      "    mean_env_wait_ms: 118.04583679578282\n",
      "    mean_inference_ms: 1.7278293561979303\n",
      "    mean_raw_obs_processing_ms: 9.234586594223986\n",
      "  time_since_restore: 91003.42678427696\n",
      "  time_this_iter_s: 504.4376480579376\n",
      "  time_total_s: 91003.42678427696\n",
      "  timers:\n",
      "    learn_throughput: 260.542\n",
      "    learn_time_ms: 15352.597\n",
      "    load_throughput: 10778.832\n",
      "    load_time_ms: 371.098\n",
      "    sample_throughput: 8.221\n",
      "    sample_time_ms: 486564.373\n",
      "    update_time_ms: 3.729\n",
      "  timestamp: 1613917569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-34-33\n",
      "  done: false\n",
      "  episode_len_mean: 142.26\n",
      "  episode_reward_max: 118.38904413532654\n",
      "  episode_reward_mean: 95.59367805044093\n",
      "  episode_reward_min: -105.04987807272472\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5236\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46796685457229614\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008288552053272724\n",
      "        model: {}\n",
      "        policy_loss: -0.0786781832575798\n",
      "        total_loss: 511.0342102050781\n",
      "        vf_explained_var: 0.6987603306770325\n",
      "        vf_loss: 511.0987548828125\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.47246175243393\n",
      "    ram_util_percent: 38.506815020862305\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06862297427505894\n",
      "    mean_env_wait_ms: 118.0054659057309\n",
      "    mean_inference_ms: 1.728048772605677\n",
      "    mean_raw_obs_processing_ms: 9.229344434382437\n",
      "  time_since_restore: 91506.885689497\n",
      "  time_this_iter_s: 503.45890522003174\n",
      "  time_total_s: 91506.885689497\n",
      "  timers:\n",
      "    learn_throughput: 260.577\n",
      "    learn_time_ms: 15350.576\n",
      "    load_throughput: 10748.637\n",
      "    load_time_ms: 372.14\n",
      "    sample_throughput: 8.217\n",
      "    sample_time_ms: 486768.692\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1613918073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 140.42\n",
      "  episode_reward_max: 118.38922858696029\n",
      "  episode_reward_mean: 91.38592287765752\n",
      "  episode_reward_min: -105.04987807272472\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5266\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5288376808166504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008779721334576607\n",
      "        model: {}\n",
      "        policy_loss: -0.08225809037685394\n",
      "        total_loss: 244.55787658691406\n",
      "        vf_explained_var: 0.8416314125061035\n",
      "        vf_loss: 244.62509155273438\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.71019553072626\n",
      "    ram_util_percent: 38.4645251396648\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0686358215296877\n",
      "    mean_env_wait_ms: 117.96378499799098\n",
      "    mean_inference_ms: 1.7282694910678185\n",
      "    mean_raw_obs_processing_ms: 9.224758524784436\n",
      "  time_since_restore: 92008.88655018806\n",
      "  time_this_iter_s: 502.00086069107056\n",
      "  time_total_s: 92008.88655018806\n",
      "  timers:\n",
      "    learn_throughput: 260.591\n",
      "    learn_time_ms: 15349.737\n",
      "    load_throughput: 10771.157\n",
      "    load_time_ms: 371.362\n",
      "    sample_throughput: 8.219\n",
      "    sample_time_ms: 486701.016\n",
      "    update_time_ms: 3.817\n",
      "  timestamp: 1613918575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-51-14\n",
      "  done: false\n",
      "  episode_len_mean: 146.02\n",
      "  episode_reward_max: 118.38922858696029\n",
      "  episode_reward_mean: 95.39460807275515\n",
      "  episode_reward_min: -105.04987807272472\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 5290\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5048798322677612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015141325071454048\n",
      "        model: {}\n",
      "        policy_loss: -0.10032626986503601\n",
      "        total_loss: 8.430785179138184\n",
      "        vf_explained_var: 0.9881623387336731\n",
      "        vf_loss: 8.505240440368652\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.36910112359551\n",
      "    ram_util_percent: 38.47724719101123\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06864618998927181\n",
      "    mean_env_wait_ms: 117.93039021822675\n",
      "    mean_inference_ms: 1.7284269435281094\n",
      "    mean_raw_obs_processing_ms: 9.220015783477972\n",
      "  time_since_restore: 92507.67228889465\n",
      "  time_this_iter_s: 498.78573870658875\n",
      "  time_total_s: 92507.67228889465\n",
      "  timers:\n",
      "    learn_throughput: 260.593\n",
      "    learn_time_ms: 15349.588\n",
      "    load_throughput: 10737.497\n",
      "    load_time_ms: 372.526\n",
      "    sample_throughput: 8.227\n",
      "    sample_time_ms: 486190.666\n",
      "    update_time_ms: 3.821\n",
      "  timestamp: 1613919074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_15-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 141.08\n",
      "  episode_reward_max: 118.38922858696029\n",
      "  episode_reward_mean: 88.93002689290537\n",
      "  episode_reward_min: -107.3067573041725\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5320\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.48957180976867676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016036346554756165\n",
      "        model: {}\n",
      "        policy_loss: -0.12040140479803085\n",
      "        total_loss: 1086.5328369140625\n",
      "        vf_explained_var: 0.608025074005127\n",
      "        vf_loss: 1086.625732421875\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.46898470097357\n",
      "    ram_util_percent: 38.46773296244784\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06865828042400379\n",
      "    mean_env_wait_ms: 117.88816196702668\n",
      "    mean_inference_ms: 1.7286137726635369\n",
      "    mean_raw_obs_processing_ms: 9.215541094601642\n",
      "  time_since_restore: 93011.05297780037\n",
      "  time_this_iter_s: 503.38068890571594\n",
      "  time_total_s: 93011.05297780037\n",
      "  timers:\n",
      "    learn_throughput: 260.568\n",
      "    learn_time_ms: 15351.097\n",
      "    load_throughput: 10591.534\n",
      "    load_time_ms: 377.66\n",
      "    sample_throughput: 8.224\n",
      "    sample_time_ms: 486385.767\n",
      "    update_time_ms: 3.848\n",
      "  timestamp: 1613919578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 140.05\n",
      "  episode_reward_max: 118.38922858696029\n",
      "  episode_reward_mean: 80.64052161749079\n",
      "  episode_reward_min: -107.3067573041725\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5351\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4718606770038605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012582182884216309\n",
      "        model: {}\n",
      "        policy_loss: -0.09653011709451675\n",
      "        total_loss: 940.9229736328125\n",
      "        vf_explained_var: 0.5720846652984619\n",
      "        vf_loss: 940.9979248046875\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.56440443213297\n",
      "    ram_util_percent: 38.44349030470914\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06867086727106841\n",
      "    mean_env_wait_ms: 117.8463413084463\n",
      "    mean_inference_ms: 1.7288022408307484\n",
      "    mean_raw_obs_processing_ms: 9.211785975004664\n",
      "  time_since_restore: 93517.2189218998\n",
      "  time_this_iter_s: 506.16594409942627\n",
      "  time_total_s: 93517.2189218998\n",
      "  timers:\n",
      "    learn_throughput: 260.59\n",
      "    learn_time_ms: 15349.806\n",
      "    load_throughput: 10517.681\n",
      "    load_time_ms: 380.312\n",
      "    sample_throughput: 8.216\n",
      "    sample_time_ms: 486867.298\n",
      "    update_time_ms: 3.833\n",
      "  timestamp: 1613920084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-16-22\n",
      "  done: false\n",
      "  episode_len_mean: 141.13\n",
      "  episode_reward_max: 118.37493702248727\n",
      "  episode_reward_mean: 78.35876333657444\n",
      "  episode_reward_min: -107.3067573041725\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5376\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.5256088376045227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009462368674576283\n",
      "        model: {}\n",
      "        policy_loss: -0.08388345688581467\n",
      "        total_loss: 501.4094543457031\n",
      "        vf_explained_var: 0.6416015625\n",
      "        vf_loss: 501.4771423339844\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.46357243319268\n",
      "    ram_util_percent: 38.43769338959212\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06868162348637416\n",
      "    mean_env_wait_ms: 117.8122085555477\n",
      "    mean_inference_ms: 1.7289530889830635\n",
      "    mean_raw_obs_processing_ms: 9.207677252935971\n",
      "  time_since_restore: 94015.06229805946\n",
      "  time_this_iter_s: 497.84337615966797\n",
      "  time_total_s: 94015.06229805946\n",
      "  timers:\n",
      "    learn_throughput: 260.6\n",
      "    learn_time_ms: 15349.207\n",
      "    load_throughput: 10478.372\n",
      "    load_time_ms: 381.739\n",
      "    sample_throughput: 8.219\n",
      "    sample_time_ms: 486663.405\n",
      "    update_time_ms: 3.768\n",
      "  timestamp: 1613920582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 145.63\n",
      "  episode_reward_max: 118.34340858200574\n",
      "  episode_reward_mean: 84.71461875629933\n",
      "  episode_reward_min: -104.54988999513048\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 5402\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46560001373291016\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006648436654359102\n",
      "        model: {}\n",
      "        policy_loss: -0.05807759240269661\n",
      "        total_loss: 155.2732696533203\n",
      "        vf_explained_var: 0.8444494009017944\n",
      "        vf_loss: 155.31997680664062\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.47549019607843\n",
      "    ram_util_percent: 38.48473389355742\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0686922814540443\n",
      "    mean_env_wait_ms: 117.77762267661575\n",
      "    mean_inference_ms: 1.7291099226470275\n",
      "    mean_raw_obs_processing_ms: 9.203317976424357\n",
      "  time_since_restore: 94514.89193749428\n",
      "  time_this_iter_s: 499.82963943481445\n",
      "  time_total_s: 94514.89193749428\n",
      "  timers:\n",
      "    learn_throughput: 260.619\n",
      "    learn_time_ms: 15348.096\n",
      "    load_throughput: 10483.485\n",
      "    load_time_ms: 381.553\n",
      "    sample_throughput: 8.23\n",
      "    sample_time_ms: 486028.273\n",
      "    update_time_ms: 3.601\n",
      "  timestamp: 1613921082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 151.13\n",
      "  episode_reward_max: 118.31820820477397\n",
      "  episode_reward_mean: 93.03315928379716\n",
      "  episode_reward_min: -104.54988999513048\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 5427\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46588629484176636\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007632530760020018\n",
      "        model: {}\n",
      "        policy_loss: -0.0638757273554802\n",
      "        total_loss: 280.8990173339844\n",
      "        vf_explained_var: 0.7463623881340027\n",
      "        vf_loss: 280.9498596191406\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.431232492997204\n",
      "    ram_util_percent: 38.42633053221288\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06870344171985562\n",
      "    mean_env_wait_ms: 117.74376743178331\n",
      "    mean_inference_ms: 1.7292855942561431\n",
      "    mean_raw_obs_processing_ms: 9.196971543897542\n",
      "  time_since_restore: 95015.25298166275\n",
      "  time_this_iter_s: 500.3610441684723\n",
      "  time_total_s: 95015.25298166275\n",
      "  timers:\n",
      "    learn_throughput: 260.664\n",
      "    learn_time_ms: 15345.419\n",
      "    load_throughput: 10469.57\n",
      "    load_time_ms: 382.06\n",
      "    sample_throughput: 8.233\n",
      "    sample_time_ms: 485855.474\n",
      "    update_time_ms: 3.592\n",
      "  timestamp: 1613921583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-41-24\n",
      "  done: false\n",
      "  episode_len_mean: 154.2\n",
      "  episode_reward_max: 118.35405133032623\n",
      "  episode_reward_mean: 99.2754836086572\n",
      "  episode_reward_min: -99.1245036832122\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5454\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4846733510494232\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006623440887778997\n",
      "        model: {}\n",
      "        policy_loss: -0.04771051183342934\n",
      "        total_loss: 155.57626342773438\n",
      "        vf_explained_var: 0.8229196071624756\n",
      "        vf_loss: 155.61268615722656\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.516363636363636\n",
      "    ram_util_percent: 38.38503496503496\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06871626289123954\n",
      "    mean_env_wait_ms: 117.70570302148782\n",
      "    mean_inference_ms: 1.7294751196604383\n",
      "    mean_raw_obs_processing_ms: 9.188781422176838\n",
      "  time_since_restore: 95516.08112931252\n",
      "  time_this_iter_s: 500.828147649765\n",
      "  time_total_s: 95516.08112931252\n",
      "  timers:\n",
      "    learn_throughput: 260.668\n",
      "    learn_time_ms: 15345.21\n",
      "    load_throughput: 10494.161\n",
      "    load_time_ms: 381.164\n",
      "    sample_throughput: 8.234\n",
      "    sample_time_ms: 485792.654\n",
      "    update_time_ms: 3.554\n",
      "  timestamp: 1613922084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 144.44\n",
      "  episode_reward_max: 118.39349617717478\n",
      "  episode_reward_mean: 86.86996045042459\n",
      "  episode_reward_min: -100.51393821107163\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5486\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4662671387195587\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012708527036011219\n",
      "        model: {}\n",
      "        policy_loss: -0.09593574702739716\n",
      "        total_loss: 837.884033203125\n",
      "        vf_explained_var: 0.6079728007316589\n",
      "        vf_loss: 837.958251953125\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.55773480662984\n",
      "    ram_util_percent: 38.398618784530385\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06872973339804499\n",
      "    mean_env_wait_ms: 117.66380970529775\n",
      "    mean_inference_ms: 1.7296831400305888\n",
      "    mean_raw_obs_processing_ms: 9.18345595904487\n",
      "  time_since_restore: 96023.5130982399\n",
      "  time_this_iter_s: 507.4319689273834\n",
      "  time_total_s: 96023.5130982399\n",
      "  timers:\n",
      "    learn_throughput: 260.725\n",
      "    learn_time_ms: 15341.832\n",
      "    load_throughput: 10326.081\n",
      "    load_time_ms: 387.369\n",
      "    sample_throughput: 8.229\n",
      "    sample_time_ms: 486088.784\n",
      "    update_time_ms: 3.581\n",
      "  timestamp: 1613922592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_16-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 139.98\n",
      "  episode_reward_max: 118.39349617717478\n",
      "  episode_reward_mean: 89.1879758260181\n",
      "  episode_reward_min: -100.51393821107163\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5515\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4440160095691681\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007366834674030542\n",
      "        model: {}\n",
      "        policy_loss: -0.052306752651929855\n",
      "        total_loss: 269.7047424316406\n",
      "        vf_explained_var: 0.7046312689781189\n",
      "        vf_loss: 269.7444152832031\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.6071129707113\n",
      "    ram_util_percent: 38.43179916317991\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06874115014425634\n",
      "    mean_env_wait_ms: 117.62692021537387\n",
      "    mean_inference_ms: 1.7298548942518515\n",
      "    mean_raw_obs_processing_ms: 9.18065413546429\n",
      "  time_since_restore: 96525.79550862312\n",
      "  time_this_iter_s: 502.2824103832245\n",
      "  time_total_s: 96525.79550862312\n",
      "  timers:\n",
      "    learn_throughput: 260.748\n",
      "    learn_time_ms: 15340.502\n",
      "    load_throughput: 10314.417\n",
      "    load_time_ms: 387.807\n",
      "    sample_throughput: 8.231\n",
      "    sample_time_ms: 485970.923\n",
      "    update_time_ms: 3.554\n",
      "  timestamp: 1613923094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-06-36\n",
      "  done: false\n",
      "  episode_len_mean: 137.64\n",
      "  episode_reward_max: 118.39542405339023\n",
      "  episode_reward_mean: 93.34721071568654\n",
      "  episode_reward_min: -105.70203195137316\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5543\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4430549442768097\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011044507846236229\n",
      "        model: {}\n",
      "        policy_loss: -0.07171367853879929\n",
      "        total_loss: 26.278682708740234\n",
      "        vf_explained_var: 0.9783812165260315\n",
      "        vf_loss: 26.331523895263672\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.46541143654114\n",
      "    ram_util_percent: 38.35523012552301\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06875123018139143\n",
      "    mean_env_wait_ms: 117.59248582940604\n",
      "    mean_inference_ms: 1.7299942823507348\n",
      "    mean_raw_obs_processing_ms: 9.179147292354722\n",
      "  time_since_restore: 97027.63077235222\n",
      "  time_this_iter_s: 501.83526372909546\n",
      "  time_total_s: 97027.63077235222\n",
      "  timers:\n",
      "    learn_throughput: 260.734\n",
      "    learn_time_ms: 15341.324\n",
      "    load_throughput: 10289.052\n",
      "    load_time_ms: 388.763\n",
      "    sample_throughput: 8.231\n",
      "    sample_time_ms: 485951.319\n",
      "    update_time_ms: 3.499\n",
      "  timestamp: 1613923596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-15-03\n",
      "  done: false\n",
      "  episode_len_mean: 133.16\n",
      "  episode_reward_max: 118.39542405339023\n",
      "  episode_reward_mean: 89.21097575811257\n",
      "  episode_reward_min: -107.31346635462283\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5575\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47607260942459106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011482289992272854\n",
      "        model: {}\n",
      "        policy_loss: -0.09403268992900848\n",
      "        total_loss: 685.237060546875\n",
      "        vf_explained_var: 0.6918187737464905\n",
      "        vf_loss: 685.3115844726562\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.605394190871365\n",
      "    ram_util_percent: 38.36528354080221\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06876275525910218\n",
      "    mean_env_wait_ms: 117.55433045998768\n",
      "    mean_inference_ms: 1.7301557845493607\n",
      "    mean_raw_obs_processing_ms: 9.178508671720747\n",
      "  time_since_restore: 97534.4449365139\n",
      "  time_this_iter_s: 506.81416416168213\n",
      "  time_total_s: 97534.4449365139\n",
      "  timers:\n",
      "    learn_throughput: 260.735\n",
      "    learn_time_ms: 15341.27\n",
      "    load_throughput: 10320.858\n",
      "    load_time_ms: 387.565\n",
      "    sample_throughput: 8.218\n",
      "    sample_time_ms: 486753.428\n",
      "    update_time_ms: 3.514\n",
      "  timestamp: 1613924103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-23-25\n",
      "  done: false\n",
      "  episode_len_mean: 137.71\n",
      "  episode_reward_max: 118.39576053671243\n",
      "  episode_reward_mean: 97.43855135668205\n",
      "  episode_reward_min: -107.31346635462283\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5602\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44221940636634827\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007441571447998285\n",
      "        model: {}\n",
      "        policy_loss: -0.06500907987356186\n",
      "        total_loss: 71.3495864868164\n",
      "        vf_explained_var: 0.920139729976654\n",
      "        vf_loss: 71.40189361572266\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.482960893854745\n",
      "    ram_util_percent: 38.3731843575419\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06877308927909531\n",
      "    mean_env_wait_ms: 117.52232623149295\n",
      "    mean_inference_ms: 1.730301866684336\n",
      "    mean_raw_obs_processing_ms: 9.176635585619612\n",
      "  time_since_restore: 98036.24931836128\n",
      "  time_this_iter_s: 501.8043818473816\n",
      "  time_total_s: 98036.24931836128\n",
      "  timers:\n",
      "    learn_throughput: 260.707\n",
      "    learn_time_ms: 15342.9\n",
      "    load_throughput: 10445.603\n",
      "    load_time_ms: 382.936\n",
      "    sample_throughput: 8.22\n",
      "    sample_time_ms: 486595.992\n",
      "    update_time_ms: 3.48\n",
      "  timestamp: 1613924605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 135.42\n",
      "  episode_reward_max: 118.39708028345218\n",
      "  episode_reward_mean: 86.6814424293701\n",
      "  episode_reward_min: -108.20790027106892\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5631\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4541889429092407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010883782058954239\n",
      "        model: {}\n",
      "        policy_loss: -0.08654586225748062\n",
      "        total_loss: 782.625244140625\n",
      "        vf_explained_var: 0.5522391200065613\n",
      "        vf_loss: 782.6931762695312\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.54303621169915\n",
      "    ram_util_percent: 38.38969359331476\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06878404687145184\n",
      "    mean_env_wait_ms: 117.48776817358333\n",
      "    mean_inference_ms: 1.7304641341571485\n",
      "    mean_raw_obs_processing_ms: 9.17493478224007\n",
      "  time_since_restore: 98538.78360366821\n",
      "  time_this_iter_s: 502.53428530693054\n",
      "  time_total_s: 98538.78360366821\n",
      "  timers:\n",
      "    learn_throughput: 260.682\n",
      "    learn_time_ms: 15344.358\n",
      "    load_throughput: 10556.637\n",
      "    load_time_ms: 378.909\n",
      "    sample_throughput: 8.227\n",
      "    sample_time_ms: 486232.038\n",
      "    update_time_ms: 3.471\n",
      "  timestamp: 1613925108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 142.42\n",
      "  episode_reward_max: 118.39708028345218\n",
      "  episode_reward_mean: 95.31215734591702\n",
      "  episode_reward_min: -108.20790027106892\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 5657\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.457278311252594\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0064457799308001995\n",
      "        model: {}\n",
      "        policy_loss: -0.06314153224229813\n",
      "        total_loss: 129.87037658691406\n",
      "        vf_explained_var: 0.8559269309043884\n",
      "        vf_loss: 129.92247009277344\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.47458100558659\n",
      "    ram_util_percent: 38.437569832402225\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06879358209132469\n",
      "    mean_env_wait_ms: 117.45771841085211\n",
      "    mean_inference_ms: 1.7306100761494239\n",
      "    mean_raw_obs_processing_ms: 9.171618487810383\n",
      "  time_since_restore: 99040.21338009834\n",
      "  time_this_iter_s: 501.42977643013\n",
      "  time_total_s: 99040.21338009834\n",
      "  timers:\n",
      "    learn_throughput: 260.653\n",
      "    learn_time_ms: 15346.056\n",
      "    load_throughput: 10625.552\n",
      "    load_time_ms: 376.451\n",
      "    sample_throughput: 8.22\n",
      "    sample_time_ms: 486590.007\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1613925610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 141.52\n",
      "  episode_reward_max: 118.39708028345218\n",
      "  episode_reward_mean: 91.08862888743343\n",
      "  episode_reward_min: -108.20790027106892\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5687\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44903984665870667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008460896089673042\n",
      "        model: {}\n",
      "        policy_loss: -0.07948212325572968\n",
      "        total_loss: 316.0011291503906\n",
      "        vf_explained_var: 0.800960123538971\n",
      "        vf_loss: 316.066162109375\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.88151724137932\n",
      "    ram_util_percent: 38.45862068965517\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06880546283073154\n",
      "    mean_env_wait_ms: 117.42320581612239\n",
      "    mean_inference_ms: 1.730811979971199\n",
      "    mean_raw_obs_processing_ms: 9.167845253267249\n",
      "  time_since_restore: 99547.9966533184\n",
      "  time_this_iter_s: 507.78327322006226\n",
      "  time_total_s: 99547.9966533184\n",
      "  timers:\n",
      "    learn_throughput: 260.555\n",
      "    learn_time_ms: 15351.827\n",
      "    load_throughput: 10536.616\n",
      "    load_time_ms: 379.629\n",
      "    sample_throughput: 8.207\n",
      "    sample_time_ms: 487376.937\n",
      "    update_time_ms: 3.52\n",
      "  timestamp: 1613926118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_17-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 140.18\n",
      "  episode_reward_max: 118.38243891327723\n",
      "  episode_reward_mean: 87.11892782788351\n",
      "  episode_reward_min: -104.86896589473784\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5717\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4141284227371216\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00811158586293459\n",
      "        model: {}\n",
      "        policy_loss: -0.07657773792743683\n",
      "        total_loss: 425.97039794921875\n",
      "        vf_explained_var: 0.7353231310844421\n",
      "        vf_loss: 426.0330810546875\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.6038942976356\n",
      "    ram_util_percent: 38.409318497913766\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688166799210907\n",
      "    mean_env_wait_ms: 117.38973558548936\n",
      "    mean_inference_ms: 1.731003723660797\n",
      "    mean_raw_obs_processing_ms: 9.165262739863397\n",
      "  time_since_restore: 100051.91476297379\n",
      "  time_this_iter_s: 503.91810965538025\n",
      "  time_total_s: 100051.91476297379\n",
      "  timers:\n",
      "    learn_throughput: 260.44\n",
      "    learn_time_ms: 15358.647\n",
      "    load_throughput: 10481.852\n",
      "    load_time_ms: 381.612\n",
      "    sample_throughput: 8.201\n",
      "    sample_time_ms: 487723.613\n",
      "    update_time_ms: 3.524\n",
      "  timestamp: 1613926622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-05-24\n",
      "  done: false\n",
      "  episode_len_mean: 139.42\n",
      "  episode_reward_max: 118.38243891327723\n",
      "  episode_reward_mean: 87.3092010228227\n",
      "  episode_reward_min: -99.1539639651303\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 5743\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4504252076148987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008290223777294159\n",
      "        model: {}\n",
      "        policy_loss: -0.06829310208559036\n",
      "        total_loss: 572.34716796875\n",
      "        vf_explained_var: 0.5892797112464905\n",
      "        vf_loss: 572.4014282226562\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.458379888268155\n",
      "    ram_util_percent: 38.467737430167595\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688263892342844\n",
      "    mean_env_wait_ms: 117.36105588966177\n",
      "    mean_inference_ms: 1.7311708017025933\n",
      "    mean_raw_obs_processing_ms: 9.16247071305064\n",
      "  time_since_restore: 100553.34527301788\n",
      "  time_this_iter_s: 501.4305100440979\n",
      "  time_total_s: 100553.34527301788\n",
      "  timers:\n",
      "    learn_throughput: 260.435\n",
      "    learn_time_ms: 15358.894\n",
      "    load_throughput: 10440.625\n",
      "    load_time_ms: 383.119\n",
      "    sample_throughput: 8.2\n",
      "    sample_time_ms: 487784.813\n",
      "    update_time_ms: 3.526\n",
      "  timestamp: 1613927124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 143.17\n",
      "  episode_reward_max: 118.37372136028051\n",
      "  episode_reward_mean: 91.30206714487505\n",
      "  episode_reward_min: -99.1539639651303\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5770\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42250144481658936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006526424083858728\n",
      "        model: {}\n",
      "        policy_loss: -0.05536552518606186\n",
      "        total_loss: 193.9767303466797\n",
      "        vf_explained_var: 0.8199180960655212\n",
      "        vf_loss: 194.0209197998047\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48100558659217\n",
      "    ram_util_percent: 38.48701117318436\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06883584798012388\n",
      "    mean_env_wait_ms: 117.3310553603252\n",
      "    mean_inference_ms: 1.7313293135656977\n",
      "    mean_raw_obs_processing_ms: 9.159199007370155\n",
      "  time_since_restore: 101054.5346596241\n",
      "  time_this_iter_s: 501.18938660621643\n",
      "  time_total_s: 101054.5346596241\n",
      "  timers:\n",
      "    learn_throughput: 260.427\n",
      "    learn_time_ms: 15359.376\n",
      "    load_throughput: 10534.029\n",
      "    load_time_ms: 379.722\n",
      "    sample_throughput: 8.211\n",
      "    sample_time_ms: 487165.523\n",
      "    update_time_ms: 3.509\n",
      "  timestamp: 1613927625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-22-13\n",
      "  done: false\n",
      "  episode_len_mean: 138.97\n",
      "  episode_reward_max: 118.33499069266497\n",
      "  episode_reward_mean: 84.75324872804698\n",
      "  episode_reward_min: -106.0289334979842\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5802\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.48007702827453613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012584340758621693\n",
      "        model: {}\n",
      "        policy_loss: -0.09932564198970795\n",
      "        total_loss: 728.4404907226562\n",
      "        vf_explained_var: 0.6476417183876038\n",
      "        vf_loss: 728.5183715820312\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.90993103448277\n",
      "    ram_util_percent: 38.470068965517235\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06884751022889775\n",
      "    mean_env_wait_ms: 117.2953505584617\n",
      "    mean_inference_ms: 1.7315061560061569\n",
      "    mean_raw_obs_processing_ms: 9.156271346885227\n",
      "  time_since_restore: 101562.37695145607\n",
      "  time_this_iter_s: 507.8422918319702\n",
      "  time_total_s: 101562.37695145607\n",
      "  timers:\n",
      "    learn_throughput: 259.223\n",
      "    learn_time_ms: 15430.755\n",
      "    load_throughput: 10585.392\n",
      "    load_time_ms: 377.879\n",
      "    sample_throughput: 8.203\n",
      "    sample_time_ms: 487652.062\n",
      "    update_time_ms: 3.492\n",
      "  timestamp: 1613928133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-30-36\n",
      "  done: false\n",
      "  episode_len_mean: 140.78\n",
      "  episode_reward_max: 118.32947700638533\n",
      "  episode_reward_mean: 88.82997558811356\n",
      "  episode_reward_min: -106.0289334979842\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5830\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.47835829854011536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010771854780614376\n",
      "        model: {}\n",
      "        policy_loss: -0.08759791404008865\n",
      "        total_loss: 685.3577880859375\n",
      "        vf_explained_var: 0.5802308320999146\n",
      "        vf_loss: 685.4270629882812\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.510055865921785\n",
      "    ram_util_percent: 38.51578212290503\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06885747574321907\n",
      "    mean_env_wait_ms: 117.26444235467844\n",
      "    mean_inference_ms: 1.7316449178343003\n",
      "    mean_raw_obs_processing_ms: 9.153492962958348\n",
      "  time_since_restore: 102064.4938955307\n",
      "  time_this_iter_s: 502.11694407463074\n",
      "  time_total_s: 102064.4938955307\n",
      "  timers:\n",
      "    learn_throughput: 259.258\n",
      "    learn_time_ms: 15428.676\n",
      "    load_throughput: 10581.754\n",
      "    load_time_ms: 378.009\n",
      "    sample_throughput: 8.202\n",
      "    sample_time_ms: 487683.671\n",
      "    update_time_ms: 3.476\n",
      "  timestamp: 1613928636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 139.6\n",
      "  episode_reward_max: 118.32947700638533\n",
      "  episode_reward_mean: 86.55391506173646\n",
      "  episode_reward_min: -106.0289334979842\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5857\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.46270233392715454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00774323008954525\n",
      "        model: {}\n",
      "        policy_loss: -0.07308743894100189\n",
      "        total_loss: 156.12130737304688\n",
      "        vf_explained_var: 0.8850777745246887\n",
      "        vf_loss: 156.18115234375\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.477902097902096\n",
      "    ram_util_percent: 38.45314685314685\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688671396590762\n",
      "    mean_env_wait_ms: 117.23489272819705\n",
      "    mean_inference_ms: 1.7317737628743544\n",
      "    mean_raw_obs_processing_ms: 9.151175244274633\n",
      "  time_since_restore: 102564.99697709084\n",
      "  time_this_iter_s: 500.5030815601349\n",
      "  time_total_s: 102564.99697709084\n",
      "  timers:\n",
      "    learn_throughput: 259.315\n",
      "    learn_time_ms: 15425.279\n",
      "    load_throughput: 10581.729\n",
      "    load_time_ms: 378.01\n",
      "    sample_throughput: 8.213\n",
      "    sample_time_ms: 487056.506\n",
      "    update_time_ms: 3.485\n",
      "  timestamp: 1613929136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-47-18\n",
      "  done: false\n",
      "  episode_len_mean: 140.78\n",
      "  episode_reward_max: 118.39086243496018\n",
      "  episode_reward_mean: 97.17839582171707\n",
      "  episode_reward_min: -106.0289334979842\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5885\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.44672891497612\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015807699412107468\n",
      "        model: {}\n",
      "        policy_loss: -0.10444864630699158\n",
      "        total_loss: 4.103693962097168\n",
      "        vf_explained_var: 0.9922036528587341\n",
      "        vf_loss: 4.18113374710083\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50251396648045\n",
      "    ram_util_percent: 38.45167597765362\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06887643826364688\n",
      "    mean_env_wait_ms: 117.20436109499063\n",
      "    mean_inference_ms: 1.7318841697148941\n",
      "    mean_raw_obs_processing_ms: 9.14814497512818\n",
      "  time_since_restore: 103066.50950145721\n",
      "  time_this_iter_s: 501.5125243663788\n",
      "  time_total_s: 103066.50950145721\n",
      "  timers:\n",
      "    learn_throughput: 259.242\n",
      "    learn_time_ms: 15429.578\n",
      "    load_throughput: 10568.217\n",
      "    load_time_ms: 378.493\n",
      "    sample_throughput: 8.213\n",
      "    sample_time_ms: 487024.19\n",
      "    update_time_ms: 3.523\n",
      "  timestamp: 1613929638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_18-55-43\n",
      "  done: false\n",
      "  episode_len_mean: 141.24\n",
      "  episode_reward_max: 118.39086243496018\n",
      "  episode_reward_mean: 99.04113578713537\n",
      "  episode_reward_min: -106.12997736688158\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5916\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4446447491645813\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007330939639359713\n",
      "        model: {}\n",
      "        policy_loss: -0.06801272928714752\n",
      "        total_loss: 325.7757263183594\n",
      "        vf_explained_var: 0.7580990791320801\n",
      "        vf_loss: 325.8312072753906\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.5625\n",
      "    ram_util_percent: 38.440416666666664\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0688861500968792\n",
      "    mean_env_wait_ms: 117.16950231676245\n",
      "    mean_inference_ms: 1.7320091447873387\n",
      "    mean_raw_obs_processing_ms: 9.14526705178258\n",
      "  time_since_restore: 103570.86333155632\n",
      "  time_this_iter_s: 504.35383009910583\n",
      "  time_total_s: 103570.86333155632\n",
      "  timers:\n",
      "    learn_throughput: 259.316\n",
      "    learn_time_ms: 15425.214\n",
      "    load_throughput: 10555.77\n",
      "    load_time_ms: 378.94\n",
      "    sample_throughput: 8.21\n",
      "    sample_time_ms: 487211.327\n",
      "    update_time_ms: 3.531\n",
      "  timestamp: 1613930143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_19-04-06\n",
      "  done: false\n",
      "  episode_len_mean: 138.77\n",
      "  episode_reward_max: 118.39086243496018\n",
      "  episode_reward_mean: 90.86260867776606\n",
      "  episode_reward_min: -106.12997736688158\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5944\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4569801688194275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010883398354053497\n",
      "        model: {}\n",
      "        policy_loss: -0.09370122104883194\n",
      "        total_loss: 565.4652099609375\n",
      "        vf_explained_var: 0.7109858393669128\n",
      "        vf_loss: 565.5403442382812\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.52175732217573\n",
      "    ram_util_percent: 38.50195258019526\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06889490639963572\n",
      "    mean_env_wait_ms: 117.13893273738398\n",
      "    mean_inference_ms: 1.7321253873039135\n",
      "    mean_raw_obs_processing_ms: 9.142997044733525\n",
      "  time_since_restore: 104073.42679667473\n",
      "  time_this_iter_s: 502.5634651184082\n",
      "  time_total_s: 104073.42679667473\n",
      "  timers:\n",
      "    learn_throughput: 259.252\n",
      "    learn_time_ms: 15429.024\n",
      "    load_throughput: 10559.05\n",
      "    load_time_ms: 378.822\n",
      "    sample_throughput: 8.208\n",
      "    sample_time_ms: 487323.336\n",
      "    update_time_ms: 3.489\n",
      "  timestamp: 1613930646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_19-12-30\n",
      "  done: false\n",
      "  episode_len_mean: 137.05\n",
      "  episode_reward_max: 118.38534078592863\n",
      "  episode_reward_mean: 88.86870043619578\n",
      "  episode_reward_min: -106.12997736688158\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5974\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43862196803092957\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006011617369949818\n",
      "        model: {}\n",
      "        policy_loss: -0.06377123296260834\n",
      "        total_loss: 181.75660705566406\n",
      "        vf_explained_var: 0.8628853559494019\n",
      "        vf_loss: 181.81011962890625\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.44396671289876\n",
      "    ram_util_percent: 38.54105409153953\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0689038510570063\n",
      "    mean_env_wait_ms: 117.1074986980277\n",
      "    mean_inference_ms: 1.7322632502674735\n",
      "    mean_raw_obs_processing_ms: 9.141648403708048\n",
      "  time_since_restore: 104578.01215744019\n",
      "  time_this_iter_s: 504.58536076545715\n",
      "  time_total_s: 104578.01215744019\n",
      "  timers:\n",
      "    learn_throughput: 259.323\n",
      "    learn_time_ms: 15424.795\n",
      "    load_throughput: 10560.106\n",
      "    load_time_ms: 378.784\n",
      "    sample_throughput: 8.213\n",
      "    sample_time_ms: 487008.488\n",
      "    update_time_ms: 3.502\n",
      "  timestamp: 1613931150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: c4001_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_c4001_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-21_19-20-54\n",
      "  done: true\n",
      "  episode_len_mean: 135.65\n",
      "  episode_reward_max: 118.37682836900566\n",
      "  episode_reward_mean: 84.91120274687219\n",
      "  episode_reward_min: -106.00795496124721\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6003\n",
      "  experiment_id: abecbdbfd48a4155b643541c7842f7d3\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.43731579184532166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011441821232438087\n",
      "        model: {}\n",
      "        policy_loss: -0.08611249923706055\n",
      "        total_loss: 731.3579711914062\n",
      "        vf_explained_var: 0.5641735792160034\n",
      "        vf_loss: 731.4244995117188\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.49972144846797\n",
      "    ram_util_percent: 38.53398328690808\n",
      "  pid: 84297\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0689123747585987\n",
      "    mean_env_wait_ms: 117.07826797938513\n",
      "    mean_inference_ms: 1.7324010880958718\n",
      "    mean_raw_obs_processing_ms: 9.140069862547081\n",
      "  time_since_restore: 105081.37180137634\n",
      "  time_this_iter_s: 503.3596439361572\n",
      "  time_total_s: 105081.37180137634\n",
      "  timers:\n",
      "    learn_throughput: 259.4\n",
      "    learn_time_ms: 15420.206\n",
      "    load_throughput: 10596.499\n",
      "    load_time_ms: 377.483\n",
      "    sample_throughput: 8.214\n",
      "    sample_time_ms: 486957.601\n",
      "    update_time_ms: 3.509\n",
      "  timestamp: 1613931654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: c4001_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         584.968</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-93.4971</td><td style=\"text-align: right;\">             118.377</td><td style=\"text-align: right;\">            -108.067</td><td style=\"text-align: right;\">           42.7742</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1148.25</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-92.2754</td><td style=\"text-align: right;\">             114.289</td><td style=\"text-align: right;\">            -108.369</td><td style=\"text-align: right;\">             45.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          1751.5</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-83.1377</td><td style=\"text-align: right;\">             118.275</td><td style=\"text-align: right;\">             -108.49</td><td style=\"text-align: right;\">             54.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2273.68</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -76.314</td><td style=\"text-align: right;\">             118.312</td><td style=\"text-align: right;\">             -108.49</td><td style=\"text-align: right;\">             68.51</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          2795.8</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-74.5112</td><td style=\"text-align: right;\">             118.312</td><td style=\"text-align: right;\">            -107.928</td><td style=\"text-align: right;\">              80.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         3319.58</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-74.3607</td><td style=\"text-align: right;\">             118.266</td><td style=\"text-align: right;\">            -106.658</td><td style=\"text-align: right;\">             82.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         3833.39</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-63.5234</td><td style=\"text-align: right;\">             118.307</td><td style=\"text-align: right;\">            -106.506</td><td style=\"text-align: right;\">             87.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4344.97</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-59.6357</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.172</td><td style=\"text-align: right;\">             94.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         4857.63</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-61.8241</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.172</td><td style=\"text-align: right;\">             99.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         5368.27</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-57.3206</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.172</td><td style=\"text-align: right;\">            105.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         5871.95</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -44.265</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -106.664</td><td style=\"text-align: right;\">            107.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         6383.15</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-37.8044</td><td style=\"text-align: right;\">             118.378</td><td style=\"text-align: right;\">            -107.508</td><td style=\"text-align: right;\">             112.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         6886.86</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-27.5816</td><td style=\"text-align: right;\">             118.325</td><td style=\"text-align: right;\">            -107.508</td><td style=\"text-align: right;\">            117.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         7398.73</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-27.6162</td><td style=\"text-align: right;\">             118.348</td><td style=\"text-align: right;\">            -107.508</td><td style=\"text-align: right;\">            114.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         7906.86</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-20.9624</td><td style=\"text-align: right;\">             118.353</td><td style=\"text-align: right;\">            -107.144</td><td style=\"text-align: right;\">            112.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         8415.26</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-18.7572</td><td style=\"text-align: right;\">             118.353</td><td style=\"text-align: right;\">             -108.65</td><td style=\"text-align: right;\">            109.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         8918.77</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-14.2859</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">             -108.65</td><td style=\"text-align: right;\">            120.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         9420.53</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-16.7852</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">             -108.65</td><td style=\"text-align: right;\">            123.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.3/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         9924.06</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-16.6352</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -107.603</td><td style=\"text-align: right;\">            125.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         10431.4</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-19.0543</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -107.603</td><td style=\"text-align: right;\">            122.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         10935.5</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-12.3108</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -107.603</td><td style=\"text-align: right;\">             120.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         11442.2</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-5.86666</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -106.867</td><td style=\"text-align: right;\">            119.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         11944.1</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> 6.99879</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -106.702</td><td style=\"text-align: right;\">            124.01</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">           12450</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> 15.2939</td><td style=\"text-align: right;\">              118.34</td><td style=\"text-align: right;\">            -106.702</td><td style=\"text-align: right;\">            123.87</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         12954.4</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> 30.2335</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">            -105.225</td><td style=\"text-align: right;\">            126.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         13457.7</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> 23.5594</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">            -105.225</td><td style=\"text-align: right;\">            125.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         13960.1</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">  36.487</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">             -105.02</td><td style=\"text-align: right;\">            127.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 13.2/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         14459.3</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> 36.2323</td><td style=\"text-align: right;\">             118.365</td><td style=\"text-align: right;\">            -105.786</td><td style=\"text-align: right;\">            131.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         14964.1</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> 32.1002</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.252</td><td style=\"text-align: right;\">            135.09</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         15459.7</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> 40.3523</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.252</td><td style=\"text-align: right;\">            140.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         15958.5</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> 47.0233</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.252</td><td style=\"text-align: right;\">            144.89</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">           16455</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> 45.0298</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.132</td><td style=\"text-align: right;\">            143.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         16955.5</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> 42.7677</td><td style=\"text-align: right;\">             118.372</td><td style=\"text-align: right;\">            -105.078</td><td style=\"text-align: right;\">            143.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         17456.9</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> 32.1405</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.078</td><td style=\"text-align: right;\">            133.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         17961.1</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> 15.5419</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -107.439</td><td style=\"text-align: right;\">            133.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         18457.8</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> 25.9761</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -107.439</td><td style=\"text-align: right;\">             130.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         18958.8</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> 29.8441</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -107.439</td><td style=\"text-align: right;\">            133.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">           19465</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> 45.9345</td><td style=\"text-align: right;\">             118.364</td><td style=\"text-align: right;\">            -107.439</td><td style=\"text-align: right;\">             133.4</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         19965.7</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> 46.3171</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -107.074</td><td style=\"text-align: right;\">            129.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         20464.8</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">  54.302</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -107.074</td><td style=\"text-align: right;\">            131.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         20965.7</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> 52.7048</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -107.074</td><td style=\"text-align: right;\">            133.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         21466.9</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> 48.8043</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -106.321</td><td style=\"text-align: right;\">            135.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         21971.4</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\"> 40.8523</td><td style=\"text-align: right;\">             118.377</td><td style=\"text-align: right;\">            -105.628</td><td style=\"text-align: right;\">            129.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         22471.9</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> 42.8976</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -105.628</td><td style=\"text-align: right;\">            129.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         22972.5</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> 62.0653</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -105.628</td><td style=\"text-align: right;\">            130.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         23480.3</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> 59.9853</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -101.459</td><td style=\"text-align: right;\">            134.84</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         23975.7</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> 74.5288</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -106.348</td><td style=\"text-align: right;\">            144.04</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         24475.1</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> 67.9829</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -106.348</td><td style=\"text-align: right;\">            142.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         24973.8</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> 67.8799</td><td style=\"text-align: right;\">             118.379</td><td style=\"text-align: right;\">            -106.348</td><td style=\"text-align: right;\">            148.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         25473.1</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> 63.9277</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -105.116</td><td style=\"text-align: right;\">            144.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         25971.5</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> 59.3821</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -105.116</td><td style=\"text-align: right;\">            142.79</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         26472.9</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> 59.0024</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -107.145</td><td style=\"text-align: right;\">            133.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         26982.7</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\"> 59.0911</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -107.145</td><td style=\"text-align: right;\">            132.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         27511.5</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> 65.3914</td><td style=\"text-align: right;\">             118.332</td><td style=\"text-align: right;\">            -107.145</td><td style=\"text-align: right;\">            128.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         28042.3</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> 65.8051</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">            -106.744</td><td style=\"text-align: right;\">            133.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         28566.9</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">  55.311</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">            -106.744</td><td style=\"text-align: right;\">            126.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         29086.6</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> 68.1601</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -101.232</td><td style=\"text-align: right;\">            136.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         29659.2</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\"> 66.3394</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -101.232</td><td style=\"text-align: right;\">             136.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         30230.8</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> 70.2979</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -100.24</td><td style=\"text-align: right;\">            141.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         30805.7</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> 74.4389</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -99.9359</td><td style=\"text-align: right;\">            141.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         31377.5</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> 70.0174</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -105.687</td><td style=\"text-align: right;\">            141.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         31949.3</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> 71.7774</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -106.745</td><td style=\"text-align: right;\">            141.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         32523.3</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> 59.1274</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -106.745</td><td style=\"text-align: right;\">            137.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         33097.1</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">  55.334</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -106.745</td><td style=\"text-align: right;\">            136.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         33669.4</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> 53.5255</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -106.745</td><td style=\"text-align: right;\">            137.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">           34239</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> 53.7095</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -107.534</td><td style=\"text-align: right;\">            146.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         34809.1</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> 72.5743</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -107.534</td><td style=\"text-align: right;\">            153.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         35380.3</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  72.234</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -107.534</td><td style=\"text-align: right;\">            156.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         35952.7</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> 76.4175</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -100.561</td><td style=\"text-align: right;\">            153.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         36525.3</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> 72.2532</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -106.235</td><td style=\"text-align: right;\">            146.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         37100.1</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> 70.3391</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -106.235</td><td style=\"text-align: right;\">            138.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         37671.1</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> 76.6779</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -106.235</td><td style=\"text-align: right;\">            139.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         38244.2</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> 68.1292</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -106.277</td><td style=\"text-align: right;\">             143.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         38817.4</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> 63.9351</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.277</td><td style=\"text-align: right;\">            139.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         39389.2</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\"> 68.3596</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.277</td><td style=\"text-align: right;\">            140.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         39957.2</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> 70.5364</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.277</td><td style=\"text-align: right;\">            144.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         40532.2</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> 74.7772</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -98.7848</td><td style=\"text-align: right;\">             138.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         41104.8</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> 78.9809</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -98.7417</td><td style=\"text-align: right;\">            142.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         41679.5</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> 66.2304</td><td style=\"text-align: right;\">             118.388</td><td style=\"text-align: right;\">            -100.335</td><td style=\"text-align: right;\">            134.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         42258.1</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\"> 66.3086</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -100.725</td><td style=\"text-align: right;\">            129.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         42831.9</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> 60.0336</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -100.725</td><td style=\"text-align: right;\">             128.4</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         43408.9</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> 55.8433</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -100.725</td><td style=\"text-align: right;\">            126.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         43981.4</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> 64.1735</td><td style=\"text-align: right;\">             118.372</td><td style=\"text-align: right;\">            -100.725</td><td style=\"text-align: right;\">            134.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">           44554</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 74.6458</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -100.479</td><td style=\"text-align: right;\">            139.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         45127.5</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> 72.4842</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -100.479</td><td style=\"text-align: right;\">            135.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         45699.6</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> 74.4792</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -100.335</td><td style=\"text-align: right;\">            141.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">           46272</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> 70.2346</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -100.486</td><td style=\"text-align: right;\">            141.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">           46842</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> 78.5976</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -101.487</td><td style=\"text-align: right;\">            145.91</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         47414.5</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> 86.7418</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.324</td><td style=\"text-align: right;\">            151.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         47986.2</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> 95.4794</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.324</td><td style=\"text-align: right;\">            152.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         48559.3</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">  86.877</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -106.324</td><td style=\"text-align: right;\">            148.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         49132.4</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">  78.536</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">             -104.59</td><td style=\"text-align: right;\">            147.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         49702.8</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> 76.2701</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">             -104.59</td><td style=\"text-align: right;\">            148.59</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         50274.3</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> 76.0907</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">             -104.59</td><td style=\"text-align: right;\">               144</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         50844.4</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> 90.7099</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -101.663</td><td style=\"text-align: right;\">            154.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         51419.6</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> 90.7374</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -105.809</td><td style=\"text-align: right;\">            151.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         51992.5</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> 95.0085</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -105.809</td><td style=\"text-align: right;\">            148.99</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         52563.9</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> 92.9886</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -105.809</td><td style=\"text-align: right;\">            148.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         53137.5</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> 88.8485</td><td style=\"text-align: right;\">             118.339</td><td style=\"text-align: right;\">            -105.809</td><td style=\"text-align: right;\">             145.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         53711.6</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> 93.1422</td><td style=\"text-align: right;\">             118.339</td><td style=\"text-align: right;\">            -102.632</td><td style=\"text-align: right;\">            147.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         54285.6</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">  87.096</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -102.632</td><td style=\"text-align: right;\">            145.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">           54857</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> 89.2414</td><td style=\"text-align: right;\">             118.394</td><td style=\"text-align: right;\">            -102.632</td><td style=\"text-align: right;\">            147.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         55434.1</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">  80.926</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -102.632</td><td style=\"text-align: right;\">            139.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         56010.5</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> 82.9837</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -97.2012</td><td style=\"text-align: right;\">            143.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         56586.3</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> 78.5377</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.916</td><td style=\"text-align: right;\">            136.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         57158.2</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> 78.3317</td><td style=\"text-align: right;\">              118.38</td><td style=\"text-align: right;\">            -105.916</td><td style=\"text-align: right;\">            137.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         57732.9</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> 78.0711</td><td style=\"text-align: right;\">              118.38</td><td style=\"text-align: right;\">            -105.916</td><td style=\"text-align: right;\">            143.85</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         58308.1</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> 78.4255</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -103.571</td><td style=\"text-align: right;\">             140.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         58882.6</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> 76.4069</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -103.571</td><td style=\"text-align: right;\">            144.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         59457.2</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\"> 74.6297</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -105.489</td><td style=\"text-align: right;\">            141.72</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         60032.4</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> 76.6534</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -105.489</td><td style=\"text-align: right;\">            139.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         60606.6</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> 72.2738</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.489</td><td style=\"text-align: right;\">            137.41</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         61191.2</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\"> 78.4317</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.489</td><td style=\"text-align: right;\">            141.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         61762.9</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\"> 86.8246</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.032</td><td style=\"text-align: right;\">             144.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         62342.6</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> 86.8017</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -105.091</td><td style=\"text-align: right;\">            144.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         62919.9</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">  93.335</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -105.091</td><td style=\"text-align: right;\">            147.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         63491.7</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> 97.3352</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.436</td><td style=\"text-align: right;\">            151.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         64066.2</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> 91.0529</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.436</td><td style=\"text-align: right;\">            146.86</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         64637.4</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\"> 89.1573</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.436</td><td style=\"text-align: right;\">            148.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         65208.2</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\"> 87.0029</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -106.436</td><td style=\"text-align: right;\">            148.73</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         65721.2</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> 85.1433</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">             -100.99</td><td style=\"text-align: right;\">            146.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         66243.3</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\"> 85.3865</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -100.431</td><td style=\"text-align: right;\">            146.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         66775.3</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> 89.4547</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -100.431</td><td style=\"text-align: right;\">            139.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">           67295</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> 84.9043</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">             -106.07</td><td style=\"text-align: right;\">            139.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.9/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">           67801</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\"> 84.8044</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">             -106.07</td><td style=\"text-align: right;\">            134.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         68305.6</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\"> 78.0839</td><td style=\"text-align: right;\">             118.347</td><td style=\"text-align: right;\">            -106.328</td><td style=\"text-align: right;\">             138.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         68832.5</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">  82.365</td><td style=\"text-align: right;\">             118.328</td><td style=\"text-align: right;\">            -106.328</td><td style=\"text-align: right;\">            142.47</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         69358.4</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> 82.4307</td><td style=\"text-align: right;\">             118.314</td><td style=\"text-align: right;\">            -106.802</td><td style=\"text-align: right;\">            142.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         69867.9</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> 82.3166</td><td style=\"text-align: right;\">             118.349</td><td style=\"text-align: right;\">            -106.896</td><td style=\"text-align: right;\">             132.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         70371.8</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\"> 80.4074</td><td style=\"text-align: right;\">             118.349</td><td style=\"text-align: right;\">            -106.896</td><td style=\"text-align: right;\">            129.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         70872.9</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\"> 82.6994</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">            -106.896</td><td style=\"text-align: right;\">             131.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">           71374</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> 82.9684</td><td style=\"text-align: right;\">              118.38</td><td style=\"text-align: right;\">            -103.769</td><td style=\"text-align: right;\">            135.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         71877.9</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\"> 83.1785</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -105.487</td><td style=\"text-align: right;\">            133.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         72376.1</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> 85.2736</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.487</td><td style=\"text-align: right;\">            139.25</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         72879.5</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> 95.6462</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.487</td><td style=\"text-align: right;\">             139.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         73381.7</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\"> 95.6876</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.487</td><td style=\"text-align: right;\">            138.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         73882.2</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> 95.8851</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -100.175</td><td style=\"text-align: right;\">             139.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         74381.8</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> 97.8148</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.633</td><td style=\"text-align: right;\">            142.07</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         74887.1</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\"> 93.4033</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.523</td><td style=\"text-align: right;\">            139.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         75387.6</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\"> 93.4257</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.523</td><td style=\"text-align: right;\">            144.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         75906.1</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\"> 89.1157</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -106.523</td><td style=\"text-align: right;\">            141.74</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">           76410</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> 85.0535</td><td style=\"text-align: right;\">             118.376</td><td style=\"text-align: right;\">            -106.523</td><td style=\"text-align: right;\">            139.19</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         76912.8</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> 91.5854</td><td style=\"text-align: right;\">             118.379</td><td style=\"text-align: right;\">            -102.284</td><td style=\"text-align: right;\">            143.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         77414.4</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> 89.3539</td><td style=\"text-align: right;\">             118.379</td><td style=\"text-align: right;\">            -105.467</td><td style=\"text-align: right;\">            142.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">           77913</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> 99.8468</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.467</td><td style=\"text-align: right;\">            146.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         78416.4</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> 93.4931</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.467</td><td style=\"text-align: right;\">            144.87</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         78917.4</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> 95.3716</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.467</td><td style=\"text-align: right;\">            145.58</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         79418.8</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\"> 101.798</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.135</td><td style=\"text-align: right;\">            147.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         79925.6</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> 95.7064</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -104.575</td><td style=\"text-align: right;\">            145.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         80429.4</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">  93.574</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -105.593</td><td style=\"text-align: right;\">            139.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         80933.3</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\"> 89.2191</td><td style=\"text-align: right;\">             118.387</td><td style=\"text-align: right;\">            -105.593</td><td style=\"text-align: right;\">            136.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         81435.8</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\"> 91.0047</td><td style=\"text-align: right;\">             118.377</td><td style=\"text-align: right;\">            -105.593</td><td style=\"text-align: right;\">            137.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         81937.2</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\"> 91.1644</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -107.678</td><td style=\"text-align: right;\">            141.33</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         82451.8</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\"> 95.1221</td><td style=\"text-align: right;\">             118.359</td><td style=\"text-align: right;\">            -107.678</td><td style=\"text-align: right;\">            138.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         82950.2</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\"> 93.0994</td><td style=\"text-align: right;\">             118.335</td><td style=\"text-align: right;\">            -107.678</td><td style=\"text-align: right;\">            142.57</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         83451.6</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\"> 97.1734</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -107.678</td><td style=\"text-align: right;\">            144.83</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         83963.5</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\"> 86.7386</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -106.731</td><td style=\"text-align: right;\">            140.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         84469.4</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> 82.4999</td><td style=\"text-align: right;\">              118.39</td><td style=\"text-align: right;\">            -108.497</td><td style=\"text-align: right;\">            134.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         84972.2</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\"> 73.9378</td><td style=\"text-align: right;\">             118.366</td><td style=\"text-align: right;\">            -108.497</td><td style=\"text-align: right;\">            132.61</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">           85475</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\"> 82.4884</td><td style=\"text-align: right;\">             118.344</td><td style=\"text-align: right;\">            -108.497</td><td style=\"text-align: right;\">            137.36</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         85978.7</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\"> 82.4412</td><td style=\"text-align: right;\">             118.346</td><td style=\"text-align: right;\">            -108.497</td><td style=\"text-align: right;\">             138.9</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         86480.1</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">  93.177</td><td style=\"text-align: right;\">             118.346</td><td style=\"text-align: right;\">            -105.149</td><td style=\"text-align: right;\">            147.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         86982.8</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\"> 90.8336</td><td style=\"text-align: right;\">             118.346</td><td style=\"text-align: right;\">            -106.965</td><td style=\"text-align: right;\">            146.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         87486.7</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\"> 92.8701</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -106.965</td><td style=\"text-align: right;\">             145.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         87988.1</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\"> 92.7061</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -107.033</td><td style=\"text-align: right;\">            145.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         88489.4</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">  94.768</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -107.033</td><td style=\"text-align: right;\">            146.88</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         88989.3</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\"> 101.208</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">            -107.033</td><td style=\"text-align: right;\">            147.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         89495.5</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\"> 90.6324</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -107.033</td><td style=\"text-align: right;\">            141.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         89997.6</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\"> 94.9958</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.978</td><td style=\"text-align: right;\">            140.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">           90499</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\"> 95.4036</td><td style=\"text-align: right;\">             118.383</td><td style=\"text-align: right;\">            -106.978</td><td style=\"text-align: right;\">            141.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         91003.4</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\"> 99.7913</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">             -106.63</td><td style=\"text-align: right;\">            141.21</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         91506.9</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\"> 95.5937</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">             -105.05</td><td style=\"text-align: right;\">            142.26</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         92008.9</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\"> 91.3859</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">             -105.05</td><td style=\"text-align: right;\">            140.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         92507.7</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\"> 95.3946</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">             -105.05</td><td style=\"text-align: right;\">            146.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         93011.1</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">   88.93</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -107.307</td><td style=\"text-align: right;\">            141.08</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         93517.2</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\"> 80.6405</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -107.307</td><td style=\"text-align: right;\">            140.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         94015.1</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> 78.3588</td><td style=\"text-align: right;\">             118.375</td><td style=\"text-align: right;\">            -107.307</td><td style=\"text-align: right;\">            141.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         94514.9</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\"> 84.7146</td><td style=\"text-align: right;\">             118.343</td><td style=\"text-align: right;\">             -104.55</td><td style=\"text-align: right;\">            145.63</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         95015.3</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\"> 93.0332</td><td style=\"text-align: right;\">             118.318</td><td style=\"text-align: right;\">             -104.55</td><td style=\"text-align: right;\">            151.13</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         95516.1</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\"> 99.2755</td><td style=\"text-align: right;\">             118.354</td><td style=\"text-align: right;\">            -99.1245</td><td style=\"text-align: right;\">             154.2</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         96023.5</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\">   86.87</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -100.514</td><td style=\"text-align: right;\">            144.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         96525.8</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">  89.188</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -100.514</td><td style=\"text-align: right;\">            139.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         97027.6</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\"> 93.3472</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -105.702</td><td style=\"text-align: right;\">            137.64</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         97534.4</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\">  89.211</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">            -107.313</td><td style=\"text-align: right;\">            133.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         98036.2</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\"> 97.4386</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -107.313</td><td style=\"text-align: right;\">            137.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         98538.8</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\"> 86.6814</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -108.208</td><td style=\"text-align: right;\">            135.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         99040.2</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\"> 95.3122</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -108.208</td><td style=\"text-align: right;\">            142.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">           99548</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\"> 91.0886</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -108.208</td><td style=\"text-align: right;\">            141.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">          100052</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\"> 87.1189</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">            -104.869</td><td style=\"text-align: right;\">            140.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">          100553</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\"> 87.3092</td><td style=\"text-align: right;\">             118.382</td><td style=\"text-align: right;\">             -99.154</td><td style=\"text-align: right;\">            139.42</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">          101055</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\"> 91.3021</td><td style=\"text-align: right;\">             118.374</td><td style=\"text-align: right;\">             -99.154</td><td style=\"text-align: right;\">            143.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">          101562</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\"> 84.7532</td><td style=\"text-align: right;\">             118.335</td><td style=\"text-align: right;\">            -106.029</td><td style=\"text-align: right;\">            138.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">          102064</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\">   88.83</td><td style=\"text-align: right;\">             118.329</td><td style=\"text-align: right;\">            -106.029</td><td style=\"text-align: right;\">            140.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">          102565</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\"> 86.5539</td><td style=\"text-align: right;\">             118.329</td><td style=\"text-align: right;\">            -106.029</td><td style=\"text-align: right;\">             139.6</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">          103067</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\"> 97.1784</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -106.029</td><td style=\"text-align: right;\">            140.78</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">          103571</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\"> 99.0411</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -106.13</td><td style=\"text-align: right;\">            141.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">          104073</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\"> 90.8626</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">             -106.13</td><td style=\"text-align: right;\">            138.77</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.1/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">          104578</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\"> 88.8687</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">             -106.13</td><td style=\"text-align: right;\">            137.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>RUNNING </td><td>192.168.178.60:84297</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">          105081</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 84.9112</td><td style=\"text-align: right;\">             118.377</td><td style=\"text-align: right;\">            -106.008</td><td style=\"text-align: right;\">            135.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.0/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/13.67 GiB heap, 0.0/4.69 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-20_14-08-46<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_c4001_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">          105081</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\"> 84.9112</td><td style=\"text-align: right;\">             118.377</td><td style=\"text-align: right;\">            -106.008</td><td style=\"text-align: right;\">            135.65</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/'\n",
    "                                              'PPO_2021-02-09_17-09-27/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_2f1df_00000_0_2021-02-09_17-09-27/' \\\n",
    "                  'checkpoint_201/checkpoint-201')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restore Agent for Testing:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m [ERROR] [1613935483.013750, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m [WARN] [1613935483.017946, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m [WARN] [1613935483.019186, 0.000000]: END Init ControllersConnection\n",
      "2021-02-21 20:24:49,121\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-21 20:24:49,247\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_164/checkpoint-164\n",
      "2021-02-21 20:24:49,248\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 164, '_timesteps_total': None, '_time_total': 82471.74639606476, '_episodes_total': 5007}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=84294)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "checkpoint_nr = 164\n",
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-02-17_15-27-03/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/' \\\n",
    "                  'checkpoint_{}/checkpoint-{}'.format(checkpoint_nr, checkpoint_nr)\n",
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 07:06:39,791\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-02-22 07:06:39,791\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m [ERROR] [1613974003.104694, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m [WARN] [1613974003.108364, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m [WARN] [1613974003.109294, 0.000000]: END Init ControllersConnection\n",
      "2021-02-22 07:06:56,780\tINFO trainable.py:99 -- Trainable.setup took 16.990 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-02-22 07:06:56,781\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-22 07:06:56,974\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-20_14-08-46/PPO_ScoutingDiscreteTask_c4001_00000_0_2021-02-20_14-08-46/checkpoint_199/checkpoint-199\n",
      "2021-02-22 07:06:56,974\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 199, '_timesteps_total': None, '_time_total': 105081.37180137634, '_episodes_total': 6003}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=5772)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "checkpoint_nr = 199\n",
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-02-20_14-08-46/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_c4001_00000_0_2021-02-20_14-08-46/' \\\n",
    "                  'checkpoint_{}/checkpoint-{}'.format(checkpoint_nr, checkpoint_nr)\n",
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import time\n",
    "time_now = time.time()\n",
    "while True:\n",
    "    episode_reward = test_traj(agent=agent, env=env)\n",
    "    if time.time() - time_now > 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m [ERROR] [1613813592.288870, 7.769000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m [WARN] [1613813592.293136, 0.003000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m [WARN] [1613813592.294096, 0.004000]: END Init ControllersConnection\n",
      "2021-02-20 10:33:17,471\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 10:33:17,599\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_1/checkpoint-1\n",
      "2021-02-20 10:33:17,600\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 580.1686522960663, '_episodes_total': 92}\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m [ERROR] [1613813777.215489, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m [WARN] [1613813777.219510, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m [WARN] [1613813777.220576, 0.000000]: END Init ControllersConnection\n",
      "2021-02-20 10:36:22,283\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 10:36:22,402\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_5/checkpoint-5\n",
      "2021-02-20 10:36:22,403\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 2726.3251144886017, '_episodes_total': 357}\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m [ERROR] [1613814064.067110, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m [WARN] [1613814064.071286, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m [WARN] [1613814064.072197, 0.000000]: END Init ControllersConnection\n",
      "2021-02-20 10:41:09,128\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 10:41:09,292\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_10/checkpoint-10\n",
      "2021-02-20 10:41:09,293\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 5285.743098020554, '_episodes_total': 563}\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m [ERROR] [1613814308.574563, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m [WARN] [1613814308.578902, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m [WARN] [1613814308.580267, 0.000000]: END Init ControllersConnection\n",
      "2021-02-20 10:45:13,949\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 10:45:14,096\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_40/checkpoint-40\n",
      "2021-02-20 10:45:14,097\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 40, '_timesteps_total': None, '_time_total': 20344.65621137619, '_episodes_total': 1464}\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m [ERROR] [1613814934.093637, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m [WARN] [1613814934.098878, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m [WARN] [1613814934.100265, 0.000000]: END Init ControllersConnection\n",
      "2021-02-20 10:55:39,156\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 10:55:39,307\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_80/checkpoint-80\n",
      "2021-02-20 10:55:39,308\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': None, '_time_total': 40383.41257071495, '_episodes_total': 2641}\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m [ERROR] [1613815425.909643, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m [WARN] [1613815425.913585, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m [WARN] [1613815425.914732, 0.000000]: END Init ControllersConnection\n",
      "2021-02-20 11:03:50,962\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-20 11:03:51,080\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-17_15-27-03/PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/checkpoint_164/checkpoint-164\n",
      "2021-02-20 11:03:51,081\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 164, '_timesteps_total': None, '_time_total': 82471.74639606476, '_episodes_total': 5007}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=16825)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=16826)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=16824)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=16827)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=16821)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=16823)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "trajectories = {}\n",
    "success_rate = {}\n",
    "runs = 15\n",
    "checkpoints = [1, 5, 10, 40, 80, 164]\n",
    "#checkpoints = [1, 5]\n",
    "for checkpoint in checkpoints:\n",
    "#for checkpoint in [1, 10]:\n",
    "    checkpoint_nr = checkpoint\n",
    "    checkpoint_path = '/home/dschori/ray_results/PPO_2021-02-17_15-27-03/' \\\n",
    "                      'PPO_ScoutingDiscreteTask_34787_00000_0_2021-02-17_15-27-03/' \\\n",
    "                      'checkpoint_{}/checkpoint-{}'.format(checkpoint_nr, checkpoint_nr)\n",
    "    agent = load(checkpoint_path=checkpoint_path, config=config)\n",
    "    trajectories['checkpoint_traj_{}'.format(checkpoint)] = {}\n",
    "    success_rate['checkpoint_success_{}'.format(checkpoint)] = {}\n",
    "    for i in range(runs):\n",
    "        episode_reward, positions = test_traj(agent=agent, env=env)\n",
    "        trajectories['checkpoint_traj_{}'.format(checkpoint)]['run{}'.format(i)] = positions\n",
    "        if episode_reward > 0:\n",
    "           success_rate['checkpoint_success_{}'.format(checkpoint)]['run{}'.format(i)] = True\n",
    "        else:\n",
    "           success_rate['checkpoint_success_{}'.format(checkpoint)]['run{}'.format(i)] = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, Success Rate:       0.00%\n",
      "Episode: 152, Success Rate:       0.00%\n",
      "Episode: 305, Success Rate:       0.00%\n",
      "Episode: 1221, Success Rate:      13.33%\n",
      "Episode: 2442, Success Rate:      33.33%\n",
      "Episode: 5007, Success Rate:      86.67%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_theme(style=\"white\")\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "img = plt.imread(\"background_env3.png\")\n",
    "ax.imshow(img, extent=[-6.1, 8.6, -7.3, 3.3])\n",
    "\n",
    "ax.set_xlabel('X- Coordinates (m)', fontsize=16)\n",
    "ax.set_ylabel('Y- Coordinates (m)', fontsize=16)\n",
    "\n",
    "ax.set_xlim([-6.1, 8.6])\n",
    "ax.set_ylim([-7.3, 3.3])\n",
    "\n",
    "def get_cmap(n, name='tab20'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct\n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "cmap = get_cmap(150)\n",
    "\n",
    "thickness = 1.0\n",
    "for i, (checkpoint_key, checkpoint_value) in enumerate(trajectories.items()):\n",
    "    line = None\n",
    "    for run_key, run_value in checkpoint_value.items():\n",
    "        x_s = [e[0] for e in run_value]\n",
    "        y_s = [e[1] for e in run_value]\n",
    "        starting_point = (x_s[0], y_s[0])\n",
    "        ending_point = (x_s[-1], y_s[-1])\n",
    "        ax.plot(*starting_point, marker='o', markersize=12, c=cmap(i * 10), alpha=0.2)\n",
    "        ax.plot(*ending_point, marker='D', markersize=12, c=cmap(i * 10), alpha=0.2)\n",
    "        line, = ax.plot(x_s, y_s, linestyle='dashed', linewidth=thickness, c=cmap(i * 10))\n",
    "    line.set_label('Episodes played: {}'.format(int(checkpoints[i]*30.5305)))\n",
    "    thickness += 0.3\n",
    "\n",
    "for i, (checkpoint_key, checkpoint_value) in enumerate(success_rate.items()):\n",
    "    results = 0.0\n",
    "    for _, success in checkpoint_value.items():\n",
    "        results += int(success)\n",
    "    print('Episode: {}, Success Rate: {:10.2f}%'.format(int(checkpoints[i]*30.5305), (results/15.)*100.))\n",
    "\n",
    "ax.legend()\n",
    "plt.savefig('training_traj.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = env.observation_space.sample()\n",
    "action = agent.compute_action(o)\n",
    "action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 84, 84, 4)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_1 (Conv2D)           (None, 42, 42, 16)   592         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 42, 42, 16)   592         observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_2 (Conv2D)           (None, 21, 21, 32)   4640        conv_value_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 21, 21, 32)   4640        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_3 (Conv2D)           (None, 11, 11, 64)   18496       conv_value_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 11, 11, 64)   18496       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_4 (Conv2D)           (None, 1, 1, 128)    991360      conv_value_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 1, 1, 128)    991360      conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_value_out (Conv2D)         (None, 1, 1, 1)      129         conv_value_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_out (Conv2D)               (None, 1, 1, 3)      387         conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           conv_value_out[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,030,692\n",
      "Trainable params: 2,030,692\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model.base_model\n",
    "obs = env.reset()\n",
    "for i in range(4):\n",
    "    obs, _, _, _ = env.step(1)\n",
    "preprocessed = agent.workers.local_worker().preprocessors[\n",
    "            \"default_policy\"].transform(obs)\n",
    "filtered_obs = agent.workers.local_worker().filters[\"default_policy\"](\n",
    "    preprocessed, update=False)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['default_policy/conv_value_1/kernel', 'default_policy/conv_value_1/bias', 'default_policy/conv1/kernel', 'default_policy/conv1/bias', 'default_policy/conv_value_2/kernel', 'default_policy/conv_value_2/bias', 'default_policy/conv2/kernel', 'default_policy/conv2/bias', 'default_policy/conv_value_3/kernel', 'default_policy/conv_value_3/bias', 'default_policy/conv3/kernel', 'default_policy/conv3/bias', 'default_policy/conv_value_4/kernel', 'default_policy/conv_value_4/bias', 'default_policy/conv4/kernel', 'default_policy/conv4/bias', 'default_policy/conv_value_out/kernel', 'default_policy/conv_value_out/bias', 'default_policy/conv_out/kernel', 'default_policy/conv_out/bias'])\n"
     ]
    }
   ],
   "source": [
    "weights = policy.get_weights()\n",
    "print(weights.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f0d2060bd30>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(weights['default_policy/conv1/kernel'][:, :, 0:3, 2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 84, 84, 4)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 42, 42, 16)        592       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 42, 42, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 21, 21, 32)        4640      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 128)         991360    \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv_out (Conv2D)            (None, 1, 1, 3)           387       \n",
      "=================================================================\n",
      "Total params: 1,015,475\n",
      "Trainable params: 1,015,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(84, 84, 4,))\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same', name='conv1')(inputs)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', name='conv2')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same', name='conv3')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(128, (11, 11), strides=(1, 1), padding='valid', name='conv4')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(3, (1, 1), strides=(1, 1), padding='valid', name='conv_out')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='mse')\n",
    "model.summary()\n",
    "\n",
    "model.layers[1].set_weights([weights['default_policy/conv1/kernel'], weights['default_policy/conv1/bias']])\n",
    "model.layers[3].set_weights([weights['default_policy/conv2/kernel'], weights['default_policy/conv2/bias']])\n",
    "model.layers[5].set_weights([weights['default_policy/conv3/kernel'], weights['default_policy/conv3/bias']])\n",
    "model.layers[7].set_weights([weights['default_policy/conv4/kernel'], weights['default_policy/conv4/bias']])\n",
    "model.layers[9].set_weights([weights['default_policy/conv_out/kernel'], weights['default_policy/conv_out/bias']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 84, 84, 4)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 21, 21, 16)        4112      \n",
      "_________________________________________________________________\n",
      "re_lu_66 (ReLU)              (None, 21, 21, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 11, 11, 32)        8224      \n",
      "_________________________________________________________________\n",
      "re_lu_67 (ReLU)              (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 1, 1, 256)         991488    \n",
      "_________________________________________________________________\n",
      "re_lu_68 (ReLU)              (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_out (Conv2D)            (None, 1, 1, 3)           771       \n",
      "=================================================================\n",
      "Total params: 1,004,595\n",
      "Trainable params: 1,004,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(84, 84, 4,))\n",
    "x = tf.keras.layers.Conv2D(16, (8, 8), strides=(4, 4), padding='same', name='conv1')(inputs)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same', name='conv2')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(256, (11, 11), strides=(1, 1), padding='valid', name='conv3')(x)\n",
    "x = tf.keras.layers.ReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(3, (1, 1), strides=(1, 1), padding='valid', name='conv_out')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='mse')\n",
    "model.summary()\n",
    "\n",
    "model.layers[1].set_weights([weights['default_policy/conv1/kernel'], weights['default_policy/conv1/bias']])\n",
    "model.layers[3].set_weights([weights['default_policy/conv2/kernel'], weights['default_policy/conv2/bias']])\n",
    "model.layers[5].set_weights([weights['default_policy/conv3/kernel'], weights['default_policy/conv3/bias']])\n",
    "model.layers[7].set_weights([weights['default_policy/conv_out/kernel'], weights['default_policy/conv_out/bias']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_policy/conv_value_1/kernel\n",
      "(8, 8, 4, 16)\n",
      "----------\n",
      "default_policy/conv_value_1/bias\n",
      "(16,)\n",
      "----------\n",
      "default_policy/conv1/kernel\n",
      "(8, 8, 4, 16)\n",
      "----------\n",
      "default_policy/conv1/bias\n",
      "(16,)\n",
      "----------\n",
      "default_policy/conv_value_2/kernel\n",
      "(4, 4, 16, 32)\n",
      "----------\n",
      "default_policy/conv_value_2/bias\n",
      "(32,)\n",
      "----------\n",
      "default_policy/conv2/kernel\n",
      "(4, 4, 16, 32)\n",
      "----------\n",
      "default_policy/conv2/bias\n",
      "(32,)\n",
      "----------\n",
      "default_policy/conv_value_3/kernel\n",
      "(11, 11, 32, 256)\n",
      "----------\n",
      "default_policy/conv_value_3/bias\n",
      "(256,)\n",
      "----------\n",
      "default_policy/conv3/kernel\n",
      "(11, 11, 32, 256)\n",
      "----------\n",
      "default_policy/conv3/bias\n",
      "(256,)\n",
      "----------\n",
      "default_policy/conv_value_out/kernel\n",
      "(1, 1, 256, 1)\n",
      "----------\n",
      "default_policy/conv_value_out/bias\n",
      "(1,)\n",
      "----------\n",
      "default_policy/conv_out/kernel\n",
      "(1, 1, 256, 3)\n",
      "----------\n",
      "default_policy/conv_out/bias\n",
      "(3,)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for key, value in weights.items():\n",
    "    print(key)\n",
    "    print(value.shape)\n",
    "    print(\"----------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f180d2a4520>,\n <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f180d2a4250>,\n <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f180d292220>,\n <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f180d292460>,\n <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f180d2929d0>,\n <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f180d27cd30>,\n <tensorflow.python.keras.layers.advanced_activations.ReLU at 0x7f180d200d90>,\n <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f180d207af0>]"
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1, 11, 11, 64)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = agent.workers.local_worker().preprocessors[\"default_policy\"].transform(obs)\n",
    "filtered_obs = agent.workers.local_worker().filters[\"default_policy\"](preprocessed, update=False)\n",
    "\n",
    "conv_1 = tf.keras.Model(inputs=model.layers[0].input, outputs=model.layers[2].output)\n",
    "conv_2 = tf.keras.Model(inputs=model.layers[0].input, outputs=model.layers[4].output)\n",
    "conv_3 = tf.keras.Model(inputs=model.layers[0].input, outputs=model.layers[6].output)\n",
    "pred_1 = conv_1.predict(filtered_obs.reshape(1, 84, 84, 4))\n",
    "pred_2 = conv_2.predict(filtered_obs.reshape(1, 84, 84, 4))\n",
    "pred_3 = conv_3.predict(filtered_obs.reshape(1, 84, 84, 4))\n",
    "pred_3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.colorbar.Colorbar at 0x7f0921bf5550>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.imshow(obs[:, :, 0])\n",
    "#np.save('obs_learning.npy', obs)\n",
    "plt.imshow(pred_3[0, :, :, 11])\n",
    "plt.colorbar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x648 with 12 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAKACAYAAACWp0mXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdfbhdZX3g/e9PSAgQAkIgBCJEC49voAhRaaFTpgRHOwq0imLLTHScZuqMnarTR6OtElqdwT59rE7bizZFa6oo4CtI6wtJiw59KmNARBQUikcMhLxQGIgQIfp7/tjrmLPXWefstc/Ze+19zv5+rutca9/3utdav5Oc++xz7/stMhNJkiRJUn1PGXQAkiRJkjTX2JCSJEmSpC7ZkJIkSZKkLtmQkiRJkqQu2ZCSJEmSpC7ZkJIkSZKkLg28IRURL42I70bE3RGxbtDxSJIkSVInMch9pCJiP+B7wDnAVuDrwGsz8zsDC0qSJEmSOhh0j9SLgLsz857MfAK4EjhvwDFJkiRJ0rT2H/DzjwV+OCG9FXjxdBfEwqXJopX9jGlKyx+9GYBth5w2kOdrRDx6867MPLJXt4uFS5MDV/bqdvsc3p48bcnNk4rcvLNUV7Y9Nvk+Sw7qYVA99kRF3oLGo9jnwFJ6z0CiGLxH/k8p4+7e1pn9liYLVvbqdvuU/v9OO3hynbn/vvb0tpNPnXyf70UPg+qxHz9ckVlR7xtT+kc/4KmDCWPQJv7e2jNGPrmrZz9EsWhpsnhlr273M0eufKAtvYztk8r8gPbnPs6iSWX2jh3Q07hmpTwIrGpQ2I+bCGQK5bfjwQ1aG6wHy/8Jt0/5HjPohlRVRZ703xYRa4G1ACw6Dl68pc9htbt4UynMokF1yeo5+hN246ADKFlcHE8ZaBTDY1P8oKf3O3Al/EIf6syF7ckt50yuznFZ6bnvmfyHI78wxB9MjFXkrWg6iAlOKqXvHEgUg/fFL5QyfqW3dWbBSnhaH+pM6f9vy89PrjPr396evuTzX5t8n9WDbM13cPe1FZm3NB7GPs9uTz7tNYMJY9Am/t7asqq39168El7R+/rymr/+o7b0W/iTSWV+s5T3HZ4zqcwDr39GbwObjb0d0gB3NxHIFMp/h1XFNwo+8v1SxjOmfI8Z9NC+rcDTJqRXAPeXC2XmhsxclZmrWNCzDx2leSsi1kbElojYwhM7Bx2ONPTa6sxPrDPSdNrqyx7ri0bXoHukvg6cGBFPB+6j9Rn3rw82pJZJvVDTlJkzPVPjw4D2rB9kFJPtGR+6cu5Aw5hPMnMDsAEgDl3Vmx/QUg9UlnugLq6Io9TpHL9fEUq5l+qlA+yhKg+Vu7Pc8wHcOcBfm5vOaU+fUDp/WMU15V6sByrKqL3OLOpRnSn922e5B2rH5EvW/7dSxsqFk8pcMlYaczpUPVTHVuTd0XgUUz777vU1rimVKdcztdeXpb2pL2/q0AP1pZj8y+tVvKIt/an8/OQb/3V7cqA9VOUeniurhr1WfA9NubXUY3tW6fxiJiuP5l3Zs2jmhIE2pDJzb0S8CfgSsB/w4cz89iBjkiRJkqROBt0jRWb+HfB3g45DkiRJkuoaeENq2NQZ0jfdNXNmmN9QKZap2lSxEMEkxYzZ1cv6Fo0kSZLUyaAXm5AkSZKkOcceqcJMeqKmu489U90Y3xuizgTLk4vjK/sUi6ayYs1dben1pSqz/j92vsfL//CTk/KuO/OC9owPdBvZKLm+PTlpmdzyRlPAw2e2p6u2Gdg9i5A0tc+tb0vG7e3vC3le5/edS26oeC8pL36wsruwRktFnWjzeEXe+vbk2PrJRVbOKBhN4zVc1Zb+myjvI9XZ78QrJuWdd/dPZxPWaNnb/n/ApnKBiv+FC89qT++quO/SmYc07OyRkiRJkqQu2SNVGO9Bmm3PlD1R/fat1mHT+E6kb2sdhmr5X0mSJM139khJkiRJUpfskSqZSc+UvVCD8GRxHB/fbo9Uv2192Ylt6Q/l99oLxP816Zqb8+q29HVfumBSmaGaEzXpN+IhNS6qmmMxKBWxlOdRubloc05Y354ubQYb11S8d1xeSq+uuO/KGUfUgBWDDqBLVXOohqlOj45ffH37yr3/K9s3Z98Ut0y65tRSunI+1Ht6Mwe+Jya9xxw0iChmYfvkrCufbE+fWfH3mHOkJEmSJEnj7JGawsRepql6p+yJkiRJkkaTPVKSJEmS1CUbUpIkSZLUJYf21VBegMIhfVKNxSeArVe0l2FdxY3Kk1D3lNJ3Mtmi8jW3VRS6r5Qu/7qr2t6xPMm8qkx59QZ/jaqmE9a3p0uLTwBw1ltLGUsml9lbSm8tpavWexgrvW8trhiyvrv83laqD/tXTIwvx1K5eMPzSumq+iq167T4BMAvXtVehjdV3Ki8QexJpfSkTWepV6ceLqXPKqVvr7hmrLQwAzdUFCqvMFMVoIaFPVKSJEmS1KVGPkqNiA8DLwd2ZOZJRd7hwFW0FnIdA16dmQ81Ec9M2RMlSZIkCZrrkfoI8NJS3jpgc2aeCGymetCPJEmSJA2dRnqkMvOrEbGylH0e+0aUbqQ1UPTtTcQzssbnlZy+fl/e18ZfrEeaja3PPnFy5p3ra1xYLvORUnps8jV7ytswTt6osTdW1ihzbJ+erXlv8frJeeX5f7sqRkKMXVHK+EF78rDfq3hY6Zrd51aUKW1AfXppTtStFZfsfayUcVVFofJ8xap5VNL0fvHZN0/OLM9TeuDaiivPak/uKs07LNc5AB5pT26t2KT5pNIc2gdK56vuu39ps9q9KysKfaaUrpgnqaExyDlSyzJzG0BxPGqAsUiSJElSbXNiuamIWAusBWDRcYMNppPxlV7Kq8SMfzLxrAl542XKq8PUcUJxHP8frFrZrJbtM71QkiRJGlmD7JHaHhHLAYrjjqkKZuaGzFyVmatYcGRjAUpzVUSsjYgtEbGFJ3YOOhxp6LXVmZ9YZ6TptNWXPdYXja5B9khdC6wBLi2O1wwwlt65c7yH57LSiaIL6VkX7cu6dXwM7vtn8KD1bbdl1/oZ3EPzVWZuADYAxKGrmllu8s6Z9m6W51ScUEqPVVxTnmNxRkWZ8rjyL9S4RqOqrc4saqjO7K7Ku7GUsbKiUGlOFMe3J2+9oeKa8jyl8nsUTKozXyvvZ1Oeu1GXc6Lmm7b6srSh+lLewg9gb+l957CKuX8P39CeHiu/xxxecePy+0WF208uZRxRSpeeW5tzouaSRnqkIuITwD8Bz4yIrRHxBloNqHMi4i7gnCItSZIkSUOvqVX7XjvFqbObeL4kSZIk9dKcWGxi4MZHWuypsyrETVPkF1PANk28xyyWbN5VLANaXsZTkiRJUt8NcrEJSZIkSZqT7JGq42c9UZfP4ibjC0vM5h4Tfb5H95Eq7K3I+1opvfvJ9vSFpc0JAa5cX8q4vsbDyxsfznRRiNKGii4uoX6qqjNlY6U6s2jB5DJ7yotC/GPFjcpv3eVryum6ynVmpotLSB3sqcirWkxior2frsgsbRr9cPlnuEqvtn35Vo/uo7nMHilJkiRJ6pINKUmSJEnqkg0pSZIkSerS6M6R2lU6Pmu6wuNzP8Y3X7uzOD5ZUVaag8rzO8rzoQB2P1jK+NP25JVvrLjojpnHJA2zcp0Zq5qbUd5s+jfbk3turrhmZfnGXQQlDany5tMPVJQ5rJTeVS5wZsVF15bSrmSsZtkjJUmSJEldGt0eqVvvKl5c0TqsWN86Lq4ou3p8ZaVXtg6b/qpIz3RlJEmSJElzmT1SkiRJktQlG1KSJEmS1KXRGNo3vp/unRM3YbutvczXxgtVbJC4tFhs4pTxjDo7L0pzyFgpvfumGhedXEpfVlGmvEnvCXUjkobbpHfPL1QU2lFKf6SUrtoketMMA5KG2MOl9JaKMntuLGWc2J5cWrHp+67DSxn+faZm2SMlSZIkSV1qpEcqIp4G/A1wNPBTYENmfjAiDqe1PuxKWp+JvzozH+p5AOOdTZWfmI+7fOpTu36veDHeWzUaHXmSJEmSqjXVI7UX+G+Z+WzgdOC/RMRzgHXA5sw8EdhcpCVJkiRpqDXStZKZ24BtxetHI+IO4FjgPOCsothG4Abg7U3E1J2it2rTIUW6PO5dmuPK49c5sKLQ9lK6vPFh1XyPx2cakfrl7oq8FaX0oiYCmePuLs2z5V8qCpXfYst16PMV17jR+9DZu7Uis1xpNK095fQNFYXK9aO0xcyuqs12DyqlqzbGVn+NtSf3nDi5SHnq2jwa2NX4HKmIWAm8ALgJWFY0ssYbW0c1HY8kSZIkdavRNmFELAY+Dbw5Mx+JiLrXrQXWArDouPoPHP8QaXc3UVbZXjpKkiRJGmWN9UhFxAJajagrMvMzRfb2iFhenF/OFGPmMnNDZq7KzFUsOLKZgKU5LCLWRsSWiNjCEzsHHY409NrqzE+sM9J02urLHuuLRldTq/YF8CHgjsx8/4RT1wJrgEuL4zU9ffCd7y1eOOZcoyUzNwAbAOLQVdnxgl3lPXB69auhaq6VmlWa33F3xdwOp3u015lFNerMpDoyk7kZvjfNDRWr+j68vj19WCOBDI22+rK0Rn258bFSxkxG+Dj/aTiNtScfrpgjVR4ZNo/qS1ND+84A/h3wrYi4tch7J60G1NUR8QbgXuCChuKRJEmSpBlratW+G4GpJkSd3UQMkiRJktQr82gBQo2mm1qHTS/el3V6sUTq4uajkSRJ0mhofPlzSZIkSZrr7JHSHPePpSPwwPrW8YSmY5kjvliVWf7HGut/HGrIP5bSFRvH7n1jI5HMWXffUJFZ/nf17XT+eHEpffzkIm5aPbXy5qsA3NF0FGpMaeGQu78zucizntNMKANgj5QkSZIkdWl+fYS2abxVfGNxdGnZ+e83WocVE5bbdClnSZIk9Zk9UpIkSZLUpRn3SEXEU4CLMvNvehjPLI1vPPmtgUahJhUbvi4dbBRzS3nzXZhvndOa6ORSeuXkIv73d/BLFXk3NR6FmnJbKX3U5CKV84AEwMcerMi8u/Ew1JRT25OvqpgPNY/nFM6mR2oB8Ne9CkSSJEmS5oppP4eMiHdPc3pBj2ORZuDy1uHWCVkr17eOrtonSZKkPuk0oONdwHXA7opzzq+SJEmSNJI6NaTuAP4iM79UPhERi4DX9iUqSZIkSRpinRpSn6NyliXQmmq5sbfhSN06tXQEDhtIIMPrp5T6lCs2l+S+ZmLRAJzYnnxVxajsqjEHo+wJ9q1dBEzefFfzW3kliYqVJebx5Pmu7QHunJhRsem35rHS//eNFUXOKqXn0QJH034rmTnlHKnM3Au8vucRSZIkSdKQa6RNWAwD/CpwQPHMT2XmxRFxOHAVrfV4x4BXZ+ZDTcSk+eKs1mH1koFGIUmSpNHS1IIRPwZ+OTOfD5wCvDQiTgfWAZsz80Rgc5GWJEmSpKHWSI9UZib7RuEvKL4SOI99Iyc3AjcAb6910/HlrnddOyHTeR6j56rWYdOyfVmnnNs6uklvy+7dcOPEQcvuJDlaNrUnr3vZ5CKrSunFfQtmbghK745fqSg0jwb5q+TJUnpscpH9X9xEIHPDj4CvTcxw893Rsqw9eVZFkfJ7yjz6M6RWj1REzLrnKiL2i4hbgR3A9Zl5E7AsM7cBFMepFraQJEmSpKHR8SO1iNgP2B0Rh2Xmj2f6oMz8CXBKRBwGfDYiTqp7bUSsBdYCsOi4VubPVpm6ZaYhaV7YURwf3Ze1ZyCBSJIkaYR07GkqGkDfA47oxQMz82FaQ/heCmyPiOUAxXHHFNdsyMxVmbmKBUf2IgxpXouItRGxJSK2wMODDkcaem11JncOOhxpqLW/x1hfNLrqDvK+ArguIj5Ia3eNHD+RmX/f6eKIOBJ4MjMfjogDgdXA+4BrgTXApcXxmu7Cl367dXDVvjaZuQHYABBxYrb12Dm3Y8SU9g1zn7VKbXVm4aps+3fabZ0ZbRXzr8t7r43YvML295jnZfvGa48MJigNyFh7csuJk4ucUkrPo/pS993hjcVxfSk/gWfUuH45sLEYJvgU4OrMvC4i/gm4OiLeANwLXFAzHkmSJEkamFoNqcx8+mwekpm3AS+oyH8QOHs295YkSZKkptUerxARC4DTgWMy86qIOBggM3/Ur+Ckzm5rHTaduS/r9OI4j7qOJUmSNFzqLn9+Mq0FJ/4K+FCR/UvAh/sUlyRJkiQNrbo9UpcB787Mj0bEQ0XeV2g1rKQB2lQ6Ag+sbx1PaDoWaRiVJsof/ZzJRey9lSY4sJR28QRpav/SniwvxDLP1d1o97nAx4rXCT8b0lf+bSNJkiRJ817dhtQYcNrEjIh4EXB3rwOSJEmSpGFXd2jfu4C/jYi/ABZGxDuA3wJ+s2+RSZIkSdKQqtUjlZnXAS8DjqQ1N+p44Ncy88t9jE2SJEmShlKtHqmIOCIzbwH+c5/jkSRJkqShV3eO1A8j4pqIeGVELOxrRJIkSZI05Oo2pI4HNgPrgAciYkNEnNnhGkmSJEmal+rOkdqZmf8zM18I/DywA/hoRNwTEX8QEcf3NUpJkiRJGiJ1V+2b6OjiawlwC3As8I2I+KPMvLSXwUnS8DmrPflbC9rTf/E/K64pbVjIGT2MRxp2K9tSH81PtqUP4MeTrnh1fL6U874exyQNqfNf057+3A2lAidPvmbxEe3p3Vf1MiJNo+5iE88FLgJ+g9aexRuB52XmfcX5PwRuA2xISZIkSZr36vZIfRX4BPCqzPzf5ZOZORYRH+h0k4jYD9gC3JeZL4+Iw4GraH1cNQa8OjMfqhmT+mh9Pj71uTiwwUgkSZKk4VN3sYnlmfmmqkbUuMx8d437/A5wx4T0OmBzZp7IvsUsJEmSJGmo1eqRyswnImIZ8CJgKRATzn24zj0iYgXwb4H3Am8tss9j34SDjcANwNvr3E/9MV1PVLmMPVMaRb+Um9vS+7G3vcBlz5h0zd/Hkn6GJA23v2if8/E+JteRsj/M321LvyuO7GlI0tDa1Z7873lNqUA5De+MpaWcE3oakqZWd47U+cDHgLuA5wLfBk4CbgRqNaSADwBvAw6ZkLcsM7cBZOa2iDiq5r0kSZIkaWDqzpF6D/D6zPxkRDyUmS+IiNfTalR1FBEvB3Zk5s0RcVa3QUbEWmAtAIuO6/Zy1VCnJ2qqa+yZkiRJ0qipO0fquMzSeqWtoXj/vub1ZwDnRsQYcCXwyxHxMWB7RCwHKI47qi7OzA2ZuSozV7HA7n2pk4hYGxFbImILPDLocKSh11Znfrpz0OFIQ639Paa8vYM0Our2SO2IiGWZuR0Yi4ifpzWKc786F2fmO4B3ABQ9Ur+bmRdFxP8DrKG1bPoaqgZ+SupaZm4ANgBEnJgDDmdeeQHfaEvfVrWnxySrS+k7KktpcNrqzMJV1ple+q2PtKf/U+fBLO/64B+XctxHapi0v8c8z/rSQ6/8Xx/r/qIP/F57+s3uI9WUuj1SfwWcWbz+E+AfgG8Cl83y+ZcC50TEXcA5uA+VJEmSpDmg7qp975vw+m8i4gbg4Mzs+mPVzLyB1up8ZOaDwNnd3kOSJEmSBqluj1SbzLwX+OeIuLfH8UiSJEnS0JtRQ6oQwIpeBSJJkiRJc0XdxSam4gTDeWJ8CfNulkF32XONog+c/I629O9/651t6fdc8d9r3MXFJjRKxtpSt8dYZal2f1tK+36j0fDpD17Ulv7b1/1KW/rph45NvujNfQxI05pNj5QkSZIkjaRpe6Qi4qNM3etUa+lzzS11eqbsiZIkSdKo6zS07+4O5/+gV4FIkiRJ0lwxbUMqMy9pKhANF3udpCncfn1b8j3xr0sFrmeo3XpDRWZpU+GXHtFEJBoZ8/D95IHvtKdPeM5g4tD8U9pMd09p/lP1DNu7+hVNb5xeSt8+kCj6otYcqYi4NSL+74hwlT5JkiRJI6/uYhOXAC8E7oyIr0TEf4qIw/sYlyRJkiQNrVoNqcz8bGa+GlgOfBj4VeCHEXFtP4OTJEmSpGHU1T5SmfloRHwceBhYAPxKh0skSZIkad6p1ZCKiAB+Gfh1Wr1RPwA+Dryub5FJMzVWHHcXx1MGFIc0lKom/ru4hDS1JYMOQJo7Hrh5ct7XTmtPL24mlCbU7ZG6n9afpVcCZ2Rm9aIhkiRJkjQC6jakzs/Mm2bzoIgYAx4FfgLszcxVxYIVVwErafUjvDozH5rNcyRY3zrsOqFIXzSoQCRJkjRP1WpIZeZNEXEo8ExKHXKZ+fddPO9fZ+auCel1wObMvDQi1hXpt3dxP0mSJElqXN05Uq8D/pzW8L7HJpxK4BmzeP55wFnF643ADdiQ0qy9tTg6rl2a7MWTs57VfBTS3FG128uOUtoNeaWWivryqVL6dU3E0Yy6Q/veC7wqM78wi2cl8OWISOAvM3MDsCwztwFk5raIOGoW95ckSZKkRtRtSO0PfHmWzzojM+8vGkvXR8SddS+MiLXAWgAWHTfLMDTvrSx6ok6YvpgkSZI0U7U25AXeB/x+RNQtP0lm3l8cdwCfBV4EbI+I5QDFsdxXPn7thsxclZmrWHDkTEOQRkZErI2ILRGxBR4ZdDjS0GurMz/dOehwpKHW/h7zL4MORxqYug2jtwC/DzwaEfdO/KpzcUQcHBGHjL8GXgLcDlwLrCmKrQGu6Sp6SZXaPnxwrpjUUVudeYof2EnTaX+PqZpDJo2GukP7Zrt+9DLgs619fdkf+HhmfjEivg5cHRFvAO4FLpjlcyQYe39xLMb2rT53cLEMhUW0rybw+Yoyz24olrnmeZOzTljWnn5d6fzWituUf9OWJ97C5AUf9lSUWVWRN91zqlQMeT36v9zTln7gZbNZQ2geOBA4aUJ66+sqCn2kkVDmnlMr8so9FitL6bGKa8ofAB1bUebJUvpbFWXKf+QvKKWrKs3KUvofa9x3hMVCOGDFvvSeZRWFtjcWznAr/7xVLP5D+d+vXH/KP/fACSva07smF+HhT5cyXjm5zMpSuhzu7or7PlDeHamih/LCp1dcOD/UXf78K7N5SGbeAzy/Iv9B4OzZ3FuSJEmSmlZraF9ELIiISyLinojYUxwviYiF/Q5wSouLL06d8LWMyS15jZ5HSl+SJElSb9Ud2vdHtBaH+C3gB8DxwLto9b2/pT+hSZIkSdJwqtuQugB4fjEUD+C7EXEL8E0G1ZA6ZfzFhPkvm24uXlTNAZFG1JID4BcmjE/+4omDi2VGzpicteKgzpetLqXLY8ZPYbLyPKWqjWpvL6UPK6UXVVxTzivHBkUPe4f7dFrSv+o3evn7rijzwMYRnxNV9mPg7okZc22Lw6r5OzOp9weW0rfVfFbZa0rpx2pcs7KUrBhtMlbOOK3iPr2Ym1M176tqztaIOhx4xYT0R06uKDTMc6Qq3mNq/f+W5+SVv8dzJl9S/h1eNRe2bGv5Z//ByWXuLs2bWlSeCwicUJoT9UDFs8aylLG3Q3AweZ5XxUigG0rpl9a47RxRd9W+6DJfkiRJkuatuj1SnwQ+HxGX0Fpd73hay6Ff3a/A1JTxT47m48p2FZ/ISJIkST1QtyH1NloNpz8HjgHuA64E3tOnuCRJkiRpaNVd/vwJ4N3Fl+aVla3DantvRtrRpbHcKyvKVO0fMdHvVuSVtraonPdTnsOztJQuz0ECTnru19vSt//tCyeVWXRm+14We9aV5nI8XBFLef5TOQ2Tx5VXjTOfiap9P8ru7NGz1KWqUezliW7l+URT5U1Ute9Ref+kqvlZ5fs+XkrfUXFN6e1+6ZmTi+wqbYK2uFSBl1bseTM2/WMAOLqUXlSe41i1l04NneYMAr1ZydfVgLtT9XNfnmfWo7m65feYqn38eG8pXd6HqSrecpmqfZAq9hmcqOr97u4OdQwmz82dNKf2iOmfOyvl33Uz+duw/Dtsfpt2jlREnBER75vi3KURcXp/wpIkSZKk4dVpsYl3Al+d4txXgN/rbTiSJEmSNPw6De07BfjiFOeuBz7U23DUUgwjOOWN+7JuHV9W87LiuL4oM8tHVXU9S5IkSZpWpx6pJcDCKc4tAA7pbTiSJEmSNPw69UjdCbwEuKbi3EsYuqnP45P2xpf0Hp/IOz7x7fhmw5mxokdqaUXe+Pc2/q2WJ+ZLnax82eS8V5XSVT9XX+tw3xsq8so9nuVNZ6HehoQlt+9fWlyiYsGHPdcdPn2ZGTxXI+rCirwrSwsQrKqYNF5eZKHs7ooJ7HUWLyn/7E76WX5O53tUjkYoLy5ROl31F0OtBR80Wqo2TC/Vl4pFhCrzJqpa8GhreQPZioVhDivNQqmzx2z5Z73qPbFc5cvxV34/Fb8nNKd1akj9CfCXEbEf8LnM/GlEPAU4n9ZS6G/td4CSJEmSNGymbUhl5scj4mhgI3BAROyi1S7fA1ycmZ+o+6CIOAy4HDgJSOA/AN8FrqK12PIY8OrMfKj7b6OwevwTj1e2DpvGO8yKXpzV55SvmDt+tvzlKwcZhSRJkiQ6z5EiM98PHAu8gtZOMa8AVmTmn3T5rA8CX8zMZwHPp7XRxTpgc2aeCGwu0pIkSZI01OpuyPsI8KWZPiQilgD/Cnhdcb8ngCci4jzgrKLYRlqzLN4+0+dM9rbi6Gaz0s+UN/sDuLuUHqso02lOUa82pu2Vqg13pZnYUpG3tDTXoWpD5Zn8DA5y3qvzndQLr6vIe7i0SWvV+0mnOYWVqjbLlprTsUeqR54B7AT+OiK+ERGXR8TBwLLM3AZQHKu2cJckSZKkoVKrR6pHzzkV+O3MvCkiPkgXw/giYi2wFoBFx9V/6mp7oiRJkiT1XlM9UluBrZl5U5H+FK2G1faIWA5QHHdUXZyZGzJzVWauYsGRjQQszWURsTYitkTEFp7YOehwpKHXVmd+Yp2RptNWX/ZYXzS6GmlIZeYDwA8j4plF1tnAd4BrgTVF3hqq96uS1KW2Dx8W+uGD1ElbndnPOiNNp62+LLK+aHQ1NbQP4LeBKyJiIXAP8HpaDbmrI+INwL3ABQ3GI2mcm9NK3em0eaikfawvmqcaa0hl5q3AqopTZzcVgyRJkiT1QlNzpCRJkiRp3rAhJUmSJEldsiElSZIkSV2yISVJkiRJXbIhJUmSJJ0Bs8cAACAASURBVEldsiElSZIkSV2yISVJkiRJXbIhJUmSJEldsiElSZIkSV2yISVJkiRJXbIhJUmSJEldsiElSZIkSV2yISVJkiRJXWqkIRURz4yIWyd8PRIRb46IwyPi+oi4qzg+tYl4JEmSJGk2GmlIZeZ3M/OUzDwFOA14DPgssA7YnJknApuLtCRJkiQNtUEM7Tsb+OfM/AFwHrCxyN8InD+AeCRJkiSpK4NoSF0IfKJ4vSwztwEUx6MGEI8kSZIkdaXRhlRELATOBT7Z5XVrI2JLRGzhyZ39CU6SJEmSamq6R+plwC2Zub1Ib4+I5QDFcUfVRZm5ITNXZeYqFhzZUKjS3NX24cMTfvggddJWZ35inZGm01Zf9lhfNLqabki9ln3D+gCuBdYUr9cA1zQcjzQvtX34sNAPH6RO2urMftYZaTpt9WWR9UWjq7GGVEQcBJwDfGZC9qXAORFxV3Hu0qbikSRJkqSZ2r+pB2XmY8ARpbwHaa3iJ0mSJElzxiBW7ZMkSZKkOc2GlCRJkiR1yYaUJEmSJHXJhpQkSZIkdcmGlCRJkiR1yYaUJEmSJHXJhpQkSZIkdcmGlCRJkiR1yYaUJEmSJHXJhpQkSZIkdcmGlCRJkiR1yYaUJEmSJHXJhpQkSZIkdamxhlREvCUivh0Rt0fEJyJiUUQcHhHXR8RdxfGpTcUjSZIkSTPVSEMqIo4F/iuwKjNPAvYDLgTWAZsz80Rgc5GWJEmSpKHW5NC+/YEDI2J/4CDgfuA8YGNxfiNwfoPxSJIkSdKMNNKQysz7gD8G7gW2Af8nM78MLMvMbUWZbcBRTcQjSZIkSbPR1NC+p9LqfXo6cAxwcERc1MX1ayNiS0Rs4cmd/QpTkiRJkmppamjfauD7mbkzM58EPgP8ArA9IpYDFMcdVRdn5obMXJWZq1hwZEMhS3NX24cPT/jhg9RJW535iXVGmk5bfdljfdHoaqohdS9wekQcFBEBnA3cAVwLrCnKrAGuaSgeaV5r+/BhoR8+SJ201Zn9rDPSdNrqyyLri0bX/k08JDNviohPAbcAe4FvABuAxcDVEfEGWo2tC5qIRxopC4EVgw5C6qUX9ff2P94Fd/9Vf58hNenu501I/Ki3994N3NjbW0qDtaB2yUYaUgCZeTFwcSn7x7R6pyRJkiRpzmhy+XNJkiRJmhdsSEmSJElSlyIzBx1DVyJiJ60BvrsGHUsXlmK8/TTf4j0+M3s2e7eoMz+o8dxhY7z9NZ/itc60GG9/zZd4rS8txttf8yneKevMnGtIAUTElsxcNeg46jLe/jLe4X7uTBlvfxnvcD5zNoy3v4x3uJ43W8bbX6MSr0P7JEmSJKlLNqQkSZIkqUtztSG1YdABdMl4+8t4h/u5M2W8/WW8w/nM2TDe/jLe4XrebBlvf41EvHNyjpQkSZIkDdJc7ZGSJEmSpIGxISVJkiRJXbIhJUmSJEldsiElSZIkSV2yISVJkiRJXbIhJUmSJEldsiElSZIkSV2yISVJkiRJXbIhJUmSJEldGnhDKiJeGhHfjYi7I2LdoOORJEmSpE4iMwf38Ij9gO8B5wBbga8Dr83M7wwsKEmSJEnqYNA9Ui8C7s7MezLzCeBK4LwBxyRJkiRJ09p/wM8/FvjhhPRW4MXTXRALlyaLVvYzpiktf/RmALYdctpAnq8R8ejNuzLzyF7dLmJpwspZ32flafdMe/7HNz/U+SanHd2xyLabj60b0uxEjTJ5f9/D6K0DOxeJp3YuM7iBCpMtrFHmiV7XmYMSDuvV7aZxVMcSy/lmxzLbWN6LYDpbeEznMk88WuNGdcpopo457afTnn947P/wo12P1/kNWEuv6stp+2+b9vzNexv6OR86h/bmNnFw5zL5YIcCdX4hz8f6vW3K95hBN6SqKvKkt/CIWAusBWDRcfDiLX0Oq9raTa1wLxnQ8zUiNsUPenvDlbB/h5/ZvZ3vsn7La6Y9//24uuM9frLl9R3LvGfBf+8cTC/U+e23Z32/o+ixkzsXOeCVncvU+HloTI2/3RnrdZ05jPG3nP767Y4l1rK0Y5lLGokVOGZ95zJjN9S4UZ0ymqk3btk97fnLVn2sx0/sTX3Zcvgl056PHQ39nA+dl/XmNgdM20/RsucjHQqsrPGgG2qUmWsumfI9ZtANqa3A0yakVwCTPgLOzA3ABoBYsqrxz0ov3hSV6UtWD9PHtl24cdABlCwujqcMNApJkiSptkHPkfo6cGJEPD0iFgIXAtcOOCZJkiRJmtZAe6Qyc29EvAn4ErAf8OHM/PYgYxpX7oWarsyc6ZnaM35cP8goJttzavHi3IGGMZ+0DYfluIHGIs0F7XWmR3MSpHnK+iK1DHpoH5n5d8DfDToOaT5pGw4bzQ+Hleaa9jpzjHVGmob1RWoZ9NA+SZIkSZpzBt4jNWzqDOmb7po5M8xvqNzXOmy6uUbZFa3D6mV9i0aSJEnqxB4pSZIkSeqSPVKFmfRETXcfe6a6sb04fr5G2fF9cmrshaMuPNKxxOuiwx4U113V+THR+Tl03rO33t6PvfjtdnsP7tGob3UusqdG3fGdoSF/2rHEJUfXeC95YP3sQ6njrBplxmoUuuGmWQbSS48POgAVPri9wz5RPdtCeK6psdF6HXtuq1HoRdOe/Xi+s+Mdfj1eWOM5T9YoMzfYIyVJkiRJXfJzx8J4D9Jse6bsieq34hP3TXcW6be1DqsXDCQaSZIkjSZ7pCRJkiSpS/ZIlcykZ8peqEEYH187Pr7dHilJkiQ1xx4pSZIkSeqSPVJTmNjLNFXvlD1RkiRJ0miyR0qSJEmSumRDSpIkSZK65NC+GsoLUDikT/PO/ks6l9n71unPv7zOgw7pXOSB62uU2d65DPd1OP/sGvdYVqNMnVikGWpqs906PrK+c5ln1SgjVXhzLB90CGLvtGd/yNNq3OOoGmU6vT/PHY30SEXEhyNiR0TcPiHv8Ii4PiLuKo5PbSIWSZIkSZqtpob2fQR4aSlvHbA5M08ENhfpoXbJ6rQ3SpIkSVIzDanM/CrwL6Xs84CNxeuNwPlNxCJJkiRJszXIOVLLMnMbQGZui4g6gyo1G4uK4+nr9+V9bfzFeiRJkiTVMydW7YuItRGxJSK28OTOQYcjSZIkacQNskdqe0QsL3qjlgM7piqYmRuADQCxZNVwT1LaWhx3lfLHe4OeNSFvvMxWundCcRz/H7xzBvcAXHFMkiRJ6t4ge6SuBdYUr9cA1wwwFkmSJEmqrZEeqYj4BHAWsDQitgIXA5cCV0fEG4B7gQuaiKXv7hzv4bmsdKLoQnrWRfuybn2kePH+GTxofdtt2bV+BvfQfBURa4G1rdRxnS84usZNO/22GPt+jZts7FykMbcMOgANkfY6c2gjz1y8+790LLN78Zdq3OnxGmUerVGm0z5v5TWjKtz5vhrP0Vw3iPpSz7E1ykw5AGqCV9Qo85kO53+txj001zXSkMrM105x6uwmni+NmrbhsDHkw2GlIdBeZ46xzkjTsL5ILXNisQlJkiRJGiaDXGxi7rixOO6psyrETVPkF13JmybeYxZDi3bdXByXzPwekiRJkmbEHilJkiRJ6pI9UnX8rCfq8lncZHxhidncY6LP9+g+kiRJkrplj5QkSZIkdcmGlCRJkiR1yYaUJEmSJHVpdOdI7SodnzVd4WXF8eTieGdxfLLHQUl9sD+wtEOZrZ+ucaNvdTi/slY4GrDR/a3fW1vXdS6z4tJpT+9efFiNB91XL56eqLHhrjTUXtzgs0Zvw90nOKBGqQP7HscwsUdKkiRJkro0up9N3npX8eKK1mHF+tZxcUXZ1QuKF69sHTb9VZFu8pNCSZIkScPCHilJkiRJ6pINKUmSJEnq0mgM7RvfT/fO7RMyb2sv87XxQguYZGmx2MQp4xl7exaaJEmSpLmnkR6piHhaRPxDRNwREd+OiN8p8g+PiOsj4q7i+NQm4pEkSZKk2WiqR2ov8N8y85aIOAS4OSKuB14HbM7MSyNiHbAOeHvPnz7e2cRl0xS6fOpTu36veDHeWzUaHXmSJEmSqjXSI5WZ2zLzluL1o8AdwLHAecDGothG4Pwm4pEkSZKk2Wi8ayUiVgIvAG4ClmXmNmg1tiLiqKbjqafordp0SJHeMbBIpK7tBR7oVKjTZrt1jPXgHuq7OlM8R73T/aBj4Dnrpy9T62O/ThtTvr9ePNJI6LTB7eONRKGpvetDf9y5UJ33j73rZxvK0Gh01b6IWAx8GnhzZj7SxXVrI2JLRGzhyZ39C1CSJEmSamjsc8eIWECrEXVFZn6myN4eEcuL3qjlTNHVk5kbgA0AsWRV1n7o+Nyo3TMOezzM0lGSJEnSKGtq1b4APgTckZkTxzJcC6wpXq8BrmkiHkmSJEmajaZ6pM4A/h3wrYi4tch7J3ApcHVEvAG4F7igp0+9873Fiyd7eltp2EXEWmBtK3XcQGOR5oK2OrPQOiNNp/095tCBxiINUiMNqcy8EYgpTp/dRAzSKGkbDhtdDIeVRlRbnTnYOiNNp/095hjri0ZWo4tNSJIkSdJ8MOqL3GrOu6l12PTifVmnL2kdFzcfjSRJkkaDPVKSJEmS1CV7pDTH/WPpCDywvnU8oelYhtWTuHS/VN/SZ+/gvK//2bRlPhR2eUv1ndqDe3Ta4FpD4T/WKPMXfY+iMfZISZIkSVKX5leP1KbxT91vLI4uez7//UbrsOLEfVkrBhOJJEmSRkfHHqmIOCgiXhARh1ScO6M/YUmSJEnS8Jq2RyoiXgT8LbAQWBAR6zPzjyYU+QKwpI/xdWlrcfzWQKNQk4ox00sHG4UkSZJGS6ceqf8XeGdmHgr8AnBRREycIjbVJruSJEmSNG91miN1EnA5QGbeGhFnAtdGxEeBNf0OTurs8tbh1glZK9e3jq7aJ0mSpD7p1CP1GHDkeCIzHwFeWuR9CnukJEmSJI2gTg2prwC/PjEjM/cA5wILcFF/SZIkSSOo09C+3wEm7TqYmU9ExK/SmjclDdCppSNw2EACGWIPAVcNOghpztj12FF86OY3dSj1vkZikeaHlYMOQL1weo0yY/0OYrhM2yOVmTsz8/tTnNubmV+t85CIWBQR/zsivhkR346IS4r8wyPi+oi4qzg+tftvQZIkSZKa1dSGvD8Gfjkzd0fEAuDGiPgC8GvA5sy8NCLWAeuAtzcUk+aFs1qH1UO0Cr8kSZLmvY4b8vZCtuwukguKrwTOAzYW+RuB85uIR5IkSZJmo1aPVEQ8JTN/OpsHRcR+wM20FqX+88y8KSKWZeY2gMzcFhFH1b7h+HLXu66dkHnfbELUnFTM/dm0bF/WKee2jm7SK0mSpD7p2CNVNIB+FBEHzOZBmfmTzDwFWAG8KCJOqnttRKyNiC0RsYUnd84mDEmSJEmatY49Upn5k4j4HnAEcP9sH5iZD0fEDbT2o9oeEcuL3qjlwI4prtkAbACIJasSgPGBgtwy25A0p43/yDy6L2vPQAKRJEnSCKk7R+oK4LqIWBMRZ0fEL49/1bk4Io6MiMOK1wcCq4E7gWuBNUWxNcA13YUvSZIkSc2ru2rfG4vj+lJ+As+ocf1yYGMxTPApwNWZeV1E/BNwdUS8AbgXuKBmPFLht1sHV+1rExFrgbWt1GHA44MMR0PlwRpljuh7FMOmrc4sPW7k9kKRutH+HnPoQGNRgy6tUWZdjTLvmW0gw6NWQyoznz6bh2TmbcALKvIfBM6ezb0lTdY2HDZW5IDDkYZeW535uVXWGWka7e8xx1hfNLJqL38eEQsi4hcj4jVF+uCIOLh/oUmSJEnScKq7/PnJtOYz/ZjWqntXAb9Ea17Ta/oWndTRba3DpjP3ZZ1eHBc3HowkSZJGRN0eqcuAd2fms4Ani7yvAGdOfYkkSZIkzU91F5t4LvCx4nUCZOaPihX4pAHaVDoCD6xvHU9oOhZJkiSNiro9UmPAaRMzIuJFwN29DkiSJEmShl3dHql3AX8bEX8BLIyIdwC/Bfxm3yKTJEmSpCFVq0cqM68DXgYcSWtu1PHAr2Xml/sYmyRJkiQNpbqr9h2RmbcA/7nP8UjSEDm5Y4kj84Udy+yMD/cimB5xc2b1U+efrzfnAR3LfOCqd3R+1IXvqxOQNLQu5vkdy1zCxT162q/N/hYrOhd5xnO/3bHMPbOPZGjUnSP1w4i4JiJeGREL+xqRJEmSJA25ug2p44HNwDrggYjYEBEufS5JkiRpJNWdI7UzM/9nZr4Q+HlgB/DRiLgnIv4gIo7va5SSJEmSNETq9khNdHTxtQT4Z+BY4BsRsa6XgUmSJEnSsKq72MRzgYuA3wB2AxuB52XmfcX5PwRuAy7tU5ySJEmSNDTq9kh9FTgEeFVmPicz3zfeiALIzDHgA51uEhH7RcQ3IuK6In14RFwfEXcVx6fO4HuQJEmSpEbV3ZB3eWY+MV2BzHx3jfv8DnAHrWGB0Fq8YnNmXloMDVwHvL1mTOqj9Tn1Erbr48AGI5EkSZKGT62GVGY+ERHLgBcBS4GYcK7WBikRsQL4t8B7gbcW2ecBZxWvNwI3YENKkiRJ0pCrO0fqfOBjwF3Ac4FvAycBNwJ1d5r8APA2WkMExy3LzG0AmbktIo6qeS/1yXQ9UeUy9kxp/utcH17DlR3L/FmtjRA/U6OMNPd94MDOm+1++fFf7FjmJRe+vBfhSAPzq3lixzKXRMciGqC6c6TeA7w+M18A/Kg4rgVurnNxRLwc2JGZtcpXXL82IrZExBae3DmTW0iSJElSz9SdI3VcZn6ylLcReAD43RrXnwGcGxG/AiwClkTEx4DtEbG86I1aTmt/qkkycwOwASCWrMqaMasLdXqiprrGnilJkiSNmro9UjuKOVIAYxHx88DPAfvVuTgz35GZKzJzJXAh8PeZeRFwLbCmKLYGuKZ25JIkSZI0IHUbUn8FnFm8/hPgH4BvApfN8vmXAudExF3AObgPldQTbcNh+dGgw5GGXludecQh5NJ02t9jHht0ONLA1F21730TXv9NRNwAHJyZd3T7wMy8gdbqfGTmg8DZ3d5D0vTahsPGCofDSh201Zmfcwi5NJ3295hjrC8aWXV7pNpk5r3AP0fEvT2OR5IkSZKG3owaUoUAVvQqEEmSJEmaK2bTkAKwO1eSJEnSyKm7/LnmufElzLtZBt1lzzX/3d2xxJ/V2ixxmDbb/UKNMr/Z9yg0wvZ8v2ORl8Q7a9zottnHIg3QKRd8r0ap9f0Oo77La5T5H32PYqhM25CKiI8yda9TraXPJUmSJGm+6dQj1enj2D/oVSAaDnV6puyJkiRJ0qibtiGVmZc0FYgkSZIkzRW15khFxK3AFcAnMnNrf0PSMLDXSZIkSZpa3VX7LgFeCNwZEV+JiP8UEYf3MS5JkiRJGlq1GlKZ+dnMfDWwHPgw8KvADyPi2n4GJ0mSJEnDqKvlzzPz0Yj4OPAwsAD4lb5EJUmSJElDrFaPVLScHREfArbTWtT+i8DT+xibJEmSJA2luj1S9wO7gSuBMzLzjv6FJM3SWHHcXRxPGVAc0lC6b9ABDL8DgJWDDmIY1VmE6NgaZW6pUabO5vDP7sF9/HNG/XJI5yKd93yHN63vXObGGvdZXKPM0g7nf7fzLQ7isRoPmj/qNqTOz8ybZvOgiBgDHgV+AuzNzFXFghVX0XrLGgNenZkPzeY5kiRJktRvtRpSmXlTRBwKPJNSmzYz/76L5/3rzNw1Ib0O2JyZl0bEuiL99i7uJ1VY3zrsOqFIXzSoQCRJkjRP1d1H6nXAn9MaLDWxzy6BZ8zi+ecBZxWvNwI3YENKkiRJ0pCrO7TvvcCrMvMLs3hWAl+OiAT+MjM3AMsycxtAZm6LiKNmcX+p8NbiuGSgUUiSJGn+qtuQ2h/48iyfdUZm3l80lq6PiDvrXhgRa4G1ACw6bpZhSJIkSdLs1Fr+HHgf8PsRUbf8JJl5f3HcAXwWeBGwPSKWAxTHHVNcuyEzV2XmKhYcOdMQNCpWLml9rab1JUmSJPVY3YbRW4DfBx6NiHsnftW5OCIOjohDxl8DLwFuB64F1hTF1gDXdBW9JEmSJA1A3aF9s132bBnw2YgYf+bHM/OLEfF14OqIeANwL3DBLJ8jidJwWA4baCzSXNBWZ452CLk0nfb3mEMHGos0SHWXP//KbB6SmfcAz6/IfxA4ezb3liYZe39xLJY/X33u4GIZkGIxlw0AEScn/FqHK67of1BzUo0NPxe9pnOZPU/WeNaCGmU6WFSjzJ6ts3/OPDSxzixfdUy+/rR3T1v+f7CqB099pEaZyhHvJSd0LsK/1CjTyVhDz4F9C/pOp85munU/L1Y32t9jjsmOF+z/vM43PanD+Us73+LUf9N5Z9o/500dy5z+w292ftieDufP7HwLnt65yBf/9Jc6lrmNkzuWeR7f6lhmMY9Oe/57PLPjPS7nP3YsM5/UGtoXEQsi4pKIuCci9hTHSyJiYb8DlCRJkqRhU/ejmj+itTjEbwE/AI4H3kVrfem39Ce0Dsa3Bd5z6oTM+4rj9oaD0XB5pHSUJEmSeqtuQ+oC4PnFUDyA70bELcA3GVRDSpIkSZIGpG5DKrrM779Txl9MmP+y6ebixecbDkaSJEnSKKm7/Pkngc9HxL+JiGdHxEuBzwFX9y80SZIkSRpOdXuk3kZrH6k/B46hNRnpSuA9fYpLjRlf6WU+rmzXg1XQJEmSpAp1lz9/Anh38SVJkiRJI23ahlREnAGcm5lvrzh3KfC5zPxav4JTE1a2DqvtvZm3Fh0AJ5w4fZnb39b5PosPmv58nY9l6uxzVOc+u2uU6cVzdtUos7fOsxqqX7ViWdH3MOa6B/75GP7Hr/5Bh1KX1bjT4x3O92pl0Vt6dJ9hcsOgA1APrX3ygx3L/EOHvcOexg873uN/PfivOpb590ds7Fjmrv9v0tank/1uh/M7vtD5Hp+9qXOZnq1GcFavbtTBVxt6znDoNEfqnUz9L/IV4Pd6G44kSZIkDb9ODalTgC9Oce564LTehiNJkiRJw6/T4JYlwEKqxycsAA7peUQClrUOp7xxX9at45sMjw8nWV+UmeWj6gy1kiRJktSmU4/UncBLpjj3kuK8JEmSJI2UTj1SfwL8ZUTsR2thiZ9GxFOA82kthf7WfgfYnfFJ1ONLen+rOC4pjsc3G86MFT1SSyvyxr+38W+1rYwkSZKkJkzbkMrMj0fE0cBG4ICI2EXrT/c9wMWZ+Ym6D4qIw4DLgZOABP4D8F3gKlpLx40Br87Mh7r/NiRJkiSpOR0XAM7M90fE5cDPA0cADwL/lJndrtv6QeCLmfmqiFgIHERrVcDNmXlpRKwD1gGTllqvbfV4r80rW4dN4yMPi16c1efM+NYDt3r8xSsHGYUkSZIk6m/I+wjwpZk+JCKWAP8KeF1xvyeAJyLiPPYtbL+R1sYRM29ISZIkSVIDajWkeuAZwE7gryPi+cDNwO8AyzJzG0BmbouIo3r72PFNRt1sViNsDzWWhemw2e74fWarFxvpNqmp35AaLg/fD59bP+gopHljQ9SZtfHZac/eVetJWzqWqHef6WORxnVata9X9gdOBS7LzBcAP6I1jK+WiFgbEVsiYgtP7uxXjJIkSZJUS1Oft24FtmbmTUX6U7QaUtsjYnnRG7Uc2FF1cWZuADYAxJJVWfupq+2JkiRJktR7jfRIZeYDwA8j4plF1tnAd4BrgTVF3hrgmibikSRJkqTZaHIGwG8DVxQr9t0DvJ5WQ+7qiHgDcC9wQYPxSPNWRKwF1rZSxw00FmkuaK8zhw40FmnYWV+klsYaUpl5K7Cq4tTZTcUgjYq24bDRxXBYaUS115ljrDPSNKwvUktTi01IkiRJ0rxhQ0qSJEmSumRDSpIkSZK65HaTkvxNIEmS1CV7pCRJkiSpSzakJEmSJKlLNqQkSZIkqUs2pCRJkiSpSzakJEmSJKlLNqQkSZIkqUs2pCRJkiSpSzakJEmSJKlLNqQkSZIkqUuNNKQi4pkRceuEr0ci4s0RcXhEXB8RdxXHpzYRjyRJkiTNRiMNqcz8bmaekpmnAKcBjwGfBdYBmzPzRGBzkZYkSZKkoTaIoX1nA/+cmT8AzgM2FvkbgfMHEI8kSZIkdWUQDakLgU8Ur5dl5jaA4njUAOKRJEmSpK402pCKiIXAucAnu7xubURsiYgtPLmzP8FJkiRJUk1N90i9DLglM7cX6e0RsRygOO6ouigzN2TmqsxcxYIjGwpVkiRJkqo13ZB6LfuG9QFcC6wpXq8Brmk4HkmSJEnqWmMNqYg4CDgH+MyE7EuBcyLiruLcpU3FI81nbcNhcTis1El7nXls0OFIQ836IrXs39SDMvMx4IhS3oO0VvGT1EOZuQHYABCxKgccjjT02uvMMdYZaRrWF6llEKv2SZIkSdKcZkNKkiRJkrpkQ0qSJEmSumRDSpIkSZK6ZENKkiRJkrpkQ0qSJEmSumRDSpIkSZK6ZENKkiRJkrpkQ0qSJEmSumRDSpIkSZK6ZENKkiRJkrpkQ0qSJEmSumRDSpIkSZK61FhDKiLeEhHfjojbI+ITEbEoIg6PiOsj4q7i+NSm4pEkSZKkmWqkIRURxwL/FViVmScB+wEXAuuAzZl5IrC5SEuSJEnSUGtyaN/+wIERsT9wEHA/cB6wsTi/ETi/wXgkSZIkaUb2b+IhmXlfRPwxcC/wOPDlzPxyRCzLzG1FmW0RcVQT8Uij5SHY++lBByHNzNgRg45AmlPe9Zd/PH2BnTc0Eoc0Cpoa2vdUWr1PTweOAQ6OiIu6uH5tRGyJiC08ubNfYUqSJElSLU0N7VsNfD8zd2bmk8BngF8AtkfEcoDiuKPq4szckJmrMnMVC45sKGRJkiRJqtZUQ+pe4PSIOCgiAjgbuAO4FlhTlFkDXNNQPJIkSZI0Y03NkbopIj4F3ALsBb4BbAAWA1dHxBtoNbYu+uWyVgAAIABJREFUaCIeab6LiLXA2lZq6UBjkeaC9jpz6EBjkYad9UVqaaQhBZCZFwMXl7J/TKt3SlIPZeYGWh9WEPFzOeBwpKHXXmeOsc5I07C+SC1NLn8uSZIkSfOCDSlJkiRJ6pINKUmSJEnqUmTOraGtEbET+BGwa9CxdGEpxvv/t3fv4XbV9Z3H3x8hCIgBEYnhrpV6wxE1VbxMh5HQUeuIM45WLR1oncm0nd7tCLZOSdo6g306XjrjY5uiJUVF0GqhtGMxKDpYRQGpxYKFKpdASABB8AKifuePtQ7snJyTs3f2OXutc8779Tz7+a29rp+TrG9Ofvu31toLaanlPbKq5u05/23N3DTC8fvGvAtrKeS1ZnZk3oW12PMudL3MdMy+M+/CWux5Z62ZRdeRAkhyRVWt6TrHsMy7sMy7uI4/KvMuLPP285jjMO/CMm8/jzkO8y6spZzXS/skSZIkaUR2pCRJkiRpRIu1I7Wx6wAjMu/CMu/iOv6ozLuwzNvPY47DvAvLvP085jjMu7CWbN5FeY+UJEmSJHVpsY5ISZIkSVJn7EhJkiRJ0ojsSEmSJEnSiOxISZIkSdKI7EhJkiRJ0ojsSEmSJEnSiOxISZIkSdKI7EhJkiRJ0ojsSEmSJEnSiDrvSCV5SZKvJrkhyeld55EkSZKkuaSqujt4sgfwT8CJwBbgi8DrquofOwslSZIkSXPYs+PjPxe4oaq+BpDkQ8BJwKwdqex1ULH3UZNJN83q+64EYOujn9PJ8bVM3HflnVX1uPnaXbJvwQHztTtpnu09D/v4eu9qZjVbx86xldVj70Pa2T1UfSfztbf5qJfnHD5+vVx5i/WihbJ11t8xXXekDgVuGXi/BXje9JWSrAPWAbD3EfC8KyYSbrp1m5t/dzZ0dHwtE5tz0/zu8ACmykfqnyfNwz5+pnc1s44NY6fYYN1qQWyc5/2NXy9XnDZ+veSXrBctlA2z/o7puiM10yciO11rWFUbaSs/K9dM/FrEMzZnxvcb1nZ3WeRYLus6wDT7te2xnaaQJEmShtb1wya2AIcPvD8MuK2jLJIkSZI0lK5HpL4IHJ3kCcCtwGuB13cbqTF9FGpX6yyakan7p9r1XabY2f3Pbide0WkMSZIkaViddqSq6vtJfgn4W2AP4H1V9ZUuM0mSJEnSXLoekaKq/gb4m65zSJIkSdKwOu9I9c0wl/TtaptFc5lfr9zaNJuvHGLdw5pm7aoFSyNJkiTNxY6UtATt8JUB7N9pFmkxsGak4VkvUsOOVGt3RqJ2tR9HpkaxrW3/aoh1n9G2r1qgLEvDDl8ZkEM8GaU5WDPS8KwXqdH1488lSZIkadFxRKo1NYI07siUI1EL7R+aZvN17fs3Nc3aFZ2kkSRJ0vLkiJQkSZIkjcgRqWl2Z2TKUaguPNi2321bR6QkSZI0OY5ISZIkSdKIHJGaxeAo02yjU45ESZIkScuTI1KSJEmSNCJHpCTN6Qw2jL2PDZwx5h5eOHYG+Ow87EPz64auAyyI8c93afnIL1kv/fPUedjHtfOwj36zIzWE6Q+g8JI+SZIkaXnz0j5JkiRJGtFERqSSvA94ObC9qo5p5x0InAccBdwIvKaq7p5Ent3lSJQkSZIkmNyI1NnAS6bNOx24pKqOBi5p30uSJElS701kRKqqPpPkqGmzTwKOb6c3AZcCp00iz7K1d9set/7heZ+fmliPJEmSpOF0eY/UqqraCtC2B8+2YpJ1Sa5IcgUP3jGxgJIkSZI0k0Xx1L6q2ghsBMjKNf2+UWlL2945bf7UaNBTBuZNrbOF0T2pbaf+Bq/bjX0AsG13N5QkSZKWrS5HpLYlWQ3Qtts7zCJJkiRJQ+tyROpC4BTgzLa9oMMs8+e6qRGe90xb0A4hPeXkh2ddfW878fbdOND6HXbLnet3Yx+SJEmSdsdERqSSnAt8Dnhyki1J3kDTgToxyfXAie17SZIkSeq9ST2173WzLDphEseXJEmSpPm0KB420bnL2vb+YZ4Kcfks89tbwDYP7uOq3c9055Vtu3L39yFJkiRpt3T5sAlJkiRJWpQckRrGQyNRZ42xk6kHS4yzj0F/NU/7kSRJkjQqR6QkSZIkaUR2pCRJkiRpRF7aJy15h/DQ947tpg3zkmNcn+06gCRJy8SNXQdYFJZvR+rOae1TdrXyqrZ9Rtte17YPznMoSZIkSYuBl/ZJkiRJ0oiW74jU1de3Ex9omsPWN+1+M6y7dkU78aqm2fyn7ftbFySaJEmSpH5zREqSJEmSRmRHSpIkSZJGtDwu7Zv6Pt3rtg3M/PKO63x+aqUV7OSg9mETx07N+P68RZMkSZK0+DgiJUmSJEkjmsiIVJLDgT8HHg/8ENhYVe9KciBwHnAUzQPrX1NVd897gKnBJt6zi5XOmn3Rnb/dTkyNVi2PgTwtXknWAeuad0d0mkVaDHasmf07zSL1nfUiNSY1IvV94I1V9VTgOOC/JnkacDpwSVUdDVzSvpc0pqraWFVrqmoNPK7rOFLv7Vgz+3YdR+o160VqTGRopaq2Alvb6fuSXAscCpwEHN+utgm4FDhtEplG045WbX50+357Z0kkSZIkdW/i90glOQp4FnA5sKrtZE11tg6eZZt1Sa5IcgUP3jGpqJIkSZI0o4ne7JNkP+AvgF+rqnuTDLVdVW0ENgJk5Zoa+oBT90Z9a7ScO9s2rZUkSZK0nE1sRCrJCppO1Aeq6qPt7G1JVrfLV+M1c5IkSZIWgUk9tS/Ae4Frq+rtA4suBE4BzmzbC+b1wNe9tZ14cF53K0mSJGl5m9SlfS8Efgb4hyRXt/N+i6YDdX6SNwA3A6+eUB5JkiRJ2m2TemrfZcBsN0SdMIkM0vJ1F3B21yEkSdKi8d2uAywKfrOsFrnLm2bz8x6eddzKpt1v8mkkSZK0PEz88eeSJEmStNg5IqVF7rPTWuD29U37pElnkSRJ0nLhiJQkSZIkjWhpjUhtnvrC3Mva1seeL30/3TSHHf3wrMO6SSJJkqTlwxEpSZIkSRrRnCNSSVYBh1fVFe37lwM/Any6qq7e5cYTt6Vt/6HTFJqkfZrmoG5TSJIkaXnZZUcqySuA9wN7JvkUcDHwkna7tyV5TVVduPAxJUmSJKk/5hqR2gCsbac/D/xRVb0LIMnJwG8DdqTUobOaZnBs9Kj1TetT+yRJkrRA5rpH6glV9YWq+gLwALB5YNmHgKNn3kySJEmSlq65OlLfSzK1zuaq+sHAsj2BPRYmliRJkiT111yX9l0DPA24pqr+7bRlxwPXLkQoaXjPntYCB3QSRJIkScvILjtSVfXiXSz+OnDqvKaRJEmSpEVgt7+Qt6q+Ouy6SfYGPgM8sj3mR6rqjCQHAucBRwE3Aq+pqrt3N5OWo+ObZu3KTlNIkiRpeZnUF/I+ALy4qp4JHAu8JMlxwOnAJVV1NHBJ+16SJEmSem23R6RGUVUFfKt9u6J9FXASDw0psAm4FDhtqJ1OPe76zsGnr986XlAtQuc1zeZVD8869hVN65f0SpIkaYEM1ZFK8oiq+uE4B0qyB3Alzbf7vLuqLk+yqqq2AlTV1iQHz7LtOmAdAHsfMU4MaRl6gObKWUmS1GsvWj/+Pi6bh31oKHN2pNoO0LeSHFBVD+zugdpHpx+b5ADgY0mOGWHbjcBGgKxcU8DD41tctbuRtCRsb9v7Hp51fydBJEmStIzMeY9U2wH6J+Cx83HAqrqH5hK+lwDbkqwGaNvtu9hUkiRJknph2HukPgBclORdwBaa+5sAqKpPzrVxkscBD1bVPUn2AdYCbwMuBE4BzmzbC0aLL/1y0/jUPkmSJE3QsB2pX2jb9dPmF/DEIbZfDWxqLxN8BHB+VV2U5HPA+UneANwMvHrIPJIkSZLUmaE6UlX1hHEOUlVfBp41w/y7gBPG2bckSZIkTdrQjz9PsgI4Djikqs5L8iiAqvr2QoWT5vblptn8oodnHde2+008jCRJkpaJob6QN8kzaB448afAe9vZ/wp43wLlkiRJkqTeGnZE6j3A71TVOUnubud9mqZjJXVo87QWuH190z5p0lkkSZK0XAzbkXo68P52uqC5pK99Ap+kntnhS6zZv9Ms0mJgzUjDs16kxlCX9gE3As8ZnJHkucAN8x1I0viqamNVramqNbBv13Gk3rNmpOFZL1Jj2BGp/w78dZI/BvZK8mbg54H/vGDJJEmSJKmnhhqRqqqLgJcCj6O5N+pI4N9X1cULmE2SJEmSemmoEakkj62qq4BfXOA8kiRJktR7w94jdUuSC5K8KsleC5pIkiRJknpu2I7UkcAlwOnA7Uk2JnnRHNtIkiRJ0pI07D1Sd1TVH1XVjwHPB7YD5yT5WpLfTXLkgqaUJEmSpB4Z9ql9gx7fvlYCVwGHAl9K8gdVdeZ8hpMkSZKWi29+MWPvY3/OmIckGsawD5t4OnAy8NPAt4BNwL+oqlvb5b8HfBmwIyVJkiRpyRt2ROozwLnAf6iqL0xfWFU3JnnnXDtJsgdwBXBrVb08yYHAecBRNF/6+5qqunvITFpA6+u7sy/LPhNMIkmSJPXPsA+bWF1VvzRTJ2pKVf3OEPv5VeDagfenA5dU1dE8/DALSZIkSeq1oUakqup7SVYBzwUOAjKw7H3D7CPJYcBPAm8FfqOdfRJwfDu9CbgUOG2Y/Wlh7Gokavo6jkxJkiRpuRr2HqlXAu8HrgeeDnwFOAa4DBiqIwW8E3gT8OiBeauqaitAVW1NcvAsx18HrANg7yOGPJwkSZIkLYxh75H6feBnq+rDSe6uqmcl+VmaTtWckrwc2F5VVyY5ftSQVbUR2AiQlWtq1O01t2FGombbxpEpSZIkLTfDdqSOqKoPT5u3Cbgd+M0htn8h8IokLwP2BlYmeT+wLcnqdjRqNc33U0mSJElSrw37sInt7T1SADcmeT7wI8Aew2xcVW+uqsOq6ijgtcAnq+pk4ELglHa1U4ALhk4uSZIkSR0ZtiP1p8CL2ul3AJ8C/h54z5jHPxM4Mcn1wIn4PVSSJEmSFoFhn9r3toHpP09yKfCoqrp29q1m3delNE/no6ruAk4YdR+SJEmS1KVhR6R2UFU3A/+c5OZ5ziNJkiRJvbdbHalWgMPmK4gkSZIkLRbDPrVvNj6KfImYeoT5KI9B97HnkiRJWq7GGZGSJEmSpGVplyNSSc5h9lGnoR59rsVlmJEpR6IkSZK03M11ad8Ncyz/3fkKIkmSJEmLxS47UlW1YVJB1C+OOmlHL5yHfZw43uZ7z0OE+y+ch51cNQ/7kCRpZ/s/cEbXETSCoe6RSnJ1kv+WxKf0SZIkSVr2hn3YxAbgx4Drknw6yX9JcuAC5pIkSZKk3hqqI1VVH6uq1wCrgfcB/w64Jcl8XCcjSZIkSYvKSN8jVVX3JfkgcA+wAnjZgqSSJEmSpB4bqiOVJMCLgdfTjEbdBHwQOHXBkkm768a2/VbbHttRDkmSJC1Zw45I3Ubz39IPAS+sqmsXLpIkSZIk9duwHalXVtXl4xwoyY3AfcAPgO9X1Zr2gRXnAUfRjCO8pqruHuc4Eqxvmjuf1L4/uasgkiRJWqKG6khV1eVJ9geeDOw3bdknRzjev66qOwfenw5cUlVnJjm9fX/aCPuTNIMk64B1zbv9O80iLQbWjDQ860VqDHuP1KnAu2ku7/vOwKICnjjG8U8Cjm+nNwGXYkdKY/uNtl3ZaYouVdVGYCNAckh1HEfqPWtGGp71IjWGvbTvrcB/qKr/O8axCrg4SQF/0hbhqqraClBVW5McPNOGO3zysfcRY0SQJEmSpPEN25HaE7h4zGO9sKpuaztLn0hy3bAb7vDJx8o1fvKhXTuqHYl60q5XkyRJknbXUF/IC7wNeEuSYdffSVXd1rbbgY8BzwW2JVkN0Lbbd3f/kiRJkjQpw3aMfh14C3BfkpsHX8NsnORRSR49NQ38BHANcCFwSrvaKcAFI6WXJEmSpA4Me2nfuM+PXgV8rPleX/YEPlhVH0/yReD8JG8AbgZePeZxJLjx7W3bXtu39hXdZZEkSdKSNOzjzz89zkGq6mvAM2eYfxdwwjj7liRJkqRJG+rSviQrkmxI8rUk97fthiR7LXTAWe3Xvnj2wGtV+9Lydu+0lyRJkjS/hr207w9oHg7x88BNwJHAf6f5op5fX5hokvrjs93v4/55iCBNyN+zYex9/Fn9j7G2f2fePHYGLht/F7xo/TzsRL319EPgL9aPtYu3PfmXx47x3Pyfsba/dOwEsH6oJwfsWo44Y/ydaGKG7Ui9GnhmeykewFeTXAX8PV11pI6dmhi4/2Xzle3EX004jCRJkqTlZNin9mXE+ZIkSZK0ZA07IvVh4K+SbKB5ut6RNI9DP3+hgmlSntG2S/HJdiu6DiBJkqQlatiO1JtoOk7vBg4BbgU+BPz+AuWSJEmSpN4a9vHn3wN+p31pSTmqadY6eiNJkiQNa5f3SCV5YZK3zbLszCTHLUwsSZIkSeqvuR428VvAZ2ZZ9mngt+c3jiRJkiT131yX9h0LfHyWZZ8A3ju/cdRov1T42F94eNbV29qJ97Tt+nadMQ+195jbS5IkScvQXCNSK4G9Zlm2Anj0/MaRJEmSpP6ba0TqOuAngAtmWPYT7fIeOaxtpx7p/Q9tu7Jtj5xsnN3WjkgdNMO8qZ9t6kfdYR1JkiRJkzBXR+odwJ8k2QP4y6r6YZJHAK+keRT6byx0QEmSJEnqm112pKrqg0keD2wCHpnkTpoxkPuBM6rq3GEPlOQA4CzgGKCAnwO+CpxH8wzuG4HXVNXdo/8YrbVTozavaprNUwNm7SjO2hN3e9edWzs18aouU0iSJEli7nukqKq3A4cC/xb4zbY9rKreMeKx3gV8vKqeAjwTuBY4Hbikqo4GLmnfS5IkSVKvDfuFvPcCf7u7B0myEvhx4NR2f98DvpfkJOD4drVNwKXAabt7nJ29qW39sllJkiRJ82fOEal58kTgDuDPknwpyVlJHgWsqqqtAG178EwbJ1mX5IokV/DgHROKLEmSJEkzG2pEap6O82zgl6vq8iTvYoTL+KpqI7ARICvX1NBHXetIlCRJkqT5N6mO1BZgS1Vd3r7/CE1HaluS1VW1NclqYPuE8kiStGCeyRnj7yQPjLmD9eNneNH4u9AS95Xb4Cnrx9rFaTx2HoLMQ82NacMRXSfQpE3k0r6quh24JcmT21knAP8IXAic0s47hZm/r0qSJEmSemVSI1IAvwx8IMlewNeAn6XpyJ2f5A3AzcCrJ5hHkiRJknbLxDpSVXU1sGaGRSdMKoMkSZIkzYdJPbVPkiRJkpYMO1KSJEmSNCI7UpIkSZI0IjtSkiRJkjSiST61T9KEJFkHrGve7d9pFmkxsGak4VkvUsMRKWkJqqqNVbWmqtbAvl3HkXrPmpGGZ71IDTtSkiRJkjQiO1KSJEmSNCI7UpIkSZI0IjtSkiRJkjQiO1KSJEmSNCI7UpIkSZI0IjtSkiRJkjSiiXSkkjw5ydUDr3uT/FqSA5N8Isn1bfuYSeSRJEmSpHFMpCNVVV+tqmOr6ljgOcB3gI8BpwOXVNXRwCXte0mSJEnqtS4u7TsB+Oequgk4CdjUzt8EvLKDPJIkSZI0ki46Uq8Fzm2nV1XVVoC2PXimDZKsS3JFkit48I4JxZQkSZKkmU20I5VkL+AVwIdH2a6qNlbVmqpaw4rHLUw4SZIkSRrSpEekXgpcVVXb2vfbkqwGaNvtE84jSZIkSSObdEfqdTx8WR/AhcAp7fQpwAUTziNJkiRJI5tYRyrJvsCJwEcHZp8JnJjk+nbZmZPKI0mSJEm7a89JHaiqvgM8dtq8u2ie4idJkiRJi0YXT+2TJEmSpEXNjpQkSZIkjciOlCRJkiSNyI6UJEmSJI3IjpQkSZIkjciOlCRJkiSNyI6UJEmSJI3IjpQkSZIkjciOlCRJkiSNyI6UJEmSJI3IjpQkSZIkjciOlCRJkiSNyI6UJEmSJI1oYh2pJL+e5CtJrklybpK9kxyY5BNJrm/bx0wqjyRJkiTtrol0pJIcCvwKsKaqjgH2AF4LnA5cUlVHA5e07yVJkiSp1yZ5ad+ewD5J9gT2BW4DTgI2tcs3Aa+cYB5pyUqyLskVSa6A73QdR+o9a0YanvUiNSbSkaqqW4E/BG4GtgLfrKqLgVVVtbVdZytw8Ezb71CwD94xicjSolZVG6tqTVWtaT63kLQr1ow0POtFakzq0r7H0Iw+PQE4BHhUkpOH3X6Hgl3xuIWKKUmSJElDmdSlfWuBr1fVHVX1IPBR4AXAtiSrAdp2+4TySJIkSdJum1RH6mbguCT7JglwAnAtcCFwSrvOKcAFE8ojSZIkSbttz0kcpKouT/IR4Crg+8CXgI3AfsD5Sd5A09l69STySJIkSdI4JtKRAqiqM4Azps1+gGZ0SpIkSZIWjUk+/lySJEmSlgQ7UpIkSZI0olRV1xlGkuQO4NvAnV1nGcFBmHchLbW8R1bVvD3nv62Zm8bIMyl9yNGHDGCOUTNYM8s7A/QjRx8yQP9+xwyTaRL6kAH6kaMPGWDx5Ji1ZhZdRwogyRXNl8AtDuZdWOYdT1/y9CFHHzKYo38ZputLpj7k6EOGvuToQ4Y+5RjUh0x9yNCXHH3IsFRyeGmfJEmSJI3IjpQkSZIkjWixdqQ2dh1gROZdWOYdT1/y9CFHHzKAOQb1IcN0fcnUhxx9yAD9yNGHDNCfHIP6kKkPGaAfOfqQAZZAjkV5j5QkSZIkdWmxjkhJkiRJUmcWVUcqyUuSfDXJDUlO7zrPdEkOT/KpJNcm+UqSX23nH5jkE0mub9vHdJ11UJI9knwpyUXt+97mTXJAko8kua79c35+z/P+ensuXJPk3CR79ylv1zU1W810ZXotdHD8nc7vjnLsdN5O6LjvS7I9yTUD86yXHTP0pma6rpc2gzVjzcyVwZrZMcOyrZmFqJdF05FKsgfwbuClwNOA1yV5WrepdvJ94I1V9VTgOOC/thlPBy6pqqOBS9r3ffKrwLUD7/uc913Ax6vqKcAzaXL3Mm+SQ4FfAdZU1THAHsBr6UnentTUbDXTlem1MGkznd8TtYvzdhLOBl4ybZ71sqM+1UzX9QLWzNlYM3OxZna0nGvmbOa5XhZNRwp4LnBDVX2tqr4HfAg4qeNMO6iqrVV1VTt9H83JeShNzk3tapuAV3aTcGdJDgN+EjhrYHYv8yZZCfw48F6AqvpeVd1DT/O29gT2SbInsC9wG/3J23lN7aJmJm6WWpjk8Wc7v7sw03m74KrqM8A3ps22Xgb0pWa6rpc2gzVjzczJmtkhw7KumYWol8XUkToUuGXg/RY6+g/XMJIcBTwLuBxYVVVboSlo4ODuku3kncCbgB8OzOtr3icCdwB/1g6Nn5XkUfQ0b1XdCvwhcDOwFfhmVV1Mf/L2qqam1UwXZqqFSZrt/J6oXZy3XbFeZtFxzXRdL2DNzMaamYU1Y83MYKx6WUwdqcwwr5ePHEyyH/AXwK9V1b1d55lNkpcD26vqyq6zDGlP4NnAe6rqWcC36cllfDNpr7M9CXgCcAjwqCQnd5tqB72pqa5rpie10IvzexGct13pTb1AtzXTk3oBa6bvrJmHj23NDFhKNbOYOlJbgMMH3h/GhIbOR5FkBU2hfqCqPtrO3pZkdbt8NbC9q3zTvBB4RZIbaYbcX5zk/fQ37xZgS1VNfZL0EZp/EPqady3w9aq6o6oeBD4KvID+5O1FTc1SM5M2Wy1M0mzn96TNdt52xXqZpgc104d6AWtmNtbMNNbMQ6yZnY1VL4upI/VF4OgkT0iyF81NaRd2nGkHSUJz3em1VfX2gUUXAqe006cAF0w620yq6s1VdVhVHUXz5/nJqjqZ/ua9HbglyZPbWScA/0hP89IMWR+XZN/23DiB5trsvuTtvKZ2UTMTtYtamGSG2c7vSZvtvO2K9TKgDzXTh3ppc1gzM7NmBlgzO+SwZnY2Xr1U1aJ5AS8D/gn4Z+C3u84zQ74X0Qxbfxm4un29DHgszZNArm/bA7vOOkP244GL2une5gWOBa5o/4z/EnhMz/NuAK4DrgHOAR7Zp7xd19RsNdPx39lDtdDBsXc6vzvKsdN5O6HjnktzvfyDNJ+cvsF62SlDr2qmy3ppj2/NWDNzZbBmdjz+sq2ZhaiXtDuWJEmSJA1pMV3aJ0mSJEm9YEdKkiRJkkZkR0qSJEmSRmRHSpIkSZJGZEdKkiRJkkZkR0qSJEmSRmRHqueS3Jhk7QSOs36ub9lO8qIkf5fkm0m+keSzSX6sXXZqkstGON5RSSrJnuNm19I1qfNfWiqsGWl41ovGZUdKQ0myErgI+N/AgcChNF+m9kCXuaSFNExH3w8DpIdZM9LwrJfFz47UIjI16pPkD5PcneTrSV46sPzSJP8zyRfaUaMLkhzYLjs+yZZp+7sxydokLwF+C/ipJN9K8vczHP5HAarq3Kr6QVV9t6ourqovJ3kq8MfA89vt72n3/5NJvpTk3iS3JFk/sL/PtO097TbPb7f5uSTXtj/f3yY5cn7+9LSUJHlMkouS3NGeKxclOaxd9uokV05b/41J/rKdfmRbQzcn2Zbkj5Ps0y47PsmWJKcluR34sxmOfWo7GvuOJN8A1rfzZzx303hHku1tXX45yTHtsrPb438iyX1JPj14zid5QZIvttt9MckLBpZdmuT32iz3Jbk4yUHtsr2TvD/JXUnuabdd1S7bP8l7k2xNcmuS30+yxzz+9aiHrBlrRsOzXqyXYdmRWnyeB3wVOAj4A+C9STKw/D8CPwccAnwf+KO5dlhVHwf+B3BeVe1XVc+cYbV/An6QZFOSlyZ5zMD21wI/D3yu3f6AdtG32zwHAD8J/EKSV7bLfrxtD2i3+Vy77LeAfw88Dvh/wLlz5dey9AiaX0BHAkcA3wX+T7vsQuAJaTr4U04Gzmmn30bzwcCxwJO1Wz8yAAAFfklEQVRoRld/Z2Ddx9OMuh4JrJvl+M8DvgYcDLx1jnP3J2jO9x+lqYWfAu4a2NdPA79HU9NXAx8ASPMhyF/T1PBjgbcDf53ksQPbvh742TbHXsBvtvNPAfYHDm+3/fn2zwhgE82/DU8CntXm+0+z/JxaOqyZhjWjYVgvDetlLlXlq8cv4EZgbTt9KnDDwLJ9gQIe376/FDhzYPnTgO8BewDHA1t2se/1wPvnyPJU4GxgC02RXAisGsh22RzbvxN4Rzt9VJt9z4Hl/xd4w8D7RwDfAY7s+u/BVzevwXN0jvWOBe4eeP8e4K3t9NOBu4FHAqHp4P/IwLrPB77eTh/f1szeuzjWqcDN0+bNeu4CL6b5IOI44BHTtjsb+NDA+/2AH9D8cvoZ4AvT1v8ccGo7fSnwloFlvwh8vJ3+OeDvgH8xbftVNJfj7jMw73XAp7r+u/Y1Py9rxprxNfzLerFexn05IrX43D41UVXfaSf3G1h+y8D0TcAKmk8hxlZV11bVqVV1GHAMzajXO2dbP8nzknyqHRr/Js0nFrvKciTwrnaY+B7gGzT/KB06H/m1dCTZN8mfJLkpyb00l4oeMHD5wCbg9e1o7c8A51fVAzSf5O0LXDlwnn28nT/ljqq6f44It0x7P+u5W1WfpPkk893AtiQb09xzuNO+qupb7baHtK+bph3nJnash9sHpr/Dw/8WnAP8LfChJLcl+YMkK9qcK4CtA1n/hObTRi1h1sxDrBnNyXp5iPUyBztSS8/hA9NHAA8Cd9J8QrLv1IL2H4PBwq5RDlJV19F80nHMLrb/IM2o1eFVtT/NfVTZxfq3AP+lqg4YeO1TVX83SjYtC28Engw8r6pW8vClogGoqs/TfOr3L2kuTZi65OJOmssPnj5wju1fVYMfRgxTC9PX2eW5W1V/VFXPofnk8keB/zaw7UM1m2Q/mks+bmtf0+8RPAK4dc5wVQ9W1YaqehrwAuDlNJfZ3kLzaeFBAzlXVtXTh/iZtbhZM7sKZ81oR9bLrsJZLw+xI7X0nJzkaUn2BX4X+EhV/YBm2HfvNA+AWAG8hWYYeso24KgkM54TSZ6S5mbKqZstD6cZrv38wPaHJdlrYLNHA9+oqvuTPJfmH5spdwA/BJ44MO+PgTcneXp7jP2TvHp3/hC0pKxob2ydeu1Jc259l+ZhJQcCZ8yw3Z/TfEr3/aq6DKCqfgj8KfCOJAcDJDk0yb8ZM+Os526SH2tHZ1fQfKBxP82lFVNeluarBfaiuY798qq6Bfgb4EeTvD7Jnkl+iuZy3YvmCpPkXyd5RvuByb00H6j8oKq2AhcD/yvJyiSPSPIjSf7VmD+/+sWasWY0POvFetltdqSWnnNoRopuB/YGfgWgqr5Jc33rWTSfNnyb5l6nKR9u27uSXDXDfu+jufnx8iTfpulAXUPzqQ3AJ4GvALcnubOd94vA7ya5j+ZGy/OndtZelvhW4LPt8O9xVfUxmps0P9QOpV8DPPRUQi1bf0PzC23qtZ7mktJ9aD79+zzNpRPTnUMzYnrOtPmnATcAn2/Ps800nzzutjnO3ZU0v1jvprls4i7gDwc2/yDNL+lvAM+huTGYqrqL5lO+N7bbvAl4eVXdydweD3yE5hfctcCnganvifuPNDcN/2Ob6SPA6lF/ZvWaNWPNaHjWi/Wy21I10hVd6rEkl9I8MOKsrrNIXUvzuNntwLOr6vqu88wkydk0D4F5S9dZJGtGGp71InBEStLS9QvAF/v6C07qIWtGGp71Ivy2ZElLTpIbaW4KfuUcq0rCmpFGYb1oipf2SZIkSdKIvLRPkiRJkkZkR0qSJEmSRmRHSpIkSZJGZEdKkiRJkkZkR0qSJEmSRmRHSpIkSZJG9P8Bf5mwdzG5SecAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "obs = np.load('obs_learning.npy')\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))\n",
    "\n",
    "cols = ['{}'.format(col) for col in ['Input State', 'Layer response', 'Layer response', 'Layer response']]\n",
    "rows = ['ConvLayer {}'.format(row) for row in range(1, 4)]\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i, 0].imshow(obs[:, :, 0], vmin=0., vmax=1., cmap='jet')\n",
    "\n",
    "for i, kernel in enumerate([0, 7, 15]):\n",
    "    axes[0, 1+i].imshow(pred_1[0, :, :, kernel], cmap='jet')\n",
    "\n",
    "for i, kernel in enumerate([1, 6, 12]):\n",
    "    axes[1, 1+i].imshow(pred_2[0, :, :, kernel], cmap='jet')\n",
    "\n",
    "for i, kernel in enumerate([7, 11, 12]):\n",
    "    axes[2, 1+i].imshow(pred_3[0, :, :, kernel], cmap='jet')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size='large')\n",
    "\n",
    "plt.setp(axes[-1, 1:], xlabel='Layer response')\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    axes[-1, i].set_xlabel(col, size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('layer_respones.png', dpi=300)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f18206d4e50>"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred_2[0, :, :, 22], cmap='jet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f1c6a40f490>"
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1440 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "shape = 21\n",
    "height = 4\n",
    "width = 4\n",
    "activations = np.zeros((shape*height, shape*width))\n",
    "k = 0\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        activations[shape*i:shape*(i+1), shape*j:shape*(j+1)] = pred_1[0, :, :, k]\n",
    "        k += 1\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(activations, cmap='jet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f1b844c8970>"
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1440 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "shape = 11\n",
    "height = 4\n",
    "width = 8\n",
    "activations = np.zeros((shape*height, shape*width))\n",
    "k = 0\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        activations[shape*i:shape*(i+1), shape*j:shape*(j+1)] = pred_2[0, :, :, k]\n",
    "        k += 1\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(activations, cmap='jet')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "tmp = np.zeros((21, 21))\n",
    "for i in range(16):\n",
    "    tmp += pred_1[0, :, :, i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f180d12a9a0>"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tmp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}