{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from scouting_gym.tasks.scouting_discrete_task import ScoutingDiscreteTask"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Register Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1612903045.619433, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1612903045.622824, 0.000000]: Start Init ControllersConnection\n",
      "[WARN] [1612903045.623587, 0.000000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box(0.0, 1.0, (84, 84, 4), float32)\n",
      "Action Space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Scouting-v0')\n",
    "\n",
    "print(\"Observation Space: {}\".format(env.observation_space))\n",
    "print(\"Action Space: {}\".format(env.action_space))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Environment State"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(1):\n",
    "    obs, _, _, _ = env.step(action=2)\n",
    "plt.imshow(obs[:, :, 0])\n",
    "print(obs.min())\n",
    "print(obs.max())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ray Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": ScoutingDiscreteTask,  # or \"corridor\" if registered above\n",
    "    \"env_config\": {\n",
    "        \"corridor_length\": 5,\n",
    "    },\n",
    "    \"num_gpus\": 1,\n",
    "    \"num_workers\": 1\n",
    "}\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 8000,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-09 21:37:28,968\tINFO services.py:1171 -- View the Ray dashboard at \u001B[1m\u001B[32mhttp://127.0.0.1:8265\u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'node_ip_address': '192.168.178.60',\n 'raylet_ip_address': '192.168.178.60',\n 'redis_address': '192.168.178.60:6379',\n 'object_store_address': '/tmp/ray/session_2021-02-09_21-37-28_430078_3339550/sockets/plasma_store',\n 'raylet_socket_name': '/tmp/ray/session_2021-02-09_21-37-28_430078_3339550/sockets/raylet',\n 'webui_url': '127.0.0.1:8265',\n 'session_dir': '/tmp/ray/session_2021-02-09_21-37-28_430078_3339550',\n 'metrics_export_port': 55103,\n 'node_id': 'f20bec9fcba9e385373b555fed1168d877e8dc0a'}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(stop_criteria, config, restorepath):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(PPOTrainer, config=config,\n",
    "                            stop=stop_criteria,\n",
    "                            checkpoint_freq=1,\n",
    "                            checkpoint_at_end=True,\n",
    "                            restore=restorepath)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean',\n",
    "                                                       )\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "def load(checkpoint_path, config):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing a trained agent.\n",
    "    :param path: Path pointing to the agent's saved checkpoint (only used for RLlib agents)\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config)\n",
    "    agent.restore(checkpoint_path)\n",
    "    return agent\n",
    "\n",
    "def test(agent, env):\n",
    "    \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-09 21:37:29,996\tINFO trainable.py:72 -- Checkpoint size is 24109446 bytes\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:33,050\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:33,050\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m [ERROR] [1612903056.157866, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m [WARN] [1612903056.160999, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m [WARN] [1612903056.161887, 0.000000]: END Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:44,110\tINFO trainable.py:99 -- Trainable.setup took 11.060 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:44,111\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:44,296\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-09_21-37-29/PPO_ScoutingDiscreteTask_a0e0c_00000_0_2021-02-09_21-37-29/tmpk4_5m957restore_from_object/checkpoint-201\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m 2021-02-09 21:37:44,296\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 201, '_timesteps_total': None, '_time_total': 109850.49397397041, '_episodes_total': 4999}\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m 2021-02-09 21:37:45,575\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 9.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=3339694)\u001B[0m None\n",
      "\u001B[2m\u001B[36m(pid=3339696)\u001B[0m None\n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_21-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 124.54838709677419\n",
      "  episode_reward_max: 118.38444732621318\n",
      "  episode_reward_mean: 76.36165389001842\n",
      "  episode_reward_min: -96.52579843274803\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5030\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36332884430885315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.042046040296554565\n",
      "        model: {}\n",
      "        policy_loss: -0.10939115285873413\n",
      "        total_loss: 634.2264404296875\n",
      "        vf_explained_var: 0.745877742767334\n",
      "        vf_loss: 634.3275146484375\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.54005102040817\n",
      "    ram_util_percent: 37.63928571428571\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07231025152342285\n",
      "    mean_env_wait_ms: 123.03301668917945\n",
      "    mean_inference_ms: 1.6323711478212601\n",
      "    mean_raw_obs_processing_ms: 9.424384848173483\n",
      "  time_since_restore: 548.8673896789551\n",
      "  time_this_iter_s: 548.8673896789551\n",
      "  time_total_s: 110399.36136364937\n",
      "  timers:\n",
      "    learn_throughput: 362.336\n",
      "    learn_time_ms: 11039.478\n",
      "    load_throughput: 7795.731\n",
      "    load_time_ms: 513.101\n",
      "    sample_throughput: 7.447\n",
      "    sample_time_ms: 537142.284\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1612903613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_21-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 122.93846153846154\n",
      "  episode_reward_max: 118.39206301338692\n",
      "  episode_reward_mean: 68.52947878681303\n",
      "  episode_reward_min: -104.87960383482677\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 5064\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.34911099076271057\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03168858587741852\n",
      "        model: {}\n",
      "        policy_loss: -0.09427706152200699\n",
      "        total_loss: 658.9390258789062\n",
      "        vf_explained_var: 0.730173647403717\n",
      "        vf_loss: 659.0238647460938\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.69817232375979\n",
      "    ram_util_percent: 39.63590078328981\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07166780838421083\n",
      "    mean_env_wait_ms: 122.04923723003078\n",
      "    mean_inference_ms: 1.6251301585110507\n",
      "    mean_raw_obs_processing_ms: 9.665621523277947\n",
      "  time_since_restore: 1085.5322859287262\n",
      "  time_this_iter_s: 536.6648962497711\n",
      "  time_total_s: 110936.02625989914\n",
      "  timers:\n",
      "    learn_throughput: 367.347\n",
      "    learn_time_ms: 10888.898\n",
      "    load_throughput: 8871.523\n",
      "    load_time_ms: 450.881\n",
      "    sample_throughput: 7.529\n",
      "    sample_time_ms: 531267.91\n",
      "    update_time_ms: 3.359\n",
      "  timestamp: 1612904149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 203\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-04-34\n",
      "  done: false\n",
      "  episode_len_mean: 117.39\n",
      "  episode_reward_max: 118.39206301338692\n",
      "  episode_reward_mean: 64.10243533848113\n",
      "  episode_reward_min: -106.57407954938027\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 5100\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3777538239955902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027607670053839684\n",
      "        model: {}\n",
      "        policy_loss: -0.10011913627386093\n",
      "        total_loss: 435.5245666503906\n",
      "        vf_explained_var: 0.8306372165679932\n",
      "        vf_loss: 435.6123046875\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.75882352941177\n",
      "    ram_util_percent: 39.59933155080214\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07122704890654502\n",
      "    mean_env_wait_ms: 121.05374739026807\n",
      "    mean_inference_ms: 1.6194300748837451\n",
      "    mean_raw_obs_processing_ms: 9.87178882279333\n",
      "  time_since_restore: 1609.8577284812927\n",
      "  time_this_iter_s: 524.3254425525665\n",
      "  time_total_s: 111460.3517024517\n",
      "  timers:\n",
      "    learn_throughput: 369.102\n",
      "    learn_time_ms: 10837.126\n",
      "    load_throughput: 9280.505\n",
      "    load_time_ms: 431.011\n",
      "    sample_throughput: 7.616\n",
      "    sample_time_ms: 525203.062\n",
      "    update_time_ms: 3.482\n",
      "  timestamp: 1612904674\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 123.43\n",
      "  episode_reward_max: 118.39973639751503\n",
      "  episode_reward_mean: 66.0876026378999\n",
      "  episode_reward_min: -106.57407954938027\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5128\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.35615918040275574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023637687787413597\n",
      "        model: {}\n",
      "        policy_loss: -0.0918046087026596\n",
      "        total_loss: 420.1271057128906\n",
      "        vf_explained_var: 0.7704351544380188\n",
      "        vf_loss: 420.2029724121094\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.09586666666666\n",
      "    ram_util_percent: 39.641866666666665\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07057611455017486\n",
      "    mean_env_wait_ms: 119.93295846473444\n",
      "    mean_inference_ms: 1.6119281064498503\n",
      "    mean_raw_obs_processing_ms: 9.98447127191076\n",
      "  time_since_restore: 2135.1983437538147\n",
      "  time_this_iter_s: 525.340615272522\n",
      "  time_total_s: 111985.69231772423\n",
      "  timers:\n",
      "    learn_throughput: 369.739\n",
      "    learn_time_ms: 10818.441\n",
      "    load_throughput: 9527.338\n",
      "    load_time_ms: 419.844\n",
      "    sample_throughput: 7.657\n",
      "    sample_time_ms: 522418.482\n",
      "    update_time_ms: 3.412\n",
      "  timestamp: 1612905199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-22-08\n",
      "  done: false\n",
      "  episode_len_mean: 123.84\n",
      "  episode_reward_max: 118.39973639751503\n",
      "  episode_reward_mean: 61.681028557918864\n",
      "  episode_reward_min: -106.57407954938027\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5159\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3658977448940277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01905224286019802\n",
      "        model: {}\n",
      "        policy_loss: -0.09183119237422943\n",
      "        total_loss: 595.0711059570312\n",
      "        vf_explained_var: 0.7773296236991882\n",
      "        vf_loss: 595.1437377929688\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.19046357615895\n",
      "    ram_util_percent: 39.65470198675497\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07015105436032552\n",
      "    mean_env_wait_ms: 119.19556238202821\n",
      "    mean_inference_ms: 1.6069065178908633\n",
      "    mean_raw_obs_processing_ms: 9.94925014152792\n",
      "  time_since_restore: 2664.37354183197\n",
      "  time_this_iter_s: 529.1751980781555\n",
      "  time_total_s: 112514.86751580238\n",
      "  timers:\n",
      "    learn_throughput: 370.288\n",
      "    learn_time_ms: 10802.416\n",
      "    load_throughput: 9619.207\n",
      "    load_time_ms: 415.835\n",
      "    sample_throughput: 7.67\n",
      "    sample_time_ms: 521515.59\n",
      "    update_time_ms: 3.384\n",
      "  timestamp: 1612905728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-30-48\n",
      "  done: false\n",
      "  episode_len_mean: 128.75\n",
      "  episode_reward_max: 118.39973639751503\n",
      "  episode_reward_mean: 60.061678788807875\n",
      "  episode_reward_min: -106.34109042456174\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5191\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37050333619117737\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020132984966039658\n",
      "        model: {}\n",
      "        policy_loss: -0.08390621095895767\n",
      "        total_loss: 873.0941162109375\n",
      "        vf_explained_var: 0.6204808354377747\n",
      "        vf_loss: 873.15771484375\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.75330634278002\n",
      "    ram_util_percent: 39.67813765182187\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06988480245347484\n",
      "    mean_env_wait_ms: 118.80406370167451\n",
      "    mean_inference_ms: 1.6042129411050277\n",
      "    mean_raw_obs_processing_ms: 9.802837411972774\n",
      "  time_since_restore: 3184.0712366104126\n",
      "  time_this_iter_s: 519.6976947784424\n",
      "  time_total_s: 113034.56521058083\n",
      "  timers:\n",
      "    learn_throughput: 370.84\n",
      "    learn_time_ms: 10786.314\n",
      "    load_throughput: 9658.68\n",
      "    load_time_ms: 414.135\n",
      "    sample_throughput: 7.702\n",
      "    sample_time_ms: 519337.025\n",
      "    update_time_ms: 3.348\n",
      "  timestamp: 1612906248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 207\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-39-33\n",
      "  done: false\n",
      "  episode_len_mean: 126.04\n",
      "  episode_reward_max: 118.39847819203922\n",
      "  episode_reward_mean: 62.08296316352289\n",
      "  episode_reward_min: -106.56819705735226\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5223\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37174192070961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010871256701648235\n",
      "        model: {}\n",
      "        policy_loss: -0.07195281237363815\n",
      "        total_loss: 354.7228698730469\n",
      "        vf_explained_var: 0.8198283314704895\n",
      "        vf_loss: 354.77838134765625\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.418400000000005\n",
      "    ram_util_percent: 39.686933333333336\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06972698767817331\n",
      "    mean_env_wait_ms: 118.5086039457089\n",
      "    mean_inference_ms: 1.602663524639035\n",
      "    mean_raw_obs_processing_ms: 9.731476917810149\n",
      "  time_since_restore: 3709.2126562595367\n",
      "  time_this_iter_s: 525.1414196491241\n",
      "  time_total_s: 113559.70663022995\n",
      "  timers:\n",
      "    learn_throughput: 371.17\n",
      "    learn_time_ms: 10776.742\n",
      "    load_throughput: 9727.782\n",
      "    load_time_ms: 411.193\n",
      "    sample_throughput: 7.714\n",
      "    sample_time_ms: 518558.902\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1612906773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-48-19\n",
      "  done: false\n",
      "  episode_len_mean: 128.89\n",
      "  episode_reward_max: 118.39864965252964\n",
      "  episode_reward_mean: 70.50949181439799\n",
      "  episode_reward_min: -106.89465794440122\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5253\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37147316336631775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012282636016607285\n",
      "        model: {}\n",
      "        policy_loss: -0.08164091408252716\n",
      "        total_loss: 278.131103515625\n",
      "        vf_explained_var: 0.8583352565765381\n",
      "        vf_loss: 278.1941223144531\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.235866666666666\n",
      "    ram_util_percent: 39.6872\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06964719806822074\n",
      "    mean_env_wait_ms: 118.27704351415736\n",
      "    mean_inference_ms: 1.6020470357376946\n",
      "    mean_raw_obs_processing_ms: 9.695159386226496\n",
      "  time_since_restore: 4235.027583360672\n",
      "  time_this_iter_s: 525.8149271011353\n",
      "  time_total_s: 114085.52155733109\n",
      "  timers:\n",
      "    learn_throughput: 371.534\n",
      "    learn_time_ms: 10766.169\n",
      "    load_throughput: 9803.36\n",
      "    load_time_ms: 408.023\n",
      "    sample_throughput: 7.721\n",
      "    sample_time_ms: 518061.246\n",
      "    update_time_ms: 3.35\n",
      "  timestamp: 1612907299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 209\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_22-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 131.11\n",
      "  episode_reward_max: 118.39864965252964\n",
      "  episode_reward_mean: 70.39507968843606\n",
      "  episode_reward_min: -106.89465794440122\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5281\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3926101326942444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016002152115106583\n",
      "        model: {}\n",
      "        policy_loss: -0.09378788620233536\n",
      "        total_loss: 477.3751525878906\n",
      "        vf_explained_var: 0.80803382396698\n",
      "        vf_loss: 477.4446105957031\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50257452574525\n",
      "    ram_util_percent: 39.63929539295393\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06958959115974306\n",
      "    mean_env_wait_ms: 118.12069330087711\n",
      "    mean_inference_ms: 1.6015595647456005\n",
      "    mean_raw_obs_processing_ms: 9.633754942849516\n",
      "  time_since_restore: 4751.792055130005\n",
      "  time_this_iter_s: 516.7644717693329\n",
      "  time_total_s: 114602.28602910042\n",
      "  timers:\n",
      "    learn_throughput: 371.645\n",
      "    learn_time_ms: 10762.951\n",
      "    load_throughput: 9893.942\n",
      "    load_time_ms: 404.288\n",
      "    sample_throughput: 7.742\n",
      "    sample_time_ms: 516666.864\n",
      "    update_time_ms: 3.333\n",
      "  timestamp: 1612907816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 134.22\n",
      "  episode_reward_max: 118.39864965252964\n",
      "  episode_reward_mean: 67.8485365884226\n",
      "  episode_reward_min: -106.89465794440122\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5312\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3889402747154236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013187848962843418\n",
      "        model: {}\n",
      "        policy_loss: -0.08308876305818558\n",
      "        total_loss: 260.45416259765625\n",
      "        vf_explained_var: 0.8716606497764587\n",
      "        vf_loss: 260.5172424316406\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.81455938697319\n",
      "    ram_util_percent: 39.69578544061303\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06965433127114633\n",
      "    mean_env_wait_ms: 118.1830953553457\n",
      "    mean_inference_ms: 1.6021898390774285\n",
      "    mean_raw_obs_processing_ms: 9.56672742321996\n",
      "  time_since_restore: 5300.760164260864\n",
      "  time_this_iter_s: 548.9681091308594\n",
      "  time_total_s: 115151.25413823128\n",
      "  timers:\n",
      "    learn_throughput: 371.005\n",
      "    learn_time_ms: 10781.52\n",
      "    load_throughput: 9910.963\n",
      "    load_time_ms: 403.593\n",
      "    sample_throughput: 7.711\n",
      "    sample_time_ms: 518744.742\n",
      "    update_time_ms: 3.368\n",
      "  timestamp: 1612908365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 136.52\n",
      "  episode_reward_max: 118.39864965252964\n",
      "  episode_reward_mean: 68.3022603010643\n",
      "  episode_reward_min: -106.89465794440122\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5342\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.34569627046585083\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010820768773555756\n",
      "        model: {}\n",
      "        policy_loss: -0.06855788826942444\n",
      "        total_loss: 630.2237548828125\n",
      "        vf_explained_var: 0.6394379138946533\n",
      "        vf_loss: 630.2757568359375\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.18577075098814\n",
      "    ram_util_percent: 39.6604743083004\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06974055146883154\n",
      "    mean_env_wait_ms: 118.28900086409611\n",
      "    mean_inference_ms: 1.6030380541280653\n",
      "    mean_raw_obs_processing_ms: 9.509131197731149\n",
      "  time_since_restore: 5832.34010887146\n",
      "  time_this_iter_s: 531.5799446105957\n",
      "  time_total_s: 115682.83408284187\n",
      "  timers:\n",
      "    learn_throughput: 372.109\n",
      "    learn_time_ms: 10749.541\n",
      "    load_throughput: 10262.115\n",
      "    load_time_ms: 389.783\n",
      "    sample_throughput: 7.736\n",
      "    sample_time_ms: 517063.931\n",
      "    update_time_ms: 3.357\n",
      "  timestamp: 1612908897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 128.22\n",
      "  episode_reward_max: 118.39781198685688\n",
      "  episode_reward_mean: 66.15294082150768\n",
      "  episode_reward_min: -106.49626118655056\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5374\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3703306317329407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012528005056083202\n",
      "        model: {}\n",
      "        policy_loss: -0.07957421988248825\n",
      "        total_loss: 550.6865844726562\n",
      "        vf_explained_var: 0.7406470775604248\n",
      "        vf_loss: 550.7471313476562\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.72814614343709\n",
      "    ram_util_percent: 39.710825439783484\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.069828025006639\n",
      "    mean_env_wait_ms: 118.36789338601075\n",
      "    mean_inference_ms: 1.6039770966385931\n",
      "    mean_raw_obs_processing_ms: 9.489333436044129\n",
      "  time_since_restore: 6350.484675168991\n",
      "  time_this_iter_s: 518.1445662975311\n",
      "  time_total_s: 116200.9786491394\n",
      "  timers:\n",
      "    learn_throughput: 372.083\n",
      "    learn_time_ms: 10750.28\n",
      "    load_throughput: 10303.365\n",
      "    load_time_ms: 388.223\n",
      "    sample_throughput: 7.764\n",
      "    sample_time_ms: 515213.645\n",
      "    update_time_ms: 3.346\n",
      "  timestamp: 1612909415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 213\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 132.11\n",
      "  episode_reward_max: 118.39781198685688\n",
      "  episode_reward_mean: 64.26172557382843\n",
      "  episode_reward_min: -106.49626118655056\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5403\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.39859068393707275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016669824719429016\n",
      "        model: {}\n",
      "        policy_loss: -0.09902878105640411\n",
      "        total_loss: 928.1426391601562\n",
      "        vf_explained_var: 0.615606427192688\n",
      "        vf_loss: 928.2164306640625\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.49919246298789\n",
      "    ram_util_percent: 39.75248990578735\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0698164683762275\n",
      "    mean_env_wait_ms: 118.31469458177338\n",
      "    mean_inference_ms: 1.6040230680890095\n",
      "    mean_raw_obs_processing_ms: 9.470671888825322\n",
      "  time_since_restore: 6871.25160741806\n",
      "  time_this_iter_s: 520.7669322490692\n",
      "  time_total_s: 116721.74558138847\n",
      "  timers:\n",
      "    learn_throughput: 372.14\n",
      "    learn_time_ms: 10748.636\n",
      "    load_throughput: 10321.921\n",
      "    load_time_ms: 387.525\n",
      "    sample_throughput: 7.769\n",
      "    sample_time_ms: 514858.597\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1612909936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 125.82\n",
      "  episode_reward_max: 118.39261575125073\n",
      "  episode_reward_mean: 51.7821219291898\n",
      "  episode_reward_min: -106.49626118655056\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 5436\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40524041652679443\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015745460987091064\n",
      "        model: {}\n",
      "        policy_loss: -0.09811822324991226\n",
      "        total_loss: 1065.9322509765625\n",
      "        vf_explained_var: 0.6816093325614929\n",
      "        vf_loss: 1066.00634765625\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.31822721598002\n",
      "    ram_util_percent: 39.85081148564294\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0698742522387255\n",
      "    mean_env_wait_ms: 118.37591755059104\n",
      "    mean_inference_ms: 1.604894211865946\n",
      "    mean_raw_obs_processing_ms: 9.471197060300153\n",
      "  time_since_restore: 7432.794239044189\n",
      "  time_this_iter_s: 561.5426316261292\n",
      "  time_total_s: 117283.2882130146\n",
      "  timers:\n",
      "    learn_throughput: 372.191\n",
      "    learn_time_ms: 10747.169\n",
      "    load_throughput: 10300.25\n",
      "    load_time_ms: 388.34\n",
      "    sample_throughput: 7.715\n",
      "    sample_time_ms: 518478.231\n",
      "    update_time_ms: 3.35\n",
      "  timestamp: 1612910497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 215\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 131.62\n",
      "  episode_reward_max: 118.39261575125073\n",
      "  episode_reward_mean: 49.6241970143333\n",
      "  episode_reward_min: -106.4034330093584\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5464\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3823293447494507\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015181094408035278\n",
      "        model: {}\n",
      "        policy_loss: -0.08965900540351868\n",
      "        total_loss: 831.5872192382812\n",
      "        vf_explained_var: 0.6600337624549866\n",
      "        vf_loss: 831.65380859375\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.07386666666667\n",
      "    ram_util_percent: 39.76026666666666\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06992486989123056\n",
      "    mean_env_wait_ms: 118.47425768232324\n",
      "    mean_inference_ms: 1.6055897994156567\n",
      "    mean_raw_obs_processing_ms: 9.451918397147683\n",
      "  time_since_restore: 7958.1811537742615\n",
      "  time_this_iter_s: 525.386914730072\n",
      "  time_total_s: 117808.67512774467\n",
      "  timers:\n",
      "    learn_throughput: 372.238\n",
      "    learn_time_ms: 10745.812\n",
      "    load_throughput: 10350.442\n",
      "    load_time_ms: 386.457\n",
      "    sample_throughput: 7.721\n",
      "    sample_time_ms: 518100.992\n",
      "    update_time_ms: 3.366\n",
      "  timestamp: 1612911023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-09_23-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 136.06\n",
      "  episode_reward_max: 118.39261575125073\n",
      "  episode_reward_mean: 58.01018434973405\n",
      "  episode_reward_min: -106.4034330093584\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5494\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3760596513748169\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01462327316403389\n",
      "        model: {}\n",
      "        policy_loss: -0.08527521044015884\n",
      "        total_loss: 749.9008178710938\n",
      "        vf_explained_var: 0.6483868360519409\n",
      "        vf_loss: 749.9640502929688\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.482795698924726\n",
      "    ram_util_percent: 39.79354838709678\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0699708609616078\n",
      "    mean_env_wait_ms: 118.58244688467141\n",
      "    mean_inference_ms: 1.6061777474905967\n",
      "    mean_raw_obs_processing_ms: 9.433881990642373\n",
      "  time_since_restore: 8479.533838272095\n",
      "  time_this_iter_s: 521.3526844978333\n",
      "  time_total_s: 118330.02781224251\n",
      "  timers:\n",
      "    learn_throughput: 372.175\n",
      "    learn_time_ms: 10747.637\n",
      "    load_throughput: 10388.371\n",
      "    load_time_ms: 385.046\n",
      "    sample_throughput: 7.718\n",
      "    sample_time_ms: 518265.303\n",
      "    update_time_ms: 3.397\n",
      "  timestamp: 1612911544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-07-51\n",
      "  done: false\n",
      "  episode_len_mean: 134.66\n",
      "  episode_reward_max: 118.38538584400176\n",
      "  episode_reward_mean: 64.49763012016085\n",
      "  episode_reward_min: -106.4034330093584\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5525\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4021989107131958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01526737306267023\n",
      "        model: {}\n",
      "        policy_loss: -0.10319027304649353\n",
      "        total_loss: 690.8093872070312\n",
      "        vf_explained_var: 0.7575576901435852\n",
      "        vf_loss: 690.8893432617188\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.37363515312916\n",
      "    ram_util_percent: 39.73635153129161\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06995180966212498\n",
      "    mean_env_wait_ms: 118.56926340409095\n",
      "    mean_inference_ms: 1.6059677677790973\n",
      "    mean_raw_obs_processing_ms: 9.41175756760745\n",
      "  time_since_restore: 9005.88105750084\n",
      "  time_this_iter_s: 526.3472192287445\n",
      "  time_total_s: 118856.37503147125\n",
      "  timers:\n",
      "    learn_throughput: 372.171\n",
      "    learn_time_ms: 10747.744\n",
      "    load_throughput: 10413.989\n",
      "    load_time_ms: 384.099\n",
      "    sample_throughput: 7.716\n",
      "    sample_time_ms: 518387.277\n",
      "    update_time_ms: 3.385\n",
      "  timestamp: 1612912071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 126.75\n",
      "  episode_reward_max: 118.3978775443949\n",
      "  episode_reward_mean: 55.807783045953876\n",
      "  episode_reward_min: -106.4034330093584\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5556\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38853704929351807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012983590364456177\n",
      "        model: {}\n",
      "        policy_loss: -0.08526013046503067\n",
      "        total_loss: 635.9664916992188\n",
      "        vf_explained_var: 0.7270710468292236\n",
      "        vf_loss: 636.031982421875\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.3870320855615\n",
      "    ram_util_percent: 39.733556149732614\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0698981121342121\n",
      "    mean_env_wait_ms: 118.48905364589629\n",
      "    mean_inference_ms: 1.605367397565822\n",
      "    mean_raw_obs_processing_ms: 9.39806908709165\n",
      "  time_since_restore: 9530.036677122116\n",
      "  time_this_iter_s: 524.1556196212769\n",
      "  time_total_s: 119380.53065109253\n",
      "  timers:\n",
      "    learn_throughput: 372.06\n",
      "    learn_time_ms: 10750.946\n",
      "    load_throughput: 10330.984\n",
      "    load_time_ms: 387.185\n",
      "    sample_throughput: 7.719\n",
      "    sample_time_ms: 518213.687\n",
      "    update_time_ms: 3.383\n",
      "  timestamp: 1612912595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 219\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-25-14\n",
      "  done: false\n",
      "  episode_len_mean: 135.22\n",
      "  episode_reward_max: 118.3978775443949\n",
      "  episode_reward_mean: 64.57938547340594\n",
      "  episode_reward_min: -104.810264303511\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5583\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3952016830444336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016168497502803802\n",
      "        model: {}\n",
      "        policy_loss: -0.09861519932746887\n",
      "        total_loss: 544.4634399414062\n",
      "        vf_explained_var: 0.7608643770217896\n",
      "        vf_loss: 544.5375366210938\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.39622132253711\n",
      "    ram_util_percent: 39.66045883940621\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06985894344809128\n",
      "    mean_env_wait_ms: 118.42396443227328\n",
      "    mean_inference_ms: 1.604926499104505\n",
      "    mean_raw_obs_processing_ms: 9.379690685295252\n",
      "  time_since_restore: 10048.95786356926\n",
      "  time_this_iter_s: 518.9211864471436\n",
      "  time_total_s: 119899.45183753967\n",
      "  timers:\n",
      "    learn_throughput: 372.177\n",
      "    learn_time_ms: 10747.578\n",
      "    load_throughput: 10105.341\n",
      "    load_time_ms: 395.83\n",
      "    sample_throughput: 7.716\n",
      "    sample_time_ms: 518422.11\n",
      "    update_time_ms: 3.423\n",
      "  timestamp: 1612913114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 128.84\n",
      "  episode_reward_max: 118.3978775443949\n",
      "  episode_reward_mean: 58.169046801264976\n",
      "  episode_reward_min: -104.810264303511\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 5616\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4071144759654999\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013936731033027172\n",
      "        model: {}\n",
      "        policy_loss: -0.09179943799972534\n",
      "        total_loss: 549.5939331054688\n",
      "        vf_explained_var: 0.8084088563919067\n",
      "        vf_loss: 549.6646118164062\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48146666666666\n",
      "    ram_util_percent: 39.63026666666667\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06981069159101434\n",
      "    mean_env_wait_ms: 118.34503471790482\n",
      "    mean_inference_ms: 1.6044976713921455\n",
      "    mean_raw_obs_processing_ms: 9.370286285722248\n",
      "  time_since_restore: 10574.980285644531\n",
      "  time_this_iter_s: 526.0224220752716\n",
      "  time_total_s: 120425.47425961494\n",
      "  timers:\n",
      "    learn_throughput: 372.919\n",
      "    learn_time_ms: 10726.177\n",
      "    load_throughput: 10151.73\n",
      "    load_time_ms: 394.022\n",
      "    sample_throughput: 7.75\n",
      "    sample_time_ms: 516153.522\n",
      "    update_time_ms: 3.405\n",
      "  timestamp: 1612913640\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 221\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 133.24\n",
      "  episode_reward_max: 118.39727598102661\n",
      "  episode_reward_mean: 68.49114500254817\n",
      "  episode_reward_min: -105.11014945249104\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 5645\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3763899505138397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011760563589632511\n",
      "        model: {}\n",
      "        policy_loss: -0.0797865092754364\n",
      "        total_loss: 423.8522644042969\n",
      "        vf_explained_var: 0.7701522707939148\n",
      "        vf_loss: 423.9142150878906\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.3380187416332\n",
      "    ram_util_percent: 39.62744310575636\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06976875578396767\n",
      "    mean_env_wait_ms: 118.28445332356154\n",
      "    mean_inference_ms: 1.6041188911869433\n",
      "    mean_raw_obs_processing_ms: 9.354661437757281\n",
      "  time_since_restore: 11098.15774178505\n",
      "  time_this_iter_s: 523.1774561405182\n",
      "  time_total_s: 120948.65171575546\n",
      "  timers:\n",
      "    learn_throughput: 372.843\n",
      "    learn_time_ms: 10728.375\n",
      "    load_throughput: 10049.343\n",
      "    load_time_ms: 398.036\n",
      "    sample_throughput: 7.762\n",
      "    sample_time_ms: 515308.75\n",
      "    update_time_ms: 3.43\n",
      "  timestamp: 1612914163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_00-51-20\n",
      "  done: false\n",
      "  episode_len_mean: 135.03\n",
      "  episode_reward_max: 118.39727598102661\n",
      "  episode_reward_mean: 58.29012656358039\n",
      "  episode_reward_min: -105.11014945249104\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 5673\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4235011637210846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017769400030374527\n",
      "        model: {}\n",
      "        policy_loss: -0.10206446051597595\n",
      "        total_loss: 1317.242431640625\n",
      "        vf_explained_var: 0.5585214495658875\n",
      "        vf_loss: 1317.3173828125\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.51655359565808\n",
      "    ram_util_percent: 39.67801899592945\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06972738914547522\n",
      "    mean_env_wait_ms: 118.21831596403736\n",
      "    mean_inference_ms: 1.6037760800646228\n",
      "    mean_raw_obs_processing_ms: 9.339964825916008\n",
      "  time_since_restore: 11614.76550936699\n",
      "  time_this_iter_s: 516.6077675819397\n",
      "  time_total_s: 121465.2594833374\n",
      "  timers:\n",
      "    learn_throughput: 372.949\n",
      "    learn_time_ms: 10725.34\n",
      "    load_throughput: 10022.723\n",
      "    load_time_ms: 399.093\n",
      "    sample_throughput: 7.765\n",
      "    sample_time_ms: 515156.135\n",
      "    update_time_ms: 3.456\n",
      "  timestamp: 1612914680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 134.75\n",
      "  episode_reward_max: 118.39604431236562\n",
      "  episode_reward_mean: 66.80768685098683\n",
      "  episode_reward_min: -105.11014945249104\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5704\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3545065224170685\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011419621296226978\n",
      "        model: {}\n",
      "        policy_loss: -0.08402487635612488\n",
      "        total_loss: 552.7821044921875\n",
      "        vf_explained_var: 0.7246250510215759\n",
      "        vf_loss: 552.8487548828125\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.5068456375839\n",
      "    ram_util_percent: 39.78859060402685\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06968143584951142\n",
      "    mean_env_wait_ms: 118.14712304427331\n",
      "    mean_inference_ms: 1.6033716757132896\n",
      "    mean_raw_obs_processing_ms: 9.325239462683353\n",
      "  time_since_restore: 12136.508862257004\n",
      "  time_this_iter_s: 521.7433528900146\n",
      "  time_total_s: 121987.00283622742\n",
      "  timers:\n",
      "    learn_throughput: 372.941\n",
      "    learn_time_ms: 10725.562\n",
      "    load_throughput: 10005.16\n",
      "    load_time_ms: 399.794\n",
      "    sample_throughput: 7.763\n",
      "    sample_time_ms: 515250.88\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1612915202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 136.45\n",
      "  episode_reward_max: 118.39604431236562\n",
      "  episode_reward_mean: 66.8536278195501\n",
      "  episode_reward_min: -101.6939903166946\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5734\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36472344398498535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009808284230530262\n",
      "        model: {}\n",
      "        policy_loss: -0.0694395899772644\n",
      "        total_loss: 240.61196899414062\n",
      "        vf_explained_var: 0.8440701365470886\n",
      "        vf_loss: 240.66650390625\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.26403743315508\n",
      "    ram_util_percent: 39.77299465240641\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06963942636529645\n",
      "    mean_env_wait_ms: 118.08350682208297\n",
      "    mean_inference_ms: 1.602971837956861\n",
      "    mean_raw_obs_processing_ms: 9.310449959286673\n",
      "  time_since_restore: 12660.724661588669\n",
      "  time_this_iter_s: 524.215799331665\n",
      "  time_total_s: 122511.21863555908\n",
      "  timers:\n",
      "    learn_throughput: 373.084\n",
      "    learn_time_ms: 10721.444\n",
      "    load_throughput: 9982.028\n",
      "    load_time_ms: 400.72\n",
      "    sample_throughput: 7.82\n",
      "    sample_time_ms: 511522.553\n",
      "    update_time_ms: 3.58\n",
      "  timestamp: 1612915726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 225\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-17-37\n",
      "  done: false\n",
      "  episode_len_mean: 131.28\n",
      "  episode_reward_max: 118.3790287753994\n",
      "  episode_reward_mean: 66.97272390365706\n",
      "  episode_reward_min: -104.82634161724816\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5766\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.375631183385849\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011179999448359013\n",
      "        model: {}\n",
      "        policy_loss: -0.06664775311946869\n",
      "        total_loss: 483.6998291015625\n",
      "        vf_explained_var: 0.7273108959197998\n",
      "        vf_loss: 483.74945068359375\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.08258575197889\n",
      "    ram_util_percent: 39.843799472295515\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06959631449704855\n",
      "    mean_env_wait_ms: 118.04850477495748\n",
      "    mean_inference_ms: 1.602573168763929\n",
      "    mean_raw_obs_processing_ms: 9.310663928175689\n",
      "  time_since_restore: 13192.02332854271\n",
      "  time_this_iter_s: 531.2986669540405\n",
      "  time_total_s: 123042.51730251312\n",
      "  timers:\n",
      "    learn_throughput: 373.154\n",
      "    learn_time_ms: 10719.447\n",
      "    load_throughput: 9911.329\n",
      "    load_time_ms: 403.579\n",
      "    sample_throughput: 7.811\n",
      "    sample_time_ms: 512113.419\n",
      "    update_time_ms: 3.615\n",
      "  timestamp: 1612916257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 136.18\n",
      "  episode_reward_max: 118.3969949840616\n",
      "  episode_reward_mean: 74.90861563666573\n",
      "  episode_reward_min: -106.34154478137519\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 5793\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4131618142127991\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012094116769731045\n",
      "        model: {}\n",
      "        policy_loss: -0.08072967827320099\n",
      "        total_loss: 189.39382934570312\n",
      "        vf_explained_var: 0.9102923274040222\n",
      "        vf_loss: 189.45616149902344\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.437753721244924\n",
      "    ram_util_percent: 39.762110960757774\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0695676720797715\n",
      "    mean_env_wait_ms: 118.02734232001227\n",
      "    mean_inference_ms: 1.6023155951656958\n",
      "    mean_raw_obs_processing_ms: 9.301950987329011\n",
      "  time_since_restore: 13709.501183748245\n",
      "  time_this_iter_s: 517.4778552055359\n",
      "  time_total_s: 123559.99515771866\n",
      "  timers:\n",
      "    learn_throughput: 373.188\n",
      "    learn_time_ms: 10718.463\n",
      "    load_throughput: 9927.838\n",
      "    load_time_ms: 402.907\n",
      "    sample_throughput: 7.817\n",
      "    sample_time_ms: 511729.483\n",
      "    update_time_ms: 3.597\n",
      "  timestamp: 1612916775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 227\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-35-08\n",
      "  done: false\n",
      "  episode_len_mean: 129.27\n",
      "  episode_reward_max: 118.3969949840616\n",
      "  episode_reward_mean: 68.58974712664153\n",
      "  episode_reward_min: -106.34154478137519\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 5829\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37948763370513916\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011703908443450928\n",
      "        model: {}\n",
      "        policy_loss: -0.08630841225385666\n",
      "        total_loss: 596.18408203125\n",
      "        vf_explained_var: 0.7297237515449524\n",
      "        vf_loss: 596.2526245117188\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.27039473684211\n",
      "    ram_util_percent: 39.720263157894735\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06953961767146864\n",
      "    mean_env_wait_ms: 118.01028946630076\n",
      "    mean_inference_ms: 1.6019961694261133\n",
      "    mean_raw_obs_processing_ms: 9.313539998185963\n",
      "  time_since_restore: 14242.50863146782\n",
      "  time_this_iter_s: 533.007447719574\n",
      "  time_total_s: 124093.00260543823\n",
      "  timers:\n",
      "    learn_throughput: 373.278\n",
      "    learn_time_ms: 10715.887\n",
      "    load_throughput: 9917.827\n",
      "    load_time_ms: 403.314\n",
      "    sample_throughput: 7.806\n",
      "    sample_time_ms: 512397.608\n",
      "    update_time_ms: 3.597\n",
      "  timestamp: 1612917308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-43-53\n",
      "  done: false\n",
      "  episode_len_mean: 127.5\n",
      "  episode_reward_max: 118.39842550449733\n",
      "  episode_reward_mean: 64.49581955380867\n",
      "  episode_reward_min: -106.34154478137519\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 5861\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4086587131023407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014609979465603828\n",
      "        model: {}\n",
      "        policy_loss: -0.0944264754652977\n",
      "        total_loss: 1178.078125\n",
      "        vf_explained_var: 0.5142092108726501\n",
      "        vf_loss: 1178.1502685546875\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.450200267022694\n",
      "    ram_util_percent: 39.72016021361816\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06952045427144717\n",
      "    mean_env_wait_ms: 117.97885140882182\n",
      "    mean_inference_ms: 1.6018020253241037\n",
      "    mean_raw_obs_processing_ms: 9.32470450554547\n",
      "  time_since_restore: 14766.96776175499\n",
      "  time_this_iter_s: 524.4591302871704\n",
      "  time_total_s: 124617.4617357254\n",
      "  timers:\n",
      "    learn_throughput: 373.405\n",
      "    learn_time_ms: 10712.234\n",
      "    load_throughput: 10025.687\n",
      "    load_time_ms: 398.975\n",
      "    sample_throughput: 7.806\n",
      "    sample_time_ms: 512438.392\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1612917833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_01-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 121.54\n",
      "  episode_reward_max: 118.39842550449733\n",
      "  episode_reward_mean: 60.63521446616135\n",
      "  episode_reward_min: -105.96584787415419\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 5892\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.404553085565567\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015702731907367706\n",
      "        model: {}\n",
      "        policy_loss: -0.10384644567966461\n",
      "        total_loss: 860.2108154296875\n",
      "        vf_explained_var: 0.6867095232009888\n",
      "        vf_loss: 860.2908935546875\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.284441489361704\n",
      "    ram_util_percent: 39.76914893617022\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06950653453007911\n",
      "    mean_env_wait_ms: 117.96080810603274\n",
      "    mean_inference_ms: 1.6015930152701514\n",
      "    mean_raw_obs_processing_ms: 9.346786389571802\n",
      "  time_since_restore: 15294.35606098175\n",
      "  time_this_iter_s: 527.3882992267609\n",
      "  time_total_s: 125144.85003495216\n",
      "  timers:\n",
      "    learn_throughput: 373.482\n",
      "    learn_time_ms: 10710.018\n",
      "    load_throughput: 10220.335\n",
      "    load_time_ms: 391.377\n",
      "    sample_throughput: 7.793\n",
      "    sample_time_ms: 513296.68\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1612918360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 124.96\n",
      "  episode_reward_max: 118.39842550449733\n",
      "  episode_reward_mean: 62.67740543385038\n",
      "  episode_reward_min: -107.4490627888949\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 5926\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3975917398929596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012994581833481789\n",
      "        model: {}\n",
      "        policy_loss: -0.08559347689151764\n",
      "        total_loss: 500.3450012207031\n",
      "        vf_explained_var: 0.7574440240859985\n",
      "        vf_loss: 500.41082763671875\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.38395225464191\n",
      "    ram_util_percent: 39.84045092838196\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06948767046337818\n",
      "    mean_env_wait_ms: 117.93805023447071\n",
      "    mean_inference_ms: 1.6013970974639238\n",
      "    mean_raw_obs_processing_ms: 9.36442021256352\n",
      "  time_since_restore: 15822.67797923088\n",
      "  time_this_iter_s: 528.3219182491302\n",
      "  time_total_s: 125673.1719532013\n",
      "  timers:\n",
      "    learn_throughput: 373.644\n",
      "    learn_time_ms: 10705.388\n",
      "    load_throughput: 10182.382\n",
      "    load_time_ms: 392.835\n",
      "    sample_throughput: 7.789\n",
      "    sample_time_ms: 513531.612\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1612918889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 231\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 120.3\n",
      "  episode_reward_max: 118.39951469513582\n",
      "  episode_reward_mean: 68.59211049811397\n",
      "  episode_reward_min: -107.4490627888949\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 5960\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37688902020454407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00973469577729702\n",
      "        model: {}\n",
      "        policy_loss: -0.07460609078407288\n",
      "        total_loss: 343.27105712890625\n",
      "        vf_explained_var: 0.7893316745758057\n",
      "        vf_loss: 343.3307800292969\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.73122476446837\n",
      "    ram_util_percent: 39.84589502018842\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06946979590045169\n",
      "    mean_env_wait_ms: 117.89959902375331\n",
      "    mean_inference_ms: 1.6011788328640475\n",
      "    mean_raw_obs_processing_ms: 9.387379383424062\n",
      "  time_since_restore: 16343.478653430939\n",
      "  time_this_iter_s: 520.800674200058\n",
      "  time_total_s: 126193.97262740135\n",
      "  timers:\n",
      "    learn_throughput: 373.696\n",
      "    learn_time_ms: 10703.882\n",
      "    load_throughput: 10241.202\n",
      "    load_time_ms: 390.579\n",
      "    sample_throughput: 7.793\n",
      "    sample_time_ms: 513298.275\n",
      "    update_time_ms: 3.605\n",
      "  timestamp: 1612919409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 122.37\n",
      "  episode_reward_max: 118.39951469513582\n",
      "  episode_reward_mean: 68.30844329309222\n",
      "  episode_reward_min: -107.4490627888949\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 5990\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3896908164024353\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012185083702206612\n",
      "        model: {}\n",
      "        policy_loss: -0.08600135147571564\n",
      "        total_loss: 339.9822692871094\n",
      "        vf_explained_var: 0.8376960754394531\n",
      "        vf_loss: 340.04974365234375\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.1184840425532\n",
      "    ram_util_percent: 39.861968085106376\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06945377551635952\n",
      "    mean_env_wait_ms: 117.86896684298218\n",
      "    mean_inference_ms: 1.6009884807007069\n",
      "    mean_raw_obs_processing_ms: 9.403574254154499\n",
      "  time_since_restore: 16870.12887263298\n",
      "  time_this_iter_s: 526.6502192020416\n",
      "  time_total_s: 126720.6228466034\n",
      "  timers:\n",
      "    learn_throughput: 373.767\n",
      "    learn_time_ms: 10701.84\n",
      "    load_throughput: 10270.834\n",
      "    load_time_ms: 389.452\n",
      "    sample_throughput: 7.778\n",
      "    sample_time_ms: 514300.741\n",
      "    update_time_ms: 3.579\n",
      "  timestamp: 1612919936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 233\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-27-39\n",
      "  done: false\n",
      "  episode_len_mean: 121.68\n",
      "  episode_reward_max: 118.39951469513582\n",
      "  episode_reward_mean: 64.17587692406539\n",
      "  episode_reward_min: -105.88207516812984\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 6023\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4002188444137573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012468689121305943\n",
      "        model: {}\n",
      "        policy_loss: -0.09416554123163223\n",
      "        total_loss: 795.3223876953125\n",
      "        vf_explained_var: 0.6846733093261719\n",
      "        vf_loss: 795.3976440429688\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.73691275167785\n",
      "    ram_util_percent: 40.00469798657718\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06944019983336709\n",
      "    mean_env_wait_ms: 117.82493538144796\n",
      "    mean_inference_ms: 1.6008359380452737\n",
      "    mean_raw_obs_processing_ms: 9.417097704119907\n",
      "  time_since_restore: 17392.488993406296\n",
      "  time_this_iter_s: 522.3601207733154\n",
      "  time_total_s: 127242.98296737671\n",
      "  timers:\n",
      "    learn_throughput: 373.795\n",
      "    learn_time_ms: 10701.062\n",
      "    load_throughput: 10317.486\n",
      "    load_time_ms: 387.691\n",
      "    sample_throughput: 7.777\n",
      "    sample_time_ms: 514366.062\n",
      "    update_time_ms: 3.429\n",
      "  timestamp: 1612920459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 125.43\n",
      "  episode_reward_max: 118.39818352064461\n",
      "  episode_reward_mean: 66.14284657843405\n",
      "  episode_reward_min: -105.88207516812984\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 6056\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3621053993701935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010412427596747875\n",
      "        model: {}\n",
      "        policy_loss: -0.07137807458639145\n",
      "        total_loss: 70.972900390625\n",
      "        vf_explained_var: 0.9519039988517761\n",
      "        vf_loss: 71.02847290039062\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.265251989389924\n",
      "    ram_util_percent: 40.052387267904514\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06942394060733718\n",
      "    mean_env_wait_ms: 117.80384705886627\n",
      "    mean_inference_ms: 1.6006649335850744\n",
      "    mean_raw_obs_processing_ms: 9.426948829497483\n",
      "  time_since_restore: 17920.99021077156\n",
      "  time_this_iter_s: 528.5012173652649\n",
      "  time_total_s: 127771.48418474197\n",
      "  timers:\n",
      "    learn_throughput: 373.749\n",
      "    learn_time_ms: 10702.374\n",
      "    load_throughput: 10308.084\n",
      "    load_time_ms: 388.045\n",
      "    sample_throughput: 7.77\n",
      "    sample_time_ms: 514788.527\n",
      "    update_time_ms: 3.45\n",
      "  timestamp: 1612920987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 118.95\n",
      "  episode_reward_max: 118.39820557669739\n",
      "  episode_reward_mean: 68.68508777033611\n",
      "  episode_reward_min: -105.70302600927543\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 6090\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3769032657146454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00883836206048727\n",
      "        model: {}\n",
      "        policy_loss: -0.07490509003400803\n",
      "        total_loss: 419.6905517578125\n",
      "        vf_explained_var: 0.7561943531036377\n",
      "        vf_loss: 419.7520446777344\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.316931216931216\n",
      "    ram_util_percent: 40.142328042328046\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06940779367379264\n",
      "    mean_env_wait_ms: 117.78173183842391\n",
      "    mean_inference_ms: 1.6005350548435646\n",
      "    mean_raw_obs_processing_ms: 9.44697858010564\n",
      "  time_since_restore: 18450.778151750565\n",
      "  time_this_iter_s: 529.7879409790039\n",
      "  time_total_s: 128301.27212572098\n",
      "  timers:\n",
      "    learn_throughput: 373.722\n",
      "    learn_time_ms: 10703.139\n",
      "    load_throughput: 10311.609\n",
      "    load_time_ms: 387.912\n",
      "    sample_throughput: 7.772\n",
      "    sample_time_ms: 514637.451\n",
      "    update_time_ms: 3.396\n",
      "  timestamp: 1612921517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_02-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 119.48\n",
      "  episode_reward_max: 118.3987819289197\n",
      "  episode_reward_mean: 77.28408257107888\n",
      "  episode_reward_min: -105.70302600927543\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 6123\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36623328924179077\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01233309879899025\n",
      "        model: {}\n",
      "        policy_loss: -0.08248881995677948\n",
      "        total_loss: 508.7405700683594\n",
      "        vf_explained_var: 0.6963388323783875\n",
      "        vf_loss: 508.80438232421875\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.94875164257556\n",
      "    ram_util_percent: 40.09145860709594\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06939537648395865\n",
      "    mean_env_wait_ms: 117.78755817760826\n",
      "    mean_inference_ms: 1.600443572974812\n",
      "    mean_raw_obs_processing_ms: 9.465584775121197\n",
      "  time_since_restore: 18984.127566576004\n",
      "  time_this_iter_s: 533.3494148254395\n",
      "  time_total_s: 128834.62154054642\n",
      "  timers:\n",
      "    learn_throughput: 373.825\n",
      "    learn_time_ms: 10700.191\n",
      "    load_throughput: 10344.24\n",
      "    load_time_ms: 386.689\n",
      "    sample_throughput: 7.749\n",
      "    sample_time_ms: 516228.445\n",
      "    update_time_ms: 3.383\n",
      "  timestamp: 1612922050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 237\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-02-52\n",
      "  done: false\n",
      "  episode_len_mean: 127.35\n",
      "  episode_reward_max: 118.3987819289197\n",
      "  episode_reward_mean: 79.64319933787448\n",
      "  episode_reward_min: -100.9123167237203\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 6150\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3793204128742218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01016020867973566\n",
      "        model: {}\n",
      "        policy_loss: -0.07364923506975174\n",
      "        total_loss: 148.41079711914062\n",
      "        vf_explained_var: 0.8893992304801941\n",
      "        vf_loss: 148.46902465820312\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.30268817204302\n",
      "    ram_util_percent: 40.11626344086022\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06938768158468336\n",
      "    mean_env_wait_ms: 117.79283874167703\n",
      "    mean_inference_ms: 1.6004149870689566\n",
      "    mean_raw_obs_processing_ms: 9.46693475808879\n",
      "  time_since_restore: 19505.68580508232\n",
      "  time_this_iter_s: 521.5582385063171\n",
      "  time_total_s: 129356.17977905273\n",
      "  timers:\n",
      "    learn_throughput: 373.826\n",
      "    learn_time_ms: 10700.163\n",
      "    load_throughput: 10297.258\n",
      "    load_time_ms: 388.453\n",
      "    sample_throughput: 7.766\n",
      "    sample_time_ms: 515081.862\n",
      "    update_time_ms: 3.372\n",
      "  timestamp: 1612922572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 134.81\n",
      "  episode_reward_max: 118.39921740094591\n",
      "  episode_reward_mean: 90.00238362991297\n",
      "  episode_reward_min: -105.06041095995295\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6179\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3751048445701599\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010476960800588131\n",
      "        model: {}\n",
      "        policy_loss: -0.06833427399396896\n",
      "        total_loss: 189.91024780273438\n",
      "        vf_explained_var: 0.8063831329345703\n",
      "        vf_loss: 189.96266174316406\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.861264822134395\n",
      "    ram_util_percent: 40.13162055335969\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06937900017141979\n",
      "    mean_env_wait_ms: 117.81148098629865\n",
      "    mean_inference_ms: 1.6003698550339704\n",
      "    mean_raw_obs_processing_ms: 9.458924686331398\n",
      "  time_since_restore: 20037.019586086273\n",
      "  time_this_iter_s: 531.333781003952\n",
      "  time_total_s: 129887.51356005669\n",
      "  timers:\n",
      "    learn_throughput: 373.698\n",
      "    learn_time_ms: 10703.84\n",
      "    load_throughput: 10277.354\n",
      "    load_time_ms: 389.205\n",
      "    sample_throughput: 7.755\n",
      "    sample_time_ms: 515765.268\n",
      "    update_time_ms: 3.368\n",
      "  timestamp: 1612923103\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 239\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-20-30\n",
      "  done: false\n",
      "  episode_len_mean: 131.97\n",
      "  episode_reward_max: 118.39921740094591\n",
      "  episode_reward_mean: 87.71435474128289\n",
      "  episode_reward_min: -105.06041095995295\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 6211\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3600043058395386\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008726473897695541\n",
      "        model: {}\n",
      "        policy_loss: -0.06986678391695023\n",
      "        total_loss: 418.94097900390625\n",
      "        vf_explained_var: 0.7331028580665588\n",
      "        vf_loss: 418.9975280761719\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.38535286284953\n",
      "    ram_util_percent: 40.13035952063915\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0693689447162325\n",
      "    mean_env_wait_ms: 117.82274746111437\n",
      "    mean_inference_ms: 1.6003345892255256\n",
      "    mean_raw_obs_processing_ms: 9.447860639676419\n",
      "  time_since_restore: 20563.72141766548\n",
      "  time_this_iter_s: 526.7018315792084\n",
      "  time_total_s: 130414.2153916359\n",
      "  timers:\n",
      "    learn_throughput: 373.603\n",
      "    learn_time_ms: 10706.543\n",
      "    load_throughput: 10312.585\n",
      "    load_time_ms: 387.876\n",
      "    sample_throughput: 7.757\n",
      "    sample_time_ms: 515695.942\n",
      "    update_time_ms: 3.293\n",
      "  timestamp: 1612923630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-29-16\n",
      "  done: false\n",
      "  episode_len_mean: 133.52\n",
      "  episode_reward_max: 118.39921740094591\n",
      "  episode_reward_mean: 89.76269450957243\n",
      "  episode_reward_min: -105.06041095995295\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6240\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.39139294624328613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011969340033829212\n",
      "        model: {}\n",
      "        policy_loss: -0.0875021368265152\n",
      "        total_loss: 193.0679473876953\n",
      "        vf_explained_var: 0.8410235047340393\n",
      "        vf_loss: 193.1372833251953\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.19347536617843\n",
      "    ram_util_percent: 40.15059920106525\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06935513337191469\n",
      "    mean_env_wait_ms: 117.83323486837696\n",
      "    mean_inference_ms: 1.6002763018824893\n",
      "    mean_raw_obs_processing_ms: 9.437443521624454\n",
      "  time_since_restore: 21089.69454741478\n",
      "  time_this_iter_s: 525.9731297492981\n",
      "  time_total_s: 130940.1885213852\n",
      "  timers:\n",
      "    learn_throughput: 373.546\n",
      "    learn_time_ms: 10708.187\n",
      "    load_throughput: 10313.293\n",
      "    load_time_ms: 387.849\n",
      "    sample_throughput: 7.76\n",
      "    sample_time_ms: 515453.509\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1612924156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 127.56\n",
      "  episode_reward_max: 118.39331475363147\n",
      "  episode_reward_mean: 91.86027214215795\n",
      "  episode_reward_min: -105.06041095995295\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 6273\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.33947816491127014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006888380739837885\n",
      "        model: {}\n",
      "        policy_loss: -0.06178026273846626\n",
      "        total_loss: 269.9424743652344\n",
      "        vf_explained_var: 0.7762779593467712\n",
      "        vf_loss: 269.9937744140625\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.33298013245033\n",
      "    ram_util_percent: 40.28582781456954\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0693395668979013\n",
      "    mean_env_wait_ms: 117.8360485768798\n",
      "    mean_inference_ms: 1.600286678916367\n",
      "    mean_raw_obs_processing_ms: 9.437720444905462\n",
      "  time_since_restore: 21618.974437475204\n",
      "  time_this_iter_s: 529.2798900604248\n",
      "  time_total_s: 131469.46841144562\n",
      "  timers:\n",
      "    learn_throughput: 373.557\n",
      "    learn_time_ms: 10707.862\n",
      "    load_throughput: 10360.047\n",
      "    load_time_ms: 386.099\n",
      "    sample_throughput: 7.747\n",
      "    sample_time_ms: 516300.185\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1612924686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 129.49\n",
      "  episode_reward_max: 118.39331475363147\n",
      "  episode_reward_mean: 89.80739836635038\n",
      "  episode_reward_min: -107.41894245995229\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 6303\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36593928933143616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010076526552438736\n",
      "        model: {}\n",
      "        policy_loss: -0.07489514350891113\n",
      "        total_loss: 318.55072021484375\n",
      "        vf_explained_var: 0.804060697555542\n",
      "        vf_loss: 318.6103210449219\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.97862796833773\n",
      "    ram_util_percent: 40.29445910290237\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06932824013408374\n",
      "    mean_env_wait_ms: 117.84688536826344\n",
      "    mean_inference_ms: 1.6002998298012432\n",
      "    mean_raw_obs_processing_ms: 9.434550362882582\n",
      "  time_since_restore: 22149.892201662064\n",
      "  time_this_iter_s: 530.9177641868591\n",
      "  time_total_s: 132000.38617563248\n",
      "  timers:\n",
      "    learn_throughput: 373.527\n",
      "    learn_time_ms: 10708.739\n",
      "    load_throughput: 10332.29\n",
      "    load_time_ms: 387.136\n",
      "    sample_throughput: 7.741\n",
      "    sample_time_ms: 516730.597\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1612925217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 243\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_03-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 123.15\n",
      "  episode_reward_max: 118.39867869760657\n",
      "  episode_reward_mean: 80.68405437559427\n",
      "  episode_reward_min: -107.41894245995229\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 6337\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38638800382614136\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00940411165356636\n",
      "        model: {}\n",
      "        policy_loss: -0.07414217293262482\n",
      "        total_loss: 300.307373046875\n",
      "        vf_explained_var: 0.8129581212997437\n",
      "        vf_loss: 300.36724853515625\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.31178807947019\n",
      "    ram_util_percent: 40.34198675496689\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06931773176173554\n",
      "    mean_env_wait_ms: 117.85443498527263\n",
      "    mean_inference_ms: 1.6003060646616203\n",
      "    mean_raw_obs_processing_ms: 9.440941454061266\n",
      "  time_since_restore: 22678.723916769028\n",
      "  time_this_iter_s: 528.8317151069641\n",
      "  time_total_s: 132529.21789073944\n",
      "  timers:\n",
      "    learn_throughput: 373.562\n",
      "    learn_time_ms: 10707.715\n",
      "    load_throughput: 10302.416\n",
      "    load_time_ms: 388.258\n",
      "    sample_throughput: 7.731\n",
      "    sample_time_ms: 517375.004\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1612925746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 118.76\n",
      "  episode_reward_max: 118.39867869760657\n",
      "  episode_reward_mean: 67.90969658424348\n",
      "  episode_reward_min: -107.41894245995229\n",
      "  episodes_this_iter: 37\n",
      "  episodes_total: 6374\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3974478244781494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016713907942175865\n",
      "        model: {}\n",
      "        policy_loss: -0.10096389800310135\n",
      "        total_loss: 997.5489501953125\n",
      "        vf_explained_var: 0.6760058999061584\n",
      "        vf_loss: 997.62451171875\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.66173333333334\n",
      "    ram_util_percent: 40.41439999999999\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06931196026203398\n",
      "    mean_env_wait_ms: 117.84413043336457\n",
      "    mean_inference_ms: 1.600277429660567\n",
      "    mean_raw_obs_processing_ms: 9.458996371804181\n",
      "  time_since_restore: 23204.338356494904\n",
      "  time_this_iter_s: 525.6144397258759\n",
      "  time_total_s: 133054.83233046532\n",
      "  timers:\n",
      "    learn_throughput: 373.579\n",
      "    learn_time_ms: 10707.241\n",
      "    load_throughput: 10375.147\n",
      "    load_time_ms: 385.537\n",
      "    sample_throughput: 7.736\n",
      "    sample_time_ms: 517092.837\n",
      "    update_time_ms: 3.431\n",
      "  timestamp: 1612926271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 245\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 120.5\n",
      "  episode_reward_max: 118.39867869760657\n",
      "  episode_reward_mean: 68.12611709596868\n",
      "  episode_reward_min: -106.01936315125033\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6403\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38961461186408997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013192914426326752\n",
      "        model: {}\n",
      "        policy_loss: -0.08568117022514343\n",
      "        total_loss: 65.19156646728516\n",
      "        vf_explained_var: 0.9548618793487549\n",
      "        vf_loss: 65.25720977783203\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.9784676354029\n",
      "    ram_util_percent: 40.42721268163804\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0693065146607158\n",
      "    mean_env_wait_ms: 117.8356198894298\n",
      "    mean_inference_ms: 1.6002390450543982\n",
      "    mean_raw_obs_processing_ms: 9.47159222265225\n",
      "  time_since_restore: 23734.42484307289\n",
      "  time_this_iter_s: 530.0864865779877\n",
      "  time_total_s: 133584.9188170433\n",
      "  timers:\n",
      "    learn_throughput: 373.709\n",
      "    learn_time_ms: 10703.522\n",
      "    load_throughput: 10362.505\n",
      "    load_time_ms: 386.007\n",
      "    sample_throughput: 7.735\n",
      "    sample_time_ms: 517125.874\n",
      "    update_time_ms: 3.454\n",
      "  timestamp: 1612926802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 123.29\n",
      "  episode_reward_max: 118.39775314424688\n",
      "  episode_reward_mean: 75.1126557353323\n",
      "  episode_reward_min: -105.14680184699387\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6434\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3633507490158081\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008570302277803421\n",
      "        model: {}\n",
      "        policy_loss: -0.07135342061519623\n",
      "        total_loss: 429.0644226074219\n",
      "        vf_explained_var: 0.7002447247505188\n",
      "        vf_loss: 429.1227722167969\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.34699599465955\n",
      "    ram_util_percent: 40.42309746328438\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06930232645239451\n",
      "    mean_env_wait_ms: 117.82627443300765\n",
      "    mean_inference_ms: 1.6001871674282546\n",
      "    mean_raw_obs_processing_ms: 9.478449373207093\n",
      "  time_since_restore: 24259.33223247528\n",
      "  time_this_iter_s: 524.9073894023895\n",
      "  time_total_s: 134109.8262064457\n",
      "  timers:\n",
      "    learn_throughput: 373.606\n",
      "    learn_time_ms: 10706.477\n",
      "    load_throughput: 10353.171\n",
      "    load_time_ms: 386.355\n",
      "    sample_throughput: 7.748\n",
      "    sample_time_ms: 516274.479\n",
      "    update_time_ms: 3.472\n",
      "  timestamp: 1612927327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-30-53\n",
      "  done: false\n",
      "  episode_len_mean: 127.17\n",
      "  episode_reward_max: 118.39995180589602\n",
      "  episode_reward_mean: 79.45895675081228\n",
      "  episode_reward_min: -105.14680184699387\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6465\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4038982093334198\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012797950766980648\n",
      "        model: {}\n",
      "        policy_loss: -0.1006801649928093\n",
      "        total_loss: 671.1112060546875\n",
      "        vf_explained_var: 0.685676634311676\n",
      "        vf_loss: 671.1925048828125\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.267287234042556\n",
      "    ram_util_percent: 40.44281914893617\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929929343814976\n",
      "    mean_env_wait_ms: 117.82996716113736\n",
      "    mean_inference_ms: 1.6001574052564382\n",
      "    mean_raw_obs_processing_ms: 9.474060235025725\n",
      "  time_since_restore: 24786.087637662888\n",
      "  time_this_iter_s: 526.7554051876068\n",
      "  time_total_s: 134636.5816116333\n",
      "  timers:\n",
      "    learn_throughput: 373.537\n",
      "    learn_time_ms: 10708.436\n",
      "    load_throughput: 10404.718\n",
      "    load_time_ms: 384.441\n",
      "    sample_throughput: 7.74\n",
      "    sample_time_ms: 516792.623\n",
      "    update_time_ms: 3.472\n",
      "  timestamp: 1612927853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-39-37\n",
      "  done: false\n",
      "  episode_len_mean: 132.98\n",
      "  episode_reward_max: 118.39995180589602\n",
      "  episode_reward_mean: 81.1533220396214\n",
      "  episode_reward_min: -115.51812555801372\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6494\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4085550606250763\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010802485048770905\n",
      "        model: {}\n",
      "        policy_loss: -0.08306783437728882\n",
      "        total_loss: 390.42413330078125\n",
      "        vf_explained_var: 0.7626205682754517\n",
      "        vf_loss: 390.4908142089844\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.373056300268104\n",
      "    ram_util_percent: 40.46353887399463\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0692980132923425\n",
      "    mean_env_wait_ms: 117.82636280612914\n",
      "    mean_inference_ms: 1.600159299578397\n",
      "    mean_raw_obs_processing_ms: 9.467822501756554\n",
      "  time_since_restore: 25309.228896856308\n",
      "  time_this_iter_s: 523.1412591934204\n",
      "  time_total_s: 135159.72287082672\n",
      "  timers:\n",
      "    learn_throughput: 373.597\n",
      "    learn_time_ms: 10706.733\n",
      "    load_throughput: 10303.502\n",
      "    load_time_ms: 388.218\n",
      "    sample_throughput: 7.752\n",
      "    sample_time_ms: 515971.515\n",
      "    update_time_ms: 3.456\n",
      "  timestamp: 1612928377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 249\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 134.56\n",
      "  episode_reward_max: 118.39995180589602\n",
      "  episode_reward_mean: 80.85583108969614\n",
      "  episode_reward_min: -115.51812555801372\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6523\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3705100119113922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009354713372886181\n",
      "        model: {}\n",
      "        policy_loss: -0.07160551846027374\n",
      "        total_loss: 447.101318359375\n",
      "        vf_explained_var: 0.6971088647842407\n",
      "        vf_loss: 447.15875244140625\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.337265415549595\n",
      "    ram_util_percent: 40.4828418230563\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929621072584825\n",
      "    mean_env_wait_ms: 117.81938511959008\n",
      "    mean_inference_ms: 1.6001894571800315\n",
      "    mean_raw_obs_processing_ms: 9.459822386462156\n",
      "  time_since_restore: 25832.122779130936\n",
      "  time_this_iter_s: 522.8938822746277\n",
      "  time_total_s: 135682.61675310135\n",
      "  timers:\n",
      "    learn_throughput: 373.5\n",
      "    learn_time_ms: 10709.496\n",
      "    load_throughput: 10294.072\n",
      "    load_time_ms: 388.573\n",
      "    sample_throughput: 7.758\n",
      "    sample_time_ms: 515586.867\n",
      "    update_time_ms: 3.452\n",
      "  timestamp: 1612928900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_04-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 133.8\n",
      "  episode_reward_max: 118.39976690190446\n",
      "  episode_reward_mean: 81.05574716238327\n",
      "  episode_reward_min: -115.51812555801372\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 6555\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3448770344257355\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00883668102324009\n",
      "        model: {}\n",
      "        policy_loss: -0.07867418974637985\n",
      "        total_loss: 444.0412902832031\n",
      "        vf_explained_var: 0.7407037615776062\n",
      "        vf_loss: 444.10650634765625\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.25073041168659\n",
      "    ram_util_percent: 40.49535192563081\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0692935270129379\n",
      "    mean_env_wait_ms: 117.81151708014092\n",
      "    mean_inference_ms: 1.6002313950560296\n",
      "    mean_raw_obs_processing_ms: 9.453415676255533\n",
      "  time_since_restore: 26359.432138442993\n",
      "  time_this_iter_s: 527.3093593120575\n",
      "  time_total_s: 136209.9261124134\n",
      "  timers:\n",
      "    learn_throughput: 373.472\n",
      "    learn_time_ms: 10710.31\n",
      "    load_throughput: 10300.951\n",
      "    load_time_ms: 388.314\n",
      "    sample_throughput: 7.756\n",
      "    sample_time_ms: 515724.919\n",
      "    update_time_ms: 3.387\n",
      "  timestamp: 1612929427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 251\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 129.17\n",
      "  episode_reward_max: 118.39976690190446\n",
      "  episode_reward_mean: 85.83263967952371\n",
      "  episode_reward_min: -106.37789142805315\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6586\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3803326189517975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009797744452953339\n",
      "        model: {}\n",
      "        policy_loss: -0.07274545729160309\n",
      "        total_loss: 558.324951171875\n",
      "        vf_explained_var: 0.6885911226272583\n",
      "        vf_loss: 558.3828125\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.15875331564986\n",
      "    ram_util_percent: 40.490053050397876\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928874154820043\n",
      "    mean_env_wait_ms: 117.80745846136251\n",
      "    mean_inference_ms: 1.6001937942611622\n",
      "    mean_raw_obs_processing_ms: 9.450316410901385\n",
      "  time_since_restore: 26887.66395688057\n",
      "  time_this_iter_s: 528.2318184375763\n",
      "  time_total_s: 136738.15793085098\n",
      "  timers:\n",
      "    learn_throughput: 373.533\n",
      "    learn_time_ms: 10708.57\n",
      "    load_throughput: 10287.053\n",
      "    load_time_ms: 388.838\n",
      "    sample_throughput: 7.758\n",
      "    sample_time_ms: 515622.783\n",
      "    update_time_ms: 3.433\n",
      "  timestamp: 1612929955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-14-42\n",
      "  done: false\n",
      "  episode_len_mean: 124.71\n",
      "  episode_reward_max: 118.39976690190446\n",
      "  episode_reward_mean: 77.69832512584883\n",
      "  episode_reward_min: -106.37223072804238\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6617\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3632063567638397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008575016632676125\n",
      "        model: {}\n",
      "        policy_loss: -0.0754043310880661\n",
      "        total_loss: 201.8621826171875\n",
      "        vf_explained_var: 0.8663883805274963\n",
      "        vf_loss: 201.92457580566406\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.32490013315579\n",
      "    ram_util_percent: 40.581091877496675\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928220287693236\n",
      "    mean_env_wait_ms: 117.80595248762413\n",
      "    mean_inference_ms: 1.6001431022087758\n",
      "    mean_raw_obs_processing_ms: 9.45096879562043\n",
      "  time_since_restore: 27414.161765813828\n",
      "  time_this_iter_s: 526.497808933258\n",
      "  time_total_s: 137264.65573978424\n",
      "  timers:\n",
      "    learn_throughput: 373.494\n",
      "    learn_time_ms: 10709.677\n",
      "    load_throughput: 10247.298\n",
      "    load_time_ms: 390.347\n",
      "    sample_throughput: 7.764\n",
      "    sample_time_ms: 515178.276\n",
      "    update_time_ms: 3.406\n",
      "  timestamp: 1612930482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 132.0\n",
      "  episode_reward_max: 118.39976690190446\n",
      "  episode_reward_mean: 81.78724985242903\n",
      "  episode_reward_min: -106.37223072804238\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 6647\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3395882546901703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009102328680455685\n",
      "        model: {}\n",
      "        policy_loss: -0.0652804970741272\n",
      "        total_loss: 602.163330078125\n",
      "        vf_explained_var: 0.6065481305122375\n",
      "        vf_loss: 602.2147216796875\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.97572559366755\n",
      "    ram_util_percent: 40.472559366754616\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06927586002715275\n",
      "    mean_env_wait_ms: 117.81394728556552\n",
      "    mean_inference_ms: 1.6000517334526334\n",
      "    mean_raw_obs_processing_ms: 9.449048254847897\n",
      "  time_since_restore: 27945.084828853607\n",
      "  time_this_iter_s: 530.9230630397797\n",
      "  time_total_s: 137795.57880282402\n",
      "  timers:\n",
      "    learn_throughput: 373.406\n",
      "    learn_time_ms: 10712.198\n",
      "    load_throughput: 10289.507\n",
      "    load_time_ms: 388.746\n",
      "    sample_throughput: 7.761\n",
      "    sample_time_ms: 515389.422\n",
      "    update_time_ms: 3.396\n",
      "  timestamp: 1612931013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-32-14\n",
      "  done: false\n",
      "  episode_len_mean: 135.05\n",
      "  episode_reward_max: 118.3991721218048\n",
      "  episode_reward_mean: 83.63744173862409\n",
      "  episode_reward_min: -106.37223072804238\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 6675\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3725070655345917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010309828445315361\n",
      "        model: {}\n",
      "        policy_loss: -0.07095495611429214\n",
      "        total_loss: 187.0923309326172\n",
      "        vf_explained_var: 0.8470891118049622\n",
      "        vf_loss: 187.14761352539062\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.324226110363384\n",
      "    ram_util_percent: 40.493135935397035\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06927126217898735\n",
      "    mean_env_wait_ms: 117.81721489263799\n",
      "    mean_inference_ms: 1.6000174089626649\n",
      "    mean_raw_obs_processing_ms: 9.44205960058755\n",
      "  time_since_restore: 28465.991718769073\n",
      "  time_this_iter_s: 520.9068899154663\n",
      "  time_total_s: 138316.4856927395\n",
      "  timers:\n",
      "    learn_throughput: 373.433\n",
      "    learn_time_ms: 10711.439\n",
      "    load_throughput: 10221.943\n",
      "    load_time_ms: 391.315\n",
      "    sample_throughput: 7.768\n",
      "    sample_time_ms: 514916.397\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1612931534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 255\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 137.62\n",
      "  episode_reward_max: 118.3991721218048\n",
      "  episode_reward_mean: 81.75838916499559\n",
      "  episode_reward_min: -102.87272157988244\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 6702\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4156583845615387\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013777113519608974\n",
      "        model: {}\n",
      "        policy_loss: -0.10282169282436371\n",
      "        total_loss: 612.8851928710938\n",
      "        vf_explained_var: 0.7260733246803284\n",
      "        vf_loss: 612.9671020507812\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.42702702702703\n",
      "    ram_util_percent: 40.51783783783784\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0692643657295874\n",
      "    mean_env_wait_ms: 117.81588355220381\n",
      "    mean_inference_ms: 1.599989229536461\n",
      "    mean_raw_obs_processing_ms: 9.429475058281788\n",
      "  time_since_restore: 28984.57059931755\n",
      "  time_this_iter_s: 518.5788805484772\n",
      "  time_total_s: 138835.06457328796\n",
      "  timers:\n",
      "    learn_throughput: 373.304\n",
      "    learn_time_ms: 10715.137\n",
      "    load_throughput: 10355.598\n",
      "    load_time_ms: 386.265\n",
      "    sample_throughput: 7.786\n",
      "    sample_time_ms: 513766.999\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1612932053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 133.93\n",
      "  episode_reward_max: 118.3991721218048\n",
      "  episode_reward_mean: 83.72954438039181\n",
      "  episode_reward_min: -99.05824168544535\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 6737\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38026756048202515\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011028818786144257\n",
      "        model: {}\n",
      "        policy_loss: -0.08933175355195999\n",
      "        total_loss: 561.9575805664062\n",
      "        vf_explained_var: 0.7296318411827087\n",
      "        vf_loss: 562.0301513671875\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.5595206391478\n",
      "    ram_util_percent: 40.54727030625832\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.069257745908199\n",
      "    mean_env_wait_ms: 117.80042400681265\n",
      "    mean_inference_ms: 1.5999936684549292\n",
      "    mean_raw_obs_processing_ms: 9.422013498121755\n",
      "  time_since_restore: 29510.404690265656\n",
      "  time_this_iter_s: 525.8340909481049\n",
      "  time_total_s: 139360.89866423607\n",
      "  timers:\n",
      "    learn_throughput: 373.345\n",
      "    learn_time_ms: 10713.943\n",
      "    load_throughput: 10319.777\n",
      "    load_time_ms: 387.605\n",
      "    sample_throughput: 7.784\n",
      "    sample_time_ms: 513862.277\n",
      "    update_time_ms: 3.235\n",
      "  timestamp: 1612932579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 257\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_05-58-22\n",
      "  done: false\n",
      "  episode_len_mean: 134.31\n",
      "  episode_reward_max: 118.39649847606634\n",
      "  episode_reward_mean: 87.83046871946243\n",
      "  episode_reward_min: -99.05824168544535\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 6765\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.35737162828445435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008245186880230904\n",
      "        model: {}\n",
      "        policy_loss: -0.05744369700551033\n",
      "        total_loss: 133.61537170410156\n",
      "        vf_explained_var: 0.8582574725151062\n",
      "        vf_loss: 133.66029357910156\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.18755020080321\n",
      "    ram_util_percent: 40.4619812583668\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06925293718327002\n",
      "    mean_env_wait_ms: 117.78800108131202\n",
      "    mean_inference_ms: 1.6000052755288174\n",
      "    mean_raw_obs_processing_ms: 9.415400140513942\n",
      "  time_since_restore: 30034.22373008728\n",
      "  time_this_iter_s: 523.8190398216248\n",
      "  time_total_s: 139884.7177040577\n",
      "  timers:\n",
      "    learn_throughput: 373.433\n",
      "    learn_time_ms: 10711.422\n",
      "    load_throughput: 10315.104\n",
      "    load_time_ms: 387.781\n",
      "    sample_throughput: 7.789\n",
      "    sample_time_ms: 513568.066\n",
      "    update_time_ms: 3.279\n",
      "  timestamp: 1612933102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 128.24\n",
      "  episode_reward_max: 118.39649847606634\n",
      "  episode_reward_mean: 85.60566402728037\n",
      "  episode_reward_min: -105.108323360269\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6796\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3514731526374817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01100051961839199\n",
      "        model: {}\n",
      "        policy_loss: -0.0751851424574852\n",
      "        total_loss: 631.9628295898438\n",
      "        vf_explained_var: 0.6407285332679749\n",
      "        vf_loss: 632.0213623046875\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.746737683089215\n",
      "    ram_util_percent: 40.47003994673768\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06925086672239267\n",
      "    mean_env_wait_ms: 117.77832740152488\n",
      "    mean_inference_ms: 1.6000539503523101\n",
      "    mean_raw_obs_processing_ms: 9.414609961651996\n",
      "  time_since_restore: 30560.631703853607\n",
      "  time_this_iter_s: 526.4079737663269\n",
      "  time_total_s: 140411.12567782402\n",
      "  timers:\n",
      "    learn_throughput: 373.38\n",
      "    learn_time_ms: 10712.95\n",
      "    load_throughput: 10305.009\n",
      "    load_time_ms: 388.161\n",
      "    sample_throughput: 7.784\n",
      "    sample_time_ms: 513891.568\n",
      "    update_time_ms: 3.297\n",
      "  timestamp: 1612933629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 129.05\n",
      "  episode_reward_max: 118.39649847606634\n",
      "  episode_reward_mean: 83.60177048380372\n",
      "  episode_reward_min: -105.108323360269\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 6828\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3544729948043823\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010000239126384258\n",
      "        model: {}\n",
      "        policy_loss: -0.07471056282520294\n",
      "        total_loss: 532.3400268554688\n",
      "        vf_explained_var: 0.7062293887138367\n",
      "        vf_loss: 532.3995361328125\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.68766954377311\n",
      "    ram_util_percent: 40.505178791615286\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06926212703198636\n",
      "    mean_env_wait_ms: 117.83010382371107\n",
      "    mean_inference_ms: 1.6006805121075827\n",
      "    mean_raw_obs_processing_ms: 9.412745186086566\n",
      "  time_since_restore: 31128.8429479599\n",
      "  time_this_iter_s: 568.2112441062927\n",
      "  time_total_s: 140979.3369219303\n",
      "  timers:\n",
      "    learn_throughput: 373.454\n",
      "    learn_time_ms: 10710.837\n",
      "    load_throughput: 10227.939\n",
      "    load_time_ms: 391.086\n",
      "    sample_throughput: 7.716\n",
      "    sample_time_ms: 518422.604\n",
      "    update_time_ms: 3.318\n",
      "  timestamp: 1612934197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-25-24\n",
      "  done: false\n",
      "  episode_len_mean: 130.17\n",
      "  episode_reward_max: 118.38580713470606\n",
      "  episode_reward_mean: 79.39752787494295\n",
      "  episode_reward_min: -105.108323360269\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6857\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3593236207962036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009192650206387043\n",
      "        model: {}\n",
      "        policy_loss: -0.06643503159284592\n",
      "        total_loss: 317.1968688964844\n",
      "        vf_explained_var: 0.7956193089485168\n",
      "        vf_loss: 317.2492980957031\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.11143617021277\n",
      "    ram_util_percent: 40.22047872340425\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06927224843781402\n",
      "    mean_env_wait_ms: 117.87998216892636\n",
      "    mean_inference_ms: 1.6012623686942706\n",
      "    mean_raw_obs_processing_ms: 9.410415330306622\n",
      "  time_since_restore: 31655.84609746933\n",
      "  time_this_iter_s: 527.0031495094299\n",
      "  time_total_s: 141506.34007143974\n",
      "  timers:\n",
      "    learn_throughput: 373.485\n",
      "    learn_time_ms: 10709.926\n",
      "    load_throughput: 10245.904\n",
      "    load_time_ms: 390.4\n",
      "    sample_throughput: 7.716\n",
      "    sample_time_ms: 518392.203\n",
      "    update_time_ms: 3.336\n",
      "  timestamp: 1612934724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 261\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 129.49\n",
      "  episode_reward_max: 118.39824125693927\n",
      "  episode_reward_mean: 79.24406138191725\n",
      "  episode_reward_min: -105.108323360269\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 6888\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.34594857692718506\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008483709767460823\n",
      "        model: {}\n",
      "        policy_loss: -0.0662190392613411\n",
      "        total_loss: 186.4908447265625\n",
      "        vf_explained_var: 0.8455460667610168\n",
      "        vf_loss: 186.54417419433594\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.209549071618035\n",
      "    ram_util_percent: 40.25928381962865\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928216808832993\n",
      "    mean_env_wait_ms: 117.93462148463153\n",
      "    mean_inference_ms: 1.6018289707504474\n",
      "    mean_raw_obs_processing_ms: 9.409357172458153\n",
      "  time_since_restore: 32184.06649661064\n",
      "  time_this_iter_s: 528.2203991413116\n",
      "  time_total_s: 142034.56047058105\n",
      "  timers:\n",
      "    learn_throughput: 373.448\n",
      "    learn_time_ms: 10711.004\n",
      "    load_throughput: 10236.371\n",
      "    load_time_ms: 390.763\n",
      "    sample_throughput: 7.716\n",
      "    sample_time_ms: 518390.816\n",
      "    update_time_ms: 3.308\n",
      "  timestamp: 1612935253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 134.7\n",
      "  episode_reward_max: 118.39824125693927\n",
      "  episode_reward_mean: 83.62067637720499\n",
      "  episode_reward_min: -104.85008915358314\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 6915\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3844476044178009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010237504728138447\n",
      "        model: {}\n",
      "        policy_loss: -0.07038916647434235\n",
      "        total_loss: 496.0287170410156\n",
      "        vf_explained_var: 0.6634261012077332\n",
      "        vf_loss: 496.08367919921875\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.21954484605087\n",
      "    ram_util_percent: 40.2809906291834\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928366145585907\n",
      "    mean_env_wait_ms: 117.95154516633418\n",
      "    mean_inference_ms: 1.6019173655563566\n",
      "    mean_raw_obs_processing_ms: 9.401972036509285\n",
      "  time_since_restore: 32707.550357341766\n",
      "  time_this_iter_s: 523.4838607311249\n",
      "  time_total_s: 142558.04433131218\n",
      "  timers:\n",
      "    learn_throughput: 373.499\n",
      "    learn_time_ms: 10709.523\n",
      "    load_throughput: 10284.31\n",
      "    load_time_ms: 388.942\n",
      "    sample_throughput: 7.721\n",
      "    sample_time_ms: 518090.868\n",
      "    update_time_ms: 3.328\n",
      "  timestamp: 1612935776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 263\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_06-51-46\n",
      "  done: false\n",
      "  episode_len_mean: 132.53\n",
      "  episode_reward_max: 118.39824125693927\n",
      "  episode_reward_mean: 85.79009329627091\n",
      "  episode_reward_min: -98.82845309206152\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 6949\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3898640275001526\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012257159687578678\n",
      "        model: {}\n",
      "        policy_loss: -0.08705633133649826\n",
      "        total_loss: 754.8258666992188\n",
      "        vf_explained_var: 0.6555668711662292\n",
      "        vf_loss: 754.8944091796875\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.25587846763541\n",
      "    ram_util_percent: 40.23817701453104\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06928193335118149\n",
      "    mean_env_wait_ms: 117.95359603329948\n",
      "    mean_inference_ms: 1.6018059597783378\n",
      "    mean_raw_obs_processing_ms: 9.398420222244923\n",
      "  time_since_restore: 33237.608710289\n",
      "  time_this_iter_s: 530.0583529472351\n",
      "  time_total_s: 143088.10268425941\n",
      "  timers:\n",
      "    learn_throughput: 373.532\n",
      "    learn_time_ms: 10708.584\n",
      "    load_throughput: 10179.306\n",
      "    load_time_ms: 392.954\n",
      "    sample_throughput: 7.722\n",
      "    sample_time_ms: 518001.435\n",
      "    update_time_ms: 3.342\n",
      "  timestamp: 1612936306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 133.12\n",
      "  episode_reward_max: 118.39824125693927\n",
      "  episode_reward_mean: 81.47117357207296\n",
      "  episode_reward_min: -106.38216724440176\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 6978\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.39508214592933655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010031311772763729\n",
      "        model: {}\n",
      "        policy_loss: -0.07565855979919434\n",
      "        total_loss: 354.84161376953125\n",
      "        vf_explained_var: 0.7663719654083252\n",
      "        vf_loss: 354.90203857421875\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.79052890528905\n",
      "    ram_util_percent: 40.269987699877\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929920923964472\n",
      "    mean_env_wait_ms: 118.00207648844453\n",
      "    mean_inference_ms: 1.6019380236310843\n",
      "    mean_raw_obs_processing_ms: 9.39443753144497\n",
      "  time_since_restore: 33807.324840307236\n",
      "  time_this_iter_s: 569.7161300182343\n",
      "  time_total_s: 143657.81881427765\n",
      "  timers:\n",
      "    learn_throughput: 369.443\n",
      "    learn_time_ms: 10827.108\n",
      "    load_throughput: 10149.911\n",
      "    load_time_ms: 394.092\n",
      "    sample_throughput: 7.652\n",
      "    sample_time_ms: 522759.33\n",
      "    update_time_ms: 3.368\n",
      "  timestamp: 1612936876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-10-32\n",
      "  done: false\n",
      "  episode_len_mean: 132.45\n",
      "  episode_reward_max: 118.39257948109737\n",
      "  episode_reward_mean: 85.37231120307699\n",
      "  episode_reward_min: -106.38216724440176\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 7006\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3483186662197113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008010245859622955\n",
      "        model: {}\n",
      "        policy_loss: -0.05721196532249451\n",
      "        total_loss: 212.0830841064453\n",
      "        vf_explained_var: 0.7778230309486389\n",
      "        vf_loss: 212.1281280517578\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.3830808080808\n",
      "    ram_util_percent: 40.35037878787878\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06933076654768668\n",
      "    mean_env_wait_ms: 118.07893922329464\n",
      "    mean_inference_ms: 1.6022921866095237\n",
      "    mean_raw_obs_processing_ms: 9.39082092427615\n",
      "  time_since_restore: 34362.60400271416\n",
      "  time_this_iter_s: 555.2791624069214\n",
      "  time_total_s: 144213.09797668457\n",
      "  timers:\n",
      "    learn_throughput: 365.255\n",
      "    learn_time_ms: 10951.265\n",
      "    load_throughput: 10001.62\n",
      "    load_time_ms: 399.935\n",
      "    sample_throughput: 7.6\n",
      "    sample_time_ms: 526296.591\n",
      "    update_time_ms: 3.384\n",
      "  timestamp: 1612937432\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 127.94\n",
      "  episode_reward_max: 118.3911407878765\n",
      "  episode_reward_mean: 89.54052084198247\n",
      "  episode_reward_min: -106.38216724440176\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 7040\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36846548318862915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007897048257291317\n",
      "        model: {}\n",
      "        policy_loss: -0.07120607048273087\n",
      "        total_loss: 170.25970458984375\n",
      "        vf_explained_var: 0.8771083950996399\n",
      "        vf_loss: 170.3188934326172\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.868\n",
      "    ram_util_percent: 40.428266666666666\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06936872774089586\n",
      "    mean_env_wait_ms: 118.16369377636036\n",
      "    mean_inference_ms: 1.6027677305677492\n",
      "    mean_raw_obs_processing_ms: 9.390226908187486\n",
      "  time_since_restore: 34887.94112968445\n",
      "  time_this_iter_s: 525.3371269702911\n",
      "  time_total_s: 144738.43510365486\n",
      "  timers:\n",
      "    learn_throughput: 365.093\n",
      "    learn_time_ms: 10956.121\n",
      "    load_throughput: 10001.031\n",
      "    load_time_ms: 399.959\n",
      "    sample_throughput: 7.601\n",
      "    sample_time_ms: 526241.612\n",
      "    update_time_ms: 3.443\n",
      "  timestamp: 1612937957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-28-06\n",
      "  done: false\n",
      "  episode_len_mean: 136.71\n",
      "  episode_reward_max: 118.3911407878765\n",
      "  episode_reward_mean: 89.60415341925747\n",
      "  episode_reward_min: -106.38216724440176\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 7065\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4166446030139923\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013184037990868092\n",
      "        model: {}\n",
      "        policy_loss: -0.09656470268964767\n",
      "        total_loss: 689.5965576171875\n",
      "        vf_explained_var: 0.6984066963195801\n",
      "        vf_loss: 689.6730346679688\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.59615894039735\n",
      "    ram_util_percent: 40.43271523178807\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06938695961377285\n",
      "    mean_env_wait_ms: 118.2059860558049\n",
      "    mean_inference_ms: 1.6030180166530983\n",
      "    mean_raw_obs_processing_ms: 9.383628867445791\n",
      "  time_since_restore: 35417.17981028557\n",
      "  time_this_iter_s: 529.23868060112\n",
      "  time_total_s: 145267.67378425598\n",
      "  timers:\n",
      "    learn_throughput: 362.539\n",
      "    learn_time_ms: 11033.288\n",
      "    load_throughput: 9998.123\n",
      "    load_time_ms: 400.075\n",
      "    sample_throughput: 7.594\n",
      "    sample_time_ms: 526706.616\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1612938486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-36-58\n",
      "  done: false\n",
      "  episode_len_mean: 134.34\n",
      "  episode_reward_max: 118.3911407878765\n",
      "  episode_reward_mean: 87.92099272091004\n",
      "  episode_reward_min: -99.4603476714637\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 7096\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37500157952308655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01283953431993723\n",
      "        model: {}\n",
      "        policy_loss: -0.09724997729063034\n",
      "        total_loss: 698.0765991210938\n",
      "        vf_explained_var: 0.6827164888381958\n",
      "        vf_loss: 698.154296875\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.613852242744066\n",
      "    ram_util_percent: 40.54960422163589\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06939583314122276\n",
      "    mean_env_wait_ms: 118.2229687330249\n",
      "    mean_inference_ms: 1.6031287007288164\n",
      "    mean_raw_obs_processing_ms: 9.378442631564404\n",
      "  time_since_restore: 35948.442828416824\n",
      "  time_this_iter_s: 531.2630181312561\n",
      "  time_total_s: 145798.93680238724\n",
      "  timers:\n",
      "    learn_throughput: 362.446\n",
      "    learn_time_ms: 11036.13\n",
      "    load_throughput: 10054.548\n",
      "    load_time_ms: 397.83\n",
      "    sample_throughput: 7.587\n",
      "    sample_time_ms: 527188.453\n",
      "    update_time_ms: 3.4\n",
      "  timestamp: 1612939018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-45-49\n",
      "  done: false\n",
      "  episode_len_mean: 131.06\n",
      "  episode_reward_max: 118.39547731734571\n",
      "  episode_reward_mean: 77.2293580192358\n",
      "  episode_reward_min: -105.8802001808491\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 7130\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3431065082550049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008472778834402561\n",
      "        model: {}\n",
      "        policy_loss: -0.06977755576372147\n",
      "        total_loss: 130.01055908203125\n",
      "        vf_explained_var: 0.9219914674758911\n",
      "        vf_loss: 130.06747436523438\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.50039577836412\n",
      "    ram_util_percent: 40.508179419525064\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06940315622627075\n",
      "    mean_env_wait_ms: 118.23035459395508\n",
      "    mean_inference_ms: 1.6031870624719602\n",
      "    mean_raw_obs_processing_ms: 9.375691606620277\n",
      "  time_since_restore: 36479.297374010086\n",
      "  time_this_iter_s: 530.8545455932617\n",
      "  time_total_s: 146329.7913479805\n",
      "  timers:\n",
      "    learn_throughput: 362.383\n",
      "    learn_time_ms: 11038.03\n",
      "    load_throughput: 10103.067\n",
      "    load_time_ms: 395.919\n",
      "    sample_throughput: 7.642\n",
      "    sample_time_ms: 523451.677\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1612939549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 270\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_07-54-39\n",
      "  done: false\n",
      "  episode_len_mean: 129.68\n",
      "  episode_reward_max: 118.39547731734571\n",
      "  episode_reward_mean: 81.19330089898682\n",
      "  episode_reward_min: -105.8802001808491\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 7159\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.35232308506965637\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01306112576276064\n",
      "        model: {}\n",
      "        policy_loss: -0.08413372188806534\n",
      "        total_loss: 262.5661926269531\n",
      "        vf_explained_var: 0.8584184050559998\n",
      "        vf_loss: 262.6304931640625\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.684676354029065\n",
      "    ram_util_percent: 40.504359313077934\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06941002960283575\n",
      "    mean_env_wait_ms: 118.23792426628793\n",
      "    mean_inference_ms: 1.6032417434272777\n",
      "    mean_raw_obs_processing_ms: 9.37499722153915\n",
      "  time_since_restore: 37009.707545518875\n",
      "  time_this_iter_s: 530.4101715087891\n",
      "  time_total_s: 146860.2015194893\n",
      "  timers:\n",
      "    learn_throughput: 362.291\n",
      "    learn_time_ms: 11040.838\n",
      "    load_throughput: 10117.451\n",
      "    load_time_ms: 395.357\n",
      "    sample_throughput: 7.637\n",
      "    sample_time_ms: 523785.471\n",
      "    update_time_ms: 3.45\n",
      "  timestamp: 1612940079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-03-26\n",
      "  done: false\n",
      "  episode_len_mean: 130.01\n",
      "  episode_reward_max: 118.39547731734571\n",
      "  episode_reward_mean: 80.98324524255646\n",
      "  episode_reward_min: -105.8802001808491\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 7190\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38195422291755676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011975212022662163\n",
      "        model: {}\n",
      "        policy_loss: -0.08833307772874832\n",
      "        total_loss: 324.47723388671875\n",
      "        vf_explained_var: 0.847353458404541\n",
      "        vf_loss: 324.5473937988281\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.64667553191489\n",
      "    ram_util_percent: 40.43377659574469\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06941623108221807\n",
      "    mean_env_wait_ms: 118.23888459789225\n",
      "    mean_inference_ms: 1.6033289318295698\n",
      "    mean_raw_obs_processing_ms: 9.376941493870792\n",
      "  time_since_restore: 37536.50109601021\n",
      "  time_this_iter_s: 526.793550491333\n",
      "  time_total_s: 147386.99506998062\n",
      "  timers:\n",
      "    learn_throughput: 362.198\n",
      "    learn_time_ms: 11043.693\n",
      "    load_throughput: 10119.119\n",
      "    load_time_ms: 395.291\n",
      "    sample_throughput: 7.639\n",
      "    sample_time_ms: 523639.369\n",
      "    update_time_ms: 3.446\n",
      "  timestamp: 1612940606\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 272\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 130.98\n",
      "  episode_reward_max: 118.38507119432211\n",
      "  episode_reward_mean: 85.17821922297034\n",
      "  episode_reward_min: -105.8802001808491\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 7220\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36194854974746704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010024256072938442\n",
      "        model: {}\n",
      "        policy_loss: -0.07011985033750534\n",
      "        total_loss: 438.0443420410156\n",
      "        vf_explained_var: 0.7122790217399597\n",
      "        vf_loss: 438.09918212890625\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.77812080536912\n",
      "    ram_util_percent: 40.32617449664429\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06941969300998256\n",
      "    mean_env_wait_ms: 118.23434205916698\n",
      "    mean_inference_ms: 1.6034069907240012\n",
      "    mean_raw_obs_processing_ms: 9.374755259648738\n",
      "  time_since_restore: 38058.417748212814\n",
      "  time_this_iter_s: 521.9166522026062\n",
      "  time_total_s: 147908.91172218323\n",
      "  timers:\n",
      "    learn_throughput: 362.031\n",
      "    learn_time_ms: 11048.787\n",
      "    load_throughput: 10062.914\n",
      "    load_time_ms: 397.499\n",
      "    sample_throughput: 7.641\n",
      "    sample_time_ms: 523476.219\n",
      "    update_time_ms: 3.411\n",
      "  timestamp: 1612941128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 131.71\n",
      "  episode_reward_max: 118.38507119432211\n",
      "  episode_reward_mean: 79.22942595241358\n",
      "  episode_reward_min: -105.33218453158977\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 7250\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3783925473690033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01028820313513279\n",
      "        model: {}\n",
      "        policy_loss: -0.08333862572908401\n",
      "        total_loss: 550.1541748046875\n",
      "        vf_explained_var: 0.7420992255210876\n",
      "        vf_loss: 550.2218017578125\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.09199475065618\n",
      "    ram_util_percent: 40.39265091863517\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06942265016359\n",
      "    mean_env_wait_ms: 118.23378203597204\n",
      "    mean_inference_ms: 1.6034889354493906\n",
      "    mean_raw_obs_processing_ms: 9.372234335143201\n",
      "  time_since_restore: 38592.20010614395\n",
      "  time_this_iter_s: 533.7823579311371\n",
      "  time_total_s: 148442.69408011436\n",
      "  timers:\n",
      "    learn_throughput: 361.897\n",
      "    learn_time_ms: 11052.881\n",
      "    load_throughput: 10136.897\n",
      "    load_time_ms: 394.598\n",
      "    sample_throughput: 7.636\n",
      "    sample_time_ms: 523848.517\n",
      "    update_time_ms: 3.423\n",
      "  timestamp: 1612941662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-29-55\n",
      "  done: false\n",
      "  episode_len_mean: 130.31\n",
      "  episode_reward_max: 118.38507119432211\n",
      "  episode_reward_mean: 79.09474967024781\n",
      "  episode_reward_min: -105.33218453158977\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 7282\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3556593656539917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008046400733292103\n",
      "        model: {}\n",
      "        policy_loss: -0.06431816518306732\n",
      "        total_loss: 292.83050537109375\n",
      "        vf_explained_var: 0.8125525712966919\n",
      "        vf_loss: 292.8826599121094\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.40815789473684\n",
      "    ram_util_percent: 40.44618421052631\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06942496642427029\n",
      "    mean_env_wait_ms: 118.23601783551679\n",
      "    mean_inference_ms: 1.6035466441877326\n",
      "    mean_raw_obs_processing_ms: 9.371829499104463\n",
      "  time_since_restore: 39124.77579474449\n",
      "  time_this_iter_s: 532.5756886005402\n",
      "  time_total_s: 148975.2697687149\n",
      "  timers:\n",
      "    learn_throughput: 365.564\n",
      "    learn_time_ms: 10942.006\n",
      "    load_throughput: 10226.877\n",
      "    load_time_ms: 391.126\n",
      "    sample_throughput: 7.689\n",
      "    sample_time_ms: 520252.098\n",
      "    update_time_ms: 3.396\n",
      "  timestamp: 1612942195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 132.46\n",
      "  episode_reward_max: 118.3886910187728\n",
      "  episode_reward_mean: 83.35827181379476\n",
      "  episode_reward_min: -105.33218453158977\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 7311\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3425547480583191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008906719274818897\n",
      "        model: {}\n",
      "        policy_loss: -0.06300731748342514\n",
      "        total_loss: 353.84796142578125\n",
      "        vf_explained_var: 0.6911104917526245\n",
      "        vf_loss: 353.89739990234375\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.42052287581699\n",
      "    ram_util_percent: 40.50104575163399\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06942795409863542\n",
      "    mean_env_wait_ms: 118.25214717421484\n",
      "    mean_inference_ms: 1.6035921121970107\n",
      "    mean_raw_obs_processing_ms: 9.369955467890632\n",
      "  time_since_restore: 39661.3459918499\n",
      "  time_this_iter_s: 536.5701971054077\n",
      "  time_total_s: 149511.8399658203\n",
      "  timers:\n",
      "    learn_throughput: 367.868\n",
      "    learn_time_ms: 10873.461\n",
      "    load_throughput: 10321.285\n",
      "    load_time_ms: 387.549\n",
      "    sample_throughput: 7.715\n",
      "    sample_time_ms: 518455.955\n",
      "    update_time_ms: 3.381\n",
      "  timestamp: 1612942732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 276\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-48-08\n",
      "  done: false\n",
      "  episode_len_mean: 135.97\n",
      "  episode_reward_max: 118.39580941565114\n",
      "  episode_reward_mean: 87.53104609220624\n",
      "  episode_reward_min: -105.33218453158977\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 7338\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3476717472076416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009480617940425873\n",
      "        model: {}\n",
      "        policy_loss: -0.06909433007240295\n",
      "        total_loss: 469.2999267578125\n",
      "        vf_explained_var: 0.6792575716972351\n",
      "        vf_loss: 469.3545837402344\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.87254408060453\n",
      "    ram_util_percent: 40.70025188916877\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06944218106007716\n",
      "    mean_env_wait_ms: 118.29156193076598\n",
      "    mean_inference_ms: 1.6037570564433796\n",
      "    mean_raw_obs_processing_ms: 9.36536120888139\n",
      "  time_since_restore: 40217.38514947891\n",
      "  time_this_iter_s: 556.0391576290131\n",
      "  time_total_s: 150067.87912344933\n",
      "  timers:\n",
      "    learn_throughput: 364.048\n",
      "    learn_time_ms: 10987.567\n",
      "    load_throughput: 10264.55\n",
      "    load_time_ms: 389.691\n",
      "    sample_throughput: 7.672\n",
      "    sample_time_ms: 521409.713\n",
      "    update_time_ms: 3.385\n",
      "  timestamp: 1612943288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_08-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 136.53\n",
      "  episode_reward_max: 118.39580941565114\n",
      "  episode_reward_mean: 79.14564286162533\n",
      "  episode_reward_min: -105.52323032972315\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 7369\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.42577964067459106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01852702721953392\n",
      "        model: {}\n",
      "        policy_loss: -0.11602400988340378\n",
      "        total_loss: 1199.9779052734375\n",
      "        vf_explained_var: 0.6293759346008301\n",
      "        vf_loss: 1200.06591796875\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.832904884318765\n",
      "    ram_util_percent: 40.518637532133674\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06946259458584826\n",
      "    mean_env_wait_ms: 118.34849367585859\n",
      "    mean_inference_ms: 1.603997160937983\n",
      "    mean_raw_obs_processing_ms: 9.359939897861125\n",
      "  time_since_restore: 40762.35188269615\n",
      "  time_this_iter_s: 544.9667332172394\n",
      "  time_total_s: 150612.84585666656\n",
      "  timers:\n",
      "    learn_throughput: 366.393\n",
      "    learn_time_ms: 10917.239\n",
      "    load_throughput: 10307.985\n",
      "    load_time_ms: 388.049\n",
      "    sample_throughput: 7.647\n",
      "    sample_time_ms: 523058.185\n",
      "    update_time_ms: 3.385\n",
      "  timestamp: 1612943833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 278\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 137.55\n",
      "  episode_reward_max: 118.3976878155662\n",
      "  episode_reward_mean: 77.25007403359892\n",
      "  episode_reward_min: -105.52323032972315\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 7398\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4034528434276581\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011728320270776749\n",
      "        model: {}\n",
      "        policy_loss: -0.07793018221855164\n",
      "        total_loss: 602.981201171875\n",
      "        vf_explained_var: 0.62846440076828\n",
      "        vf_loss: 603.0413208007812\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.38916776750329\n",
      "    ram_util_percent: 40.5443857331572\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06948366589806465\n",
      "    mean_env_wait_ms: 118.39865715950458\n",
      "    mean_inference_ms: 1.6042411251738866\n",
      "    mean_raw_obs_processing_ms: 9.353458141439141\n",
      "  time_since_restore: 41292.729105472565\n",
      "  time_this_iter_s: 530.377222776413\n",
      "  time_total_s: 151143.22307944298\n",
      "  timers:\n",
      "    learn_throughput: 366.288\n",
      "    learn_time_ms: 10920.374\n",
      "    load_throughput: 10381.488\n",
      "    load_time_ms: 385.301\n",
      "    sample_throughput: 7.649\n",
      "    sample_time_ms: 522971.039\n",
      "    update_time_ms: 3.421\n",
      "  timestamp: 1612944363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-14-59\n",
      "  done: false\n",
      "  episode_len_mean: 133.02\n",
      "  episode_reward_max: 118.3976878155662\n",
      "  episode_reward_mean: 75.17532373335648\n",
      "  episode_reward_min: -105.52323032972315\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 7430\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3691219091415405\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010335390456020832\n",
      "        model: {}\n",
      "        policy_loss: -0.07603420317173004\n",
      "        total_loss: 283.4937438964844\n",
      "        vf_explained_var: 0.7773721218109131\n",
      "        vf_loss: 283.5540466308594\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.305235602094236\n",
      "    ram_util_percent: 40.51675392670157\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06949693533174083\n",
      "    mean_env_wait_ms: 118.43573541676615\n",
      "    mean_inference_ms: 1.604398965273203\n",
      "    mean_raw_obs_processing_ms: 9.35150265498284\n",
      "  time_since_restore: 41828.68137764931\n",
      "  time_this_iter_s: 535.9522721767426\n",
      "  time_total_s: 151679.17535161972\n",
      "  timers:\n",
      "    learn_throughput: 366.123\n",
      "    learn_time_ms: 10925.28\n",
      "    load_throughput: 10322.615\n",
      "    load_time_ms: 387.499\n",
      "    sample_throughput: 7.641\n",
      "    sample_time_ms: 523473.059\n",
      "    update_time_ms: 3.408\n",
      "  timestamp: 1612944899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 136.16\n",
      "  episode_reward_max: 118.3976878155662\n",
      "  episode_reward_mean: 89.8180215661961\n",
      "  episode_reward_min: -106.1800133472685\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 7456\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.37231138348579407\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011143573559820652\n",
      "        model: {}\n",
      "        policy_loss: -0.07171215116977692\n",
      "        total_loss: 389.7134704589844\n",
      "        vf_explained_var: 0.6836399435997009\n",
      "        vf_loss: 389.768310546875\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.33488063660477\n",
      "    ram_util_percent: 40.52175066312998\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06950029131288073\n",
      "    mean_env_wait_ms: 118.44942270458586\n",
      "    mean_inference_ms: 1.6044624901248203\n",
      "    mean_raw_obs_processing_ms: 9.346769151518554\n",
      "  time_since_restore: 42356.51848053932\n",
      "  time_this_iter_s: 527.8371028900146\n",
      "  time_total_s: 152207.01245450974\n",
      "  timers:\n",
      "    learn_throughput: 365.96\n",
      "    learn_time_ms: 10930.154\n",
      "    load_throughput: 10279.254\n",
      "    load_time_ms: 389.133\n",
      "    sample_throughput: 7.645\n",
      "    sample_time_ms: 523213.818\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1612945427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 129.56\n",
      "  episode_reward_max: 118.38907803189579\n",
      "  episode_reward_mean: 79.15793387668813\n",
      "  episode_reward_min: -106.1800133472685\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 7491\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.4181641936302185\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01874413527548313\n",
      "        model: {}\n",
      "        policy_loss: -0.11580529063940048\n",
      "        total_loss: 903.9219360351562\n",
      "        vf_explained_var: 0.741776704788208\n",
      "        vf_loss: 904.0093994140625\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.52149410222805\n",
      "    ram_util_percent: 40.55032765399738\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06950381122408152\n",
      "    mean_env_wait_ms: 118.46017629196584\n",
      "    mean_inference_ms: 1.6045572335347642\n",
      "    mean_raw_obs_processing_ms: 9.347407631388204\n",
      "  time_since_restore: 42891.106843948364\n",
      "  time_this_iter_s: 534.5883634090424\n",
      "  time_total_s: 152741.60081791878\n",
      "  timers:\n",
      "    learn_throughput: 365.925\n",
      "    learn_time_ms: 10931.188\n",
      "    load_throughput: 10295.921\n",
      "    load_time_ms: 388.503\n",
      "    sample_throughput: 7.634\n",
      "    sample_time_ms: 523991.819\n",
      "    update_time_ms: 3.4\n",
      "  timestamp: 1612945962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 282\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 131.38\n",
      "  episode_reward_max: 118.37268002476236\n",
      "  episode_reward_mean: 68.3818894872424\n",
      "  episode_reward_min: -106.1800133472685\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 7522\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.45041367411613464\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015993593260645866\n",
      "        model: {}\n",
      "        policy_loss: -0.1078135147690773\n",
      "        total_loss: 917.2972412109375\n",
      "        vf_explained_var: 0.6977779269218445\n",
      "        vf_loss: 917.3807373046875\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.34323258869909\n",
      "    ram_util_percent: 40.58055190538765\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0695060439899176\n",
      "    mean_env_wait_ms: 118.46810521919463\n",
      "    mean_inference_ms: 1.6046388531288642\n",
      "    mean_raw_obs_processing_ms: 9.347778173045032\n",
      "  time_since_restore: 43424.396802425385\n",
      "  time_this_iter_s: 533.2899584770203\n",
      "  time_total_s: 153274.8907763958\n",
      "  timers:\n",
      "    learn_throughput: 365.813\n",
      "    learn_time_ms: 10934.56\n",
      "    load_throughput: 10282.908\n",
      "    load_time_ms: 388.995\n",
      "    sample_throughput: 7.617\n",
      "    sample_time_ms: 525124.248\n",
      "    update_time_ms: 3.439\n",
      "  timestamp: 1612946495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-50-29\n",
      "  done: false\n",
      "  episode_len_mean: 127.44\n",
      "  episode_reward_max: 118.398246663027\n",
      "  episode_reward_mean: 61.838339995953405\n",
      "  episode_reward_min: -105.69028246594925\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 7552\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.40359172224998474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009850187227129936\n",
      "        model: {}\n",
      "        policy_loss: -0.07553484290838242\n",
      "        total_loss: 327.4836730957031\n",
      "        vf_explained_var: 0.8231552839279175\n",
      "        vf_loss: 327.54425048828125\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.292772667542714\n",
      "    ram_util_percent: 40.60972404730617\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06950834913283394\n",
      "    mean_env_wait_ms: 118.47616458931476\n",
      "    mean_inference_ms: 1.604710811935626\n",
      "    mean_raw_obs_processing_ms: 9.350908301096528\n",
      "  time_since_restore: 43957.74543118477\n",
      "  time_this_iter_s: 533.3486287593842\n",
      "  time_total_s: 153808.23940515518\n",
      "  timers:\n",
      "    learn_throughput: 365.729\n",
      "    learn_time_ms: 10937.065\n",
      "    load_throughput: 10275.164\n",
      "    load_time_ms: 389.288\n",
      "    sample_throughput: 7.618\n",
      "    sample_time_ms: 525075.996\n",
      "    update_time_ms: 3.525\n",
      "  timestamp: 1612947029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 284\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_09-59-24\n",
      "  done: false\n",
      "  episode_len_mean: 130.14\n",
      "  episode_reward_max: 118.398246663027\n",
      "  episode_reward_mean: 74.5172555204358\n",
      "  episode_reward_min: -105.69028246594925\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 7582\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.36293870210647583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008887550793588161\n",
      "        model: {}\n",
      "        policy_loss: -0.06467510014772415\n",
      "        total_loss: 324.4293212890625\n",
      "        vf_explained_var: 0.7121971845626831\n",
      "        vf_loss: 324.4804992675781\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.196330275229364\n",
      "    ram_util_percent: 40.61114023591088\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06950992011721735\n",
      "    mean_env_wait_ms: 118.48866594726613\n",
      "    mean_inference_ms: 1.6047330625390026\n",
      "    mean_raw_obs_processing_ms: 9.350276528556721\n",
      "  time_since_restore: 44492.287269592285\n",
      "  time_this_iter_s: 534.5418384075165\n",
      "  time_total_s: 154342.7812435627\n",
      "  timers:\n",
      "    learn_throughput: 365.758\n",
      "    learn_time_ms: 10936.201\n",
      "    load_throughput: 10273.911\n",
      "    load_time_ms: 389.336\n",
      "    sample_throughput: 7.615\n",
      "    sample_time_ms: 525267.905\n",
      "    update_time_ms: 3.562\n",
      "  timestamp: 1612947564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_10-08-25\n",
      "  done: false\n",
      "  episode_len_mean: 136.03\n",
      "  episode_reward_max: 118.398246663027\n",
      "  episode_reward_mean: 82.95946381570985\n",
      "  episode_reward_min: -105.69028246594925\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 7610\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.39392825961112976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014838993549346924\n",
      "        model: {}\n",
      "        policy_loss: -0.09969846904277802\n",
      "        total_loss: 1159.6898193359375\n",
      "        vf_explained_var: 0.4761207401752472\n",
      "        vf_loss: 1159.766845703125\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.633808290155436\n",
      "    ram_util_percent: 40.68821243523316\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06951423446589448\n",
      "    mean_env_wait_ms: 118.50896110344944\n",
      "    mean_inference_ms: 1.6048002376053325\n",
      "    mean_raw_obs_processing_ms: 9.346438786145281\n",
      "  time_since_restore: 45033.333627939224\n",
      "  time_this_iter_s: 541.0463583469391\n",
      "  time_total_s: 154883.82760190964\n",
      "  timers:\n",
      "    learn_throughput: 362.993\n",
      "    learn_time_ms: 11019.489\n",
      "    load_throughput: 10185.356\n",
      "    load_time_ms: 392.721\n",
      "    sample_throughput: 7.61\n",
      "    sample_time_ms: 525620.522\n",
      "    update_time_ms: 3.608\n",
      "  timestamp: 1612948105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: a0e0c_00000\n",
      "  \n",
      "Result for PPO_ScoutingDiscreteTask_a0e0c_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-10_10-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 132.52\n",
      "  episode_reward_max: 118.398246663027\n",
      "  episode_reward_mean: 83.5106970852512\n",
      "  episode_reward_min: -105.63192916897212\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 7644\n",
      "  experiment_id: cbe2f70fbfd44ae186b60705bb56fcbb\n",
      "  hostname: workstation\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38366809487342834\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013387883082032204\n",
      "        model: {}\n",
      "        policy_loss: -0.09720607101917267\n",
      "        total_loss: 1059.09375\n",
      "        vf_explained_var: 0.5738667249679565\n",
      "        vf_loss: 1059.170654296875\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.178.60\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.12960102960102\n",
      "    ram_util_percent: 40.683912483912486\n",
      "  pid: 3339694\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06952904049834233\n",
      "    mean_env_wait_ms: 118.53967438018982\n",
      "    mean_inference_ms: 1.6049730312102413\n",
      "    mean_raw_obs_processing_ms: 9.34611090836681\n",
      "  time_since_restore: 45577.51814699173\n",
      "  time_this_iter_s: 544.1845190525055\n",
      "  time_total_s: 155428.01212096214\n",
      "  timers:\n",
      "    learn_throughput: 366.731\n",
      "    learn_time_ms: 10907.189\n",
      "    load_throughput: 10278.318\n",
      "    load_time_ms: 389.169\n",
      "    sample_throughput: 7.626\n",
      "    sample_time_ms: 524551.309\n",
      "    update_time_ms: 3.609\n",
      "  timestamp: 1612948649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: a0e0c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>     </td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">          110399</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\"> 76.3617</td><td style=\"text-align: right;\">             118.384</td><td style=\"text-align: right;\">            -96.5258</td><td style=\"text-align: right;\">           124.548</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">          110936</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\"> 68.5295</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">             -104.88</td><td style=\"text-align: right;\">           122.938</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">          111460</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\"> 64.1024</td><td style=\"text-align: right;\">             118.392</td><td style=\"text-align: right;\">            -106.574</td><td style=\"text-align: right;\">            117.39</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">          111986</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\"> 66.0876</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.574</td><td style=\"text-align: right;\">            123.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">          112515</td><td style=\"text-align: right;\">824000</td><td style=\"text-align: right;\">  61.681</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.574</td><td style=\"text-align: right;\">            123.84</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">          113035</td><td style=\"text-align: right;\">828000</td><td style=\"text-align: right;\"> 60.0617</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.341</td><td style=\"text-align: right;\">            128.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">          113560</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\">  62.083</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.568</td><td style=\"text-align: right;\">            126.04</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">          114086</td><td style=\"text-align: right;\">836000</td><td style=\"text-align: right;\"> 70.5095</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.895</td><td style=\"text-align: right;\">            128.89</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">          114602</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\"> 70.3951</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.895</td><td style=\"text-align: right;\">            131.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">          115151</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\"> 67.8485</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.895</td><td style=\"text-align: right;\">            134.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">          115683</td><td style=\"text-align: right;\">848000</td><td style=\"text-align: right;\"> 68.3023</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.895</td><td style=\"text-align: right;\">            136.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">          116201</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\"> 66.1529</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.496</td><td style=\"text-align: right;\">            128.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">          116722</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\"> 64.2617</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.496</td><td style=\"text-align: right;\">            132.11</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">          117283</td><td style=\"text-align: right;\">860000</td><td style=\"text-align: right;\"> 51.7821</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -106.496</td><td style=\"text-align: right;\">            125.82</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          117809</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\"> 49.6242</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -106.403</td><td style=\"text-align: right;\">            131.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">          118330</td><td style=\"text-align: right;\">868000</td><td style=\"text-align: right;\"> 58.0102</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -106.403</td><td style=\"text-align: right;\">            136.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">          118856</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\"> 64.4976</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">            -106.403</td><td style=\"text-align: right;\">            134.66</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">          119381</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\"> 55.8078</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.403</td><td style=\"text-align: right;\">            126.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">          119899</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\"> 64.5794</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -104.81</td><td style=\"text-align: right;\">            135.22</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">          120425</td><td style=\"text-align: right;\">884000</td><td style=\"text-align: right;\">  58.169</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -104.81</td><td style=\"text-align: right;\">            128.84</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">          120949</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\"> 68.4911</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -105.11</td><td style=\"text-align: right;\">            133.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">          121465</td><td style=\"text-align: right;\">892000</td><td style=\"text-align: right;\"> 58.2901</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">             -105.11</td><td style=\"text-align: right;\">            135.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">          121987</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\"> 66.8077</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">             -105.11</td><td style=\"text-align: right;\">            134.75</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">          122511</td><td style=\"text-align: right;\">900000</td><td style=\"text-align: right;\"> 66.8536</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -101.694</td><td style=\"text-align: right;\">            136.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">          123043</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\"> 66.9727</td><td style=\"text-align: right;\">             118.379</td><td style=\"text-align: right;\">            -104.826</td><td style=\"text-align: right;\">            131.28</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">          123560</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\"> 74.9086</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.342</td><td style=\"text-align: right;\">            136.18</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">          124093</td><td style=\"text-align: right;\">912000</td><td style=\"text-align: right;\"> 68.5897</td><td style=\"text-align: right;\">             118.397</td><td style=\"text-align: right;\">            -106.342</td><td style=\"text-align: right;\">            129.27</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">          124617</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\"> 64.4958</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.342</td><td style=\"text-align: right;\">             127.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">          125145</td><td style=\"text-align: right;\">920000</td><td style=\"text-align: right;\"> 60.6352</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.966</td><td style=\"text-align: right;\">            121.54</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.4/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">          125673</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\"> 62.6774</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -107.449</td><td style=\"text-align: right;\">            124.96</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">          126194</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\"> 68.5921</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -107.449</td><td style=\"text-align: right;\">             120.3</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">          126721</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\"> 68.3084</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -107.449</td><td style=\"text-align: right;\">            122.37</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">          127243</td><td style=\"text-align: right;\">936000</td><td style=\"text-align: right;\"> 64.1759</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.882</td><td style=\"text-align: right;\">            121.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">          127771</td><td style=\"text-align: right;\">940000</td><td style=\"text-align: right;\"> 66.1428</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.882</td><td style=\"text-align: right;\">            125.43</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">          128301</td><td style=\"text-align: right;\">944000</td><td style=\"text-align: right;\"> 68.6851</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.703</td><td style=\"text-align: right;\">            118.95</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          128835</td><td style=\"text-align: right;\">948000</td><td style=\"text-align: right;\"> 77.2841</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -105.703</td><td style=\"text-align: right;\">            119.48</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">          129356</td><td style=\"text-align: right;\">952000</td><td style=\"text-align: right;\"> 79.6432</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -100.912</td><td style=\"text-align: right;\">            127.35</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.5/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">          129888</td><td style=\"text-align: right;\">956000</td><td style=\"text-align: right;\"> 90.0024</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">             -105.06</td><td style=\"text-align: right;\">            134.81</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">          130414</td><td style=\"text-align: right;\">960000</td><td style=\"text-align: right;\"> 87.7144</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">             -105.06</td><td style=\"text-align: right;\">            131.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">          130940</td><td style=\"text-align: right;\">964000</td><td style=\"text-align: right;\"> 89.7627</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">             -105.06</td><td style=\"text-align: right;\">            133.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">          131469</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\"> 91.8603</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">             -105.06</td><td style=\"text-align: right;\">            127.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">          132000</td><td style=\"text-align: right;\">972000</td><td style=\"text-align: right;\"> 89.8074</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -107.419</td><td style=\"text-align: right;\">            129.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">          132529</td><td style=\"text-align: right;\">976000</td><td style=\"text-align: right;\"> 80.6841</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.419</td><td style=\"text-align: right;\">            123.15</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">          133055</td><td style=\"text-align: right;\">980000</td><td style=\"text-align: right;\"> 67.9097</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -107.419</td><td style=\"text-align: right;\">            118.76</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">          133585</td><td style=\"text-align: right;\">984000</td><td style=\"text-align: right;\"> 68.1261</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.019</td><td style=\"text-align: right;\">             120.5</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">          134110</td><td style=\"text-align: right;\">988000</td><td style=\"text-align: right;\"> 75.1127</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.147</td><td style=\"text-align: right;\">            123.29</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">          134637</td><td style=\"text-align: right;\">992000</td><td style=\"text-align: right;\">  79.459</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -105.147</td><td style=\"text-align: right;\">            127.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">          135160</td><td style=\"text-align: right;\">996000</td><td style=\"text-align: right;\"> 81.1533</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -115.518</td><td style=\"text-align: right;\">            132.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">          135683</td><td style=\"text-align: right;\">1000000</td><td style=\"text-align: right;\"> 80.8558</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -115.518</td><td style=\"text-align: right;\">            134.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">          136210</td><td style=\"text-align: right;\">1004000</td><td style=\"text-align: right;\"> 81.0557</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -115.518</td><td style=\"text-align: right;\">             133.8</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">          136738</td><td style=\"text-align: right;\">1008000</td><td style=\"text-align: right;\"> 85.8326</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.378</td><td style=\"text-align: right;\">            129.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">          137265</td><td style=\"text-align: right;\">1012000</td><td style=\"text-align: right;\"> 77.6983</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.372</td><td style=\"text-align: right;\">            124.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">          137796</td><td style=\"text-align: right;\">1016000</td><td style=\"text-align: right;\"> 81.7872</td><td style=\"text-align: right;\">               118.4</td><td style=\"text-align: right;\">            -106.372</td><td style=\"text-align: right;\">               132</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">          138316</td><td style=\"text-align: right;\">1020000</td><td style=\"text-align: right;\"> 83.6374</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -106.372</td><td style=\"text-align: right;\">            135.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">          138835</td><td style=\"text-align: right;\">1024000</td><td style=\"text-align: right;\"> 81.7584</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -102.873</td><td style=\"text-align: right;\">            137.62</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">          139361</td><td style=\"text-align: right;\">1028000</td><td style=\"text-align: right;\"> 83.7295</td><td style=\"text-align: right;\">             118.399</td><td style=\"text-align: right;\">            -99.0582</td><td style=\"text-align: right;\">            133.93</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">          139885</td><td style=\"text-align: right;\">1032000</td><td style=\"text-align: right;\"> 87.8305</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -99.0582</td><td style=\"text-align: right;\">            134.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          140411</td><td style=\"text-align: right;\">1036000</td><td style=\"text-align: right;\"> 85.6057</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.108</td><td style=\"text-align: right;\">            128.24</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">          140979</td><td style=\"text-align: right;\">1040000</td><td style=\"text-align: right;\"> 83.6018</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.108</td><td style=\"text-align: right;\">            129.05</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">          141506</td><td style=\"text-align: right;\">1044000</td><td style=\"text-align: right;\"> 79.3975</td><td style=\"text-align: right;\">             118.386</td><td style=\"text-align: right;\">            -105.108</td><td style=\"text-align: right;\">            130.17</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">          142035</td><td style=\"text-align: right;\">1048000</td><td style=\"text-align: right;\"> 79.2441</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.108</td><td style=\"text-align: right;\">            129.49</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">          142558</td><td style=\"text-align: right;\">1052000</td><td style=\"text-align: right;\"> 83.6207</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -104.85</td><td style=\"text-align: right;\">             134.7</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">          143088</td><td style=\"text-align: right;\">1056000</td><td style=\"text-align: right;\"> 85.7901</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -98.8285</td><td style=\"text-align: right;\">            132.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">          143658</td><td style=\"text-align: right;\">1060000</td><td style=\"text-align: right;\"> 81.4712</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -106.382</td><td style=\"text-align: right;\">            133.12</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">          144213</td><td style=\"text-align: right;\">1064000</td><td style=\"text-align: right;\"> 85.3723</td><td style=\"text-align: right;\">             118.393</td><td style=\"text-align: right;\">            -106.382</td><td style=\"text-align: right;\">            132.45</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">          144738</td><td style=\"text-align: right;\">1068000</td><td style=\"text-align: right;\"> 89.5405</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -106.382</td><td style=\"text-align: right;\">            127.94</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">          145268</td><td style=\"text-align: right;\">1072000</td><td style=\"text-align: right;\"> 89.6042</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -106.382</td><td style=\"text-align: right;\">            136.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">          145799</td><td style=\"text-align: right;\">1076000</td><td style=\"text-align: right;\">  87.921</td><td style=\"text-align: right;\">             118.391</td><td style=\"text-align: right;\">            -99.4603</td><td style=\"text-align: right;\">            134.34</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">          146330</td><td style=\"text-align: right;\">1080000</td><td style=\"text-align: right;\"> 77.2294</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">             -105.88</td><td style=\"text-align: right;\">            131.06</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">          146860</td><td style=\"text-align: right;\">1084000</td><td style=\"text-align: right;\"> 81.1933</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">             -105.88</td><td style=\"text-align: right;\">            129.68</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">          147387</td><td style=\"text-align: right;\">1088000</td><td style=\"text-align: right;\"> 80.9832</td><td style=\"text-align: right;\">             118.395</td><td style=\"text-align: right;\">             -105.88</td><td style=\"text-align: right;\">            130.01</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">          147909</td><td style=\"text-align: right;\">1092000</td><td style=\"text-align: right;\"> 85.1782</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">             -105.88</td><td style=\"text-align: right;\">            130.98</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.6/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">          148443</td><td style=\"text-align: right;\">1096000</td><td style=\"text-align: right;\"> 79.2294</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">            -105.332</td><td style=\"text-align: right;\">            131.71</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">          148975</td><td style=\"text-align: right;\">1100000</td><td style=\"text-align: right;\"> 79.0947</td><td style=\"text-align: right;\">             118.385</td><td style=\"text-align: right;\">            -105.332</td><td style=\"text-align: right;\">            130.31</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">          149512</td><td style=\"text-align: right;\">1104000</td><td style=\"text-align: right;\"> 83.3583</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">            -105.332</td><td style=\"text-align: right;\">            132.46</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">          150068</td><td style=\"text-align: right;\">1108000</td><td style=\"text-align: right;\">  87.531</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.332</td><td style=\"text-align: right;\">            135.97</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">          150613</td><td style=\"text-align: right;\">1112000</td><td style=\"text-align: right;\"> 79.1456</td><td style=\"text-align: right;\">             118.396</td><td style=\"text-align: right;\">            -105.523</td><td style=\"text-align: right;\">            136.53</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">          151143</td><td style=\"text-align: right;\">1116000</td><td style=\"text-align: right;\"> 77.2501</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.523</td><td style=\"text-align: right;\">            137.55</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">          151679</td><td style=\"text-align: right;\">1120000</td><td style=\"text-align: right;\"> 75.1753</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.523</td><td style=\"text-align: right;\">            133.02</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">          152207</td><td style=\"text-align: right;\">1124000</td><td style=\"text-align: right;\">  89.818</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -106.18</td><td style=\"text-align: right;\">            136.16</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">          152742</td><td style=\"text-align: right;\">1128000</td><td style=\"text-align: right;\"> 79.1579</td><td style=\"text-align: right;\">             118.389</td><td style=\"text-align: right;\">             -106.18</td><td style=\"text-align: right;\">            129.56</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">          153275</td><td style=\"text-align: right;\">1132000</td><td style=\"text-align: right;\"> 68.3819</td><td style=\"text-align: right;\">             118.373</td><td style=\"text-align: right;\">             -106.18</td><td style=\"text-align: right;\">            131.38</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">          153808</td><td style=\"text-align: right;\">1136000</td><td style=\"text-align: right;\"> 61.8383</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -105.69</td><td style=\"text-align: right;\">            127.44</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">          154343</td><td style=\"text-align: right;\">1140000</td><td style=\"text-align: right;\"> 74.5173</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -105.69</td><td style=\"text-align: right;\">            130.14</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.8/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">          154884</td><td style=\"text-align: right;\">1144000</td><td style=\"text-align: right;\"> 82.9595</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">             -105.69</td><td style=\"text-align: right;\">            136.03</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 12.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/8 CPUs, 1/1 GPUs, 0.0/13.72 GiB heap, 0.0/4.74 GiB objects (0/1.0 accelerator_type:GTX)<br>Result logdir: /home/dschori/ray_results/PPO_2021-02-09_21-37-29<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n<thead>\n<tr><th>Trial name                          </th><th>status  </th><th>loc                   </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_ScoutingDiscreteTask_a0e0c_00000</td><td>RUNNING </td><td>192.168.178.60:3339694</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">          155428</td><td style=\"text-align: right;\">1148000</td><td style=\"text-align: right;\"> 83.5107</td><td style=\"text-align: right;\">             118.398</td><td style=\"text-align: right;\">            -105.632</td><td style=\"text-align: right;\">            132.52</td></tr>\n</tbody>\n</table><br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_path, analysis = train(stop_criteria=stop,\n",
    "                                  config=config,\n",
    "                                  restorepath='/home/dschori/ray_results/'\n",
    "                                              'PPO_2021-02-09_17-09-27/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_2f1df_00000_0_2021-02-09_17-09-27/' \\\n",
    "                  'checkpoint_201/checkpoint-201')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Restore Agent for Testing:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m WARNING:tensorflow:From /home/dschori/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m Instructions for updating:\n",
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m non-resource variables are not supported in the long term\n",
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m [ERROR] [1612618532.740385, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m [WARN] [1612618532.745021, 0.000000]: Start Init ControllersConnection\n",
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m [WARN] [1612618532.746567, 0.000000]: END Init ControllersConnection\n",
      "2021-02-06 14:35:38,052\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-06 14:35:38,209\tINFO trainable.py:328 -- Restored on 192.168.178.60 from checkpoint: /home/dschori/ray_results/PPO_2021-02-04_21-27-48/PPO_ScoutingDiscreteTask_729ff_00000_0_2021-02-04_21-27-48/checkpoint_185/checkpoint-185\n",
      "2021-02-06 14:35:38,210\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 185, '_timesteps_total': None, '_time_total': 100741.15248703957, '_episodes_total': 4508}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=12396)\u001B[0m None\n"
     ]
    }
   ],
   "source": [
    "checkpoint_nr = 185\n",
    "checkpoint_path = '/home/dschori/ray_results/PPO_2021-02-04_21-27-48/' \\\n",
    "                  'PPO_ScoutingDiscreteTask_729ff_00000_0_2021-02-04_21-27-48/' \\\n",
    "                  'checkpoint_{}/checkpoint-{}'.format(checkpoint_nr, checkpoint_nr)\n",
    "agent = load(checkpoint_path=checkpoint_path, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import time\n",
    "time_now = time.time()\n",
    "while True:\n",
    "    episode_reward = test(agent=agent, env=env)\n",
    "    if time.time() - time_now > 200:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-49841455",
   "language": "python",
   "display_name": "PyCharm (MasterThesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}