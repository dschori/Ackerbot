{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import rospy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from neuroracer_gym import neuroracer_env\n",
    "from tf_agents.environments import tf_py_environment, utils\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
    "\n",
    "# just to register env:\n",
    "from neuroracer_gym.tasks.neuroracer_discrete_task import NeuroRacerTfAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] [1609759330.691125, 0.000000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1609759330.697017, 0.006000]: Start Init ControllersConnection\n",
      "[WARN] [1609759330.698069, 0.006000]: END Init ControllersConnection\n",
      "[ERROR] [1609759333.172889, 2.470000]: NOT Initialising Simulation Physics Parameters\n",
      "[WARN] [1609759333.177111, 2.470000]: Start Init ControllersConnection\n",
      "[WARN] [1609759333.178668, 2.470000]: END Init ControllersConnection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n",
      "time_step_spec.observation: BoundedArraySpec(shape=(30,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=10.0)\n",
      "time_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\n",
      "time_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\n",
      "time_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "rospy.init_node('neuroracer_qlearn', anonymous=True, log_level=rospy.INFO)\n",
    "\n",
    "env = NeuroRacerTfAgents()\n",
    "env_eval = NeuroRacerTfAgents(val=True)\n",
    "\n",
    "print('action_spec:', env.action_spec())\n",
    "print('time_step_spec.observation:', env.time_step_spec().observation)\n",
    "print('time_step_spec.step_type:', env.time_step_spec().step_type)\n",
    "print('time_step_spec.discount:', env.time_step_spec().discount)\n",
    "print('time_step_spec.reward:', env.time_step_spec().reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collided, Cumulated Reward: 109, n_steps: 34\n"
     ]
    }
   ],
   "source": [
    "utils.validate_py_environment(env, episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "time_step = env.reset()\n",
    "\n",
    "for _ in range(1):\n",
    "    time_step = env.step(np.array(2, dtype=np.int32))\n",
    "    print(time_step.reward)\n",
    "\n",
    "cumulative_reward = time_step.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedArraySpec(shape=(30,), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=10.0)\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_spec())\n",
    "print(env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(env)\n",
    "env_eval = tf_py_environment.TFPyEnvironment(env_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc_layer_params = (128, )\n",
    "dropout_layer_params = (0.15, )\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    dropout_layer_params=dropout_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (128,)\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    env.observation_spec(),\n",
    "    env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    n_step_update=1,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gradient_clipping=1.0,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "print(env.batch_size)\n",
    "print(agent.collect_data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(30,), dtype=tf.float32, name='observation', minimum=array(0., dtype=float32), maximum=array(10., dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-3)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = reinforce_agent.ReinforceAgent(\n",
    "    env.time_step_spec(),\n",
    "    env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter,\n",
    "    debug_summaries=True,\n",
    "    summarize_grads_and_vars=True)\n",
    "agent.initialize()\n",
    "\n",
    "print(env.batch_size)\n",
    "print(agent.collect_data_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics.\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=2000)\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "    episode_counter = 0\n",
    "    environment.reset()\n",
    "\n",
    "    while episode_counter < num_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        # Add trajectory to the replay buffer\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1\n",
    "\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collided, Cumulated Reward: 162, n_steps: 28\n",
      "Collided, Cumulated Reward: 147, n_steps: 42\n",
      "Starting training...\n",
      "Collided, Cumulated Reward: 138, n_steps: 41\n",
      "Collided, Cumulated Reward: 77, n_steps: 24\n",
      "step = 1: loss = -0.818091094493866\n",
      "Collided, Cumulated Reward: 160, n_steps: 46\n",
      "Collided, Cumulated Reward: 199, n_steps: 60\n",
      "step = 2: loss = 0.6095919609069824\n",
      "Collided, Cumulated Reward: 229, n_steps: 65\n",
      "Collided, Cumulated Reward: 101, n_steps: 31\n",
      "step = 3: loss = 1.991593599319458\n",
      "Collided, Cumulated Reward: 211, n_steps: 64\n",
      "Collided, Cumulated Reward: 96, n_steps: 27\n",
      "step = 4: loss = 1.2532529830932617\n",
      "Collided, Cumulated Reward: 148, n_steps: 43\n",
      "Collided, Cumulated Reward: 150, n_steps: 44\n",
      "step = 5: loss = 0.7624613046646118\n",
      "Collided, Cumulated Reward: 224, n_steps: 67\n",
      "Collided, Cumulated Reward: 114, n_steps: 36\n",
      "step = 6: loss = 0.008882641792297363\n",
      "Collided, Cumulated Reward: 188, n_steps: 59\n",
      "Collided, Cumulated Reward: 141, n_steps: 42\n",
      "step = 7: loss = -1.25008225440979\n",
      "Collided, Cumulated Reward: 104, n_steps: 31\n",
      "Collided, Cumulated Reward: 140, n_steps: 41\n",
      "step = 8: loss = -0.16798490285873413\n",
      "Collided, Cumulated Reward: 168, n_steps: 52\n",
      "Collided, Cumulated Reward: 135, n_steps: 39\n",
      "step = 9: loss = -0.3028731346130371\n",
      "Collided, Cumulated Reward: 152, n_steps: 45\n",
      "Collided, Cumulated Reward: 141, n_steps: 42\n",
      "step = 10: loss = -0.1621323823928833\n",
      "Collided, Cumulated Reward: 153, n_steps: 47\n",
      "Collided, Cumulated Reward: 142, n_steps: 43\n",
      "step = 11: loss = -2.0932059288024902\n",
      "Collided, Cumulated Reward: 168, n_steps: 49\n",
      "Collided, Cumulated Reward: 91, n_steps: 27\n",
      "step = 12: loss = -0.9828701019287109\n",
      "Collided, Cumulated Reward: 150, n_steps: 46\n",
      "Collided, Cumulated Reward: 149, n_steps: 43\n",
      "step = 13: loss = -0.3337136507034302\n",
      "Collided, Cumulated Reward: 161, n_steps: 51\n",
      "Collided, Cumulated Reward: 150, n_steps: 44\n",
      "step = 14: loss = -2.131869077682495\n",
      "Collided, Cumulated Reward: 153, n_steps: 45\n",
      "Collided, Cumulated Reward: 140, n_steps: 42\n",
      "step = 15: loss = -1.5361747741699219\n",
      "Collided, Cumulated Reward: 148, n_steps: 47\n",
      "Collided, Cumulated Reward: 146, n_steps: 51\n",
      "step = 16: loss = -3.0367186069488525\n",
      "Collided, Cumulated Reward: 199, n_steps: 67\n",
      "Collided, Cumulated Reward: 143, n_steps: 42\n",
      "step = 17: loss = -0.7041692733764648\n",
      "Collided, Cumulated Reward: 151, n_steps: 44\n",
      "Collided, Cumulated Reward: 141, n_steps: 43\n",
      "step = 18: loss = -1.1386727094650269\n",
      "Collided, Cumulated Reward: 150, n_steps: 45\n",
      "Collided, Cumulated Reward: 135, n_steps: 41\n",
      "step = 19: loss = -0.2876865863800049\n",
      "Collided, Cumulated Reward: 153, n_steps: 49\n",
      "Collided, Cumulated Reward: 138, n_steps: 48\n",
      "step = 20: loss = -0.6456854343414307\n",
      "Collided, Cumulated Reward: 53, n_steps: 24\n",
      "Collided, Cumulated Reward: 54, n_steps: 30\n",
      "step = 20: Average Return = 3.032686233520508\n",
      "Collided, Cumulated Reward: 134, n_steps: 43\n",
      "Collided, Cumulated Reward: 193, n_steps: 64\n",
      "step = 21: loss = -0.4073023796081543\n",
      "Collided, Cumulated Reward: 105, n_steps: 31\n",
      "Collided, Cumulated Reward: 130, n_steps: 40\n",
      "step = 22: loss = -1.887284517288208\n",
      "Collided, Cumulated Reward: 151, n_steps: 44\n",
      "Collided, Cumulated Reward: 400, n_steps: 173\n",
      "step = 23: loss = -0.7604472637176514\n",
      "Collided, Cumulated Reward: 217, n_steps: 66\n",
      "Collided, Cumulated Reward: 477, n_steps: 170\n",
      "step = 24: loss = -1.5962903499603271\n",
      "Collided, Cumulated Reward: 178, n_steps: 54\n",
      "Collided, Cumulated Reward: 150, n_steps: 44\n",
      "step = 25: loss = -1.3230572938919067\n",
      "Collided, Cumulated Reward: 186, n_steps: 58\n",
      "Collided, Cumulated Reward: 126, n_steps: 37\n",
      "step = 26: loss = -2.003277540206909\n",
      "Collided, Cumulated Reward: 224, n_steps: 67\n",
      "Collided, Cumulated Reward: 232, n_steps: 69\n",
      "step = 27: loss = -1.8911786079406738\n",
      "Collided, Cumulated Reward: 155, n_steps: 51\n",
      "Collided, Cumulated Reward: 144, n_steps: 48\n",
      "step = 28: loss = -0.3126528263092041\n",
      "Collided, Cumulated Reward: 158, n_steps: 47\n",
      "Collided, Cumulated Reward: 171, n_steps: 50\n",
      "step = 29: loss = -0.6137809753417969\n",
      "Collided, Cumulated Reward: 243, n_steps: 78\n",
      "Collided, Cumulated Reward: 60, n_steps: 20\n",
      "step = 30: loss = -1.0255553722381592\n",
      "Collided, Cumulated Reward: 188, n_steps: 60\n",
      "Collided, Cumulated Reward: 228, n_steps: 65\n",
      "step = 31: loss = -3.2119815349578857\n",
      "Collided, Cumulated Reward: 152, n_steps: 44\n",
      "Collided, Cumulated Reward: 82, n_steps: 24\n",
      "step = 32: loss = -1.5882019996643066\n",
      "Collided, Cumulated Reward: 152, n_steps: 46\n",
      "Collided, Cumulated Reward: 94, n_steps: 29\n",
      "step = 33: loss = -0.9331462383270264\n",
      "Collided, Cumulated Reward: 210, n_steps: 64\n",
      "Collided, Cumulated Reward: 141, n_steps: 44\n",
      "step = 34: loss = -2.4059295654296875\n",
      "Collided, Cumulated Reward: 500, n_steps: 318\n",
      "Collided, Cumulated Reward: 83, n_steps: 25\n",
      "step = 35: loss = 10.37564468383789\n",
      "Collided, Cumulated Reward: 150, n_steps: 47\n",
      "Collided, Cumulated Reward: 144, n_steps: 44\n",
      "step = 36: loss = 1.9245433807373047\n",
      "Collided, Cumulated Reward: 172, n_steps: 60\n",
      "Collided, Cumulated Reward: 145, n_steps: 42\n",
      "step = 37: loss = -0.6460796594619751\n",
      "Collided, Cumulated Reward: 147, n_steps: 45\n",
      "Collided, Cumulated Reward: 145, n_steps: 47\n",
      "step = 38: loss = -4.413754463195801\n",
      "Collided, Cumulated Reward: 160, n_steps: 48\n",
      "Collided, Cumulated Reward: 341, n_steps: 101\n",
      "step = 39: loss = 4.766353607177734\n",
      "Collided, Cumulated Reward: 160, n_steps: 48\n",
      "Collided, Cumulated Reward: 183, n_steps: 69\n",
      "step = 40: loss = -4.9975996017456055\n",
      "Collided, Cumulated Reward: 59, n_steps: 42\n",
      "Collided, Cumulated Reward: 281, n_steps: 95\n",
      "step = 40: Average Return = 119.94823455810547\n",
      "Collided, Cumulated Reward: 151, n_steps: 46\n",
      "Collided, Cumulated Reward: 97, n_steps: 30\n",
      "step = 41: loss = 1.1558828353881836\n",
      "Collided, Cumulated Reward: 184, n_steps: 61\n",
      "Collided, Cumulated Reward: 91, n_steps: 27\n",
      "step = 42: loss = -2.1214165687561035\n",
      "Collided, Cumulated Reward: 149, n_steps: 44\n",
      "Collided, Cumulated Reward: 142, n_steps: 45\n",
      "step = 43: loss = -2.9689440727233887\n",
      "Collided, Cumulated Reward: 159, n_steps: 47\n",
      "Collided, Cumulated Reward: 345, n_steps: 109\n",
      "step = 44: loss = -0.342517614364624\n",
      "Collided, Cumulated Reward: 335, n_steps: 124\n",
      "Collided, Cumulated Reward: 111, n_steps: 36\n",
      "step = 45: loss = 3.3572423458099365\n",
      "Collided, Cumulated Reward: 158, n_steps: 50\n",
      "Collided, Cumulated Reward: 92, n_steps: 28\n",
      "step = 46: loss = -5.1739325523376465\n",
      "Collided, Cumulated Reward: 153, n_steps: 47\n",
      "Collided, Cumulated Reward: 170, n_steps: 50\n",
      "step = 47: loss = -0.6466813087463379\n",
      "Collided, Cumulated Reward: 171, n_steps: 62\n",
      "Collided, Cumulated Reward: 136, n_steps: 50\n",
      "step = 48: loss = -4.1753644943237305\n",
      "Collided, Cumulated Reward: 151, n_steps: 44\n",
      "Collided, Cumulated Reward: 96, n_steps: 72\n",
      "step = 49: loss = 2.4648609161376953\n",
      "Collided, Cumulated Reward: 156, n_steps: 50\n",
      "Collided, Cumulated Reward: 98, n_steps: 37\n",
      "step = 50: loss = -1.9469085931777954\n",
      "Collided, Cumulated Reward: 155, n_steps: 53\n",
      "Collided, Cumulated Reward: 54, n_steps: 17\n",
      "step = 51: loss = -2.952913999557495\n",
      "Collided, Cumulated Reward: 158, n_steps: 48\n",
      "Collided, Cumulated Reward: 159, n_steps: 50\n",
      "step = 52: loss = -5.137331008911133\n",
      "Collided, Cumulated Reward: 213, n_steps: 62\n",
      "Collided, Cumulated Reward: 226, n_steps: 70\n",
      "step = 53: loss = -4.573004245758057\n",
      "Collided, Cumulated Reward: 162, n_steps: 48\n",
      "Collided, Cumulated Reward: 186, n_steps: 68\n",
      "step = 54: loss = -3.766474723815918\n",
      "Collided, Cumulated Reward: 134, n_steps: 44\n",
      "Collided, Cumulated Reward: 149, n_steps: 59\n",
      "step = 55: loss = -2.1097614765167236\n",
      "Collided, Cumulated Reward: 124, n_steps: 39\n",
      "Collided, Cumulated Reward: 142, n_steps: 53\n",
      "step = 56: loss = -4.324059009552002\n",
      "Collided, Cumulated Reward: 148, n_steps: 46\n",
      "Collided, Cumulated Reward: 27, n_steps: 9\n",
      "step = 57: loss = -1.9116052389144897\n",
      "Collided, Cumulated Reward: 180, n_steps: 54\n",
      "Collided, Cumulated Reward: 195, n_steps: 59\n",
      "step = 58: loss = -2.810441017150879\n",
      "Collided, Cumulated Reward: 132, n_steps: 43\n",
      "Collided, Cumulated Reward: 309, n_steps: 112\n",
      "step = 59: loss = 7.126943111419678\n",
      "Collided, Cumulated Reward: 155, n_steps: 53\n",
      "Collided, Cumulated Reward: 207, n_steps: 63\n",
      "step = 60: loss = -5.024715423583984\n",
      "Collided, Cumulated Reward: 58, n_steps: 26\n",
      "Collided, Cumulated Reward: 37, n_steps: 17\n",
      "step = 60: Average Return = -3.9383392333984375\n",
      "Collided, Cumulated Reward: 145, n_steps: 50\n",
      "Collided, Cumulated Reward: 161, n_steps: 51\n",
      "step = 61: loss = -2.2688169479370117\n",
      "Collided, Cumulated Reward: 104, n_steps: 46\n",
      "Collided, Cumulated Reward: 221, n_steps: 66\n",
      "step = 62: loss = -1.1887856721878052\n",
      "Collided, Cumulated Reward: 133, n_steps: 54\n",
      "Collided, Cumulated Reward: 321, n_steps: 125\n",
      "step = 63: loss = 0.6408557891845703\n",
      "Collided, Cumulated Reward: 128, n_steps: 44\n",
      "Collided, Cumulated Reward: 209, n_steps: 175\n",
      "step = 64: loss = 0.4133216142654419\n",
      "Collided, Cumulated Reward: 223, n_steps: 67\n",
      "Collided, Cumulated Reward: 157, n_steps: 58\n",
      "step = 65: loss = -3.91642165184021\n",
      "Collided, Cumulated Reward: 162, n_steps: 55\n",
      "Collided, Cumulated Reward: 145, n_steps: 49\n",
      "step = 66: loss = -8.715665817260742\n",
      "Collided, Cumulated Reward: 181, n_steps: 56\n",
      "Collided, Cumulated Reward: 144, n_steps: 53\n",
      "step = 67: loss = -2.23960280418396\n",
      "Collided, Cumulated Reward: 159, n_steps: 48\n",
      "Collided, Cumulated Reward: 173, n_steps: 59\n",
      "step = 68: loss = -4.1043853759765625\n",
      "Collided, Cumulated Reward: 188, n_steps: 61\n",
      "Collided, Cumulated Reward: 93, n_steps: 28\n",
      "step = 69: loss = -5.046486854553223\n",
      "Collided, Cumulated Reward: 315, n_steps: 120\n",
      "Collided, Cumulated Reward: 151, n_steps: 48\n",
      "step = 70: loss = 2.6440327167510986\n",
      "Collided, Cumulated Reward: 163, n_steps: 49\n",
      "Collided, Cumulated Reward: 126, n_steps: 45\n",
      "step = 71: loss = -1.2375541925430298\n",
      "Collided, Cumulated Reward: 179, n_steps: 62\n",
      "Collided, Cumulated Reward: 154, n_steps: 53\n",
      "step = 72: loss = -2.010636329650879\n",
      "Collided, Cumulated Reward: 172, n_steps: 58\n",
      "Collided, Cumulated Reward: 146, n_steps: 51\n",
      "step = 73: loss = -3.4335074424743652\n",
      "Collided, Cumulated Reward: 117, n_steps: 52\n",
      "Collided, Cumulated Reward: 156, n_steps: 53\n",
      "step = 74: loss = -8.404265403747559\n",
      "Collided, Cumulated Reward: 184, n_steps: 65\n",
      "Collided, Cumulated Reward: 143, n_steps: 42\n",
      "step = 75: loss = -3.8104758262634277\n",
      "Collided, Cumulated Reward: 314, n_steps: 120\n",
      "Collided, Cumulated Reward: 333, n_steps: 132\n",
      "step = 76: loss = 13.098106384277344\n",
      "Collided, Cumulated Reward: 196, n_steps: 59\n",
      "Collided, Cumulated Reward: 215, n_steps: 67\n",
      "step = 77: loss = -4.205838203430176\n",
      "Collided, Cumulated Reward: 148, n_steps: 49\n",
      "Collided, Cumulated Reward: 150, n_steps: 45\n",
      "step = 78: loss = 1.635025978088379\n",
      "Collided, Cumulated Reward: 347, n_steps: 140\n",
      "Collided, Cumulated Reward: 176, n_steps: 54\n",
      "step = 79: loss = 3.370150089263916\n",
      "Collided, Cumulated Reward: 170, n_steps: 54\n",
      "Collided, Cumulated Reward: 299, n_steps: 108\n",
      "step = 80: loss = 7.071803092956543\n",
      "Collided, Cumulated Reward: 59, n_steps: 48\n",
      "Collided, Cumulated Reward: 285, n_steps: 106\n",
      "step = 80: Average Return = 122.5975341796875\n",
      "Collided, Cumulated Reward: 154, n_steps: 47\n",
      "Collided, Cumulated Reward: 165, n_steps: 56\n",
      "step = 81: loss = -4.121549606323242\n",
      "Collided, Cumulated Reward: 170, n_steps: 62\n",
      "Collided, Cumulated Reward: 133, n_steps: 51\n",
      "step = 82: loss = -8.767382621765137\n",
      "Collided, Cumulated Reward: 170, n_steps: 61\n",
      "Collided, Cumulated Reward: 141, n_steps: 42\n",
      "step = 83: loss = -5.170519828796387\n",
      "Collided, Cumulated Reward: 193, n_steps: 64\n",
      "Collided, Cumulated Reward: 136, n_steps: 43\n",
      "step = 84: loss = -3.8331596851348877\n",
      "Collided, Cumulated Reward: 162, n_steps: 49\n",
      "Collided, Cumulated Reward: 150, n_steps: 48\n",
      "step = 85: loss = -1.8247485160827637\n",
      "Collided, Cumulated Reward: 138, n_steps: 62\n",
      "Collided, Cumulated Reward: 130, n_steps: 65\n",
      "step = 86: loss = -8.27374267578125\n",
      "Collided, Cumulated Reward: 344, n_steps: 132\n",
      "Collided, Cumulated Reward: 110, n_steps: 38\n",
      "step = 87: loss = 4.755800724029541\n",
      "Collided, Cumulated Reward: 169, n_steps: 52\n",
      "Collided, Cumulated Reward: 321, n_steps: 115\n",
      "step = 88: loss = -4.08973503112793\n",
      "Collided, Cumulated Reward: 174, n_steps: 58\n",
      "Collided, Cumulated Reward: 936, n_steps: 536\n",
      "step = 89: loss = 6.70793342590332\n",
      "Collided, Cumulated Reward: 224, n_steps: 75\n",
      "Collided, Cumulated Reward: 157, n_steps: 61\n",
      "step = 90: loss = -5.400759220123291\n",
      "Collided, Cumulated Reward: 152, n_steps: 46\n",
      "Collided, Cumulated Reward: 203, n_steps: 71\n",
      "step = 91: loss = -2.6417369842529297\n",
      "Collided, Cumulated Reward: 154, n_steps: 45\n",
      "Collided, Cumulated Reward: 140, n_steps: 45\n",
      "step = 92: loss = -0.8954493999481201\n",
      "Collided, Cumulated Reward: 156, n_steps: 49\n",
      "Collided, Cumulated Reward: 202, n_steps: 66\n",
      "step = 93: loss = 1.6916718482971191\n",
      "Collided, Cumulated Reward: 150, n_steps: 45\n",
      "Collided, Cumulated Reward: 39, n_steps: 13\n",
      "step = 94: loss = 3.164498805999756\n",
      "Collided, Cumulated Reward: 154, n_steps: 50\n",
      "Collided, Cumulated Reward: 103, n_steps: 32\n",
      "step = 95: loss = -2.248981475830078\n",
      "Collided, Cumulated Reward: 150, n_steps: 44\n",
      "Collided, Cumulated Reward: 27, n_steps: 9\n",
      "step = 96: loss = 0.9076263904571533\n",
      "Collided, Cumulated Reward: 173, n_steps: 52\n",
      "Collided, Cumulated Reward: 147, n_steps: 44\n",
      "step = 97: loss = 4.918850421905518\n",
      "Collided, Cumulated Reward: 158, n_steps: 47\n",
      "Collided, Cumulated Reward: 145, n_steps: 45\n",
      "step = 98: loss = -0.1680281162261963\n",
      "Collided, Cumulated Reward: 205, n_steps: 66\n",
      "Collided, Cumulated Reward: 149, n_steps: 47\n",
      "step = 99: loss = 0.14872336387634277\n",
      "Collided, Cumulated Reward: 148, n_steps: 47\n",
      "Collided, Cumulated Reward: 160, n_steps: 47\n",
      "step = 100: loss = 0.6455250978469849\n",
      "Collided, Cumulated Reward: 162, n_steps: 52\n",
      "Collided, Cumulated Reward: 148, n_steps: 51\n",
      "step = 100: Average Return = 102.2687759399414\n",
      "Collided, Cumulated Reward: 145, n_steps: 45\n",
      "Collided, Cumulated Reward: 86, n_steps: 27\n",
      "step = 101: loss = 3.0780553817749023\n",
      "Collided, Cumulated Reward: 157, n_steps: 47\n",
      "Collided, Cumulated Reward: 156, n_steps: 51\n",
      "step = 102: loss = -0.24717259407043457\n",
      "Collided, Cumulated Reward: 157, n_steps: 46\n",
      "Collided, Cumulated Reward: 141, n_steps: 40\n",
      "step = 103: loss = 1.0450390577316284\n",
      "Collided, Cumulated Reward: 150, n_steps: 47\n",
      "Collided, Cumulated Reward: 138, n_steps: 41\n",
      "step = 104: loss = -5.22901725769043\n",
      "Collided, Cumulated Reward: 106, n_steps: 31\n",
      "Collided, Cumulated Reward: 149, n_steps: 48\n",
      "step = 105: loss = -0.30808621644973755\n",
      "Collided, Cumulated Reward: 154, n_steps: 49\n",
      "Collided, Cumulated Reward: 322, n_steps: 100\n",
      "step = 106: loss = -3.2846271991729736\n",
      "Collided, Cumulated Reward: 198, n_steps: 65\n",
      "Collided, Cumulated Reward: 139, n_steps: 42\n",
      "step = 107: loss = -1.2297792434692383\n",
      "Collided, Cumulated Reward: 171, n_steps: 52\n",
      "Collided, Cumulated Reward: 154, n_steps: 45\n",
      "step = 108: loss = 1.4959239959716797\n",
      "Too slow, Cumulated Reward: -20, n_steps: 2629\n",
      "Collided, Cumulated Reward: 141, n_steps: 42\n",
      "step = 109: loss = 53.850486755371094\n",
      "Collided, Cumulated Reward: 211, n_steps: 64\n",
      "Collided, Cumulated Reward: 968, n_steps: 524\n",
      "step = 110: loss = 11.113032341003418\n",
      "Collided, Cumulated Reward: 248, n_steps: 76\n",
      "Collided, Cumulated Reward: 275, n_steps: 235\n",
      "step = 111: loss = -2.679025173187256\n",
      "Collided, Cumulated Reward: 196, n_steps: 60\n",
      "Collided, Cumulated Reward: 141, n_steps: 61\n",
      "step = 112: loss = -3.931924819946289\n",
      "Collided, Cumulated Reward: 151, n_steps: 48\n",
      "Collided, Cumulated Reward: 343, n_steps: 130\n",
      "step = 113: loss = 1.766242504119873\n",
      "Collided, Cumulated Reward: 162, n_steps: 50\n",
      "Collided, Cumulated Reward: 138, n_steps: 49\n",
      "step = 114: loss = -3.6477110385894775\n",
      "Collided, Cumulated Reward: 144, n_steps: 56\n",
      "Collided, Cumulated Reward: 164, n_steps: 50\n",
      "step = 115: loss = -10.411081314086914\n",
      "Collided, Cumulated Reward: 328, n_steps: 126\n",
      "Collided, Cumulated Reward: 184, n_steps: 61\n",
      "step = 116: loss = 4.36983585357666\n",
      "Collided, Cumulated Reward: 140, n_steps: 52\n",
      "Collided, Cumulated Reward: 83, n_steps: 64\n",
      "step = 117: loss = 1.2691287994384766\n",
      "Collided, Cumulated Reward: 210, n_steps: 63\n",
      "Collided, Cumulated Reward: 76, n_steps: 55\n",
      "step = 118: loss = 4.88457727432251\n",
      "Collided, Cumulated Reward: 278, n_steps: 246\n",
      "Collided, Cumulated Reward: 165, n_steps: 54\n",
      "step = 119: loss = 9.149967193603516\n",
      "Collided, Cumulated Reward: 136, n_steps: 49\n",
      "Collided, Cumulated Reward: 110, n_steps: 40\n",
      "step = 120: loss = -10.109496116638184\n",
      "Collided, Cumulated Reward: 55, n_steps: 25\n",
      "Collided, Cumulated Reward: 54, n_steps: 30\n",
      "step = 120: Average Return = 4.032766342163086\n",
      "Collided, Cumulated Reward: 140, n_steps: 44\n",
      "Collided, Cumulated Reward: 146, n_steps: 53\n",
      "step = 121: loss = -10.632509231567383\n",
      "Collided, Cumulated Reward: 174, n_steps: 196\n",
      "Collided, Cumulated Reward: 164, n_steps: 55\n",
      "step = 122: loss = 8.769061088562012\n",
      "Collided, Cumulated Reward: 138, n_steps: 54\n",
      "Collided, Cumulated Reward: 94, n_steps: 54\n",
      "step = 123: loss = -7.557358264923096\n",
      "Collided, Cumulated Reward: 127, n_steps: 57\n",
      "Collided, Cumulated Reward: 126, n_steps: 49\n",
      "step = 124: loss = -8.671465873718262\n",
      "Collided, Cumulated Reward: 322, n_steps: 124\n",
      "Collided, Cumulated Reward: 163, n_steps: 57\n",
      "step = 125: loss = 5.858670234680176\n",
      "Collided, Cumulated Reward: 132, n_steps: 45\n",
      "Collided, Cumulated Reward: 175, n_steps: 151\n",
      "step = 126: loss = 1.534437894821167\n",
      "Collided, Cumulated Reward: 219, n_steps: 74\n",
      "Collided, Cumulated Reward: 272, n_steps: 243\n",
      "step = 127: loss = 0.8994898796081543\n",
      "Collided, Cumulated Reward: 338, n_steps: 125\n",
      "Collided, Cumulated Reward: 299, n_steps: 229\n",
      "step = 128: loss = 13.800992012023926\n",
      "Collided, Cumulated Reward: 284, n_steps: 161\n",
      "Collided, Cumulated Reward: 155, n_steps: 56\n",
      "step = 129: loss = 8.685831069946289\n",
      "Collided, Cumulated Reward: 151, n_steps: 47\n",
      "Collided, Cumulated Reward: 272, n_steps: 99\n",
      "step = 130: loss = 4.066676616668701\n",
      "Collided, Cumulated Reward: 328, n_steps: 104\n",
      "Collided, Cumulated Reward: 252, n_steps: 84\n",
      "step = 131: loss = 6.7706298828125\n",
      "Collided, Cumulated Reward: 155, n_steps: 51\n",
      "Collided, Cumulated Reward: 907, n_steps: 547\n",
      "step = 132: loss = 5.5121917724609375\n",
      "Collided, Cumulated Reward: 152, n_steps: 51\n",
      "Collided, Cumulated Reward: 144, n_steps: 51\n",
      "step = 133: loss = -5.397274494171143\n",
      "Collided, Cumulated Reward: 90, n_steps: 26\n",
      "Collided, Cumulated Reward: 117, n_steps: 56\n",
      "step = 134: loss = -2.8320024013519287\n",
      "Collided, Cumulated Reward: 151, n_steps: 45\n",
      "Collided, Cumulated Reward: 145, n_steps: 45\n",
      "step = 135: loss = -0.2395191192626953\n",
      "Collided, Cumulated Reward: 169, n_steps: 52\n",
      "Collided, Cumulated Reward: 144, n_steps: 42\n",
      "step = 136: loss = 2.1612629890441895\n",
      "Collided, Cumulated Reward: 155, n_steps: 49\n",
      "Collided, Cumulated Reward: 147, n_steps: 49\n",
      "step = 137: loss = -2.9582414627075195\n",
      "Collided, Cumulated Reward: 157, n_steps: 56\n",
      "Collided, Cumulated Reward: 96, n_steps: 29\n",
      "step = 138: loss = -1.9405546188354492\n",
      "Collided, Cumulated Reward: 94, n_steps: 30\n",
      "Collided, Cumulated Reward: 149, n_steps: 47\n",
      "step = 139: loss = -2.6666369438171387\n",
      "Collided, Cumulated Reward: 121, n_steps: 40\n",
      "Collided, Cumulated Reward: 72, n_steps: 24\n",
      "step = 140: loss = -0.7463840246200562\n",
      "Collided, Cumulated Reward: 102, n_steps: 32\n",
      "Collided, Cumulated Reward: 100, n_steps: 124\n",
      "step = 140: Average Return = 48.19984817504883\n",
      "Collided, Cumulated Reward: 82, n_steps: 25\n",
      "Collided, Cumulated Reward: 143, n_steps: 42\n",
      "step = 141: loss = -1.6266505718231201\n",
      "Collided, Cumulated Reward: 146, n_steps: 45\n",
      "Collided, Cumulated Reward: 86, n_steps: 26\n",
      "step = 142: loss = 3.9409735202789307\n",
      "Collided, Cumulated Reward: 96, n_steps: 32\n",
      "Collided, Cumulated Reward: 99, n_steps: 36\n",
      "step = 143: loss = -0.4500088691711426\n",
      "Collided, Cumulated Reward: 141, n_steps: 44\n",
      "Collided, Cumulated Reward: 96, n_steps: 31\n",
      "step = 144: loss = -2.600886821746826\n",
      "Collided, Cumulated Reward: 85, n_steps: 30\n",
      "Collided, Cumulated Reward: 55, n_steps: 17\n",
      "step = 145: loss = -3.0390095710754395\n",
      "Collided, Cumulated Reward: 101, n_steps: 31\n",
      "Collided, Cumulated Reward: 121, n_steps: 37\n",
      "step = 146: loss = -0.5388603210449219\n",
      "Collided, Cumulated Reward: 128, n_steps: 47\n",
      "Collided, Cumulated Reward: 155, n_steps: 50\n",
      "step = 147: loss = 5.45033597946167\n",
      "Collided, Cumulated Reward: 94, n_steps: 29\n",
      "Collided, Cumulated Reward: 120, n_steps: 36\n",
      "step = 148: loss = 0.5513837337493896\n",
      "Collided, Cumulated Reward: 88, n_steps: 29\n",
      "Collided, Cumulated Reward: 92, n_steps: 27\n",
      "step = 149: loss = -5.612255096435547\n",
      "Collided, Cumulated Reward: 115, n_steps: 40\n",
      "Collided, Cumulated Reward: 152, n_steps: 54\n",
      "step = 150: loss = -8.461132049560547\n",
      "Collided, Cumulated Reward: 90, n_steps: 28\n",
      "Collided, Cumulated Reward: 85, n_steps: 28\n",
      "step = 151: loss = -0.8388444781303406\n",
      "Collided, Cumulated Reward: 90, n_steps: 30\n",
      "Collided, Cumulated Reward: 82, n_steps: 28\n",
      "step = 152: loss = -2.0468356609344482\n",
      "Collided, Cumulated Reward: 89, n_steps: 32\n",
      "Collided, Cumulated Reward: 76, n_steps: 33\n",
      "step = 153: loss = -3.5348715782165527\n",
      "Collided, Cumulated Reward: 83, n_steps: 54\n",
      "Collided, Cumulated Reward: 81, n_steps: 28\n",
      "step = 154: loss = -0.7236284017562866\n",
      "Collided, Cumulated Reward: 114, n_steps: 53\n",
      "Collided, Cumulated Reward: 118, n_steps: 55\n",
      "step = 155: loss = 0.7527434825897217\n",
      "Collided, Cumulated Reward: 117, n_steps: 48\n",
      "Collided, Cumulated Reward: 92, n_steps: 27\n",
      "step = 156: loss = 1.4991048574447632\n",
      "Collided, Cumulated Reward: 90, n_steps: 33\n",
      "Collided, Cumulated Reward: 97, n_steps: 44\n",
      "step = 157: loss = 6.946772575378418\n",
      "Collided, Cumulated Reward: 107, n_steps: 46\n",
      "Collided, Cumulated Reward: 81, n_steps: 31\n",
      "step = 158: loss = 0.7059286832809448\n",
      "Collided, Cumulated Reward: 108, n_steps: 46\n",
      "Collided, Cumulated Reward: 99, n_steps: 37\n",
      "step = 159: loss = 2.142043113708496\n",
      "Collided, Cumulated Reward: 82, n_steps: 40\n",
      "Collided, Cumulated Reward: 75, n_steps: 31\n",
      "step = 160: loss = 2.3849549293518066\n",
      "Collided, Cumulated Reward: 174, n_steps: 616\n",
      "Collided, Cumulated Reward: 50, n_steps: 33\n",
      "step = 160: Average Return = 62.68505096435547\n",
      "Collided, Cumulated Reward: 102, n_steps: 36\n",
      "Collided, Cumulated Reward: 81, n_steps: 28\n",
      "step = 161: loss = -2.560032606124878\n",
      "Collided, Cumulated Reward: 93, n_steps: 33\n",
      "Collided, Cumulated Reward: 94, n_steps: 48\n",
      "step = 162: loss = -1.6918045282363892\n",
      "Collided, Cumulated Reward: 97, n_steps: 35\n",
      "Collided, Cumulated Reward: 88, n_steps: 37\n",
      "step = 163: loss = 5.842660903930664\n",
      "Collided, Cumulated Reward: 93, n_steps: 54\n",
      "Collided, Cumulated Reward: 97, n_steps: 37\n",
      "step = 164: loss = -2.202535629272461\n",
      "Collided, Cumulated Reward: 113, n_steps: 46\n",
      "Collided, Cumulated Reward: 79, n_steps: 31\n",
      "step = 165: loss = -0.8195319175720215\n",
      "Collided, Cumulated Reward: 96, n_steps: 45\n",
      "Collided, Cumulated Reward: 57, n_steps: 58\n",
      "step = 166: loss = 0.322632372379303\n",
      "Collided, Cumulated Reward: 93, n_steps: 37\n",
      "Collided, Cumulated Reward: 38, n_steps: 16\n",
      "step = 167: loss = -2.891364097595215\n",
      "Collided, Cumulated Reward: 88, n_steps: 53\n",
      "Collided, Cumulated Reward: 92, n_steps: 32\n",
      "step = 168: loss = 0.8246556520462036\n",
      "Collided, Cumulated Reward: 90, n_steps: 37\n",
      "Collided, Cumulated Reward: 75, n_steps: 26\n",
      "step = 169: loss = 1.9439129829406738\n",
      "Collided, Cumulated Reward: 92, n_steps: 42\n",
      "Collided, Cumulated Reward: 60, n_steps: 25\n",
      "step = 170: loss = -1.3544516563415527\n",
      "Collided, Cumulated Reward: 89, n_steps: 42\n",
      "Collided, Cumulated Reward: 109, n_steps: 43\n",
      "step = 171: loss = 5.861771583557129\n",
      "Collided, Cumulated Reward: 91, n_steps: 36\n",
      "Collided, Cumulated Reward: 61, n_steps: 56\n",
      "step = 172: loss = -1.1724915504455566\n",
      "Collided, Cumulated Reward: 89, n_steps: 35\n",
      "Collided, Cumulated Reward: 86, n_steps: 27\n",
      "step = 173: loss = -2.464470386505127\n",
      "Collided, Cumulated Reward: 74, n_steps: 50\n",
      "Collided, Cumulated Reward: 75, n_steps: 31\n",
      "step = 174: loss = -1.4975091218948364\n",
      "Collided, Cumulated Reward: 84, n_steps: 59\n",
      "Collided, Cumulated Reward: 96, n_steps: 35\n",
      "step = 175: loss = 6.328635215759277\n",
      "Collided, Cumulated Reward: 83, n_steps: 27\n",
      "Collided, Cumulated Reward: 113, n_steps: 40\n",
      "step = 176: loss = -2.08225679397583\n",
      "Collided, Cumulated Reward: 89, n_steps: 60\n",
      "Collided, Cumulated Reward: 83, n_steps: 37\n",
      "step = 177: loss = 0.5529811382293701\n",
      "Collided, Cumulated Reward: 151, n_steps: 47\n",
      "Collided, Cumulated Reward: 58, n_steps: 17\n",
      "step = 178: loss = 0.05221793055534363\n",
      "Collided, Cumulated Reward: 88, n_steps: 31\n",
      "Collided, Cumulated Reward: 78, n_steps: 31\n",
      "step = 179: loss = -6.14805793762207\n",
      "Collided, Cumulated Reward: 90, n_steps: 33\n",
      "Collided, Cumulated Reward: 39, n_steps: 14\n",
      "step = 180: loss = -4.363258361816406\n",
      "Collided, Cumulated Reward: 61, n_steps: 35\n",
      "Collided, Cumulated Reward: 49, n_steps: 43\n",
      "step = 180: Average Return = 5.961637496948242\n",
      "Collided, Cumulated Reward: 130, n_steps: 129\n",
      "Collided, Cumulated Reward: 39, n_steps: 14\n",
      "step = 181: loss = -0.6650328636169434\n",
      "Collided, Cumulated Reward: 86, n_steps: 41\n",
      "Collided, Cumulated Reward: 110, n_steps: 39\n",
      "step = 182: loss = 5.041824817657471\n",
      "Collided, Cumulated Reward: 95, n_steps: 34\n",
      "Collided, Cumulated Reward: 106, n_steps: 51\n",
      "step = 183: loss = 1.1629066467285156\n",
      "Collided, Cumulated Reward: 99, n_steps: 43\n",
      "Collided, Cumulated Reward: 110, n_steps: 54\n",
      "step = 184: loss = -1.2530063390731812\n",
      "Collided, Cumulated Reward: 75, n_steps: 35\n",
      "Collided, Cumulated Reward: 65, n_steps: 27\n",
      "step = 185: loss = -1.7469764947891235\n",
      "Collided, Cumulated Reward: 114, n_steps: 45\n",
      "Collided, Cumulated Reward: 95, n_steps: 34\n",
      "step = 186: loss = 3.9331483840942383\n",
      "Collided, Cumulated Reward: 113, n_steps: 46\n",
      "Collided, Cumulated Reward: 26, n_steps: 9\n",
      "step = 187: loss = 0.8207722306251526\n",
      "Collided, Cumulated Reward: 103, n_steps: 39\n",
      "Collided, Cumulated Reward: 35, n_steps: 12\n",
      "step = 188: loss = -1.2946693897247314\n",
      "Collided, Cumulated Reward: 93, n_steps: 32\n",
      "Collided, Cumulated Reward: 72, n_steps: 24\n",
      "step = 189: loss = -1.5701276063919067\n",
      "Collided, Cumulated Reward: 88, n_steps: 36\n",
      "Collided, Cumulated Reward: 102, n_steps: 44\n",
      "step = 190: loss = 0.9183275699615479\n",
      "Collided, Cumulated Reward: 91, n_steps: 33\n",
      "Collided, Cumulated Reward: 115, n_steps: 51\n",
      "step = 191: loss = -1.680072546005249\n",
      "Collided, Cumulated Reward: 99, n_steps: 51\n",
      "Collided, Cumulated Reward: 80, n_steps: 42\n",
      "step = 192: loss = 0.7013278007507324\n",
      "Collided, Cumulated Reward: 135, n_steps: 44\n",
      "Collided, Cumulated Reward: 90, n_steps: 50\n",
      "step = 193: loss = -4.679493427276611\n",
      "Collided, Cumulated Reward: 96, n_steps: 31\n",
      "Collided, Cumulated Reward: 109, n_steps: 40\n",
      "step = 194: loss = -2.6199393272399902\n",
      "Collided, Cumulated Reward: 89, n_steps: 30\n",
      "Collided, Cumulated Reward: 92, n_steps: 57\n",
      "step = 195: loss = 5.1794538497924805\n",
      "Collided, Cumulated Reward: 93, n_steps: 31\n",
      "Collided, Cumulated Reward: 91, n_steps: 30\n",
      "step = 196: loss = -5.77363920211792\n",
      "Collided, Cumulated Reward: 133, n_steps: 42\n",
      "Collided, Cumulated Reward: 69, n_steps: 24\n",
      "step = 197: loss = 9.917681694030762\n",
      "Collided, Cumulated Reward: 95, n_steps: 33\n",
      "Collided, Cumulated Reward: 69, n_steps: 25\n",
      "step = 198: loss = 1.1275510787963867\n",
      "Collided, Cumulated Reward: 96, n_steps: 32\n",
      "Collided, Cumulated Reward: 60, n_steps: 39\n",
      "step = 199: loss = 2.3847544193267822\n",
      "Collided, Cumulated Reward: 96, n_steps: 52\n",
      "Collided, Cumulated Reward: 85, n_steps: 34\n",
      "step = 200: loss = 2.7591023445129395\n",
      "Collided, Cumulated Reward: 50, n_steps: 53\n",
      "Collided, Cumulated Reward: 47, n_steps: 41\n",
      "step = 200: Average Return = -0.15601539611816406\n",
      "Collided, Cumulated Reward: 114, n_steps: 39\n",
      "Collided, Cumulated Reward: 51, n_steps: 16\n",
      "step = 201: loss = -2.026686191558838\n",
      "Collided, Cumulated Reward: 126, n_steps: 45\n",
      "Collided, Cumulated Reward: 86, n_steps: 51\n",
      "step = 202: loss = 3.7879478931427\n",
      "Collided, Cumulated Reward: 94, n_steps: 29\n",
      "Collided, Cumulated Reward: 93, n_steps: 55\n",
      "step = 203: loss = -4.682301998138428\n",
      "Collided, Cumulated Reward: 101, n_steps: 55\n",
      "Collided, Cumulated Reward: 110, n_steps: 37\n",
      "step = 204: loss = -1.6130571365356445\n",
      "Collided, Cumulated Reward: 112, n_steps: 37\n",
      "Collided, Cumulated Reward: 86, n_steps: 29\n",
      "step = 205: loss = -5.7068376541137695\n",
      "Collided, Cumulated Reward: 84, n_steps: 28\n",
      "Collided, Cumulated Reward: 96, n_steps: 38\n",
      "step = 206: loss = 0.8451504111289978\n",
      "Collided, Cumulated Reward: 100, n_steps: 38\n",
      "Collided, Cumulated Reward: 77, n_steps: 27\n",
      "step = 207: loss = -2.6106624603271484\n",
      "Collided, Cumulated Reward: 119, n_steps: 54\n",
      "Collided, Cumulated Reward: 78, n_steps: 38\n",
      "step = 208: loss = -1.553276777267456\n",
      "Collided, Cumulated Reward: 114, n_steps: 35\n",
      "Collided, Cumulated Reward: 95, n_steps: 37\n",
      "step = 209: loss = 4.0584025382995605\n",
      "Collided, Cumulated Reward: 106, n_steps: 59\n",
      "Collided, Cumulated Reward: 64, n_steps: 22\n",
      "step = 210: loss = -0.29837924242019653\n",
      "Collided, Cumulated Reward: 111, n_steps: 56\n",
      "Collided, Cumulated Reward: 77, n_steps: 46\n",
      "step = 211: loss = -1.3484100103378296\n",
      "Collided, Cumulated Reward: 75, n_steps: 33\n",
      "Collided, Cumulated Reward: 103, n_steps: 33\n",
      "step = 212: loss = -0.7007709741592407\n",
      "Collided, Cumulated Reward: 89, n_steps: 35\n",
      "Collided, Cumulated Reward: 56, n_steps: 53\n",
      "step = 213: loss = -3.176407814025879\n",
      "Collided, Cumulated Reward: 89, n_steps: 28\n",
      "Collided, Cumulated Reward: 131, n_steps: 48\n",
      "step = 214: loss = 6.0596818923950195\n",
      "Collided, Cumulated Reward: 101, n_steps: 47\n",
      "Collided, Cumulated Reward: 115, n_steps: 40\n",
      "step = 215: loss = -6.080535888671875\n",
      "Collided, Cumulated Reward: 116, n_steps: 47\n",
      "Collided, Cumulated Reward: 101, n_steps: 34\n",
      "step = 216: loss = 0.5790166854858398\n",
      "Collided, Cumulated Reward: 151, n_steps: 46\n",
      "Collided, Cumulated Reward: 82, n_steps: 30\n",
      "step = 217: loss = 1.612309455871582\n",
      "Collided, Cumulated Reward: 89, n_steps: 30\n",
      "Collided, Cumulated Reward: 73, n_steps: 24\n",
      "step = 218: loss = -5.086497783660889\n",
      "Collided, Cumulated Reward: 112, n_steps: 51\n",
      "Collided, Cumulated Reward: 83, n_steps: 32\n",
      "step = 219: loss = 3.8090124130249023\n",
      "Collided, Cumulated Reward: 85, n_steps: 32\n",
      "Collided, Cumulated Reward: 101, n_steps: 31\n",
      "step = 220: loss = -3.273911952972412\n",
      "Collided, Cumulated Reward: 275, n_steps: 1077\n",
      "Collided, Cumulated Reward: 50, n_steps: 27\n",
      "step = 220: Average Return = 112.65272521972656\n",
      "Collided, Cumulated Reward: 90, n_steps: 37\n",
      "Collided, Cumulated Reward: 107, n_steps: 39\n",
      "step = 221: loss = -1.9188874959945679\n",
      "Collided, Cumulated Reward: 83, n_steps: 26\n",
      "Collided, Cumulated Reward: 114, n_steps: 43\n",
      "step = 222: loss = -1.8307950496673584\n",
      "Collided, Cumulated Reward: 95, n_steps: 36\n",
      "Collided, Cumulated Reward: 95, n_steps: 32\n",
      "step = 223: loss = -5.273349761962891\n",
      "Collided, Cumulated Reward: 117, n_steps: 41\n",
      "Collided, Cumulated Reward: 142, n_steps: 43\n",
      "step = 224: loss = 3.556243658065796\n",
      "Collided, Cumulated Reward: 90, n_steps: 29\n",
      "Collided, Cumulated Reward: 83, n_steps: 27\n",
      "step = 225: loss = -4.427361488342285\n",
      "Collided, Cumulated Reward: 128, n_steps: 45\n",
      "Collided, Cumulated Reward: 120, n_steps: 39\n",
      "step = 226: loss = -2.151808977127075\n",
      "Collided, Cumulated Reward: 128, n_steps: 41\n",
      "Collided, Cumulated Reward: 114, n_steps: 37\n",
      "step = 227: loss = -5.7471394538879395\n",
      "Collided, Cumulated Reward: 94, n_steps: 30\n",
      "Collided, Cumulated Reward: 143, n_steps: 43\n",
      "step = 228: loss = 3.0144379138946533\n",
      "Collided, Cumulated Reward: 154, n_steps: 46\n",
      "Collided, Cumulated Reward: 132, n_steps: 39\n",
      "step = 229: loss = 2.0360875129699707\n",
      "Collided, Cumulated Reward: 92, n_steps: 28\n",
      "Collided, Cumulated Reward: 141, n_steps: 43\n",
      "step = 230: loss = -1.4569709300994873\n",
      "Collided, Cumulated Reward: 95, n_steps: 29\n",
      "Collided, Cumulated Reward: 146, n_steps: 47\n",
      "step = 231: loss = 1.8323192596435547\n",
      "Collided, Cumulated Reward: 150, n_steps: 45\n",
      "Collided, Cumulated Reward: 106, n_steps: 34\n",
      "step = 232: loss = -1.2682186365127563\n",
      "Collided, Cumulated Reward: 154, n_steps: 46\n",
      "Collided, Cumulated Reward: 95, n_steps: 28\n",
      "step = 233: loss = -0.5060635805130005\n",
      "Collided, Cumulated Reward: 151, n_steps: 45\n",
      "Collided, Cumulated Reward: 99, n_steps: 32\n",
      "step = 234: loss = -2.095827341079712\n",
      "Collided, Cumulated Reward: 97, n_steps: 30\n",
      "Collided, Cumulated Reward: 138, n_steps: 42\n",
      "step = 235: loss = -0.658130943775177\n",
      "Collided, Cumulated Reward: 196, n_steps: 62\n",
      "Collided, Cumulated Reward: 106, n_steps: 32\n",
      "step = 236: loss = 4.949477195739746\n",
      "Collided, Cumulated Reward: 155, n_steps: 46\n",
      "Collided, Cumulated Reward: 194, n_steps: 59\n",
      "step = 237: loss = -0.37241673469543457\n",
      "Collided, Cumulated Reward: 100, n_steps: 31\n",
      "Collided, Cumulated Reward: 98, n_steps: 31\n",
      "step = 238: loss = -3.396787643432617\n",
      "Collided, Cumulated Reward: 87, n_steps: 27\n",
      "Collided, Cumulated Reward: 324, n_steps: 99\n",
      "step = 239: loss = 1.2439711093902588\n",
      "Collided, Cumulated Reward: 153, n_steps: 52\n",
      "Collided, Cumulated Reward: 7523, n_steps: 9474\n",
      "step = 240: loss = 1.8823747634887695\n",
      "Collided, Cumulated Reward: 280, n_steps: 96\n",
      "Collided, Cumulated Reward: 336, n_steps: 135\n",
      "step = 240: Average Return = 257.39813232421875\n",
      "Collided, Cumulated Reward: 144, n_steps: 44\n",
      "Collided, Cumulated Reward: 184, n_steps: 56\n",
      "step = 241: loss = 1.2265496253967285\n",
      "Collided, Cumulated Reward: 205, n_steps: 63\n",
      "Collided, Cumulated Reward: 109, n_steps: 34\n",
      "step = 242: loss = -0.47766125202178955\n",
      "Collided, Cumulated Reward: 233, n_steps: 72\n",
      "Collided, Cumulated Reward: 109, n_steps: 63\n",
      "step = 243: loss = 6.608588218688965\n",
      "Collided, Cumulated Reward: 161, n_steps: 48\n",
      "Collided, Cumulated Reward: 146, n_steps: 54\n",
      "step = 244: loss = -0.05693662166595459\n",
      "Collided, Cumulated Reward: 155, n_steps: 51\n",
      "Collided, Cumulated Reward: 82, n_steps: 58\n",
      "step = 245: loss = -0.5123147964477539\n",
      "Collided, Cumulated Reward: 127, n_steps: 41\n",
      "Collided, Cumulated Reward: 129, n_steps: 49\n",
      "step = 246: loss = -7.885280609130859\n",
      "Collided, Cumulated Reward: 146, n_steps: 51\n",
      "Collided, Cumulated Reward: 124, n_steps: 39\n",
      "step = 247: loss = 7.125764846801758\n",
      "Collided, Cumulated Reward: 74, n_steps: 29\n",
      "Collided, Cumulated Reward: 134, n_steps: 51\n",
      "step = 248: loss = -4.121632099151611\n",
      "Collided, Cumulated Reward: 179, n_steps: 58\n",
      "Collided, Cumulated Reward: 96, n_steps: 39\n",
      "step = 249: loss = -0.2940634489059448\n",
      "Collided, Cumulated Reward: 100, n_steps: 38\n",
      "Collided, Cumulated Reward: 68, n_steps: 25\n",
      "step = 250: loss = -4.928067684173584\n",
      "Collided, Cumulated Reward: 99, n_steps: 49\n",
      "Collided, Cumulated Reward: 105, n_steps: 50\n",
      "step = 251: loss = -3.1679649353027344\n",
      "Collided, Cumulated Reward: 120, n_steps: 46\n",
      "Collided, Cumulated Reward: 121, n_steps: 44\n",
      "step = 252: loss = -5.544950485229492\n",
      "Collided, Cumulated Reward: 55, n_steps: 20\n",
      "Collided, Cumulated Reward: 92, n_steps: 35\n",
      "step = 253: loss = -1.8692731857299805\n",
      "Collided, Cumulated Reward: 82, n_steps: 32\n",
      "Collided, Cumulated Reward: 68, n_steps: 26\n",
      "step = 254: loss = -3.7357866764068604\n",
      "Collided, Cumulated Reward: 76, n_steps: 32\n",
      "Collided, Cumulated Reward: 126, n_steps: 42\n",
      "step = 255: loss = 3.1102185249328613\n",
      "Collided, Cumulated Reward: 143, n_steps: 51\n",
      "Collided, Cumulated Reward: 87, n_steps: 48\n",
      "step = 256: loss = -1.3060390949249268\n",
      "Collided, Cumulated Reward: 76, n_steps: 36\n",
      "Collided, Cumulated Reward: 74, n_steps: 25\n",
      "step = 257: loss = -4.6676530838012695\n",
      "Collided, Cumulated Reward: 76, n_steps: 28\n",
      "Collided, Cumulated Reward: 117, n_steps: 53\n",
      "step = 258: loss = 1.6879793405532837\n",
      "Collided, Cumulated Reward: 68, n_steps: 38\n",
      "Collided, Cumulated Reward: 55, n_steps: 24\n",
      "step = 259: loss = -2.9787821769714355\n",
      "Collided, Cumulated Reward: 79, n_steps: 28\n",
      "Collided, Cumulated Reward: 75, n_steps: 25\n",
      "step = 260: loss = -4.1961870193481445\n",
      "Collided, Cumulated Reward: 65, n_steps: 37\n",
      "Collided, Cumulated Reward: 44, n_steps: 18\n",
      "step = 260: Average Return = 4.226961135864258\n",
      "Collided, Cumulated Reward: 75, n_steps: 25\n",
      "Collided, Cumulated Reward: 82, n_steps: 42\n",
      "step = 261: loss = -0.47710558772087097\n",
      "Collided, Cumulated Reward: 62, n_steps: 24\n",
      "Collided, Cumulated Reward: 64, n_steps: 49\n",
      "step = 262: loss = 2.3744449615478516\n",
      "Collided, Cumulated Reward: 117, n_steps: 39\n",
      "Collided, Cumulated Reward: 95, n_steps: 52\n",
      "step = 263: loss = 6.020286560058594\n",
      "Collided, Cumulated Reward: 79, n_steps: 48\n",
      "Collided, Cumulated Reward: 124, n_steps: 48\n",
      "step = 264: loss = -0.47377169132232666\n",
      "Collided, Cumulated Reward: 98, n_steps: 39\n",
      "Collided, Cumulated Reward: 70, n_steps: 36\n",
      "step = 265: loss = -4.0382914543151855\n",
      "Collided, Cumulated Reward: 66, n_steps: 23\n",
      "Collided, Cumulated Reward: 104, n_steps: 35\n",
      "step = 266: loss = -0.32356178760528564\n",
      "Collided, Cumulated Reward: 66, n_steps: 28\n",
      "Collided, Cumulated Reward: 83, n_steps: 45\n",
      "step = 267: loss = -6.146334648132324\n",
      "Collided, Cumulated Reward: 114, n_steps: 55\n",
      "Collided, Cumulated Reward: 64, n_steps: 39\n",
      "step = 268: loss = 4.249057769775391\n",
      "Collided, Cumulated Reward: 85, n_steps: 36\n",
      "Collided, Cumulated Reward: 119, n_steps: 44\n",
      "step = 269: loss = -4.768996238708496\n",
      "Collided, Cumulated Reward: 98, n_steps: 37\n",
      "Collided, Cumulated Reward: 122, n_steps: 47\n",
      "step = 270: loss = -7.609249114990234\n",
      "Collided, Cumulated Reward: 71, n_steps: 40\n",
      "Collided, Cumulated Reward: 122, n_steps: 39\n",
      "step = 271: loss = -0.9989583492279053\n",
      "Collided, Cumulated Reward: 92, n_steps: 39\n",
      "Collided, Cumulated Reward: 86, n_steps: 34\n",
      "step = 272: loss = -5.646405220031738\n",
      "Collided, Cumulated Reward: 71, n_steps: 25\n",
      "Collided, Cumulated Reward: 71, n_steps: 59\n",
      "step = 273: loss = -3.228837490081787\n",
      "Collided, Cumulated Reward: 90, n_steps: 39\n",
      "Collided, Cumulated Reward: 55, n_steps: 22\n",
      "step = 274: loss = -1.6552146673202515\n",
      "Collided, Cumulated Reward: 83, n_steps: 43\n",
      "Collided, Cumulated Reward: 102, n_steps: 49\n",
      "step = 275: loss = 0.9843763709068298\n",
      "Collided, Cumulated Reward: 83, n_steps: 29\n",
      "Collided, Cumulated Reward: 64, n_steps: 28\n",
      "step = 276: loss = -2.728145122528076\n",
      "Collided, Cumulated Reward: 102, n_steps: 45\n"
     ]
    }
   ],
   "source": [
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(env, agent.policy, 2)\n",
    "returns = [avg_return]\n",
    "losses = []\n",
    "print('Starting training...')\n",
    "for _ in range(1000):\n",
    "    \n",
    "    # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "    collect_episode(env, agent.collect_policy, 2)\n",
    "\n",
    "    # Use data from the buffer and update the agent's network.\n",
    "    experience = replay_buffer.gather_all()\n",
    "    train_loss = agent.train(experience)\n",
    "    replay_buffer.clear()\n",
    "        \n",
    "    losses.append(train_loss.loss.numpy())\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        avg_return = compute_avg_return(env, agent.policy, 2)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    # observation = tf.ones((1080))\n",
    "    # observation = tf.reshape(time_step.observation, [1080])\n",
    "    # time_step = ts.restart(observation)\n",
    "    # time_step = ts.restart(time_step.observation, 1)\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(env.time_step_spec(),\n",
    "                                                env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=env.batch_size,\n",
    "    max_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "class ExperienceReply(object):\n",
    "    def __init__(self, agent, environment):\n",
    "        self._replay_buffer = TFUniformReplayBuffer(\n",
    "            data_spec=agent.collect_data_spec,\n",
    "            batch_size=environment.batch_size,\n",
    "            max_length=50000)\n",
    "\n",
    "        self._random_policy = RandomTFPolicy(environment.time_step_spec(),\n",
    "                                             environment.action_spec())\n",
    "\n",
    "        self._fill_buffer(environment, self._random_policy, steps=100)\n",
    "\n",
    "        self.dataset = self._replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=64,\n",
    "            num_steps=2).prefetch(3)\n",
    "\n",
    "        self.iterator = iter(self.dataset)\n",
    "\n",
    "    def _fill_buffer(self, environment, policy, steps):\n",
    "        for _ in range(steps):\n",
    "            self.timestamp_data(environment, policy)\n",
    "\n",
    "    def timestamp_data(self, environment, policy):\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        timestamp_trajectory = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "        self._replay_buffer.add_batch(timestamp_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_return = compute_avg_return(env, agent.policy, 1)\n",
    "returns = [avg_return]\n",
    "losses = []\n",
    "# iterator = iter(dataset)\n",
    "experience_replay = ExperienceReply(agent, env)\n",
    "for _ in range(150000):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    # collect_data(env, agent.collect_policy, replay_buffer, 1)\n",
    "    # collect_data(env, random_policy, replay_buffer, 1)\n",
    "    for _ in range(1):\n",
    "        experience_replay.timestamp_data(env, agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    # experience, unused_info = next(iterator)\n",
    "    experience, unused_info = next(experience_replay.iterator)\n",
    "    \n",
    "    train_loss = agent.train(experience).loss\n",
    "    \n",
    "    losses.append(train_loss.numpy())\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % 10000 == 0:\n",
    "        avg_return = compute_avg_return(env_eval, agent.policy, 1)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, 10000 + 1, 1)\n",
    "plt.plot(losses)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    experience_replay.timestamp_data(env, agent.collect_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        # five_secs_ago = self.timer2 - rospy.Duration(5) # Time minus Duration is a Time\n",
    "        if time.time() - self.timer2 > 30.0:\n",
    "            if self._get_distance(self.last_pos, self._get_pos_x_y()) < 1.2:\n",
    "                print('new break after: {}'.format(self._get_distance(self.last_pos, self._get_pos_x_y())))\n",
    "                self._episode_ended = True\n",
    "                if self.val:\n",
    "                    reward = self._compute_dist_from_origin()\n",
    "                else:\n",
    "                    reward = -10.\n",
    "                return ts.termination(np.array(self._state, dtype=np.float32), reward=reward)\n",
    "            else:\n",
    "                self.last_pos = self._get_pos_x_y()\n",
    "            self.timer2 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
